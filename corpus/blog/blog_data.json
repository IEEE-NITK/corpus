[
    {
        "layout": "post",
        "title": "Introduction to IEEE-NITK Chapter",
        "author_github": "salman-bhai",
        "date": "2017-05-14T23:32:44",
        "image": "/assets/img/",
        "description": "Introductory post to IEEE NITK",
        "tags": [
            "Introduction"
        ],
        "categories": [
            "CompSoc",
            "Diode",
            "Piston"
        ],
        "github_username": "salman-bhai",
        "comments": true,
        "text": "[{% load static %}]()\nIEEE is the world's largest professional association advancing innovation and technological excellence for the benefit of humanity. IEEE and its members inspire a global community to innovate for a better tomorrow through its highly-cited publications, conferences, technology standards, and professional and educational activities. IEEE is the trusted \"voice\" for engineering, computing and technology information around the globe.\n\nIEEE is incorporated under the Not-for-Profit Corporation Law of the state of New York. It was formed in 1963 by the merger of the Institute of Radio Engineers (IRE, founded 1912) and the American Institute of Electrical Engineers (AIEE, founded 1884).\n\n![IEEE NITK Team]({% static '/blog/assets/img/blog-image.png' %})\n\n## Beginning of IEEE NITK\n\nStarted in 1988, IEEE NITK has progressed exponentially and involved itself in various regional and global technical activities. Our effort has been fruitful in the form of various awards and laurels, the Best Student Branch Award (2007), GINI Student Branch Award (2010) and The Darrel Chong Student Activity Award (Silver Medal). Now one of the largest student branches worldwide, we are getting bigger and better.\n\n## Mission\nIEEE's core purpose is to foster technological innovation and excellence for the benefit of humanity.\n\n### Vision\nIEEE will be essential to the global technical community and to technical professionals everywhere, and be universally recognized for the contributions of technology and of technical professionals in improving global conditions.\n\nIEEE as a professional club provides learning opportunities within the engineering sciences, research, and technology. The goal of the IEEE education programs is to ensure the growth of skill and knowledge in the electricity-related technical professions and to foster individual commitment to continuing education among IEEE members, the engineering and scientific communities, and the general public.\n\nIEEE NITK is recognized as a dynamic body of enthusiastic engineers with a passion to create change with innovation. With membership strength of 600, IEEE NITK strives towards excellence and justifies IEEE as a pioneer of technological innovations. IEEE NITK actively organizes intra-branch activities, institute level activities and actively participates in initiatives of its affinity groups such as WIE (Women in Engineering) and SIGHT (Special Interest Group for Humanitarian Technologies). IEEE NITK's participation in GINI (Global Integrated Network of IEEE) has been of great importance to the growth of the student branch.\n\nIEEE NITK provides its student members with good platforms for sharing knowledge and implementing technology in practical projects through Special Interest Groups (SIGs).\n\n![IEEE NITK Team]({% static '/blog/assets/img/introduction-to-ieee/IEEE_2017.JPG' %})\n\nThe primary SIGs of IEEE NITK are: Diode, Piston and Python.\n\nApart from that secondary SIGs include WIE(Women in Engineering) and SIGHT(Special Interest Group for Humanitarian Technologies).\n\nIn broad terms the objectives of the club are:\n\n* Add on to the student member benefits by involving student members in different innovative projects and research.\n* Improve global presence of the student chapter by taking part in international competitions, conferences and talks.\n* Act as an effective platform to share and build technical knowledge inside the college and support.",
        "author_name": "Salman Shah",
        "slug": "introduction-to-ieee"
    },
    {
        "layout": "post",
        "title": "Installing Ubuntu For Windows Users",
        "date": "2017-05-23T20:03:44",
        "image": "/assets/img/",
        "description": "Detailed Instructions on installing Ubuntu on your Windows machine",
        "tags": [
            "Linux",
            "Ubuntu"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "hrily",
        "comments": true,
        "text": "[{% load static %}]()\n# Installing Ubuntu For Windows Users\n\n![Ubuntu]({% static '/blog/assets/img/installing_ubuntu/ubuntu.png' %})\n\nHie\n\nInstalling Ubuntu on your machine is really helpfull in college days. It comes free and there are lots of open source softwares available for it.\nLast year, a lot of people came to my room for installing ubuntu on their machine. I once thought of opening shop for installing ubuntu (Just Kidding :P)\n\nSo, this brief blog gives instructions on installing Ubuntu alongside Windows as a dual boot OS. \n\n### 1. Creating a partition for Ubuntu\n\nFirst thing we need to do is create a partition to install ubuntu. If you already have a partition, empty it and format it. If you don't have one, follow this [page](http://www.wikihow.com/Partition-Your-Hard-Drive-in-Windows-7).\n*Remeber the size of the partition you desire, it will be usefull later.*\n\n### 2. Create Bootable Live USB\n\n[Download](https://www.ubuntu.com/download/desktop) and create a live USB or DVD. In Windows, my favorite tool to create a live USB is [Universal USB Installer](http://www.pendrivelinux.com/universal-usb-installer-easy-as-1-2-3/).\n\n### 3. Boot up the Live USB\n\nNow insert the USB into machine, restart and boot into USB drive from the **Boot Options Menu**. Google for hotkey to get into **Boot Menu Options** of your machine model.\n\n### 4. Start Installation\n\nWhen prompted for options on booting, select **Install Ubuntu**\n\n![Install Ubuntu]({% static '/blog/assets/img/installing_ubuntu/install_ubuntu.jpg' %})\n\nSelect *Continue* for first few screens\n\n![First Few Screens]({% static '/blog/assets/img/installing_ubuntu/first_few.jpg' %})\n\n### 5. Preparing Partition for Ubuntu\n\nAfter few screens, you will be prompted for **Installation Type**. Choose **Something Else** and continue.\n\n![Installation Type]({% static '/blog/assets/img/installing_ubuntu/something_else.jpg' %})\n\nNow select desired partition (match the size of partition with one you created in step one), and click on **-** to delete the partition\n\n![Ubuntu]({% static '/blog/assets/img/installing_ubuntu/delete_partition.jpg' %})\n\n### 6. Create root, swap and home partition\n\nNow selecting the same partition, click **+** to create **root** partition.\nEnter size *14000 MB*, select *Logical Partition type*, *Begining of space*, and select **/** as *Mount Point*. Create the partition.\n\n![Create Root Partition]({% static '/blog/assets/img/installing_ubuntu/create_root.jpg' %})\n\nFollow similar steps for *swap* and *home*\n\n#### swap\n+ Size: 16000 (two times your RAM)\n+ Logical Partition\n+ Begining of Space\n+ Use As: swap area\n\n![Create Swap Partition]({% static '/blog/assets/img/installing_ubuntu/create_swap.jpg' %})\n\n#### home\n+ size: (remaining space)\n+ Logical Partition\n+ Begining of Space\n+ Mount Point: /home\n\n![Create Home Partition]({% static '/blog/assets/img/installing_ubuntu/create_home.jpg' %})\n\nNow select **Install Now**\n\n### 7. Follow next screens\n\nYour ubuntu installation should begin by now, follow the screens prompted for user name and password and general system setup. \nAfter few minutes, the installation will complete\n\n### 8. Reboot and Enjoy!\n\nYour ubuntu installation is complete. Reboot the machine to get into Ubuntu. Don't worry, you can still use Windows.\n\nIf you ran into any issue during installation, try googling it or you can always ping me over mail.\n\nUntill then, \nThis is *hrily* signing off...",
        "author_name": "Hrishi Hiraskar",
        "slug": "installing-ubuntu"
    },
    {
        "layout": "post",
        "title": "Dealing with Underdetermined Systems",
        "author_github": "AbhijithKamath",
        "date": "2017-07-02T00:00:00",
        "image": "/assets/img/",
        "description": "Mathematical models of engineering systems are inherently underdetermined",
        "tags": [
            "SPS",
            "Image Processing",
            "Linear Algebra"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "AbhijithKamath",
        "comments": true,
        "text": "[{% load static %}]()\n![incidence matrix](https://wikimedia.org/api/rest_v1/media/math/render/svg/ee0208dc07116e4d32bcc9952a015b43b63d91a2)\n\n## What is meant by \"under-constrained\" system?\n\nI will start off this segment with my favourite quote.\n\n*\"To the ones who tread the path less taken; To the ones who believe the answer to everything lies in data.\"*\n\nThe answer to all problems lies in how much you know about it. Most engineering problems and their math models are inherently under-constrained. It is not surprising, we have all seen physics problems give under-constrained systems. The best example is in the definition of electric potential. All electric potentials are with reference to potentials at infinity where the potential is assumed to be zero. In classical terms, it would sound like this.\n\n*\"If the potential at infinity is zero, the potential at x is V(x)\"*\n\nEngineering systems have to mean a lot more than *\"if\"* s. At the end of the day, we have to be able to use it and it has to make sense. An under-constrained system like this is seen more often than an engineer like to see and we have to find a way around it while keeping reality. Let's look at some interesting examples.\n\n### Linear Networks\n\nMost electric circuits, linear in nature and electronic circuits are linearised, are underdetermined. Starting from the point when we have to define potentials at nodes stating *\"if the potential at infinity was zero, then the potential here is ..\"*. To realise, analyse, debug and re-create such devices are impossible in that case. We construct a model using classical graph theory. Linear components give an incidence matrix for the linear network. The system equation then reduces to the normal equation. Even then, the square matrix constructed out of the incidence matrix A, will not be invertible because A has linearly dependent columns. How do we fix this? How do we have all our systems working if we can't debug them?\n\nThe insight is in localising infinity. Engineers call it circuit ground node. The simplest setup is to set it at zero and reduce the square matrix by one row and one column which then gives an invertible system. Now everything is merry and we move on debugging and building better circuits!\n\n![Ground Node](https://i.stack.imgur.com/bpC7a.png)\n\n### Optical Flow\n\nThis problem is more in my research areas. Optical flow algorithms for calculating motion of objects in a video. The foundations layed by Horn and Schunk are based on *\"brightness constancy\"*, a very simple idea and practical in most cases since most videos are shot at more than 25fps. Turns out this is another under-constrained system. Optical flow is a displacement vector of every pixel in a movie frame. So that means every pixel needs two numbers to give its optical flow and we only have one constraint, brightness constancy. How do we deal with this?\n\nThis is more of what Indians like to call \"Jugaad\". We will expect more and then come up with another constraint. If brightness constancy gave us two conditions, we would never think of this. Jugaad for real! We will expect motions in the video to be smooth. Now of course, this wouldn't happen in a Harry Potter movie where things magically appear and disappear, but let's keep things \"real\" :P The solutions now are given from a functional that Horn and Schunk construct based on the smoothness constraint and brightness constancy. Two conditions to give two numbers and we can happily move on!\n\nThere are faster, traditional methods to use the smoothness constraint and this time jugaad is more real! It is so jugaad-y that it lead me to write this blog on *'Dealing with under-constrained systems'*. But it makes good sense in most cases and gives good results and that's what matters. The idea that Lucas-Kanade proposed is in using the smoothness constraint in using the same optical flow in a nine point neighbourhood. Now it suddenly becomes over-determined and the system is solved by a least-squares approximation over the nine-points. This now gives smooth variations in the optical flow of the frame! Problem solved and we make it better!\n\n![Hamburg Taxi Sequence](https://www.researchgate.net/profile/Alexandru_Telea/publication/250199688/figure/fig3/AS:298282552184833@1448127510079/Fig-6-The-taxi-sequence-Original-image-left-Flow-extraction-without-the-projection.png)\n\n### Sub-Nyquist Sampling and Reconstruction\n\nThis is something everyone wants. Sub-Nyquist sampling, slower sampling rates and money to invest in more accuracy. Well, Shannon-Nyquist designed a theorem for the entire sampling framework and there is proof to why you cannot perfectly reconstruct every signal while sampling below Nyquist rate. Why? Simply because the system then becomes under-constrained. To what signals can we develop plausible constraints?\n\nWe look at a class of signals called *'FRI'* or signals with Finite Rate of Innovation. These signals are seen in transrecivers where the signal received is only a combination of delayed and attenuated versions of the signal that is transmitted. Now we have two parameters for every replication of the transmitted signal that is received and we need just twice the number of samples to the number of replications received. Then the system has enough constraints to solve! Job done again! And we see again that under-constrained systems are very much solvable under certain situations.",
        "author_name": "Abhijith Kamath",
        "slug": "dealing-with-underdetermined-systems"
    },
    {
        "layout": "post",
        "title": "Getting to love the Terminal",
        "author_github": "kaushiksk",
        "date": "2017-05-24T10:48:33",
        "image": "/assets/img/",
        "description": "Getting started with exploring the Bash shell",
        "tags": [
            "Unix",
            "Terminal",
            "Bash"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "kaushiksk",
        "comments": true,
        "text": "[{% load static %}]()\n## I Just Got Ubuntu!\n<sub>But I'm not so sure about this command line</sub>\n\nEarly days with the Unix command line interface, or the terminal (bash shell by default) can be pretty annoying and intimidating.\n\nBut it doesn't have to be. Getting to know the shell and writing shell scripts is exciting as well as extremely useful and something everyone should get familiar with. Once you get a hang of it you'll start loving the flexibility Unix provides [in no time](https://xkcd.com/456/). But I wont go into too much details about shell scripting here, I'll leave resources at the end so you start learning on your own.\n\nIn this post I'll focus a bit more about making your life easier with a few tricks on the command line.\nWe'll mainly talk about bash [aliases](https://en.wikipedia.org/wiki/Alias_(command)) and the `.bashrc` file.\n\nIf you're completely new to Unix then it might be a good idea to have a look at this [book](http://linux-training.be/files/books/LinuxFun.pdf) to get familiar with some basic commands and then come back here.\n\n## Aliases\n\nBash aliases are placeholders. You can think of them as shortcut commands for a longer command. They are extremely useful when you want to perform some operation that requires you to type a long command with a whole lot of options frequently.\n\nFor e.g this line prints out all the hidden files and folders in the current directory.\n\n`$ ls -A|grep '^\\..*'`\n\n<sub>**Note:** Whenever you see a $ in a line of code online, it is an indicator that it is a shell command and should not be typed in while you write the command in your terminal, i.e, you will only copy paste `ls -A|grep '^\\..*'` into your terminal from above.</sub>\n\nThe above command lists out all the files in the current directory, pipes the output to the `grep` command which then filters out only those files which have a `.` in the beginning. (Hidden files in begin with a `.` in Unix).\n\nNow what if you want to go to all your favourite folders and check all the hidden files there? You'll have to navigate there and type out the whole command again. Of course, you can press the `Up Arrow Key` to access your command history and keep pressing it until you reach this command, but that can get pretty boring.\n\n There's a better way.\n\n`$ alias lhid='ls -A|grep '^\\..*''`\n\n<sub>Make sure there no whitespace around the '=' sign. The shell is very particular about that.</sub>\n\nNow type `lhid` in your terminal and it should perform the same operation as the command we aliased it to.\n\nYou can remove the alias by typing `unalias lhid`. We'll not do that for now.\n\nSome other common aliases you would like to use can be\n\n```\nalias ..=\"cd ..\"\nalias ...=\"cd ../..\"\nalias myfolder=\"cd path/to/my/folder\"\n```\n\nYou can now just use the placeholders instead of the entire command. I use a lot of aliases to `cd` into my frequent folders and it really saves a lot of time and a lot of typing.\n\nBut there's a catch. These placeholders we created are available only inside the *current shell*. Open up a new terminal and type `lhid` and it will tell you that the command was not found. This is because the variables you create in a shell are local to that shell only. \n\n## .bashrc\n\nThankfully there's the [.bashrc](https://superuser.com/questions/49289/what-is-the-bashrc-file#49292) file. This is the file that is run every time you fire up a new terminal. So all we need to do is add our aliases at the end of our `.bashrc` file and they'll be loaded every time we open a new terminal.\n\nThe `.bashrc` file exists in your home directory. Open it with any editor. I'll use nano here, you can use vi or gedit or any editor you wish.\n\n`$ nano ~/.bashrc`\n\n<sub>**Note**: The `~` is a placeholder for the user's home directory.</sub>\n\nNow move all the way down to the bottom of the file and add the following lines there.\n\n```\nalias lhid='ls -A|grep '^\\..*''\nalias ..=\"cd ..\"\nalias ...=\"cd ../..\"\nalias myfolder=\"cd path/to/my/folder\"\n\n```\n\nSave the file and exit (`Ctrl X + Y` in nano).\n\nNow open up a new terminal and try using those aliases and you'll see that it works like a charm! This is because those commands in the `.bashrc` file are run every time you open a new terminal.\n\nYou can notice that the terminal opens up in your home directory by default. Changing this is simple.\n\nOpen the `.bashrc` file and at the end add the line `cd path/to/my/directory`. For e.g if you want every new terminal to open up the Downloads folder just add the line `cd ~/Downloads` at the end and that's it!\n\nAnd with just a few simple commands you've already made your life so much more easier on the terminal!\n\n### Resources:\n\n[Basic Unix commands](http://freeengineer.org/learnUNIXin10minutes.html)\n\n[Bash Scripting Guide](http://www.tldp.org/LDP/Bash-Beginners-Guide/html/Bash-Beginners-Guide.html)",
        "author_name": "Kaushik S Kalmady",
        "slug": "getting-to-love-the-terminal"
    },
    {
        "layout": "post",
        "title": "Hello World",
        "author_github": "adithyabhatkajake",
        "date": "2017-05-26T10:48:33",
        "image": "/assets/img/",
        "description": "A not so simple Hello World (PS: No need to buy the full version)",
        "tags": [
            "GCC",
            "C",
            "Programming",
            "HelloWorld"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "adithyabhatkajake",
        "comments": true,
        "text": "[{% load static %}]()\n# A dissection of the hello world C program\n\nDISCLAMER: The contents on this page are strictly R rated. With R standing for Rigorous material. This is not a beginner's guide to C. This is more of an article for intermediate/advanced users to explore C, in depth. I am not responsible for any earthquakes, alien invasions or epidemics caused, directly/indirectly from this material.\n\n\nGiven below is a typical C program taught in most of the textbooks.\n\n~~~~\n#include <stdio.h> // An include statement: Preprocessor Directive\n\nvoid main() // An entry function\n{\n    printf(\"Hello world\\n\"); \n}\n~~~~\n\nThe entire post does not even begin to comprehend the amount of work put in ,just to run a simple hello world program. To compile and run this program, a lot of juggling is done by gcc and the operating system. Of course, the standard books and online resources will tell you about how the program works, what each line is, and the technical jargon associated with them, but that is not why we are here today. Let us explore the bland hello world program and work out it's internals. I am pretty sure this will be a worthwhile trip. So hold on tight!\n\n(Trivia: *The first Hello world was written in the BCPL language and was used during the development of the C compiler, by Dennis Ritchie. Since then, the hello world program was deified by all the computer enthusiasts.*)\n\nJust to recap, here are the fundamental elements of the program:\n\n1. ` #include <stdio.h> ` is a preprocessor directive.\n2. `void` is a data type.\n3. `main()` is the entry point of the program.\n4. `printf()` is a C function that is used to print formatted (printf - print format) strings.\n\n![Introduction]({% static '/blog/assets/img/Hello-World/intro.png' %})\n\nLet us start unravelling some of the complexities of the example program. \n\n## Preprocessor directives\n\nThe first step performed by the gcc compiler is to preprocess the source file. The compilation begins after the preprocessing is complete and there are no errors. The preprocessor directives are commands that gcc understands, and start with the symbol `#`. `#include` is a preprocessor directive that tells the compiler to include the contents of a  file and paste it in place of the directive.\n\nFor example:\n`#include <stdio.h>`\n\nis replaced by\n\n~~~~\n# 1 \"temp.c\"\n# 1 \"<built-in>\"\n# 1 \"<command-line>\"\n# 31 \"<command-line>\"\n# 1 \"/usr/include/stdc-predef.h\" 1 3 4\n# 32 \"<command-line>\" 2\n# 1 \"temp.c\"\n# 1 \"/usr/include/stdio.h\" 1 3 4\n# 28 \"/usr/include/stdio.h\" 3 4\n# 1 \"/usr/include/bits/libc-header-start.h\" 1 3 4\n# 33 \"/usr/include/bits/libc-header-start.h\" 3 4\n# 1 \"/usr/include/features.h\" 1 3 4\n# 410 \"/usr/include/features.h\" 3 4\n# 1 \"/usr/include/sys/cdefs.h\" 1 3 4\n# 441 \"/usr/include/sys/cdefs.h\" 3 4\n# 1 \"/usr/include/bits/wordsize.h\" 1 3 4\n# 442 \"/usr/include/sys/cdefs.h\" 2 3 4\n...\n\nextern int fprintf (FILE *__restrict __stream,\n      const char *__restrict __format, ...);\n\nextern int printf (const char *__restrict __format, ...);\n\nextern int sprintf (char *__restrict __s,\n      const char *__restrict __format, ...) __attribute__ ((__nothrow__));\n\n...\n\n~~~~\n> Pro Tip: Save a C file with just \"#include &lt;stdio.h&gt;\" in it. Then run `gcc -c -i &lt;file.c&gt; -o &lt;file.i&gt; to see the actual preprocessing in action. GCC's -E flag, outputs the file after performing the preprocessing. Internally, during compilation, it stores it as .i file which is later used as the input for the compiler.\n\nThe preprocessing and compilation, both of them are done by the same program: cc1. The `cc1` executable is a part of the gcc suite of programs. But, hang on! What is this *&lt;stdio.h&gt;*? Angle brackets(yeah, that is what <> are actually called. Not less/greater than symbols) tells the preprocessor that the file inside is present in the standard library path. It is usually defined in an environment variable called C_INCLUDE_PATH. You can also add directories to search for headers with the `-I` flag in gcc. (Example: `gcc -I foo/bar <file>.c` will make gcc search for .h files in foo/bar/ directory first, and then the remaining directories in C_INCLUDE_PATH). For shorthand purposes, you can write `#include \"foo.h\"` if you want gcc to add foo.h from the current directory.\n\n We also see that printf is actually defined here. Which is why, folks! you get warnings if you use printf without including the library. But, hold on. Where is the implementation of the printf? To answer the question, please buy the full version here.\n\nCompilation\n-----------\n\nThe preprocessed file is then parsed and compiled by the `cc1` executable. The source code is then transformed from the C code to assembly code. All the basic data types like the void, int, char \\*, etc are validated by the compiler. Also, syntax checking happens here. If you enable all warnings (`with gcc -Wall`: Warnings-all), any compile time errors are detected here and thrown out to the user. It also checks variable usage, references and loop invariant checks too. The entire compilation consists of generating an Abstract Syntax Tree (AST), and then converting it into a GENERIC format(literally). GENERIC trees are then gimplified by a gimplifier also called gimplication into GIMPLE format (Gimply, mind blowing). Then a tree SSA pass is performed. This pass performs several optimizations like:\n\n1. Remove useless statements <br>\n2. Building the control flow graph<br>\n3. Find all referenced variables<br>\n4. Warn for unintialized variables<br>\n5. Dead code elimination<br>\n6. Dominator optimizations<br>\n7. Forward propogation of single variables<br>\n8. Loop optimizations<br>\n\n...<br>\n and many more optimizations, each of which deserve a 15 hour course in it's entirety. (Nope, I am not going to explain them here)\n \n\n~~~~\n\t.file\t\"temp.c\"\n\t.section\t.rodata\n.LC0:\n\t.string\t\"Hello World\"\n\t.text\n\t.globl\tmain\n\t.type\tmain, @function\nmain:\n.LFB0:\n\t.cfi_startproc\n\tpushq\t%rbp\n\t.cfi_def_cfa_offset 16\n\t.cfi_offset 6, -16\n\tmovq\t%rsp, %rbp\n\t.cfi_def_cfa_register 6\n\tmovl\t$.LC0, %edi\n\tcall\tputs\n\tnop\n\tpopq\t%rbp\n\t.cfi_def_cfa 7, 8\n\tret\n\t.cfi_endproc\n.LFE0:\n\t.size\tmain, .-main\n\t.ident\t\"GCC: (GNU) 6.3.1 20170306\"\n\t.section\t.note.GNU-stack,\"\",@progbits\n~~~~\n> Pro Tip: Use the source code shown above. Compile it with `gcc -S <file>.c`. This makes gcc to output the file after preprocessing and compiling and stops further processing of the file.\n\nDon't worry, compilation is almost over. A step called RTL pass is performed at the end of this phase, which does some cleanup and the final assembly code is ready.\n\n## Assembling\n\nThe assembling is done by the executable called `as` which is a part of the binutils package. This program is responsible for converting the generic assembly code into a code that is understood by the local machine. Also, ever wondered why all the C programs always start with main? \n\nAs a reward for reading the article so much, here is a nice way to write a C program without main. (WHAT?? Yes, it is possible)\n\n~~~~\n#include <stdio.h> // for our beloved printf()\n#include <stdlib.h> // for our beloved exit()\n\n\nvoid _start()\n{\n    printf(\"Hello world\\n\");\n    exit(0);\n}\n~~~~\n\n> Compile it with `gcc -nostartfiles <file>.c` and run `./a.out`\n\nThe _start program calls main internally and that my dear friend, is why C programs always begin with main. \n\nThe assembly code generated has extra elements that take care of the arguments passed to the program, the environment is setup and hard-wired to start main (or whatever entry function, if you read the article in the right order) after performing some checks. Exception handling is also done by this program. Ever wondered why only C compiled programs give segmentation fault (core dumped) errors? Wait a minute, I did not write any exception handling  code. That's right. GCC adds boilerplate code that automatically terminates the program when wrong addresses are accessed and immediately packs it's stuff up, prepares a dump of the core and terminates the program.\n\nThe assembler creates an object file (.o). You can make gcc stop at this stage, by running `gcc -c <file>.c`. This is usually done in large projects. Multiple .c object files are created. Then they are linked together by the linker into one giant executable by the linker.\n\n## Linking\n\nOne more question, that has not been answered by our dissection so far is: Where is the implementation of printf? How come I can't see the implementation of printf?\n\nThis is done in the final phase. A basic set of shared libraries called `libc`, `linux-vdso` and `ld-linux-x86-64` are attached to any basic C program. During assembly, dangling references are made to functions. Kind of like, \n\n> User: hey computer, use printf here and here\n\n> Computer: But I don't know what printf is. ERR..\n\n> User: wait, don't throw an error. I will tell you where it is, later. Just assume it is defined for now.\n\n> Computer: RRMM, oh. Okay.\n\n\nThe linker now points to all the dangling functions from these shared libraries. The linker fixes all references to functions. The linker is actually a program called `ld`, another member of the gcc core team.\n\n### NOTE: \nShared libraries are C programs, that are compiled before hand and can be attached to any program. There are two kinds of libraries:\n\n1.Static libraries<br>\n2.Dynamic libraries\n\nNothing to be worried about. You have used both of these kinds of libraries. You just don't recognize them. Remember compiling C programs with the `-lm` option to enable math operations (Other examples are `-lpthread`, `-fopenmp`,etc), these are shared objects attached to the program. Shared libraries are .so files which are compiled in advance. This is usually done, in order to save memory. For example, if 20 processes are using the math library, then it is a wiser choice to seperate all the math code and leave references to it. During runtime, the process finds the required shared library in memory and resolves all the addresses.\n\nThe naming convention for shared libraries is somewhat peculiar. If the file is called libxyz.so, it has to be linked with `-lxyz` and vice versa.\n\nStatic libraries are just, well static. They are code, that linked during compile time. They are usually .a (archive) files. One can make dynamic libraries be attached statically into the executable by using `gcc -static <file>.c -lm`. [Notice the drastic increase in size of the executable output]. You can find out what are the libraries dependent on the executable by running `ldd <executable>`. You can also make the linker look for addition object files in other directories by using the `-L` flag in gcc. (Example: `gcc -Lsome_dir/ foo.o bar.o` where bar.o is in some_dir/)\n\nI hope by now, you are clear on why using printf throws a compiler warning but works anyhow.\n\n## Conclusion\n\nThat's it folks. The C program is now ready to do your bidding. I am sure, next time someone says a \"Hello world\" program is easy, you can blow their brains out. Also, the programming languages evolve in a very fascinating way. For example, the token parsing program of the C code is written in C++. But C++ was written in C. This continues backwards until Dennis Ritchie first wrote C using assembly. Later C was rewritten using C. This is sometimes referred to as bootstrapping. Loading oneself using oneself. On a similar note, I hope this article stimulates the readers to read more :)",
        "author_name": "Adithya Bhat",
        "slug": "hello-world"
    },
    {
        "layout": "post",
        "title": "How to Get Started with Bots",
        "author_github": "trmittal",
        "date": "2017-06-01T18:28:33",
        "image": "/assets/img/",
        "description": "A guest entry to the world of bots",
        "tags": [
            "Bots"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "trmittal",
        "comments": true,
        "text": "[{% load static %}]()\n# How to get Started with bots\n\nHave you ever been fascinated by robots - Awestrucking objects which are capable of performing some cool tasks without needing a human intervention (well, unless they decide to go mad! :P) ? And certainly the thought of how these genius creations work and can I make one too? must have crossed your mind. Being in a similar frame of mind couple of years back and having made some basic bots, I thought of writing this article to help the curious people. \n\n\n### Getting Stuff together\n\nIt's surely hard to shell out a few bucks but it's all worth here. First and foremost you will need a microcontroller. To the beginners out there with no previous experiences with one I will recommend to use Arduino Uno but if you already had enough of Arduino go with MSP430(you will certainly learn more here) from TI. Then comes the robot chassis: buy one which is not too small and strong enough. You will need two DC motors, two wheels, one castor wheel. Also L293d - a motor driver (It is an IC if you didn't know) which is required to provide extra current to motors. Apart from this you will need a breadboard along with few LEDs and 330Ohm resistors(for testing), jumper wires (M2M, M2F, F2F), sensor components(I will come to these later) and also I highly recommend having a multimeter handy. Lastly a power source: a few 9V batteries ( they drain quickly, you can try rechargable ones) and if you don't mind shelling out more, buy 12V lead acid battery which are much better than your 9V ones.\n\nWhere do I buy these? If you are in Bangalore, head to SP road, Lamington road for Mumbaikars ( I am unaware of other cities). You can also buy it online on robocraze.com, robokits.co.in, amazon.in and many other sites.\n\nTry catching hold of your friend with similar interests and spilt the bill.\n\nOnce done with testing on breadboard, people generally prefer to make it permanent by soldering it onto a board which will require a soldering iron.\n\n### Start Building\n\nI believe you are well-versed with Basic C programming and that is enough to get started. Install Arduino IDE and learn basic GPIO functions of Arduino and test them out with LEDs ( use resistor in series to prevent your LED from burning) and breadboards. Go through the example codes in Arduino IDE ( Arduino's website is the best place for answers to your doubts). \nNext learn to use the motor driver (here you will have to use your batteries). [This](http://www.instructables.com/id/Control-your-motors-with-L293D-and-Arduino/) article will help you. \n\n### Integrating the Sensor\n\nIR sensors are the most basic and versatile sensors. They can be used to tell black lines apart from white, measure the distance in analog and digital form, detect obstacle among many others. Refer [this](http://maxembedded.com/2013/08/how-to-build-an-ir-sensor/) article which does a very good job of explaining it. You can build a line-following, wall-following,etc with IR sensor. \n\nThere are many other interesting sensors like [light-sensing](http://www.buildcircuit.com/darklight-sensor-using-transistor/), then accelerometers, gyro, etc which detect the movement of the robot which you can experiment with once you are done with your first one. \n\nAnd if you want your robot to have arms then servo/stepper motor is the answer.\n\n![Raspberry Pi]({% static '/blog/assets/img/how-to-get-started-with-bots/rpi.jpg' %})\n\n### You are almost there\n\nOnce you have tested your sensor and motor circuits individually, it's time to bring them together. Write a code ( I leave this to you) which will process the sensor data and intruct the motors. You can improve the efficiency of your code using algorithms like [PID](http://brettbeauregard.com/blog/2011/04/improving-the-beginners-pid-introduction/). \nThen fix your motors, castor wheel to the chassis. Attach your sensors at the required place.Finally rest your Arduino and sensors on top of  the chassis along with the battery and you are ready to go.\n\n### What's next?\n\nAfter you build these basic robots, you should try and procure data in other forms like images of surroundings, noises,etc and process them using image/audio processing techniques. Send data to and fro using bluetooth, WiFi. You can perform certain tasks with your robot by including a robotic arm controlled by a stepper/servo motor. Further improvisation is endless and your imagination is the only limit.\n\n### Useful Tips\n\n* Make sure all your modules share a common ground.\n* Always have a switch for your robot.\n* Disconnect wires from pins 0 and 1 on Arduino while uploading code.\n* Have a provision to use [variable speed for motors](http://www.instructables.com/id/Arduino-DC-motor-speed-and-direction-L293D/)\n* Try making your circuit permanent by soldering to avoid lose connections.\n* Never lose hope and keep building!!",
        "author_name": "Tarun Mittal",
        "slug": "how-to-get-started-with-bots"
    },
    {
        "layout": "post",
        "title": "Introduction to GPU Computing",
        "author_github": "vilas897",
        "date": "2017-06-04T12:00:00",
        "image": "/assets/img/",
        "description": "An overview of how GPUs and Graphic rendering works along with an introduction to GPUs",
        "tags": [
            "High Performance Computing",
            "Game Development"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "vilas897",
        "comments": true,
        "text": "[{% load static %}]()\n# A Gamer's priced possession - Graphic Cards!\n\nLet's say that you have bought a brand new computer with one of the latest i7 processors but without installing a separate graphics card. And your friend is still using his 5 year PC with an old i3 processor, but with a high end graphics card installed. Both of you play the same game simultaneously on your own computers. But you notice that the gaming performance is better on his PC even though your computer is brand new. You scratch your head not understanding what's going on here. You have a much better CPU and everyone knows that the CPU is the \"brain\" or the mastermind of the computer.\n\n![GPU]({% static '/blog/assets/img/introduction_to_gpu_computing/gpu.jpg' %})\n\nSo lets dive a bit deeper into the technical details. Most of you may already know - GPU stands for **Graphics Processing Unit** and CPU stands for **Central Processing Unit**. Both of them are basically microprocessor chips that reside on a PCB board (A CPU resides on a motherboard while a GPU resides on a graphics card board). From a high level point of view, both of them appear to be the same. After all, they are just crunching a lot of numbers in binary, your typical 0s and 1s, +'s and -'s. So what is the need of a separate GPU when the CPU is capable of doing the same tasks?\n\n## CPU vs GPU\n\nThe difference between the two comes in the way they are designed. What tasks can both of them can handle, how fast or efficiently do they handle the same task, how well do they manage multiple tasks together. Things like that.\n\n![CPU vs GPU]({% static '/blog/assets/img/introduction_to_gpu_computing/cpu_vs_gpu.png' %})\n\nCPUs are general purpose processors that are designed to handle a very wide range of tasks. Be it some simple addition, multiplication operations, browsing the internet, playing audio/videos, creating/deleting/searching files in your PC etc. But a CPU has a limited number of cores or processing units. A dual-core CPU has 2 cores, each of which can run only a single process or task simultaneously. (Of course, it is possible to execute multiple processes virtually on each core by the means of **hyper-threading** where a single core is split into various logical CPUs. Read more about multi core CPUs and hyper-threading over [here](https://www.howtogeek.com/194756/cpu-basics-multiple-cpus-cores-and-hyper-threading-explained/)).\n\nGPUs are processors that are designed to take a very specialist workload of rendering 2D/3D graphics and it does it much more efficiently. GPUs generally have around hundreds of cores as compared to a CPU. So given a task - lets say adding 2 matrices having a dimension of 1000 x 1000, the GPU can divide the work or region of data to compute among all the cores, which work in parallel and produce the result almost immediately.\n\nHowever, a single GPU core has a much lower performance as compared to a single CPU core due to following reasons:\n1. GPU cores have a lesser clock frequency (proportional to number of instructions executed per second) as compared to a typical CPU core.\n2. GPUs have a smaller instruction set. They deal with mostly floating point arithmetic and instructions like branch predictors are absent. Hence, they struggle while dealing with a lot of conditional statements.\n3. They have smaller cache sizes.\n4. Hyper-threading is not present in a single GPU core.\n\nGPUs rely mainly on their massively parallel structure. Lets say you give the GPU 2 numbers to add. Sure, you might say it calculates the result slower than a CPU. But if you give the same GPU a million numbers to add, it will return the result almost instantly as compared to a CPU. So what the GPU cares about is not the time taken to do an individual task, but rather what it does when bulk data is given to it.  \n\n## Why is the job of rendering graphics suitable for a GPU?\n\n![Need For Speed]({% static '/blog/assets/img/introduction_to_gpu_computing/nfs.jpg' %})\n\n Say you're playing Need for speed. What exactly happens while rendering the game screen graphics? Usually, the data is given in the form of a 3D model. So basically there is a car of so and so height, width and depth situated at some position given by a set of 3D coordinates in the world. Similarly, there are buildings, trees, people etc. which are the objects of your world. So firstly, you get the coordinate data which says where exactly is something located. Then you get the information about the color of objects. This is also called the texture. Now, we divide the complex surfaces into smaller triangles or quadrilaterals as they are simple shapes that can be managed easily. Any complex shapes in this world can be broken into triangles. The more the number of triangles you have, the more detailed your objects are.\n\n![Rabbit]({% static '/blog/assets/img/introduction_to_gpu_computing/rabbit.png' %})\n\n Next, you have a \"camera\" or the place and angle that you are viewing the entire scene from. Obviously, you can see only 2D images on a computer screen, so the 3D objects in your model have to be projected onto a 2D screen. Firstly, you discard or keep aside the objects that are not visible to your camera and then proceed towards projected all objects in what is called the camera's view space onto a 2D screen. This is followed by rasterization which is to convert the image in vector graphics format to pixels for output on a display screen. After all this, you proceed towards moving your camera so that it gives an impression of moving in a 3D world.\n\n![3D v/s 2D]({% static '/blog/assets/img/introduction_to_gpu_computing/3d_2d.png' %})\n\nSo as you can see, there are different types of calculations happening which involves mostly three dimensional matrix operations, representing the 3D coordinates, the RGB values for the colors, and transferring all the pixels of the image into a memory buffer. Floating point arithmetic operations need to be supported as the coordinates are all floating point numbers. A lot of three dimensional plane equations need to be solved. Most of these operations can be parallelized because the results do not depend on each other. If there is an operation which say has to be done for every vertex in our model, or every pixel in the image, since there are millions of such vertices and pixels, we can divide the data into batches, and compute all units in a batch independently and in parallel. This kind of work is exactly what a GPU is designed for.\n\n## General Purpose GPU Computing\n\n![GPU Acceleration]({% static '/blog/assets/img/introduction_to_gpu_computing/gpu_accel.png' %})\n\nOver the years, people realized that some types of computational work can be done better by a GPU as compared to a CPU. They realized this could be applied in situations where the data in motion moves much like images, and the required processing is well-suited for vector and matrix mathematical operations. Subsequently, the term \"General Purpose GPUs\" or GPGPUs was coined and GPU accelerating computing was used to accelerate applications such as deep learning, analytics, scientific computations etc. Frameworks such as OpenCL, CUDA etc. came into existence that abstracted away the underlying GPGPU architecture that made it easier for programmers to work with it. GPU-accelerated computing offloads compute-intensive portions of the application from the CPU to the GPU, while the remainder of the code still runs on the CPU. From a user's perspective, applications simply run much faster.\n\n## Interested to learn more about GPU Computing?\n\nGreat! :) You could get started with learning how to code using one of the frameworks used for heterogeneous parallel computing such as OpenCL or CUDA C/C++ and maybe try out a few programs on your own. (Although, first check if your graphics card is compatible with these frameworks)\n\nLinks:\n* OpenCL:\n    1. [Fixstars OpenCL Programming Book](https://www.fixstars.com/en/opencl/book/)\n\n* CUDA:\n    1. [CUDA C/C++ Basics - Nvidia Corporation](http://www.nvidia.com/docs/io/116711/sc11-cuda-c-basics.pdf)\n    2. [CUDA C programming guide - Nvidia Developers Zone](http://docs.nvidia.com/cuda/cuda-c-programming-guide/#axzz4j3ZskSHg)",
        "author_name": "Vilas M",
        "slug": "introduction-to-gpu-computing"
    },
    {
        "layout": "post",
        "title": "Why Not Chemical Engineering",
        "author_github": "Anupama",
        "date": "2017-06-07T06:48:33",
        "image": "/assets/img/",
        "description": "Why Chemical Enginnering is not a bad choice after all!",
        "tags": [
            "Chemical",
            "Career Advice"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "Anupama",
        "comments": true,
        "text": "[{% load static %}]()\nI don\u2019t know what kind of fascination it is, probably parents sermon their little ones or the Indian school teachers are rather pedagogues, but this one notion of securing admission for undergraduation in a leading institute with specialisation in computer science is considered to be victory. I too was once a prey to this concept. The D-day shattered my dreams and landed me to choose one among the core branches. Hence, by elimination, the same technique one might use in the exams (xD), I chose chemical.\n\n**Preliminary thoughts:** Chemical engineering = Chemistry. Bury myself. Very disinclined to the idea of studying chemistry, infact chemistry is despicable. \n\n**Society:** People are incorrigible, my neighbour even blurted it out unconscionably \u2013 \u201cSomehow try to get a branch change to ECE, not much future in chemical you see.\u201d \n \u201cWhy did you choose chemical of all branches?\u201d\n\u201cOh mechanical? Not for girls, good that you chose chemical.\u201d\n\nNow that I\u2019ve chosen my branch turned a deaf ear to people\u2019s prophecies.\n\n**The present:** Absolutely no regrets. \n\n![Chemical Engineering]({% static '/blog/assets/img/why-not-chemical-engineering/chemical-engineering.jpg' %})\n\nChemical engineering infact is a convergence of physics and mathematics. Chemistry is just the basis. A graduate might perform groundbreaking scientific research, work as a field engineer, or occupy a senior management position. Graduates employed by multinational companies often travel all over the world.\n\nOne aspect that dissuades students is the salary package one receives soon after graduation. One might argue, but salary is a concern. Frankly, I too had similar thoughts. But let me set it right, a chemical engineer requires specific, additional long periods of tireful training pertaining to the needs and type of the company. During the probation periods, one is paid and is also given privilege to a number of benefits, let alone the safety and risk management steps the company considers for you. Hence the difference. As you progress, your pay-scale shoots, probably in 5-10 years of experience. \n\nEven those graduates who choose not to accept jobs in industry are highly employable in other areas because they are experienced at solving problems and have good analytical and management skills. Chemical engineering graduates can go on to successful careers in finance, consultancy or scientific journalism, to name but a few.\n\nChemical engineers work to improve the quality of people's lives. Most items used by people everyday have needed chemical engineers to produce them. Chemical engineers are helping to end starvation, disease and poverty. They are working to save the environment by developing alternative technologies to combat acid rain, lead pollution and the greenhouse effect.\n\nChemical also offers a wide range of areas for specialization. Nanotechnology, petrochemicals, material sciences, colloidal and polymer science, agricultural production, biotechnology, catalysis, electro-mechanical systems, energy, kinetics, modelling and simulation, instrumentation, industrial safety and pollution control, environment, chemical process and plant design, manufacturing\u2026\u2026\u2026\n\n### Chemical engineering @NITK\n\nThe topmost institute for undergraduate studies in chemical engineering among NITs and in the **top fifteen in India**. These are just the statistics picked up from the internet, what\u2019s more vital is the holistic development as a student as well as expertise in the subject. A well balanced curriculum having both theory and practice, well qualified and friendly faculty have been the hall mark of the department right from its inception. Every faculty will know you inside-out in just one semester! They give good suggestions and always encourage you for your betterment.\n\nI opine, given the experience of professors on-site and off-site, their concern towards every student, beyond any doubt should be one of the unparalled options. \n\nA considerable amount of PSUs along with private companies do come to pick up eligible students on campus. Over and above, one also has an innumerable opportunities for higher studies in India and abroad. Though these programs are common across other branches as well, the most exhilarating facet is live experimentation and multifaceted instrumentation. \n\nRecently, I had been working on self healing materials, as a replacement for muscles in the human body, and for two days I just saw myself enthralled with cutting the material and joining it back again using different crosslinkers. (xD), and it\u2019s not that I have lost my touch with computer science (I once craved for), I model it by coding. See, it\u2019s MULTIFACETED.\n\nPing me for any thoughts and doubts!",
        "author_name": "Anupama G",
        "slug": "why-not-chemical-engineering"
    },
    {
        "layout": "post",
        "title": "The Class of NP",
        "date": "2017-06-09T16:32:44",
        "image": "/assets/img/",
        "description": "The basic understanding of problems belonging to class of NP, NP hard and NP complete.",
        "tags": [
            "Algorithms",
            "P v/s NP"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "isha97",
        "comments": true,
        "text": "[{% load static %}]()\nMany of us have this notion that NP stands for \"Non Polynomial\", the problems which can't be solved in polynomial time. This is not true. First of all, NP consist of only decision problem (given any problem, the answer should be yes or no) solvable in polynomial time by a theoretical non-deterministic Turing machine. Well, I am not going to cover non-deterministic Turing machine in this blog, so I will give another verifier based definition.\n\n### Verifier Based Definition:\nConsider a decision problem X. Let I be an instance of X for which the answer is \u201cyes\u201d. Now if we can indeed verify in polynomial time that the answer to I is \u201cyes\u201d, then we have a verifier for I. If there is a verifier for all such instances of X, then X is said to be in NP. Consider an example:\n\nSubset sum problem: Given a set of natural numbers, is it possible to find a subset such that sum of all numbers in that subset is zero. Now for any instance I, I can simply add up all the numbers of the subset to verify if the sum is zero. So, I have a polynomial time Verifier. Therefore, the problem belong to NP. \n \nNow, the next thing is whether we can find in polynomial time, the answer to that problem. Wait! Here by polynomial time, we mean polynomial in terms of input size. If my input is just a number, polynomial time implies polynomial in the number of bits required to store that number. So, the polynomial time in case of finding whether a number is prime or not will be log(n) (The binary representation will require log(n) bits).\n\nIf we can find the answer in polynomial time, the problem is in the class of 'P'. So yeah, P is a subset of NP. Now, if you ask me whether it's a proper subset or not, then you are asking me a million dollar question. Whether P = NP is one of the most famous question in Computer Science and nobody has been able to attack it.(You can try your hand!)\n \n### NP Hard \nSo, if it's not a proper subset than what other problems does NP consist of? Well let me introduce one more class of problems - NP Hard.\n\nThe problems in NP hard are the set of problems which are \"atleast\" as hard as any other problem in NP. So, if you can find a polynomial time algorithm for a problem which is NP hard, then you can find a polynomial time solution for all the problems in NP. So, we can reduce any problem in NP to any problem in NP hard. This reduction will be done in polynomial time. But wait, although the class of problems is called \"NP\" hard, all the problems in NP hard need not belong to the class NP. Why? All NP hard problems are not decision problems. Now you will think that some NP hard problems must belong to NP and such problems are called NP complete(NPC) problems. So, NP complete is intersection of NP and NP hard as shown in the figure below. \n\n![Euler diagram for P, NP, NP-complete, and NP-hard set of problems]({% static '/blog/assets/img/the-class-of-np/P_np_np-complete_np-hard.svg.png' %})\n \nThe above diagram is taken from wikipedia. It is widely believed that P != NP, but no proofs yet. \n\n \n### Resources:\n[NP](https://en.wikipedia.org/wiki/NP) <br>\n[List of NP Problems](https://en.wikipedia.org/wiki/List_of_NP-complete_problems)\n \nYou can follow the book \u2018Algorithms\u2019 by U.V. vazirani and \u2018Introduction to Algorithms\u2019 by Tardos for getting a better understanding.",
        "author_name": "Isha Tarte",
        "slug": "the-class-of-np"
    },
    {
        "layout": "post",
        "title": "Lighting Up your Room!",
        "author_github": "AbhijithKamath",
        "date": "2017-06-10T10:48:33",
        "image": "/assets/img/",
        "description": "Three components that dance to music",
        "tags": [
            "CAS",
            "SPS",
            "EEE"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "AbhijithKamath",
        "comments": true,
        "text": "[{% load static %}]()\n## Apparatus:\n\n1.**LED strips** <br>\nYou should find these in most home decor stores. They could turn out to be expensive depending upon the quality of light you expect. There are RGB strips available too! <br>\n\n2.**TIP 31 switch**<br>\nNow this is a power transistor that can withstand large currents that the LED strips demand\n\n3.**AUX cable**<br>\nThis is to connect your music player to the device\n\n4.**12V battery**<br>\nThe LED strips demand a lot of current, so get one that supplies enough. There are rechargeable lead acid batteries available. They are cheaper, but bulkier than pocket Li-Po batteries. Sort yourself a powerful 12V battery!\n\n5.**AUX female adapter**<br>\nTo connect your music player to the device. I haven't been able to find these at stores, you'll find them on [here](http://www.electroncomponents.com/)\n\n6.**Speakers**<br> To listen to audio!\n\n7.**AUX female to female spiltter**<br>\nSo that you can feed into the device and listen at the same time.\n\n8.**Soldering tools**<br>\nThis is something you should already have!\n\n## Setup\n\nWe will use the transistor to switch the LEDs depending on the audio input that is received. If you have RGB LEDs you can use RC filters to separate out bass and treble and make it cooler. We will stick to regular strips for now.\n\nWe want the transistor to operate as a switch, so we will use the LEDs as loads across the source and the collector. The audio input will be given to the base. And that is basically all you need!\n\nGet the components on to a perforated board and solder the connections once you check them. It is advised not to use a breadboard if you're using longer LED strips, you could leave residues on the breadboard or even completely damage it.\n\n![Transistor Switch]({% static '/blog/assets/img/lighting-up-your-room/plans.jpeg' %})\n\nThat's how to make your room cooler! Engineering FTW!\n\n### Resources:\n\n[Check out for more help!](http://www.instructables.com/id/Sync-LED-to-Music/)",
        "author_name": "Abhijith Kamath",
        "slug": "lighting-up-your-room"
    },
    {
        "layout": "post",
        "title": "Implementing Google Image Search Style Grid",
        "date": "2017-06-10T12:10:44",
        "image": "/assets/img/",
        "description": "A walkthrough of implementing image grid similar to Google Search",
        "tags": [
            "Web Design",
            "Grid"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "hrily",
        "comments": true,
        "text": "[{% load static %}]()\n# Implementing Google Image Search Style Grid\n\nI love design principles of Google. The way they present things is really awesome. One such presentation is Google Image Search.\n\n![Google Image Search]({% static '/blog/assets/img/google_image_search/google_image_search.png' %})\n\nLook at the way they arrange images. Each row has a fixed width, but their are no columns. The images have arranges themselves, intelligently, just enough to fit the row. I always wondered, what algorithm they must have used to achieve this.\n\n### What I thought\n\nSo my search began. Initially I thought they might have used some kind of knapsack algorithm to fit images to the row width. It means rearragning the positions of images, shifting them from one row to another. But if you inspect, the images don't change their positions, yet they can fit in the row, amazing!\n\nI searched the internet to find whats behind the amazing grid. It turns out that, the technique which is used is far simpler than knapsack and yet brilliant. So, here I present you a really simple technique to achive the **Google Image Search style grid**\n\n*NOTE: This post does not provide any 'ready to copy+paste' code. I will only provide pseudo code, you are free to implement it in any language you prefer.*\n\n### Initial intuition\n\nIf we look closely, we can see that the images in a row have equal height. Also, the heights of rows are not same. So the first intuition is to make the heights of images equal. Lets say we have list of `images` which stores `height` and `width` of each image.\n\n~~~~\nSet CONST_HEIGHT := 160\nFor each image in images:\n\tResize image to height CONST_HEIGHT, constraint ratio\n~~~~\n\n### Building a Row\n\nSo now that we have normalized the image heights, let's build a row. First we'll have a look at the pseudo code, explanation is on the way...\n\n~~~~\n/**\n * Procedure to build a single row\n * parameter: images - list of images\n * returns: row - list of images in the built row\n */\nBUILD_ROW(images):\n\tSET row_width := 0\n\tInitialize row_list\n\tWhile row_width < MAX_WIDTH AND images.size > 0:\n\t\tDequeue an image from images\n\t\tAppend image to row_list\n\t\trow_width := row_width + image.width\n\tIf row_width > MAX_WIDTH:\n\t\tCreate new image row_image, with height = CONST_HEIGHT and width = row_width\n\t\tResize row_image to width MAX_WIDTH, constraint ratio\n\t\tFor each image in row_list:\n\t\t\tResize image to height row_image.height, constraint ratio\n\treturn row_list\n~~~~\n\nSo let's see whats happening here. First we intialized row_list which stores the images in that particular row, and a variable row_width which will store the total width of the row. \nNow we keep adding images to row from original list until the row_width exceeds the `MAX_WIDTH`, which is maximum width of the screen. Now we have a row which looks something like this.\n\n![Row]({% static '/blog/assets/img/google_image_search/row.jpg' %})\n\nNow comes the trick. We won't try to fit the images in row, instead we will try to fit our created row in the maximum width of the screen. So we create an dummy `row_image`, which represents the entire row, with height equal to `CONST_HEIGHT` and width equal to `row_width`. Now we resize the `row_image` to width equal to `MAX_WIDTH` and then we resize all the images in `row_list` to height equal to the new height of `row_image`.\nHere's a depiction of the process...\n\n![Resized Row]({% static '/blog/assets/img/google_image_search/resized_row.jpg' %})\n\n### Building the Grid\n\nNow that we know how to build a row, building the grid is easy.\n\n~~~~\n/**\n * Procedure to build the grid\n * parameter: images - list of images\n * returns: grid - list of rows\n */\nBUILD_GRID(images):\n\tInitialize grid\n\tWhile images.size > 0:\n\t\tSET row := BUILD_ROW(images)\n\t\tAppend row to grid\n\treturn grid\n~~~~\n\nNice, we just made a Google Image Search style grid, where images fit in a row, intelligently. \n\n### There's more\n\nHold on, there's little more. While resizing the images, due to floating point divisions and floor operations, we lose some units in the row. If we neglect them, the rows won't have equal widths. To overcome this, we just need to know the units we have lost and divide them equally among the images.\n\n~~~~\nBUILD_ROW(images):\n\t...\n\t// After resizing the images\n\t// Calculate width of resized row\n\tSET row_width := 0\n\tFor each image in row_list:\n\t\trow_width := row_widt-h + image.width\n\t// Calculate the lost units\n\tSET delta := MAX_WIDTH - row_width\n\t// Divide the lost units among the images\n\tWhile delta > 0:\n\t\tFor each image in row_list:\n\t\t\timage.width := image.width + 1\n\t\t\tdelta := delta - 1\n\treturn row_list\n~~~~\n\nYou might be thinking that, aren't we changing image aspect ratio here? Yes, but one can lose a single unit in a single division. So the maximum number of lost units is equal to number of images in row. So width of each image will be increased by atmost one, which isn't a big deal. Also you can center crop the images, so that there isn't any empty space left out.\n\nGo ahead, try implementing it in your next web or mobile app. If you find any issues, you can always mail me...\n\nThat's it guys, until then, \nThis is *hrily* signing of...",
        "author_name": "Hrishi Hiraskar",
        "slug": "google-image-search-grid"
    },
    {
        "layout": "post",
        "title": "Introduction to Reverse Engineering",
        "author_github": "jeshventhraja",
        "date": "2017-06-14T12:10:44",
        "image": "/assets/img/",
        "description": "Learn how to break your code and make it better!",
        "tags": [
            "CTF",
            "Ethical Hacking",
            "Security"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "jeshventhraja",
        "comments": true,
        "text": "[{% load static %}]()\nI am sure you would have fixed a bug if you have done programming. Going through your code, identifying places where it could have gone wrong, trying out different inputs to make it easier, each one with your own different methods! You also would have learnt to make your debugging process simpler with your experience for sure. Congratulations for your first step towards an important skill required for reverse engineering :)\n\nLets begin with a simple intuitive understanding of reverse engineering. You have a toy gun in which you can load bullets and shoot. One fine day, it stops working! (Hmm, not a fine day maybe). Having \u201cJugaad\u201d skills, you will surely not throw it :P You open it and understand how it works. You find that it works on spring mechanism. Being a little technical (:P), You provide potential energy when you compress the spring and at a stage, it gets locked. It is released by pressing the trigger (releasing the lock effectively) and give the bullet kinetic energy and BOOM!\nYou find the trigger was displaced and unable to release the lock and hence you fix it! Feels good isn\u2019t it? :D Not a bad day after all :) Similar process takes place with a software product too. From the application, you extract the source code and understand the logic, flow of information and the loopholes.\n\nPeople who enjoy debugging will love this field. But don\u2019t think it\u2019s very easy. A complete understanding of the system is very important. Say in the case of your toy gun, an improper understanding could have worsened the situation of the gun. You can start by learning how your computer works, going deeper and deeper eventually. Slowly you will start identifying possible bugs and might end up finding out how it is taken care off also. This also is an important step because this is how you learn great ideas!\n\nI will share how I gained interest in this field with the same example I saw. Instead of explaining the whole assembly code obtained from the executable of a program, I will concentrate on attacking the loophole found.\n\nThis a simple program which takes a password as input and grants access only if the password is \u201cpassword1\u201d or \u201cpassword2\u201d. You will soon find out this is not true! Let us go through the program.\n\n![Image1]({% static '/blog/assets/img/introduction-to-reverse-engineering/4.png' %})\n\nThe program intends to grant access only when auth_flag is 1, which happens when right password is submitted. But any positive value of auth_flag will also grant us access. See an example run of the program.\n\n![Image2]({% static '/blog/assets/img/introduction-to-reverse-engineering/3.png' %})\n\nThe program gave access with not only password1 and password2 but also a sequence of \u2018a\u2019s. Lets see what the flaw here is.\n\n![Image3]({% static '/blog/assets/img/introduction-to-reverse-engineering/5.png' %})\n\n![Image4]({% static '/blog/assets/img/introduction-to-reverse-engineering/1.png' %})\n\nWe use gdb to find the addresses of password_buffer and auth_flag variables and find that there is 28 bytes gap between them. Hence any string of greater length will write in the address of auth_flag giving it a positive value and hence grant access. Hence we have broken into the password checking system!! In fact, this is a common method called buffer overflow. Well, that was fun. **Fire in the** loop**hole**!(CS fans here? :P ). Unfortunately, most compilers do not allow overflow these days. We have allowed overflow in this program by using the command \u2018-zexecstack -fno-stack-protector -g\u2019 while compiling (Don\u2019t worry, even I cannot read that xD).\n\nLets see what you can do with the understanding you gained. You can:\n*\treplicate the product\n*\tfix bug issues\n*\tadd extra features\n*\tacquire extra access\n*\tfind loopholes to understand the flaws in your software\n*\tOr do it for fun :)\n\nAll these things can result into ethical or unethical hacking, hackers and crackers respectively. An interesting question for everyone. Is it legal? If you have got the software legally, you are free to reverse engineer it but replicating it exactly will be a problem. Check this interesting solution to the problem! :P\n\n![Image5]({% static '/blog/assets/img/introduction-to-reverse-engineering/6.jpg' %})\n\nHow to start?\n1. \u201cHacking \u2013 The art of exploitation\u201d is a good book\n2. Learn how computers work at lower level\n3. Programming in assembly language\n4. Tools like gdb, radare2 will help you disassemble the executable\n5. Play CTFs \u2013 They are fun and will help you improve your skills!!\n\nSo, That was about it! Have a great Summer!\nAny doubts, feel free to ask :) !",
        "author_name": "Jeshventh Raja",
        "slug": "introduction-to-reverse-engineering"
    },
    {
        "layout": "post",
        "title": "Through the eyes of a Computer",
        "author_github": "mohita",
        "date": "2017-06-14T13:10:44",
        "image": "/assets/img/",
        "description": "Learn how to make your computers see the world like you do",
        "tags": [
            "Computer Vision",
            "Image Processing",
            "Artificial Intelligence"
        ],
        "categories": [
            "CompSoc",
            "Diode"
        ],
        "github_username": "mohita",
        "comments": true,
        "text": "[{% load static %}]()\n![Intro]({% static '/blog/assets/img/through-the-eyes-of-a-computer/image001.jpg' %})\n\n**Robot** -- A word that might remind you of a Rajnikanth starrer or give you the picture of a metallic body going here and there and doing things that humans do quite naturally, on their own. All of us have at least once thought of owning a robot that could do our assignments, or just do things that we are lazy to do. And then, as you enter your college, you take online courses for learning how to build robots that follow lines, identify colours and do all sorts of things that you think it should do. You brag about it in front of your friends and later come to me and say, \u201cOh well, I have designed a robot that can follow a line!\u201d and I would ask you a simple question. Can your robot see the line? Now, when I ask you this question, I mean to know if the robot can see like humans. A robot isn\u2019t fully automated if you restrict what it should see. You can use sensors to detect lines but that doesn\u2019t mean that the robot has its own vision. Our ability to see and perceive the things around us is perhaps the greatest ability we all have. And when you pass on this ability to a computer or any machine per se, that\u2019s what you called Computer Vision or more generally, Machine vision. Many of you would have already heard a lot about it, it\u2019s the future of artificial intelligence. But truly speaking, this futuristic topic isn\u2019t that new.\n\nHave you heard about the Robot Sony Aibo, if not you got to look at this video!\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/RDdpqlZ1N4Y\" frameborder=\"0\" allowfullscreen></iframe>\n\nThese robots were conceptualized in the 1990s and made available to the general public in 1999. They went on to become very popular and stayed in the market till 2005. Millions of models were sold world wide! But what made these robots so popular?\n\n![img2]({% static '/blog/assets/img/through-the-eyes-of-a-computer/image002.jpg' %})\n\nThey were one of a kind. They could identify charging stations around the house and charge themselves. They behaved like actual dogs, they would grow and learn based on their owner ,who could teach their dog whatever they wanted using visual cues. They made use of the most efficient CV and ML algorithms of those times. Many of you would have heard of the famous SIFT ( Scale Invariant Feature Transform ) detector. It was patented by David Lowe ( who is currently working as a research scientist at Google) in 1999. This feature detection algorithm was readily used in Aibo making it very autonomous. Eventually, Sony came up with Sony Qrio, which was a human-like robot that was so efficient that it could learn a dance by looking at the choreography!\nHave a look at this video!\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/Qv_4eR-P58E\" frameborder=\"0\" allowfullscreen></iframe>\n\nIt\u2019s not only the robots that steal the show. We all are fascinated by the idea of self-driving cars and would want to own one. There are numerous other examples illustrating how Computer Vision had its aura in the past and has been continuing to amaze us with inventions happening almost every day.\n\nWith the advancements happening in this field, it\u2019s good to have some genuine idea of what takes you into this field and how you can build upon it.\n\nI\u2019m pretty sure, that by now, you are ready to take a dive into the world of Computer Vision!\n\nFirst of all, let me make somethings clear! There are many misconceptions people have about Computer Vision, atleast I was one of them and I always thought it was a perfect blend of Image Processing and Machine Learning. So, I went on to learn Image processing and Machine learning separately and when I arrived at a real CV problem during my internship, I realised that this perfect blend isn\u2019t complete without a few other important ingredients that go into the making. So, I am going to tell you some important topics you need to know before you start with Computer Vision. Of course having the knowledge of Image processing helps us a lot in getting acquainted with some basics of Feature Extraction. But the real world of Computer Vision is full of cameras and geometry.\n\n![img3]({% static '/blog/assets/img/through-the-eyes-of-a-computer/image003.jpg' %})\n\nDidn\u2019t like Geometry in school? I bet you will start loving it when you are introduced to problems that transform basic geometry into a higher level of application. Loved geometry, you will start loving it more! :P\n\nMost of the online tutorials on Computer Vision properly cover the basics of Cameras and Geometry but the major struggle is understanding the algorithms without a proper knowledge of Linear algebra. Reducing huge matrices and finding eigenvectors (oh! Jargons: P) never made so much sense when we were doing them in our 3rd semester. But as they say, everything happens for good :P. You will realise that it is indeed true and it\u2019s worth spending time on basics of linear algebra. It not only helps in the understanding the existing algorithms, but also helps in getting the intuition with which those great scientists came out with those algorithms. And when you know this formula, you are ready to devise algorithms of your own!\n\n![img4]({% static '/blog/assets/img/through-the-eyes-of-a-computer/image004.jpg' %})\n\nWhen you start learning the epipolar constraints and camera calibration, it does get intimidating at first. But don\u2019t worry my friend, it\u2019s just a phase :P. It\u2019s just a simple change of coordinates \u2013 You project your point in the world frame to the camera frame (using Extrinsics) and then bring it from the camera frame to the image frame(using Intrinsics). Well, it\u2019s not this simple , but trust me, it\u2019s really interesting. And if you want to go further, you can use Machine Learning to unleash the power of your CV algorithms.\n\nI will give some references in the end so that you can also embark onto this journey.\n\nTalking about applications, I forgot to mention about Virtual Reality (ahh! That\u2019s what we wanted to hear :P). There is an enormous scope of Computer Vison in this field. Imagine clicking pictures of a place you are holidaying at and your friend gets to see the entire place in a 3D view in Real time! Won\u2019t that be cool? Well, of course it will be unless you spent lakhs to actually go to that place :P.\n\nThat was a lot of introduction I suppose. So, let\u2019s start learning!\nHere are some good resources to get started :D\n* [Introduction to Computer Vision by Udacity](https://www.udacity.com/course/introduction-to-computer-vision--ud810) - A well designed and fun filled course that takes you from the theory to applications\n* This course is good for deep intuition in the image processing part\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/715uLCHt4jE\" frameborder=\"0\" allowfullscreen></iframe>\n* The book \u201cMulti view Geometry\u201d for those who want to go into the depths of geometry.\n\nThese are the resources that should be more than enough to get a little idea on how you do things. To actually do things, you can solve problem sets online or pick up OpenCV documentation and start exploring. By the way, OpenCV is a library that has millions of functions already implemented for you so that you can get yourself involved in what\u2019s yet to be done :)\n\nNow that you have read this loooong post, Go explore the fascinating and futuristic world of Computer Vision!\n\nHappy CV :)",
        "author_name": "Mohiitaa Chowdhury",
        "slug": "through-the-eyes-of-a-computer"
    },
    {
        "layout": "post",
        "title": "The Fourth Industrial Revolution",
        "author_github": "vasista",
        "date": "2017-06-16T13:10:44",
        "image": "/assets/img/",
        "description": "What the future beholds!",
        "tags": [
            "IoT",
            "Automation"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "vasista",
        "comments": true,
        "text": "[{% load static %}]()\nAutomation, Internet of Things, Genetic engineering, cloud computing, renewable energy, sustainable development. Welcome to the Future. Welcome to the 4th Industrial revolution.\n\n![image1]({% static '/blog/assets/img/the-fourth-industrial-revolution/pic1.jpg' %})\n\nProfessor Klaus Schwab, Founder and Executive Chairman of the World Economic Forum published the \u2018The Fourth Industrial Revolution\u2019. Through this he explained its impact on economies, countries, politics and even ideologies. The fourth industrial revolution focuses on making our industries more productive with automation cloud computing and IoT, shying away from fossil fuels and making super-soldiers (through genetic engineering!). That last one is questionable though\u2026\n\nGenetic engineering will help us in not only rectifying defective genes, but also opens up a possibility to make the new generation more Charles-Darwin-\u2018fit\u2019. With the onset of technologies like CRISPR, it is exciting and apprehensive of what could happen in the future.\n\n![image2]({% static '/blog/assets/img/the-fourth-industrial-revolution/pic2.jpg' %})\n\nAutomation, IoT and cloud computing collectively will help in making every aspect of our lives more comfortable*. From household to industries, robots will replace human labour and intervention and give us freedom from the concept of work and let us choose our real passions. Elon Musk rightly said that automation will force governments to give a universal basic income.\n\nFinally we come to sustainable development. In the future, there will be a stronger force towards renewable energy and undo the effects of climate change and mass extinctions, making this planet a better place (although the fourth industrial also focuses on cheaper space exploration technologies and colonization, so we are good either way!!).\n\nThere is a lot of heated debate about the effects of the fourth industrial revolution. This means that there is going to be some serious strategic changes for economists, politicians and top businessmen\n\n![image3]({% static '/blog/assets/img/the-fourth-industrial-revolution/pic3.jpg' %})\n\nLeave it to them for the decision making. But no matter what, the fourth industrial revolution is happening and it\u2019s happening fast! Your opinion is not going to stop it. So what does the fourth industrial revolution mean for engineers and scientists?\nIt\u2019s simple, it means that we have a lot of work to do!\n\nWe are in a race very much like the space-race between Russia and USA had. We need to come up with innovative and profound technologies as soon as possible. We need to make it affordable, accessible and original. Like every other industrial revolution, this will be a game changer for the world.\n\nSo in two words\u2026 Carpe Diem!\n\n### THE FOURTH INDUSTRIAL REVOULTION\n#### Coming soon...\n\n*unless the robots take over and a scene from the Terminator becomes a reality",
        "author_name": "Vasista Ayyagari",
        "slug": "the-fourth-industrial-revolution"
    },
    {
        "layout": "post",
        "title": "Fluid mechanics in everyday life",
        "author_github": "shreyas",
        "date": "2017-06-18T12:10:44",
        "image": "/assets/img/",
        "description": "Thought Fluid mechanics is a subject involving equations and complex mathematical formula? Here are some everyday applications which you may not have thought about",
        "tags": [
            "Fluid Mechanics"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "shreyas",
        "comments": true,
        "text": "[{% load static %}]()\n## Applying Makeup!\n![Image1]({% static '/blog/assets/img/fluid-mech-everyday-life/1.jpg' %})\n\nWhen you put on eye shadow or powder, you are most definitely applying shear and pressure forces. When you put foundation on a sponge and smear it over your face you can feel the shear stress acting on your face. Furthermore, the shear stress increases as the foundation moves from the sponge to your face. This causes no slip at the boundary, which allows the foundation to stick to your face rather than slide off. This is a great example of Couette Flow!!! It is amazing to realize how much fluid mechanics applies to our lives. I'm excited about using all that I have learned to better understand the world.\n\n## Using Lotion!!!\n![Image2]({% static '/blog/assets/img/fluid-mech-everyday-life/2.jpg' %})\n\nToday I thought about the fluid mechanics used in applying lotion. When you squeeze the container to get lotion out, you are actually applying a pressure force to the container. In addition, once the container is set upright, the lotion slowly slips down the side of the container. Since the lotion is coming into contact with the sides of the container shear forces are present. This situation is representative of Couette flow.\n\n## Washing Dishes!!!\n![Image3]({% static '/blog/assets/img/fluid-mech-everyday-life/3.jpg' %})\n\nAs I was washing dishes today, the extra soap on the dishes became slippery. I realized that this was because the soap was acting as a lubricant for the dish. The viscosity of the soap lessened the effect of the shear forces on the plate. This is what causes the plate to slip out of your hand!!!\n\n## Flags!\n![Image4]({% static '/blog/assets/img/fluid-mech-everyday-life/4.jpg' %})\n\nToday I thought about how beautiful a flag looks when it waves. I realized that the the wind blowing on the flag actually produces a drag force on the flag. This force acts on both sides of the flag. This drag is due to the wind blowing over the side of the flag. Thus the force itself is actually the shear force acting on the flag. So the next time you see a flag waving, think about the fluid mechanics involved!!!!\n\n## Making Coffee!\n![Image5]({% static '/blog/assets/img/fluid-mech-everyday-life/5.jpg' %})\n\nAs I was pouring creamer into my coffee today, I thought about the design of the container. I noticed the flow of my creamer into the cup was laminar rather than turbulent and wondered why. I noticed the container had a curved point from which the fluid flowed. I decided that this design inhibited the flow of the creamer particles close to this point. As a result, this allows the remaining creamer to flow slower, which leads to a less turbulent flow.\n\n## Blood Pressure!!!!!\n![Image6]({% static '/blog/assets/img/fluid-mech-everyday-life/6.jpg' %})\n\nWhen I went for a general health check up I was talking to some nurses about how the blood pressure meter works. I was thrilled by all the fluid mechanics involved! To begin with, the cuff is pumped with air by use of a hand pump. Clearly there are frictional losses in this method. Once the cuff is full of air, it cuts off the blood circulation in the artery, which causes the pressure to build up. The top number in your blood pressure measurement tells you the pressure of your blood when the artery is not pumping blood, and the bottom number tells you the pressure of your blood when blood flow continues. The flow of blood through your veins is much like that of a fluid flowing through a pipe. The flow of blood is steady and laminar, so the velocity profile would be parabolic due to friction occurring at the walls of the arteries. This velocity profile would be altered if there is plaque build up in the artery.\n\n## Forces on a Soap Dispenser!\n![Image7]({% static '/blog/assets/img/fluid-mech-everyday-life/7.jpg' %})\n\nAs I was washing my hands today, I got soap from a soap dispenser that was attached to the wall. I remembered that by using the momentum equation, you can calculate the force the screws need to hold the soap dispenser up. This is the same approach you would take to find the forces needed to hold a pipe in place. You would need to sum the forces in the y-direction and use a variable for the force holding the soap dispenser in place. You would then need to consider whether you are analyzing the dispenser when soap is coming out or when the dispenser isn\u2019t being used. If you choose the later, the problem would become very simple because you would not have any momentum coming out. It was rewarding to recognize a fluid mechanics problem in disguise!\n\n## Showering!\n![Image8]({% static '/blog/assets/img/fluid-mech-everyday-life/8.jpg' %})\n\nToday as I was taking a shower, I was mindful of the momentum of the water coming out of the faucet. I realized that the momentum of the water is what washes off the soap. In addition as you increase the velocity of the water, you also increase the momentum.\n\n## Toothpaste!\n![Image9]({% static '/blog/assets/img/fluid-mech-everyday-life/9.jpg' %})\n\nToday as I brushed my teeth, I was reminded of the extreme difference between the viscosity of water and the viscosity of the toothpaste. Water is a Newtonian fluid that has a very low viscosity and flows freely at room temperature. My toothpaste, on the other hand, has a very high viscosity. It is actually considered a Non-Newtonian plastic fluid because a shear force must be applied to it before it will move! This works to my advantage when I brush my teeth! Think how hard it would be to put toothpaste on your toothbrush if it just poured out of the container when you opened the cap!",
        "author_name": "Shreyas Hegde",
        "slug": "fluid-mechanics-in-everyday-life"
    },
    {
        "layout": "post",
        "title": "Promises, Closures and Potatoes",
        "author_github": "viggi-v",
        "date": "2017-06-23T00:00:00",
        "image": "/assets/img/",
        "description": "A very basic introduction to some lesser explored stuff in javascript",
        "tags": [
            "JavaScript",
            "Programming"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "viggi",
        "comments": true,
        "text": "[{% load static %}]()\n# Promises, Closures and Potatoes.\n\nWell this article is mainly about Promises and Closures, two concepts that\nmost of us don't really care about in the beginning, but in reality, we have been\nusing it a lot. \"Promises and Closures\" wouldn't sound cool so I thought of slightly innovative title.\n\n## So what are promises?\nThey are not new, and they are pretty popular for a long time.\n![image1]({% static '/blog/assets/img/promises-closures-and-potatoes/image1.png' %})\nPromises gives you this feature of asynchronous nature in the code. It actually lets a third agent interact with your code: let's take a small example.\nAssume that we are doing a function `getTextFromFile()` which will take unpredictable time.\n```\n    function getTextFromFile(fileEvent){\n                var input = fileEvent.target;\n\n                var reader = new FileReader();\n                reader.onload = function(){\n                    var text = reader.result;\n                    return text;\n                };\n                reader.onerror = function(){\n                    console.log(Error(reader.error));\n                }\n                reader.readAsText(input.files[0])\n            }\n```\nThis method would not let you really specify the actions to be done on file read externally. It just returns the text, and it could actually take unpredictable time,\nsuch that the following snippet may not work the right way:\n```\n    var myText = getTextFromFile(fileEvent);\n    console.log(myText);\n```\nThe same applies for AJAX requests or any other asynchronous event.\nSo we have a much better way of dealing this: using a **promise**.\nAll promises need not be fulfilled, it could be **resolved** or it could be **rejected**.\nIt could take some time to **resolve**/**reject** a promise, and either way it would be **settled** :).\nLet's rewrite the `getTextFromFile()` :\n```\n    function getTextFromFile(fileEvent){\n        return new Promise(function(resolve,reject){\n            var input = fileEvent.target;\n            var reader = new FileReader();\n            reader.onload = function(){\n                resolve(reader.result);\n            };\n            reader.onerror = function(){\n                reject(Error(reader.error));\n            }\n            reader.readAsText(input.files[0])\n        });\n    }\n```\nnow we can use this function to handle file read much better way:\n```\n    getTextFromFile(fileEvent).then(function(text){\n        // yaay! I did it!\n        console.log(text);\n    }).catch(function(error){\n        // oops! something went wrong.\n        console.log(error);\n    })\n```\nSo you have the anonymous functions in `.then` and `.catch` to work asychronously, ie they will wait for the promise to resolve/reject.\nPromises are much more flexible, and they could be chained.\n```\n    function middleGuy(text){\n        return new Promise(function(resolve){\n            console.log(\"Hello from the middle guy!\");\n            // just to show that this could be done.\n            resolve(text);\n        });\n    }\n```\n and boom! we can do this:\n```\n    getTextFromFile(fileEvent)\n        .then(middleGuy)\n        .then(function(text){\n            // yaay! I did it!\n            console.log(text);\n        }).catch(function(error){\n            // oops! something went wrong.\n            console.log(error);\n        });\n```\n[This](https://gist.github.com/viggi-v/cf996ad839a7b9525dc0e1ea89d45346) is a simple demo of promises with the above code.\n\n\n\nThere are plenty of resources out there for Promises,\nand my favourite ones are:\n* [Scotch Blog](https://scotch.io/tutorials/javascript-promises-for-dummies)\n* [Article by Jake Archibald](https://developers.google.com/web/fundamentals/getting-started/primers/promises)\n\n## Closures\n\nClosures were this one tricky thing in the beginning for me, and quoting *Rohit Varkey*, batch of '16,\n> It (closures) guarantees you no side effects. Plus this way you have everything you need that is used in the function wrapped along with it. No ugly global changes that could **** everything up. This leads to interesting things like lazy evaluation, loop fusion and even easier process migration.\n\nThis was about closures in a general perspective, but from the view of a js developer like me, this offers a scope that persists just for my function.\nAn example would be, (from w3Schools)\n```\n    var add = (function () {\n        var counter = 0;\n        return function () {\n            return counter += 1;\n            }\n    })();\n\n    add(); // prints 1\n    add(); // prints 2\n    add(); // prints 3\n    // counter is not accessible here.\n```\n\na good set of resources would be:\n* [W3 Schools](https://www.w3schools.com/js/js_function_closures.asp)\n* [Medium article by Eric Elliott](https://medium.com/javascript-scene/master-the-javascript-interview-what-is-a-closure-b2f0d2152b36)",
        "author_name": "Vighnesh Velayudhan",
        "slug": "promises-closures-and-potatoes"
    },
    {
        "layout": "post",
        "title": "Graphene - A Wonder Material",
        "date": "2017-06-24T16:00:00",
        "image": "/assets/img/",
        "description": "An article about Graphene",
        "tags": [
            "Graphene"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "samarth",
        "comments": true,
        "text": "[{% load static %}]()\n# GRAPHENE \u2013 A WONDER MATERIAL\n\n\u201cI have never even heard of this term graphene before\u201d, might be your thought process presently. Read further to understand why I call it a wonder material.\nWhat\u2019s running in your mind at this point of time due to similarity in what you have studied is Graphite, an allotrope of Carbon which is very commonly found. But, what\u2019s Graphene then?\nYes, graphite and graphene are in fact the same except for a few structural differences which brings in unexpected properties.\nGraphite consists of various layers of Carbon atoms where each layer is arranged in a honeycomb lattice and the layers are held together by weak force, van der Waals\u2019 forces.  We all know that Graphite is the most stable form of carbon. (Diamond is not the most stable even though it very hard). So graphene is mono-layered graphite. If you have ever drawn with a sharp pencil, you have probably manufactured grapheme in a very crude way. This is the world\u2019s thinnest material and what made researchers work in this area is due to the extra ordinary properties graphene possesses.\n\n![1]({% static '/blog/assets/img/graphene/1.jpg' %})\n\n## History:\nGraphene revolution was started in October 2004, when condensed-matter physicists reported that they had prepared graphene\u2014two-dimensional sheets of carbon atoms\u2014and observed the electric field effect in their samples1. It was not long before this new material attracted the attention of the electron-device community, and today a growing number of groups around the globe are successfully fabricating graphene transistors.\n\n## General Properties\n\nExcellent mobility\nMany methods to open up a band-gap as graphene in its large-area form has zero band gap and behaves as a semi-metal. By constraining large-area graphene in one dimension or biasing bilayer graphene or even  by applying strain.\nPossibility to make large-area transistors and fabricating nano-ribbons with well-defined widths and clean edges\n\n## Mechanical Properties\n\nGraphene\u2019s stand-out properties is its inherent strength. Due to the strength of its 0.142 Nm-long carbon bonds, graphene is the strongest material ever discovered, with an ultimate tensile strength of 130,000,000,000 Pascals (or 130 gigapascals), compared to 400,000,000 for A36 structural steel, or 375,700,000 for Aramid (Kevlar). Not only is graphene extraordinarily strong, it is also very light at 0.77milligrams per square metre (for comparison purposes, 1 square metre of paper is roughly 1000 times heavier). It is often said that a single sheet of graphene (being only 1 atom thick), sufficient in size enough to cover a whole football field, would weigh under 1 single gram. What makes this particularly special is that graphene also contains elastic properties, being able to retain its initial size after strain.\n\n## APPLICATIONS\n### Graphene in Transistors:\n\n\n![2]({% static '/blog/assets/img/graphene/2.jpg' %})\n\nVarious configurations of Transistors using Graphene to increase mobility of charge carriers in channel since graphene has offers higher mobility compared to just Si. (Red region represents Graphene material in all the above structures)\n\n![3]({% static '/blog/assets/img/graphene/3.jpg' %})\n\nScaling down in transistor size and increase in number of transistors per chip over time.\n Source: Review Article on Graphene Transistors by Nature Nano Technology\n\n### Graphene for desalination purposes:\nResearch in this direction will aid in solving the water crises in places where people are suffering from water shortage. Researchers have invented a graphene sieve capable of desalinating sea water where the graphene sheath acts as a semi permeable membrane. This in turn aids in separating water from the dissolved salts. But, we do not want all the salts in water to be separated, so regulation of the size of pores in the graphene material is very necessary and a critical step in this process. Passing through multiple layers of graphene may also be considered an alternative for effective desalination.\n\n\n![4]({% static '/blog/assets/img/graphene/4.jpg' %})\n\nGraphene has so many interesting properties and applications. If you are interested, go through them from any online source. I don\u2019t want to make this article boring by including every possible detail.  :P",
        "author_name": "Samarth Bonthala",
        "slug": "graphene"
    },
    {
        "layout": "post",
        "title": "Hyperloop",
        "date": "2017-06-25T01:00:00",
        "image": "/assets/img/",
        "description": "Basics of working of Hyperloop",
        "tags": [
            "hyperloop"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "pranaypraveen",
        "comments": true,
        "text": "[{% load static %}]()\n# Hyperloop\n\u00a0\n![1]({% static '/blog/assets/img/hyperloop/1.jpg' %})\n\nOne of the most valued and important entity nowadays is time. To help the people around the world to save their \u00a0\u00a0travelling time, Elon Musk came up with an innovative idea of hyperloop. Hyperloop is a proposed mode of passenger and freight transportation that would propel a pod-like vehicle through a reduced-pressure tube that could potentially exceed airliner speeds. The pods would accelerate to cruising speed gradually using a linear electric motor and glide above their track using passive magnetic levitation or air bearings. The tubes could also go above ground on columns or underground, eliminating the dangers of grade crossings. It is hoped that the system will be highly energy-efficient, quiet, and autonomous. The concept, created \u00a0in 2012, incorporates reduced-pressure tubes in which pressurized capsules ride on an air cushion driven by linear induction motors and air compressors. The Hyperloop concept has been explicitly open-sourced by Musk and SpaceX, and others have been encouraged to take the ideas and further develop them. Also \u00a0a few companies have been formed, and several interdisciplinary student-led teams are working to advance the technology. \u00a0Lots of efforts are put to create this innovative \u00a0concept but it does surely has a limitation which is that \u00a0hyperloop is too susceptible to disruption from a power outage or terror attacks to be considered safe.\nThe major reasons why it is difficult to operate high speed vehicles is due to presence of large amount of drag force or air resistance. This resistance becomes substantial when the vehicles reach high speeds. The vactrain (vacuum train) \u00a0concept theoretically eliminates these obstacles by employing magnetically levitating trains in evacuated (airless) or partly evacuated tubes, allowing for speeds of thousands of miles per hour. The hyperloop operates at a pressure of one millibar or 100Pa of pressure.\n\n![2]({% static '/blog/assets/img/hyperloop/2.jpg' %})\n\u00a0\nSome critics of Hyperloop focus on the experience\u2014possibly unpleasant and frightening\u2014of riding in a narrow, sealed, and windowless capsule inside a sealed steel tunnel, that is subjected to significant acceleration forces; high noise levels due to air being compressed and ducted around the capsule at near-sonic speeds; and the vibration and jostling. Even if the tube is initially smooth, ground may shift due to seismic activity. At high speeds, even minor deviations from a straight path may add considerable buffeting. This is in addition to the obvious practical and logistical questions regarding how to best deal with equipment malfunction, accidents, and emergency evacuations. \u00a0\n\u00a0\nHere are the four main features of the hyperloop\n1. The passenger capsules aren't propelled by air pressure like in vacuum tubes, but by two electromagnetic motors. It is aimed to travel at a top speed of 760 miles per hour.\n![3]({% static '/blog/assets/img/hyperloop/3.jpg' %})\n2. The tube tracks do have a vaccuum, but not completely free of air. Instead, they have low pressure air inside of them. Most things moving through air tubes will end up compressing the air in the front thus, providing a cushion of air that slows the object down. But the hyperloop will feature a compressor fan in the front of the capsule. The compressor fan can redirect air to the back of the capsule, but mostly air will be sent to the air bearings.\n![4]({% static '/blog/assets/img/hyperloop/4.jpg' %})\n3. \u00a0Air bearings are ski like paddles that levitate the capsules above the surface of the tube to reduce friction.\n![5]({% static '/blog/assets/img/hyperloop/5.jpg' %})\n4. The tube track is designed to be immune to weather and earthquakes. They are also designed to be self-powering and unobsrtuctive. The pillars that rise the tube abobe the ground have a small foot-print that can sway in the case of an earthquake. Each of the tube sections can move around flexibly of the train ships because there isn't a constant track that capsules rely on. The solar panels present on the top of the track\u00a0 provide the periodic motors with power.\n\u00a0\n![6]({% static '/blog/assets/img/hyperloop/6.jpg' %})",
        "author_name": "Pranay Praveen",
        "slug": "hyperloop"
    },
    {
        "layout": "post",
        "title": "Introduction to big data and hadoop",
        "date": "2017-07-08T00:00:00",
        "author_github": "aditya5558",
        "image": "/assets/img/",
        "description": "An overview of big data and where hadoop comes into the picture.",
        "tags": [
            "Big Data"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "aditya5558",
        "comments": true,
        "text": "[{% load static %}]()\n## What is 'Big data'?\n\nIn simple terms, Big data is data which can\u2019t comfortably be processed on a single machine. \nWe live in a world increasingly driven by data. Try googling about big data and you will find some staggering facts like :\n* The amount of data produced by us from the beginning of time till 2003 was 5 billion gigabytes. The same amount was created in every two days in 2011, and in every ten minutes in 2013 !!!\n* 90% of the world\u2019s data was generated in the last few years.\n* A report from McKinsey & Co. stated that by 2009, companies with more than 1,000 employees already had more than 200 terabytes of data of their customer\u2019s lives stored.\n\n\n\nBig Data is often characterized by **3 Vs** (well sometimes even more :P) \n\n* **Volume** refers to the size of data that you\u2019re dealing with.\n* **Variety** refers to the fact that the data is often coming from lots of different sources and in many different formats.\n* **Velocity** refers to the speed at which the data is being generated, and the speed at which it needs to be made available for processing. \n\n\n## How useful is Big Data ?\n\nIt is very useful....\n\nThink about an e-commerce website. How does Amazon recommend similar products to what you have looked at in the past?\n\nEarlier we could only store records of actual purchases. But nowadays,we can store and process all of our Web server log files, along with the purchase data that\u2019s in our traditional data warehouse, with this data we can give the customer a much better shopping experience which should directly translate into bigger profits. \n\nAnother example is Facebook, every day we feed Facebook\u2019s data beast with mounds of information in the form of likes, messages and picture uploads. With data like this, Facebook knows who our friends are, what we look like, where we are, what we are doing, our likes, our dislikes, and so much more. With this information it is able to suggest us friends, target us with activities and advertisements, use our past activities to get to know our interests and a lot more.\n\nIn fact, Facebook Inc. analytics chief Ken Rudin says, \u201cBig Data is crucial to the company\u2019s very being\u201d.  \n\n## Challenges with Big Data...\n\n* Storing huge amounts of data.\n* Storing data in varying formats.\n* Time taken to process the data.\n\nThis is where Hadoop comes into the picture.\n\n## Meet the Elephant - Hadoop !\n![Hadoop Logo](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcR8I9CmKx2HWXSiBdGBM1VFacBN5QGnPnG6ULd4QlI5wQrjJ1vd)\n\nHadoop is an open source, Java-based programming framework that supports the processing and storage of extremely large data sets in a distributed computing environment. Hadoop with its distributed processing, handles large volumes of structured and unstructured data more efficiently than the traditional enterprise data warehouse.\n\n### How does Hadoop help ?\n\nThe **key concept** in hadoop is that we split the the data up and store it across a collection of machines, known as a cluster. Then, when we want to process the data, we process it where it\u2019s actually stored. Rather than retrieving the data from a central server, instead it\u2019s already on the cluster, and we can process it in place.\n\n### History...\n\nIt all started when Google published a paper about their distributed file system and about their processing framework, MapReduce in 2003. At the same time computer scientists Doug Cutting and Mike Cafarella were working on an Open Source web search engine called Nutch. They needed something scalable because the web was you know, billions of pages - terabytes, petabytes, of data, that needed to be processed. So they decided to reimplement what Google had done in open source. After a couple of years Nutch was up and running on 20 to 40 machines. It wasn't perfect, it wasn't totally reliable, but it worked. In January, 2006 Doug went to work for Yahoo and it was there that \u2018Hadoop\u2019 came into being. After years of development within the open source community, Hadoop 1.0 became publically available in November 2012 as part of the Apache project sponsored by the Apache Software Foundation.\n\n### The Hadoop ecosystem\n\nThe core of Apache Hadoop consists of a storage part, known as [Hadoop Distributed File System (HDFS)](https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html), and a processing part called [Hadoop MapReduce](https://hadoop.apache.org/docs/r1.2.1/mapred_tutorial.html). [Hadoop Yet Another Resource Negotiator (YARN)](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) is the brain of the Hadoop Ecosystem, which provides resource management and scheduling for user applications.\n\nToday, Hadoop is a collection of related subprojects that fall under the umbrella of infrastructure for distributed computing.\n\nThese include:\n* **Flume** : A tool used to collect and move huge amounts of streaming data into HDFS.\n* **Hbase** : An open source, nonrelational, distributed database.\n* **Hive** : A data warehouse that provides data summarization, query and analysis.\n* **Oozie** : A server-based workflow scheduling system to manage Hadoop jobs.\n* **Phoenix** : An open source, massively parallel processing, relational database engine for Hadoop that is based on Apache Hbase.\n* **Pig** : A high-level platform for creating programs that run on Hadoop.\n* **Sqoop** : A tool to transfer bulk data between Hadoop and relational databases.\n* **Spark** : A fast engine for big data processing capable of streaming and supporting SQL, machine learning and graph processing.\n* **Storm** : An open source data processing system.\n* **ZooKeeper** : An open source configuration, synchronization and naming registry service for large distributed systems.\n\n#### Some great resources to get started with Hadoop :\n\n* [Setting up Hadoop on your systems](https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html)\n* [Udacity course on Hadoop and mapreduce](https://www.udacity.com/course/intro-to-hadoop-and-mapreduce--ud617)\n* [Yahoo Hadoop Tutorial](https://developer.yahoo.com/hadoop/tutorial/)\n* [Hadoop - The definitive guide (pdf)](http://javaarm.com/file/apache/Hadoop/books/Hadoop-The.Definitive.Guide_4.edition_a_Tom.White_April-2015.pdf)\n\nHappy Hadooping!",
        "author_name": "A Aditya",
        "slug": "big-data"
    },
    {
        "layout": "post",
        "title": "Fascinating World Of Compliant Mechanisms",
        "date": "2017-07-08T00:00:00",
        "author_github": "shuvam96",
        "image": "/assets/img/",
        "description": "The world around us is fast changing. Soon most of the machines will work on Compliant Mechanisms. Never heard of it eh? Come lets see what this revolutionary technique is and what scope does it have in powering future machines.",
        "tags": [
            "Compliant mechanisms"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "shuvam96",
        "comments": true,
        "text": "[{% load static %}]()\n## What is a Mechanism?\n\nA mechanism is a mechanical device used to transfer or transform motion, force, or energy. Traditional rigid-body mechanisms consist of rigid links connected at movable joints. Whatever machines you see in your day to day life has some underlying mechanisms that govern its motion to produce the desired output. For example A NUT CRACKER works on the principle of MECHANICAL ADVANTAGE based on LEVER MECHANISM. This mechanism transfers energy from the input to the output. \n\nSince energy is conserved between the input and output (neglecting friction losses), the output force may be much larger than the input force, but the output displacement is much smaller than the input displacement. As mentioned earlier these mechanisms involve relative motion of rigid bodies often at very high speeds which various problems like Frictional Losses, Non Linear behavior, Non uniform Heat dissipation, etc. Thus modern day scientists are working on mechanisms that more or less achieve the same motion and force transfer with the help of flexible links and linkless joints. \n\nThese 2 terms have tremendous significance when its comes to the emerging and exciting field of COMPLIANT MECHANISMS. So lets see what these wonders are and what are they capable of.\n\n\n### Compliant Mechanisms\n![Compliant Mechanism]({% static '/blog/assets/img/compliantMechanisms/pic 5.jpg' %})\n\nA compliant mechanism also transfers or transforms motion, force, or energy. Unlike rigid-link mechanisms, however, compliant mechanisms gain at least some of their mobility from the deflection of flexible members rather than from movable joints only. Basically not all the links of a mechanism need to be flexible for it to be termed as A Compliant System but some important links must be flexible. Fully compliant mechanisms are very unstable and unreliable.\n\nCompliant mechanisms rely upon elastic deformation to perform their function of transmitting and/or transforming motion and force. From an overall perspective that considers performance, manufacturability, economy of material, scalability to micro and Nano sizes, adaptability to smart actuations and embedded sensors, resistance to wear, etc., compliant mechanisms are preferable over rigid body mechanisms.\n\n![alt text]({% static '/blog/assets/img/compliantMechanisms/pic 1.png' %})\n\nLarge number of compliant mechanisms are constructed of rigid links that are interconnected by Flexure Hinges design to undergo relatively low levels of rotation that traditional Revolute Pair Joints. A relatively less number of compliant mechanisms have compliant links in addition to the Flexure Hinges designed to undergo large deformation.\n\n![alt text]({% static '/blog/assets/img/compliantMechanisms/pic 3.jpeg' %})\n\nCurrently available design techniques for compliant mechanisms can be grouped broadly into the following three categories based on the methods used as well as the type of mechanisms created using them.\n- Flexural pivot-based compliant mechanisms.\n- Flexible beam-based compliant mechanisms.\n- Fully compliant, elastic continua.\n\n### Flexural pivot-based compliant mechanism\n\nFlexural pivot-based designs use narrow sections connecting relatively rigid segments. Thus, compliance is lumped to a few portions of the mechanism. They are usually of monolithic construction. They can be systematically designed either by starting from an available rigid-body linkage or an intuitively conceived linkage. So in this type mainly the joints are flexible which connects rigid links with each other. The biggest problem faced in this type is the force and momentum transfer is very limited. Compliant mechanisms of this type are often restricted to a small range of motion. Their applications are in precision instrumentation and many consumer products.\n\n### Flexible beam-based compliant mechanisms\n\nFlexible beam-based compliant designs extend the range of motion because the slender beam-like segments are designed to undergo large deformations. These are not always of monolithic construction, as they may have some rigid segments and kinematic joints. Thus, they are sometimes partially compliant. Unlike in flexural pivot- based designs, the compliance is distributed in flexible beam-based designs.\n\n![alt text]({% static '/blog/assets/img/compliantMechanisms/pic 2.png' %})\n\n## Design of Compliant Mechanisms\n\nThe first step in the design of a compliant mechanism is to establish a kinematicaly functional design that generates the desired output motion when subjected to prescribed input forces. This is called topological synthesis. Although the size and shape of individual elements can be optimized to a certain extent in this stage, local constraints such as stress and buckling constraints cannot be imposed while the topology is being determined. Once a feasible topology is established, performance constraints can be imposed during the following stage in which size and shape optimization are performed.\n\nTopology synthesis\u2014which involves generation of a functional design in the form of a feasible topology starting from input/output force/motion specifications,\n\nSize and shape optimization\u2014to meet performance requirements such as maximum stress, motion amplification or force amplification etc.\n\nGeneral Design procedure involves Systematic methods of design of compliant mechanisms starting from functional specifications. First is deriving the topology of a compliant mechanism given the desired input forces and output displacements. Next, is optimizing the size and shape of various elements of a compliant mechanism in order to satisfy prescribed mechanical or geometric advantage, stress constraints, size constraints etc.\n\n![alt text]({% static '/blog/assets/img/compliantMechanisms/pic 6.png' %})\n\n## Advantages of Compliant Mechanisms\n\nThere are a number of reasons why a compliant mechanism may be considered for use in a particular application.\n\nOne advantage of compliant mechanisms is the potential for a dramatic reduction in the total number of parts required to accomplish a specified task.\n\nSome mechanisms may be manufactured from an injection-moldable material and constructed of one piece. For example, consider the fully compliant crimping mechanism shown in the figure below, along with its pseudo- rigid-body model. Due to symmetry, only half the mechanism is shown. The number of components required for the compliant mechanism are considerably less than for the rigid mechanism. The reduction in part count may simplify manufacturing and reduce manufacturing and assembly time and cost.\n\n![alt text]({% static '/blog/assets/img/compliantMechanisms/pic 7.png' %})\n\nIt is possible to realize a significant reduction in weight by using a compliant mechanism over their rigid- body counterparts. This may be a significant factor in aerospace and other applications. Compliant mechanisms have also benefited companies by reducing the weight and shipping costs of consumer products.\n\nThe reduction in the total number of parts and joints offered by compliant mechanisms is a significant advantage in the fabrication of micro mechanisms. Compliant micro mechanisms may be fabricated using technology and materials similar to those used in the fabrication of integrated circuits.\n\n## Disadvantages of Compliant Mechanism\n\nPerhaps the largest challenge is the relative difficulty in analyzing and designing compliant mechanisms. Knowledge of mechanism analysis methods and the deflection of flexible members is required. The combination of the two bodies of knowledge in compliant mechanisms requires not only an understanding of both, but also an understanding of the interactions of the two in a complex system. Since many of the flexible members undergo large deflections, linearized beam equations are no longer valid. Nonlinear equations must be used that account for the geometric nonlinearities caused by large deflections.\n\n## Conclusion\n\nI believe the field of Complaint Mechanisms is still in its nascent stage and lots of research and quality work needs to be done regarding the fabrication of proper materials and design of flexible linkages. On the brighter side some of the greatest and brightest minds of the country are working on this field to expand its applications. \n\nDr. G K Ananthasuresh from IISc Bangalore and Dr. Anupam Saxena from IIT Kanpur are among those few who have contributed quite a lot towards this field. I consider myself lucky to have got the opportunity to meet both of them personally here in NITK when they visited our college last semester for a 2 Day workshop on Kinematics and Mechanism conducted by Dr. Somasekhar Rao sir.\n\nBelow are the links to the references. Thank You\n* [Article 1](https://link.springer.com/article/10.1023/A:1011265810471)\n* [Article 2](https://link.springer.com/article/10.1007/s11465-009- 0066-1)\n* [Article 3](http://www.tandfonline.com/doi/abs/10.1080/08905459708945415)\n* [Introduction to compliant mechanisms](https://compliantmechanisms.byu.edu/content/intro-compliant-mechanisms)\n* [Journal](http://www.mecheng.iisc.ernet.in/~suresh/journal/J25YinSureshDistributed.pdf)\n* Compliant Mechanisms: Design of Flexure Hinges By Nicolae Lobontiu\n* Compliant Mechanisms By Larry L. Howell",
        "author_name": "Shuvam Senapati",
        "slug": "compliant-mechanisms"
    },
    {
        "layout": "post",
        "title": "All about FinFETS",
        "date": "2017-07-08T00:00:00",
        "author_github": "megha68",
        "image": "/assets/img/",
        "description": "Exploring the world of 3D transistors which extends Moores law.",
        "tags": [
            "FinFETS"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "megha68",
        "comments": true,
        "text": "[{% load static %}]()\n# Introduction\n\nHello folks, today I\u2019m going to discuss in detail about FinFET technology. The demand for smartphones and tablets with better performance and longer battery life has been driving the industry to come up with chips that are faster, smaller and use less power. To remain on this treadmill known as Moore\u2019s Law, chipmakers have in recent years relied on a series of technological breakthroughs, most recently 3-D transistors known as FinFETs.\n\n\nThe FinFET technology promises to provide the deliver superior levels of scalability needed to ensure that the current progress with increased levels of integration within integrated circuits can be maintained.FinFET technology has recently seen a major increase in adoption for use within integrated circuits.This type of FET gained its name from Profs. Chenming Hu, Tsu-Jae King-Liu and Jeffrey Bokor at the University of California, Berkeley who were the first to coin the term as a result of the shape of the structure.\n\n\nIntel and Samsung are betting billions of dollars on FinFET technology because it is power efficient and delivers superior processing power. Intel has been using FinFET technology since 2011. It had produced 22nm Ivy Bridge processor for the first time based on this technology means FinFET isn\u2019t the brand new technology for Chipset makers, but it did take lot of time to hit the mobile SoC industry. Mobile chipmakers are now trying to tape out the FinFET based SoCs for their high-end mobiles.\n\n\n# What is FinFET technology?\nThe FinFET technology is a technological breakthrough in recent years. It redefined the entire chipset industry with newly implemented 3D transistors. What is 3D transistor? The traditional planar transistors contain Source and Drain parts, which separated by High K-Dielectric surface. The entire power flow goes through Dielectric surface gate only. The 3D transistor drain part raised like Fin structure in FinFET transistors, so these transistors are called FinFETs. Usually, this kind of protruding parts will allow power to pass through all three parts at the same time. Also, it will allow multiple transistors to share a common gate and common contacts so the ultimate power supply consumption will significantly be reduced. In simple words, FinFET technology will allow chipset makers to build ultra-powerful processors, which only take a little power when compared to traditional planar transistors.\n![alt text](https://www.semiwiki.com/forum/attachments/content/attachments/5665d1355855218-planar-vs-3d-finfet-jpg)\n\n\n\n\n\n# Why is it so revolutionary?\nThe traditional transistor takes significantly higher amount of power so the entire chipset will consume a greater amount of power because a chipset contains billions of transistors. Due to this high power consumption, the sustainability of the processor will decrease along with heat increase, which chipset makers don\u2019t like. Talking about FinFET transistors, as we said, these will use standard drain and contacts so it takes less power and expose less heat so the processor performance won\u2019t fall like the traditional chip. In the meantime, the processor can deliver superior returns. This is the main reason; all the chip makers are interested in FinFET technology. There is another important reason that is, FinFET transistors will take a lot less power at the stand-by time. For example, consider the Intel desktop processor, which fabricated at 22nm, assume it usually takes the 1.0V at X amount of clock speed. Compare the same configuration with FinFET technology-based processor, this new technology will only consume 0.7v means we see around 37% decrease in power at same speeds. These two key reasons are ultimately pushing all chipset makers toward this new technology.\n\n\n# Conclusion\n\n\nTo summarize, you may have got some knowledge about what is FinFET and how it works? And Why it is so important ?. These FinFET transistors are going to drive billions of devices in coming years. We will see more processor in future using this technology, and more chip makers are trying to deploy their chips with FinFETs including Samsung, Qualcomm, Intel.\n\n\n\n\nYou can visit following pages for more information:\n* [All about FinFETS](https://www.engineersgarage.com/articles/all-about-finfet)\n* [FinFET Technology Review Paper](https://www.design-reuse.com/articles/41330/cmos-soi-finfet-technology-review-paper.html)",
        "author_name": "Megha Mantur",
        "slug": "finfets"
    },
    {
        "layout": "post",
        "title": "Continuously Variable Transmission",
        "date": "2017-07-15T00:00:00",
        "image": "/assets/img/",
        "description": "Why change the gear when there is CVT which does it for you without any jerks?",
        "tags": [
            "CVT",
            "Transmission",
            "Constant Engine RPM",
            "Infinite gear ratio"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "prajwal",
        "comments": true,
        "text": "[{% load static %}]()\n# Introduction\nA Continuously variable transmission is a type of power transmission which be called as an efficient replacement for a conventional automobile gearbox with a gearshift. A CVT is a system of belt and pulleys which is used to achieve infinite gear ratios between the 2 transmission shafts. The CVT consists of 2 pulleys \u2013 the primary and the secondary. Both the pulleys have one movable sheave and one stationary sheave. A belt (V shaped) goes over these pulleys. The angle of the V belt is equal to the angle between the facing sides of movable and stationary sheave of the pulley. The movable sheave makes the belt to slide up or down in the pulley as it moves towards and away from the stationary sheave. This is how infinite gear ratios are achieved in a CVT. But the main reason why a CVT is used is because, after a certain speed is achieved , the rpm of the engine remains constant irrespective of the increase in the speed of the vehicle as the throttle is held open.\n\n![alt text]({% static '/blog/assets/img/cvt/1.jpg' %})\n\n#### Explanation of the working through graph analysis\nThe below image shows the graph of working of a manual transmission system . After the max rpm is reached in 1st gear, there is a grilling sound which is an indication to change the gear. When the gear is shifted to 2nd , the rpm comes down due to the inertial effect of the increased load from the higher gear. This is due to the substantial difference in gear ratio between 1st and 2nd gear. Further, the transmission runs in the 2nd gear and while shifting to 3rd gear, the engine rpm comes down again.\n\n![alt text]({% static '/blog/assets/img/cvt/2.jpg' %})\n\nBut consider a transmission which has infinite gear ratios in which there is an infinitesimal difference between  2 gear ratios. So, while shifting the gear, the dip in engine rpm is very small which is why it can be assumed that the rpm remains constant. \n![alt text]({% static '/blog/assets/img/cvt/3.jpg' %})\nThe job of a CVT tuner is to set the constant working engine rpm to be equal to the rpm at which maximum torque or maximum power is attained, according to our requirement. For this, we have a set of springs and flyweights, the right combination of which will lead us to the desired working rpm.\n\n#### Use of Springs and Flyweights\nAt zero rpm, the belt rests lowest in the primary and highest in secondary pulley. The primary pulley has a set of pressure springs and flyweights. The pressure spring force keeps the movable sheave away from the stationary sheave. The flyweights are pivoted at one end in the movable sheave of the primary pulley. When the rpm increases, the centrifugal force(mw2r)on the flyweight increases which make them open out radially. While this happens, it presses against the pressure spring. When it has sufficient amount of centrifugal force, it will suppress the pressure spring and thus, the movable sheave moves towards the stationary sheave raising the belt higher in the primary pulley. This is known as primary pulley engagement. The secondary pulley too, has a torque spring whose inertial force has to be overcome in order to engage the secondary pulley. This engagement happens when the tension in the belt is sufficient to suppress the torque spring in the secondary.\nWhen both sheaves are engaged , the belt rises in the primary and comes down in the secondary(as belt length remains constant).So, a new higher gear ratio is obtained at every instant which increases the load at every instant and thus, keeps the engine rpm constant while the output vehicle speed increases.\n\n#### Importance of modeling a CVT\nMathematical modeling and design optimization of the working of the CVT is very important in order to achieve a good efficiency of power transmission. It also reduces time as it prevents the iterative tests to be conducted to achieve best performance. Here, the objective function is chosen as the 0ptimization of power/torque which is governed by variables such as spring constants (k), flyweights(w) , rpm and constraints like friction. The governing equations of CVT can also be input in a simulation software in order to check the results ,and from the results obtained ,new inputs which are closer to the required values can be simulated and so on a loop can be created which finally converges to the desired value.",
        "author_name": "Prajwal Bharadwaj",
        "slug": "cvt"
    },
    {
        "layout": "post",
        "title": "Multitenancy",
        "date": "2017-07-15T00:00:00",
        "author_github": "gitmenonsandu",
        "image": "/assets/img/",
        "description": "There is no doubt that our world is data driven. It is the most important asset of any business. But why would anyone trust a business with their data? The answer is Multitenancy.",
        "tags": [
            "Multitenancy",
            "Cloud Computing",
            "SaaS"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "gitmenonsandu",
        "comments": true,
        "text": "[{% load static %}]()\n# What is Multitenancy?\nAccording to Wikipedia, the term \"software multitenancy\" refers to a\u00a0software architecture\u00a0in which a single\u00a0instance\u00a0of\u00a0software\u00a0runs on a server and serves multiple tenants. With a multitenant architecture, a\u00a0software application\u00a0is designed to provide every tenant a dedicated share of the resources.\n\n![Single tenancy vs Multitenancy]({% static '/blog/assets/img/multitenancy/difference.png' %})\n\nThe initial aim of a multitenant architecture was to optimise resource allocation in an application. Instead of providing multiple instances of the same software, a single instance of the software was used to serve multiple tenants. But times have changed. Today, 15 years or more after the creation of application and database multi-tenancy, things are different; the economic necessity for multi-tenancy at the application tier has disappeared. We have huge clouds offering cheap infrastructure and tools creating new instances and tearing them down again without any of the manual interventions that used to be necessary.\n\n### Then why do we still need multitenant architecture?\n\nTrust, or the lack thereof, is the number one factor blocking the adoption of [Software as a Service (SaaS)](https://en.wikipedia.org/wiki/Software_as_a_service).  SaaS applications provide customers with centralized, network-based access to data with less overhead than is possible when using a locally-installed application. But in order to take advantage of the benefits of SaaS, an organization must surrender a level of control over its own data, trusting the SaaS vendor to keep it safe and away from prying eyes. \nTo earn this trust, one of the highest priorities for a prospective SaaS architect is creating a SaaS data architecture that is both robust and secure enough to satisfy tenants or clients who are concerned about surrendering control of vital business data to a third party, while also being efficient and cost-effective to administer and maintain.\n\n![dB Schema]({% static '/blog/assets/img/multitenancy/db_schema.png' %})\n\nIn Multi-tenant applications, one instance serves more than one organization, but at the same time provides virtual isolation to data and applications from other tenants of the application. Multitenancy is applied to data, operating system and even hardware. But in this article we talk only about data.\n\n## Machine learning and Multitenancy\n\nWe live in the world of artificial intelligence and machine learning. Every application and website has some form of intelligence incorporated into it. But the learning curve of the machine learning models depend on the data provided by the user. Do you see how multitenancy gets applied here? Let me explain with a scenario. You have deployed a website which provides description of an image that you upload. Like you see in many cognitive services, if you put a picture of a guy trekking, it will say \" A picture of a guy trekking\". The training model is trained with some data which you had collected. But since you have deployed and people are using the, you decide you can improve the model by retraining the model with the new pictures and also correct the model based on the feedback of the results that you provide. Sounds good right, you have a cool website which learns on its own through time. But there is one problem. What if one day, a user mails you asking whether the pictures he upload are private and are not shared. You say that they are private and cannot be accessed by anyone. He decides to check it out by uploading an image from his own account. He gave a feedback regarding the prediction. Then he creates a new account and tries with the same picture. He gets the updated result, ie. the result considering his feedback. This makes him feel insecure about using your website. This is the trust issue that we talked about earlier. \n\nIn the case of SaaS applications, the application vendor does not own the customer data. So it cannot use one customers data to train a model which is used by other customers.\nTo tackle this issue, we need an architecture which does not use one user's data to retrain a model which is used by other users. This virtual isolation is provided by a multitenant architecture. The simple way to tackle this problem is by creating a new model for each tenant. Whenever a new user signs up, create a new instance for that user with the basic training done in the mode. All further predictions and retraining for that user will be done to the model exclusive to him. This ensures data isolation while model retraining. The disadvantage of this is memory. A separate model for every user consumes a lot of memory. The other approach is to intelligently share the models among some users. Instead of creating a new model for each user, only create an instant of the model. All retraining can be done to this instant. The instant can be destroyed and put away to secondary memory when the user is inactive. Though the second method has a lot of overhead, because of the memory it saves, this is method is preferred. You might be confuse thinking how creating new instances of models support multitenancy. Isn't this the opposite of multitenancy. No, this is because we are still using only one instance of the application. Only the machine learning model that it accesses varies.\n\n### Choosing an Approach\n\n![Approach]({% static '/blog/assets/img/multitenancy/approach.png' %})\n\nFactors influencing \n1. Economic considerations\n2. Security considerations\n3. Tenant considerations\n4. Law and order considerations\n5. Complexity considerations\n\n\n#### Resources\n1. [Design Patterns for multitenant SaaS application](https://docs.microsoft.com/en-us/azure/sql-database/sql-database-design-patterns-multi-tenancy-saas-applications)\n2. [Implementing multitenancy using namespace](https://cloud.google.com/appengine/docs/standard/python/multitenancy/multitenancy)",
        "author_name": "Sandeep Menon",
        "slug": "multitenancy"
    },
    {
        "layout": "post",
        "title": "Cuba and Candela - Thin shell structures and their strength through curvature",
        "date": "2017-07-22T00:00:00",
        "author_github": "prafulitti",
        "image": "/assets/img/",
        "description": "Strength through curvature is an idea that shapes our world, and it has its roots in geometry.A look into how this idea was used in Cuba(Havana) and its magnificent thin shell structures and also an introduction to Felix Candela , one of the greatest structural Engineer",
        "tags": [
            "Structural engineering",
            "Felix Candela"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "prafulitti",
        "comments": true,
        "text": "[{% load static %}]()\nBefore plunging into understanding the beauty of the thin shell structures or even  knowing whatgwho the candela in the title refers to. Let us imagine a scenario where you're at an eatery. You pick up a slice of pizza, and  you're about to take a bite. But it flops over and sags about your fingers. Maybe you should have gone for fewer toppings? No really. Enough experience with pizzas has taught us that folding the pizza slice into a U-shape keeps the pizza from flopping over .\n\n![img1]({% static '/blog/assets/img/cuba-and-candela/image001.jpg' %})\n\nBehind this simple intuition there lies a powerful mathematical result about curved surfaces. One that'92s so startling that its discoverer, the mathematical genius Carl Friedrich Gauss, named it Theorema Egregium, Latin for excellent or remarkable theorem.\nGauss's wanted to define curvature in such a way that bending does not affect the curvature. On any surface, let us say  spherical , one can trace many paths to come down from the top. Gauss intended to use all these choices into account. Here's how it works. Starting at any point, find the two most extreme paths that you can choose (i.e. the most concave path and the most convex path). Then multiply the curvature of those paths together (curvature is positive for concave paths, zero for flat paths, and negative for convex paths). The number you get is Gauss's definition of the curvature at that point.\n\nLet us take the case of a Cylinder, the two extreme paths from any point are namely, a straight path and a completely circular path. Since the straight path has zero curvature, thus the Gaussian curvature of a Cylinder is zero. That's the reason why engineers say cylinder is flat. Similarly curvature calculations with sphere, we get the Gaussian curvature to be a non zero value. \n\nSo spheres are curved while cylinders are flat. You can bend a sheet of paper into a tube, but you can never bend it into a ball. A surprising consequence of this result is that you can take a surface and bend it any way you like, so long as you don't stretch, shrink or tear it, and the Gaussian curvature stays the same. That's because bending does not change the distances between any two points. Similarly, have you ever tried gift wrapping any spherical object , let's say a football(ggmu). No matter how you bend a sheet of paper, it'll always retain a trace of its original flatness, so you end up with a crinkled mess.\n\nNow back to pizza, initially the pizza was flat, Gaussian curvature was zero. So when the pizza flopped while eating the breadth of the pizza was straight maintaining the Gaussian curvature zero. Now instead by forcing it to bend we are forcing it to become flat in the other direction..Once you recognize this idea, you start seeing it everywhere. By curving a sheet in one direction, you force it to become stiff in the other direction.\n\nNow let us consider the scenario of an egg. Take an egg from your fridge and try to squeeze it by putting your fingers around it. You'll surprised to see that you can't. This is due to the fact that egg shells are curved on both sides, thus giving higher rigidity.\n\nShape for a nuclear power plant cooling tower also incorporates curvature in both directions. This shape, called a hyperboloid, minimizes the amount of material required to build it\n\n![img2]({% static '/blog/assets/img/cuba-and-candela/image002.jpg' %})\n\nHyperboloid Parabaloid  is another shape that uses double curvature. You can look into the infamous punch of a Mantis Shrimp to know more regarding the exceptional strength of this shape.\n\n![img3]({% static '/blog/assets/img/cuba-and-candela/image003.jpg' %})\n\nSo what makes this Hyperboloid Parabaloid shape so strong? It has to do with how it balances pushes and pulls. All structures have to support weight, and ultimately transfer this weight down to the ground. They can do this in two different ways. There's compression, where the weight squeezes an object by pushing inwards. An arch is an example of a structure that exists in pure compression. And then there's tension, where the weight pulls at the ends of an object, stretching it apart. The hyperbolic paraboloid combines the best of both. The concave part is stretched in tension while the convex part is squeezed in compression Through double curvature, this shape strikes a delicate balance between these push and pull forces, allowing it to remain thin yet surprisingly strong.\n\nThe strength of this  shape was well understood by the Spanish-Mexican Architect and Engineer Felix Candela. He built structures that took the hyperbolic paraboloid shape. When you hear the word concrete, you might think of heavy, boxy constructions. Yet Candela was able to use the hyperbolic paraboloid shape to build huge structures that expressed the incredible thinness that concrete can provide. His lightweight, graceful structures might seem delicate, but in fact they're immensely strong, and built to last. Felix Candela is one of the greatest structural engineers of the twentieth century. Candelas work demonstrates three ideals which are essential to the education of structural engineers and to anyone with an appreciation for the built environment: Firstly to conserve natural resources by minimizing materials. Secondly , to reduce cost by intelligent design . Lastly creating beautiful structures. His structures were a testimony to all the three ideals.\n\nIn the late 1950's his influence spread across the world and one such country which embraced this type of thin shell structures was Cuba. Cubans have an international reputation for their spirited high-quality art, which is manifested in mediums such as paintings, sculptures, cinema, music, as well as the design of structures.\nHere are a few of the thin shell structures in and around Havana which came up during the 1950's in consequence to the famous partnership between Max Borges and Felix Candela.\n\n![The colorful outdoor shells frame the entrance to the famous Tropicana Nightclub in Havana, Cuba]({% static '/blog/assets/img/cuba-and-candela/image004.jpg' %})\n\n![The Nunez Galvez tomb]({% static '/blog/assets/img/cuba-and-candela/image005.jpg' %})\n\n![Parque Jos'e9 Mart'ed, a sports complex]({% static '/blog/assets/img/cuba-and-candela/image006.jpg' %})\n\nThere was a lot of optimism and hope that permeated  Cuba during the forties and fifties. As can be witnessed in these remarkable structures.\n\nBut then Castro happened. And left Cuba Crawling. Numb.",
        "author_name": "Praful Itti",
        "slug": "cuba-and-candela-thin-shell-structures-and-their-strength-through-curvature"
    },
    {
        "layout": "post",
        "title": "Number Theory",
        "author_github": "adithyabhatkajake",
        "date": "2017-07-23T10:48:33",
        "image": "/assets/img/",
        "description": "Number Theory",
        "tags": [
            "Number Theory",
            "Group",
            "Ring",
            "Rng",
            "Field",
            "Finite Field",
            "Galois Field"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "adithyabhatkajake",
        "comments": true,
        "use_math": true,
        "text": "[{% load static %}]()\n# The Mathematics of 1,2,3, ...\n\n## Number Theory\n\nNumber theory is a field of mathematics that deals with the study of numbers and their properties. It is just like how chemists study different elements, their reactivity, their compounds, etc, in number theory mathematicians boil numbers, titrate them, react with numbers and form number complexes (complex numbers).\n\n### Group\n\nA group is a set of elements $G = \\{...\\}$ and an operation $#$. For G to be called a Group with respect to $#$, it must satisfy the following four group axioms or conditions:\n- Closure: For any $x$,$y$ $\\in G$, $x$ $#$ $y$ $\\in G$\n- Associativity: If $x$, $y$, $z$ $\\in G$, then ($x$ $#$ $y$) $#$ $z = x$ $#$ ($y$ $#$ $z$)\n- Identity: $\\exists e \\in G$ such that for any $x \\in G$,  $x$ $#$ $e = e$ $#$ $x$ = $x$ \n- Invertibility: For every $x \\in G$, there must exist an element $x^{-1}$ or $y \\in G$ such that, $x$ $#$ $y = y$ $#$ $x = e$, where $e$ is the identity element.\n\nIntegers form a group with the operation of addition. Let us check to see if they follow the rules of the Group Axioms:\n- Closure: For any two integers a and b, a+b is also an integer.\n- Associativity: Obviously\n- Identity: 0 is the identity integer\n- Invertibility: For every integer a, -a is also an integer and a + (-a) is 0.\n\n** Note that, group operations need not be commutative. **\n\nThe order of a group $G$, is the number of elements in the set $G$.\n\nThere are many interesting Groups in nature. Some of them are called symmetric groups and abelian groups. Let us discuss them one by one.\n\n\n#### Symmetric Groups\n\nIn general, symmetric groups are defined on a set $X$ and the group elements consist of bijective functions defined from X to X and the group operation is composition of functions. \n\nLet us take two elements in the group. Each element is a bijective function from X to X. Let $f$ and $g$ be two such functions.\n\nNow, $f:X\\rightarrow X$ and $g:X \\rightarrow X$.\n\nConsider the composition of f and g, $g o f:X \\rightarrow X$ also belongs to the group by definition.\nIdentity elements always exist in these groups and therefore they form a symmetric group.\n\nHere, taking the first N natural numbers, the bijection becomes a mere permutation of re-arrangement.\n\nOn a set $X_n$, for example consisting of numbers 1 to n.\n\\begin{equation}\nS_n = { 1, 2, 3, ... , n-1, n }\n\\end{equation}\n\nthe symmetric group on the set $S_n$ consists of all the permutations of the numbers from 1 to n taken all at once. \n\nFor example, let\n\\begin{equation}\nS_n = {1,2,3}\n\\end{equation}\nThen it's symmetric group is:\n\\begin{equation}\nG = {(1,2,3),(1,3,2),(2,3,1),(2,1,3),(3,1,2),(3,2,1)}\n\\end{equation}\n\nFor $n$ elements there are $n!$ permutations. Hence, the order of the symmetric group of $S_n$ is $n!$.\nBy this definition, a Rubik's cube, molecules like ammonia, permuations of numbers and millions of other objects in the real world that contain states can be mathematically modelled into a symmetric group.\n\n#### Abelian Groups\n\nAn abelian group is a special form of a group, which adds the condition of commutativity to the group. A group $G$ is said to be an abelian group with respect to an operation $#$, if $G$ is a group and:\n\nFor any $x,y\\in G$, $x$ $#$ $y = y$ $#$ $x$\n\nThe concept of Abelian Groups are important when it comes to rings, fields and finite fields. \n\n### Ring\n\nA ring is an abelian group $R$ but it has two operations, usually denoted as $+$ and $*$. But in order to make things simpler, let us consider them as $#1$ and $#2$.\n\nConditions for a Group $R$ to be a ring:\n1. $R$ is an abelian group (i.e closure, associativity, commutative, identity and invertibility in $#1$)\n2. $R$ is monoid in $#2$, i.e\n    + For any $x,y,z\\in R$, ($x$ $#2$ $y$) $#2$ $z = x$ $#2$ ($y$ $#2$ $z$) [Associative]\n    + There exists an element $e \\in R$, such that for any element $x \\in R$, $x$ $#2$ $e = e$ $#2$ $x = x$ [Identity]\n\n3. The two operations must be distributive over the first operation $#1$. For all $x,y,z \\in R$:\n    + $x$ $#2$ ($y$ $#1$ $z$) = ($x$ $#2$ $y$) $#1$ ($x$ $#2$ $z$) [Left distributivity]\n    + ($y$ $#1$ $z$) $#2$ $x$ = ($y$ $#2$ $x$) $#1$ ($z$ $#2$ $x$) [Right distributivity]\n    \n** Note: Other sources use, $+$ and $*$ to denote the operations $#1$ and $#2$ respectively.**\n\n\nThere is an interesting concept here called the *rng* (without the $i$), also known as the pseudo ring.\nA rng has all the properties of a ring except that it need not have a multiplicative identity.\nConsider, the set of even integers,\n\\begin{equation}\nS = {...,-8,-6,-4,-2,0,2,4,6,8,...}\n\\end{equation}\nand the operations addition $+$ and multiplication $*$. They form a $rng$ but not a $ring$ because there is no multiplicative identity $1$ in $S$.\n\nThe set of Integers $Z$ with $+$ and $*$, however can claim the title of the **LORD of the RINGS**. Because they form a ring. Get it?Nope? Moving on!\n\n### Field\n\nA set $F$(see how I am conveniently changing the letters of the set as the definition changes, get used to it when reading math books) is said to be a Field with respect to the operations $+$ and $*$ if the following field axioms are satisfied:\n- ($F,+$) must be an abelian group\n- ($F-\\\\{0\\\\},\\*$) must be an abelian group [0 is the additive identity]\n\n#### Characteristic of a Field:\nA positive integer n is said to be a characteristic of a field $F$, if\n\\begin{equation}\nn . 1 = 0\n\\end{equation}\ni.e.\n\\begin{equation}\n1 + 1 + ... + 1 = 0 \n\\end{equation}\nHere, 0 is the identity of the $+$ operation. If there is no positive integer n that satisfies this condition, then the field is said to have a zero characteristic.\n\nWe need not take only 1 here. In fact, for any element $x \\in F$, $n.x = 0$ is true. We also want the least positive positive integer n that satisfies this condition for all the elements in the set $F.\n\n#### Subfield\n\nA field $(G,+,\\*)$ is said to be a subfield of $(F,+,\\*)$, if $0$ in $F = 0$ in $G$, both the operations are the same and $G\\subset F$.\n\n### Prime Field\n\nGiven a prime number $p$, we can build a prime number field.\nLet the $+$ operation be a $+modulo$ p operation:if the sum exceeds p, we mod it with p.\nLet the $\\*$ operation be a $\\*modulo$ p operation:if the product exceeds p, we mod it with p.\nAdd these two operations with a set consisting of integers from $0$ to $p-1$, we get a prime field.\n\nThis field is commonly referred to as $\\mathbb{Z}_p$.\n\nFor example, \n\\begin{equation}\n\\mathbb{Z}_7 = \\{0,1,2,3,4,5,6\\}\n\\end{equation}\n\n### Finite Field\n\nBy the blessings from the gods of obviousness, a finite field is a field containing a finite number of elements in its set. One interesting finite field is the finite field of prime order, also called the Galois Field ($GF(p)$ or $\\mathbb{Z}/p\\mathbb{Z}$ or $\\mathbb{F}_{p}$). \n\nThe elements of a Galois Field are actually generally defined as,\n\n\\begin{align}\n    gf(p^n) = && (0,1,2,..., p-1) \\cup \\\\\\\n               && (p,p+1,p+2,...,p+p-1) \\cup \\\\\\\n               && (p^2, p^2+1,p^2+2,...,p^2+p-1) \\cup ... \\cup \\\\\\\n               && (p^{n-1},p^{n-1}+1, p^{n-1}+2,...,p^{n-1}+p-1)\n\\end{align}\nwhere $p \\in \\mathbb{P}$ and $n \\in \\mathbb{Z}^{+}$\n\nLet us try to compute $gf(3^2)$:\n\n\\begin{equation}\ngf(3^2) = \\{0,1,2,3,4,5\\}\n\\end{equation}\n\nThe operations here are $+modulo3$ and $*modulo3$.\n\nOne can easily note that, $gf(p^1)$ is nothing but the prime field $\\mathbb{Z}_p$. However, $gf(p^n)$ is considered to be an extension to the prime field $\\mathbb{Z}_p$. The addition of the new elements to the set do not violate any of the field properties.\n\nAll of this number physics is beautiful actually. Mathematics is the purest form of art and knowledge to have ever been invented by mankind. It is certainly very true that numbers never lie. In the upcoming blogs, let us explore the application of number theory in cryptography.\n\nFeel free to contact me at dth.bht@gmail.com for any queries, complaints or suggestions.\n\nSigning off, Yours truly, the Hermitsage",
        "author_name": "Adithya Bhat",
        "slug": "number-theory"
    },
    {
        "layout": "post",
        "title": "Log file parsing and formats of report generation",
        "date": "2017-07-23T00:00:00",
        "author_github": "karthik",
        "image": "/assets/img/",
        "description": "How to parse Log FIles generated by Jenkins and present the data in a readable format",
        "tags": [
            "File Parsing",
            "Logging"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "karthik",
        "comments": true,
        "text": "[{% load static %}]()\nHello everyone, today I will be talking about the core part of my work during my summer internship at Samsung Semiconductor India R&D Center, Bengaluru. Our project team consisted of four members and we were given the task to parse log files generated by a testing framework called as Jenkins (now a very popular tool for continuous integration and deployment of source code) and present the parsed data in a HTML, XLSX and JSON format. We were also required to develop plotting and charting utilities for the parsed data in the Excel files.\n\n### Log file parsing\n\nLog files are simple text file containing huge amount of information. These files are however unstructured and the data they contain is not in a presentable format and it cannot be grasped quickly. There is no organisation of data and no scope for visualization of the data. Therefore, parsing is a very important step if you want to work upon, present, analyze the data in your log files.\nWe initially thought of using some existing popular tools like the MS log parser, the ELK (Elastic Search, Logstash, Kibana) stack, etc. But these tools require the log files to retain a fixed format throughout the file and proved to be unsuitable for our use. We then moved on to developing Python scripts (parsers) for the log files. Python gets its immense power through its humungous number of modules and a simple import of these modules can get any of your work done!\n\n### Parsing\n\nThe python re module (regular expression) contains most of the functions that a parsing utility of any language would provide you with. This coupled with the string handling functions in Python will get your parsing work done. By parsing, I mean that the you will be able to extract the data from the log files and put them in Python data structures (list, dictionaries, strings and their combinations)\n\n### Making the JSON file\n\nMaking the JSON file is perhaps the most important step after parsing. The parsed data present in the Python data structures will be dumped to a JSON file. The JSON file format provides us with a very easy way to retrieve information just like data retrieval in arrays and dictionaries. The python JSON module can write data from a data structure to a JSON file and can load the data from a JSON file to a data structure\n\n### Writing to MS Excel\n\n There are many Python modules for achieving this task. They include xlwt, openpyxl, xlrd, xlsxwriter, etc. The most popular is the xlsxwriter, perhaps because of its excellent online documentation and its extensive and diverse functionalities. Also, this is the only module that supports plotting on an Excel sheet and hence we opted for this module. The python data structures containing the parsed data are written in a tabular format in an Excel worksheet.\n\n### Plotting on MS Excel\n\nThe xlsxwriter module contains utilities for graphing and charting. You can decide the type of chart (line, bar, column, pie), set its axes (units, scale, labels, linear/logarithmic), set chart legends, add data labels, formatting (chart color, plot color, trend lines, gridlines, border) and many more features.\n\n### HTML report generation\n\n The xlsx files are suitable when you want to store and export the parsed data. However, for presentation purposes, the HTML format is more suitable. There are command-line based tools like the sswriter and unoconv which help us with this. These are generic tools which can help in conversion of files from one format to another.\n\n To conclude, I wanted to share my learnings during my internship tenure at Samsung and provide you with the knowledge of how data should be parsed and presented and the python modules/tools that can be used to achieve the same.",
        "author_name": "Karthik",
        "slug": "log-file-parsing-and-report-generation"
    },
    {
        "layout": "post",
        "title": "3D Printing",
        "author_github": "nandini",
        "date": "2017-07-28T15:32:44",
        "image": "/assets/img/",
        "description": "Everyone has heard of 3D printing...But what goes beyond the scenes of a 3D printer is a lesser known fact.Let's explore the world of 3D printing!",
        "tags": [
            "3D Printing",
            "Additive Manufacturing",
            "SIS"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "nandini",
        "comments": true,
        "text": "[{% load static %}]()\n# 3-D Printing-A world beyond 2D\n\n\n### <center> The emerging world of 3D printing </center>\n\n3D Printing is something all of us have come across in today\u2019s world. It\nmay seem like a new technology but history points out that it is no\nrecent development.[ In fact, 3D printing has its roots in the 1980s. In\n1984,\u00a0Chuck Hull, invented a process called \u201cstereolithography\u201d\u00a0which\nused UV lasers to solidify photopolymer that created\u00a03D parts layer by\nlayer. A technology which initially began with the aim of prototyping\nhas today secured its place in the manufacturing industry. Today its use\nis far more widespread than we can possibly imagine. Did you know 3D\nprinting techniques\u00a0have been featured in movies such as IronMan, The\nHobbit, Jurassic Park, Avatar, The Muppets, Terminator Salvation, Real\nSteel,\u00a0Paranorman, The Avengers and Paranorman to design some props.\nThese are some lesser known facts about 3D printing. To know more about\nthis very popular technology read on. :)\n\n![3D Printer]({% static '/blog/assets/img/3D-Printing/3D_printer.jpg' %})\n\n## How does 3D printing work?\n\n3D printing is the process of making a three-dimensional object from a\n3D model or other electronic data source.\n\nEvery 3D print starts as a digital 3D design file \u2013 like a blueprint \u2013\nfor a physical object. Trying to print without a design file is like\ntrying to print a document on a sheet of paper without a text file. This\ndesign file is sliced into thin layers which are then sent to the 3D\nprinter.\n\n3D printable models can be created with a computer-aided design package\nor\u00a0a 3D scanner. The printing can take hours to complete depending on\nthe size, and the printed objects are often post-processed to reach the\ndesired finish.\n\nSuccessive layers of a given material (in liquid, powder, paper or sheet\nform) are\u00a0applied under computer control using\u00a0a 3D printer. Since\nlayers are added, it is often also referred to as additive\nmanufacturing. \u00a0\n\n\n[Materials](https://www.3dhubs.com/what-is-3d-printing#materials) used\nvary by printer type, ranging from plastics to rubber, sandstone, metals\nand alloys - with more and more materials appearing on the market every\nyear. Of late, 3D printing has also been used to make human body parts,\nusing human tissue !!\n\nThe applications of 3-D printing are widespread. From industrial\napplications in manufacturing, prototyping and customization to\nBiomedical devices like dental implants and prototypes of body ,3D\nprinting seems to be the new kid on the block ......\n\n## Recent Developments in 3D printing\n\n- There are now 3D printers to print very long objects or multiple objects in the same print job.  An extension of this has led to **3D printing on a moving platform**. As the object is completed, it moves down the belt and then pops off the belt when it is done. \n- 3D printers can be controlled remotely and the printers can be operated as **\u201clights out manufacturing\u201d**, meaning no one has to be present while the printers do their work\n- **SLS (selective laser sintering) printing** - this new technology is able to use a broader range of materials with differing levels of strength, flexibility and stiffness than conventional 3D printers.\u00a0\n- 3D printing is slowly beginning to be used for human body parts. The day is not far when someone when someone will have a 3D printed heart.\n\n## What does the future hold?\n\nAfter successfully establishing a 3D printing technology, there are now\nplans to add another dimension to it. Yes, a 4-D printing technology is\nin the making. 4D printing is 3D printing\nplus another dimension tacked on: the variable element of time. In other\nwords, 4D printing involves 3D-printing objects that can eventually do\none or both of two things: self-assemble and/or change properties based\nupon certain conditions.\n\nSo let\u2019s wait and watch what revolutionary innovations does this new\ntechnology bring along!",
        "author_name": "Nandini",
        "slug": "3d-printing"
    },
    {
        "layout": "post",
        "title": "Ever wondered why exactly are high lift devices needed? Let me tell you...",
        "author_github": "akula-sri-charan",
        "date": "2017-07-28T17:32:44",
        "image": "/assets/img/",
        "description": "This article explains the need for the usage of high lift devices",
        "tags": [
            "High Lift devices"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "akula-sri-charan",
        "comments": true,
        "use_math": true,
        "text": "[{% load static %}]()\n# Ever wondered why exactly are high lift devices needed? Let me tell you...\n\nLift generated by wings of an aircraft is estimated by the following relationship,\n\n\\begin{equation}\nL = \\frac{1}{2}\\rho v^2 A C_L\n\\end{equation}\n\nWhere:\n\n$L$ = lift force generated\n\n$\\rho$ = air density\n\n$v$ = true air speed\n\n$A$ = lifting area of the wing \n\n$C_L$ = coefficient of lift\n\nClearly, same amount of lift can be generated by various permutations and combinations of $\\rho$, $v$, $A$ and $C_L$. \n\nNow, let us think about situations when $v$ is quite less, take-off or landing for instance, that the lift generated cannot balance the total weight of the flight. \nAt such situations, we need to alter the other three factors in order to make the aircraft fly. \n$\\rho$ is something that is quite out of our control. \nHence, we need to do something which increases the other two factors. Here is where high lift devices come into the play.\n\n## Knowing CL\n\nOne should know how $C_L$ behaves, before one can commend the work done by the high lift devices. \nAirfoils are of various types. \nOne of the basic types of differentiating them is into symmetrical and cambered airfoils.\n\n![Types of Airfoils]({% static '/blog/assets/img/High-Lift-devices/img1.jpg' %})\n\nThe above figure represents the difference between symmetrical and cambered airfoils schematically. \nLet us look what effect cambering has on the lift of an airfoil.\n\n![Effects of cambering on Lift]({% static '/blog/assets/img/High-Lift-devices/img2.png' %})\n\nIt can be seen that having a camber in the airfoil increases the highest attainable lift coefficient. \nHowever, there is a decrease in the critical angle of attack, which is the highest angle of attack that can be deployed without letting the airfoil to stall.\n\n## What is this stall then?\n\nAir does have certain viscosity and it does develop a boundary layer. \nThus, the molecules in the boundary layer experience viscous forces against each other. \nIn addition to this, they might also have to face adverse pressure gradients along their way ahead along the boundary layer. \nHence, the molecules which become devoid of energy while they try to overcome both viscous forces and adverse pressure gradients, cease to continue in the boundary layer and get separated from the skin of the airfoil. \nThis is what is called as a stall.\n\n![Tired Man]({% static '/blog/assets/img/High-Lift-devices/phew.jpg' %})\n![Dhoni Boost]({% static '/blog/assets/img/High-Lift-devices/boost.jpg' %})\n\nWhen M. S. Dhoni, or Sachin Tendulkar if you think of him, faces a similar situation, he takes a cup of Boost in order to continue playing. \nOn similar lines, if one can, somehow, provide energy to the flow, the stall will be delayed thus making it possible to increase the critical angle of attack.\n\nSo, in order to increase the lift supporting area of the wing and to increase the lift coefficient either by increasing camber or by delaying stall or both, some special devices are used known as High Lift Devices.\n\n## High Lift Devices\n\nThere are various ways in which the aforesaid objectives can be met. \nFollowing are some of those.\n\n### Flaps\n\nThese are the high lift devices (HLDs) that are generally deployed at the trailing edge of the wings. \nThese devices, change the effective camber of the wing cross sections. \nFlaps are again of different types. \nThe following figure illustrates some.\n\n![Flaps]({% static '/blog/assets/img/High-Lift-devices/flaps.gif' %})\n\n### Slats and Slots\n\nThese are the HLDs usually placed at the leading edge of the wings. \nSlats are small, retractable airfoils attached to the wing leading edges, generally just before the ailerons so that the latter remain operable even when rest of the wing enters stall. \nWhen slats are made open, they create slots (gaps) between them and the main airfoil and high pressure air on the lower surface of the airfoil is directed to the upper surface, making the flow over the upper surface smoother and delaying the stall to higher angles of attack. \nThe effect of flaps and slats is shown below.\n\n![Chords]({% static '/blog/assets/img/High-Lift-devices/chords.gif' %})\n\n![Slats]({% static '/blog/assets/img/High-Lift-devices/slats1.png' %})\n![Slats]({% static '/blog/assets/img/High-Lift-devices/slats2.png' %})\n  \n### Boundary layer control and Blown Flaps\n\nThese come under the powered high lift systems. \nGenerally, these systems involve blowing of air from some powered source, like compressor of an engine or exhaust of the engine, using ducts and other mechanisms to achieve the desired lift characteristics. \nHere is a small description of such devices.\n\n![Engine Air]({% static '/blog/assets/img/High-Lift-devices/img3.jpg' %})\n\n### Leading Edge Root Extensions\n\nA LERX typically consist of a small triangular fillet attached to the wing leading edge root and to the fuselage. \nIn normal flight the LERX generates little lift. \nAt higher angles of attack, however, it generates a vortex\u00a0that is positioned to lie on the upper surface of the main wing. \nThe swirling action of the vortex increases the speed of airflow over the wing, so reducing the pressure and providing greater lift. \nLERX systems are notable for the potentially large angles in which they are effective.\n\n![LERX Systems]({% static '/blog/assets/img/High-Lift-devices/lerx.png' %})\n![Graphical Lerx]({% static '/blog/assets/img/High-Lift-devices/lerx.jpg' %})",
        "author_name": "Sri Charan Akula",
        "slug": "high-lift-devices"
    },
    {
        "layout": "post",
        "title": "An Introduction to Reinforcement Learning",
        "author_github": "aneesh297",
        "date": "2017-07-28T13:32:44",
        "image": "/assets/img/",
        "description": "A decision making AI which learns from its mistakes.",
        "tags": [
            "Machine Learning",
            "AI",
            "Bots"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "aneesh297",
        "comments": true,
        "text": "[{% load static %}]()\n# An Introduction to Reinforcement Learning\n\nImagine you are trying to train a dog to do fetch a ball. What you\u2019d normally do is that you would reward it a snack every time it fetched the ball. The dog would then associate the snack with the act of fetching and would do it every time in the hope of a treat. Ladies and gentlemen, this is Reinforcement Learning. Cue the curtains. We\u2019re done here. ;)\n\n### Wait what? You didn\u2019t tell me what Reinforcement Learning actually is.\nAll jokes aside, Reinforcement Learning is the area of machine learning which resembles how we humans learn something. __Attempt, Make mistakes, Learn, Master__. The agents or the models learn for themselves to achieve successful strategies that lead to the greatest long-term rewards. This is a paradigm of learning from trial-and-error, solely from rewards or punishments. \n\nSurely, you must have heard of DeepMind\u2019s [AlphaGo](https://deepmind.com/research/alphago/) program? Yes, the one who beat the world No.1 ranked player in Go. This was built using RL. \n![AlphaGo]({% static '/blog/assets/img/Reinforcement-Learning/ago.jpg' %})\n\n### So what does the model consist of?\nHere we normally refer to the model as the agent. The agent is presented with an environment. This environment consists of states. In each state, an agent can take several actions. For each state-action pair, there is an associated reward which is collected when the agent takes this action in this state. For example, an agent playing a racing game would be rewarded every time it took the correct turn. This environment is represented by something known as the Markov Decision Process (MDP).  \n![Markov Decision Process]({% static '/blog/assets/img/Reinforcement-Learning/mdp.png' %})\n\nThe job of the agent is to maximize the total reward as it interacts with the environment and moves through the states. \n\n### What are the applications of this?\nReinforcement Learning is on the rise right now. I have already mentioned AlphaGo. A lot of current research focuses on using bots trained using RL to play video games. The same company DeepMind designed a Deep Q Network to play Atari games (paper link [here](https://storage.googleapis.com/deepmind-media/dqn/DQNNaturePaper.pdf)).\nAlso, [OpenAI ](https://openai.com/) an AI research company founded by Elon Musk currently has a platform named [Gym](https://gym.openai.com/) which is a toolkit for creating RL algorithms. Take a look if you\u2019re interested. \n\nIn the industry, RL can be used to solve many problems requiring optimization. For example, when a modified e-commerce website has to be tested against its previous version to see which UI sells products better, A/B testing is used. Here, half of the users see the original website and the other half see the modified one. If the modified version of the site turns out to be doing better, this version is chosen. However, the website makes a small loss due to the fact that half of the users saw a suboptimal version of the site, hence losing a few purchases. \nThis issue can be solved using Bandit Testing. This is based on the Multi Armed Bandit [problem](https://en.wikipedia.org/wiki/Multi-armed_bandit). Here, the division of users is only initially 50/50. A reinforcement learning model initially *explores* the two possibilities and after it figures out the better performing version, it *exploits* it. It starts reducing the percentage of the users who see the poorer performing version of the site so as to increase its reward, i.e. more purchases. By the end, the model will eliminate the poorer version completely.  \n\n### How do I make a reinforcement learning agent?\n\nFirst, you will need to define your environment. You should be able to define your problem as a MDP. You\u2019ll need to define what the states are, what actions you can take in each state. Also, you\u2019ll need to define a numerical reward for each state-action pair. Then if you\u2019re using [Q-Learning](https://en.wikipedia.org/wiki/Q-learning), you need to construct a Q-table consisting of Q values. These Q values corresponding to each state-action pair represents the discounted sum of rewards from the initial state to the current state. \nUsing a table can cause data storage problems if there are many state and action pairs. We can instead use a neural network to estimate the Q value and determine the most optimal play. DeepMind used a Deep Q-Network to play Atari games. If you wanted to get started with Reinforcement Learning, [Gym](https://gym.openai.com/) by OpenAI is a great place to start. Here are a few more resources to help understand the theory and get started with Reinforcement Learning:\n\n* [Gym by OpenAI](https://blog.openai.com/openai-gym-beta/)\n* [Deep Q-Networks](https://deepmind.com/research/dqn/)\n* [Theory behind RL and MDPs](http://reinforcementlearning.ai-depot.com/)\n* [A Udacity course on RL](https://in.udacity.com/course/reinforcement-learning--ud600/)\n* [A short code introduction](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-0-q-learning-with-tables-and-neural-networks-d195264329d0)",
        "author_name": "Aneesh Aithal",
        "slug": "reinforcement-learning"
    },
    {
        "layout": "post",
        "title": "Is Random really Random?",
        "author_github": "subbalakshmi",
        "date": "2017-08-04T03:48:33",
        "image": "/assets/img/",
        "description": "Random implies indeterminacy (One can't possibly know, what will happen next). At the same time, the result of a computer program is deterministic (If you know the algo, then you know the answer). If that is the case, then is it possible that the random numbers that a computer generates is not truly random?",
        "tags": [
            "random",
            "PRNG",
            "TRNGs",
            "cryptography",
            "Lehmer's RNG"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "subbalakshmi",
        "comments": true,
        "text": "[{% load static %}]()\nRandom numbers have been used for many thousands of years. \nRoll a dice. \nFlip a coin. \nPick a card from a deck. \nWhatever the result may be in these three situations, it\u2019s completely random each time and there\u2019s no way of predicting it by looking at the previous outcomes. \nIn other words, the end result is left to random chance.\n\n![Dice]({% static '/blog/assets/img/random-number-generator/dice.jpg' %})\n\nRandom number generators in a computer are similar: they\u2019re an attempt to achieve an unpredictable, random result. \nFrom creating random keys for data encryption to random shuffling cards in a poker website, generation of random numbers is vital.\n\n![Gambling]({% static '/blog/assets/img/random-number-generator/gamble.jpg' %})\n\nNow, to generate random numbers we might use the traditional `import random` (python), `randn` (MATLAB) and other packages and functions in other platforms.  \nAnd what do they do? They present to us a randomly picked number.\nSimple. \nBut have you ever pondered how the computer picks that particular number?\n\nWe all know that a computer is good at following instructions. \nIt cannot create, or think for itself (until fully functioning Artificial Intelligence becomes a reality). \nIf a computer is asked to pick a random number it won\u2019t be able to do so, until and unless it\u2019s told how to do it and this way, the true purpose of randomness is lost.\n\n**Moment of Truth**: Majority of the random number generators used by computers are not truly random. \nThe number generated is usually a result of an algorithm.\n\nThe random numbers generated by such a way are termed `pseudo-random` i.e it appears random but is actually just a predetermined sequence of numbers. \nThe sequence is determined by an initial value known as `seed`.\nThe Lehmer's Random Number Generation algorithm is one such example of pseudo random generation.\n\nWhat defines randomness is uncertainty. \nOne can never know what will happen next. \nBy using algorithms to generate random numbers this property is lost. \nAnyone who gets hold of the algorithm can easily figure out the number.\n\nIf random numbers are really generated by a deterministic algorithm, how on earth can we use it for applications as important as cryptography?\n\nIn order to generate numbers that are truly random, computers must find a seed that is truly random. \nThis is done by measuring a physical occurrence, or reaction, which happens naturally at random.\n\nMost secure random number generators have an `entropy gathering service` continously running on a machine. \nIts job is to harvest entropy from multiple sources over time.\nIn other words, it  gathers random information from the environment, stores this information, and then returns it when asked to generate a random number. \nFor example, the computer can measure white noise, or can measure the timing of various events like keystrokes and packet arrivals, and combine these in a theoretically valid way to generate numbers randomly.\n\n![Comic]({% static '/blog/assets/img/random-number-generator/comic.jpg' %})\n\nAt the end of the day, randomness isn\u2019t limited to securing information and gambling. \nIt\u2019s so much more. \nLook around you. \nThe electronic noise in our devices is random. \nRadioactive Decay is random.\nThe world is governed by chance. \nRandomness stalks us every day of our lives.Its what keeps us curious, intrigued and excited all along the way!",
        "author_name": "N Subbalakshmi",
        "slug": "random-number-generator"
    },
    {
        "layout": "post",
        "title": "Of a tooth and a pie",
        "author_github": "Shwetha",
        "date": "2017-08-04T00:00:00",
        "image": "/assets/img/",
        "description": "A prologue to using Raspberry Pi as a bluetooth device.",
        "tags": [
            "Bluetooth",
            "Raspberry Pi 3",
            "Internship"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Shwetha",
        "comments": true,
        "text": "[{% load static %}]()\nAt first, I thought I would write about the wonder creation of yesteryear-Bluetooth. Give you a little insight into how it works, the various layers of Bluetooth, it's profiles and so on. \n\nBut I guess Wikipedia would do a much better job at it. So, I decided to share something I learned during my internship- how we can **make use of the inbuilt Bluetooth controller in Raspberry Pi 3 to implement some of the profiles of Bluetooth.**\n\nFirst, you need to know that **Raspberry Pi** is nothing but a Linux machine and can be governed by some commands that you can type in the terminal. Also, there will be a repository associated with each Raspbian version (Raspbian is an OS), from which you can download software packages and perform various tasks on your Pi. \n\nSecond, though the latest model of Raspberry Pi, the 3B model, is equipped with an inbuilt bluetooth controller, you need to carry out some steps to enable it. The most widely used package to carry this out is `bluez`. \n\nWith a `sudo apt-get install bluez` command, you can install it in your Pi. \n\nThere is another package called `bluetooth` which also needs to be installed and then you are good to go. This offers a tool called `bluetoothctl` and once this is launched, you can scan, pair, trust or connect any Bluetooth device to the Pi just like you can in a mobile phone.\n\nNow, I hope you know what a **Bluetooth profile** is. It is nothing but a specification concerning a particular aspect of Bluetooth-based wireless communication between devices. For example, transferring files is associated with the profile FTP, hearing quality music is associated with A2DP, VoIP calls or using a bluetooth headset to make handsfree calls is associated with HSP/HFP and so on. \n\nSo basically, if you connect a Bluetooth device to your Raspberry Pi, you can transfer files to and from the device, or play music on your device and hear it through speakers connected to your Pi, or play a music on the Pi and hear it through the bluetooth headset connected to your Pi. The applications are endless.\n\nI am going to just give you a sneak-peek on how to get started on doing all this. We will start with the easiest one, the A2DP profile which is an acronym for **Advanced Audio Distribution Profile**. Using this profile, you can play high quality music both on the Pi output (speakers or HDMI analog out) or on a bluetooth headset connected to it. \n\nFor this, you need to install a sound server package called `Pulseaudio` which deals with all the sound streams passing via Bluetooth. Pulseaudio provides you with the option to select a source (from where the audio is taken) or a sink \\(to where the audio must be played). It also allows you to select the Bluetooth profile in which you want your Bluetooth device connected to the Pi work and can even do the same for Pi as well. \n\nThat is, if you are connecting a bluetooth headset to the Pi, it can either be used as an A2DP sink or can be used to attend calls for HSP/HFP (Headset/Handsfree) purposes. Once you configure the source and sink to route your audio through the Pi, you are done. Now, the Pi can act like a music player and all you have to do is feed in your playlist into the Pi.\n\nComing to transfer of files, which is namely the FTP or File Transfer Protocol, again, you have readily available packages like `ussp-push` and `obexpushd` which are once again armed with ready-to-go commands to enable easy transfer of files from Raspberry Pi to your bluetooth device and vice-versa. \n\nA small glitch, however, occurs when you try to use the Raspberry Pi as a **handsfree** device. By handsfree device, I mean the exact handsfree device you have in your cars, using which you can attend calls coming to a device connected via bluetooth to it. Our Pi too can perform the same task, but since we require live transmission of audio or more precisely voice data, the inbuilt bluetooth controller faces trouble and is not feasible owing to its huge latency and difference in the sampling rates of the two devices. \n\nBut you can always go for a Bluetooth dongle in that case. Using the inbuilt bluetooth you can, however, place calls or answer them and do other related operations using a software for telephony called `ofono`. \n\nA joint effort of Intel and Nokia, this package is available for download in the repository and provides test files which you can run to do any call related operations, provided your connected device supports it. While implementing this, the problem I faced was that audio could be heard neither through the Pi nor through the connected mobile phone. \n\nI guess I have introduced you to a plausible aspect of Bluetooth usage in Raspberry Pi. It is a fun field to explore and the possibilities are plenty. Please do check these sites out for more details.\n\n[Bluetooth file transfer](http://www.jamesrobertson.eu/clf/2014/jun/28/sending-a-file-over-bluetooth-from-your-phone-to.html)",
        "author_name": "Shwetha S V",
        "slug": "raspberry-pi-bluetooth"
    },
    {
        "layout": "post",
        "title": "Smart Phones and Health issues",
        "date": "2017-08-04T1:0:0",
        "image": "/assets/img/",
        "author_github": "akshay",
        "github_username": "akshay",
        "description": "Ever wondered what happens to your body while using your smartphones for a long time?",
        "tags": [
            "Smartphone",
            "Negative effects"
        ],
        "categories": [
            "Piston"
        ],
        "comments": true,
        "text": "[{% load static %}]()\nThe way smartphones have changed our lives is amazing. Technology has been ever bringing multiple facets of our daily lives within the scope of smartphones, which in turn gets smaller and slimmer. No more does our parents bother us to go and pay telephone or electricity bills (if you were a 90\u2019s kid :P) neither does shopkeepers curse you for not having suitable denominations. It has all come down to few swipes and touches.\nAs new innovations brought the world closer and time shorter, it has not come without any drawbacks, same with the case of smartphones. Again, it\u2019s the way we use it that puts the blot on this device, than the device itself doing the wrong.\nMost of us are already aware of the dangers a non-protected cyber world pose. But most of us neglect the health issues which can lead to heavy complications in the long run.\n\n### Mind your posture\n\n![image1]({% static '/blog/assets/img/Smartphones-and-health/image1.jpg' %})\nThe picture above clearly shows our tendency to lean our necks forward while using mobile phones. something now known as **text neck** syndrome. Whether you know about increased stresses at support in a cantilever beam (which is similar to our neck when bent forward) or not, this can lead to chronic neck and back pain in the long run.\n\n### Well\u2026.what was I just reading?\nEver felt you were getting distracted repeatedly while reading something? Are you skipping many articles without reading them completely? Your attention span might be severely affected by nothing other than prolonged exposure to smart media. The contents in modern media have been designed to penetrate your mind with much less effort from your brain. So, it\u2019s just normal that brain finds it difficult to comprehend stuff through relatively strenuous activities like reading (yes, you heard it right)\n\n### Blue light, Blurred vision\nWhat comes out from smartphones and laptop screens are generally called blue violet light and will lead to deterioration of cells in the back of the eye. Moreover, we tend not to blink our eyes while staring at such screens and usually bring the phones more closer to our eyes. So, if you wish to see the screen for a further time in your life, take good care of  eyes.\n\n### Relationships \u2013 Virtual or Real ?\nWhat is date or a dinner if the people involved does no talking, but stares at their screens. Only a very few care to talk with co-passengers on a trip. Relationships converges to mere texts, or occasional calls. \n\n### Anxiety, Sleep and many more..\nExposure to blue light before going to sleep have adverse effects on biological cycle, leading to disturbed sleep. Anxiety, obesity and depression are few other issues due to over usage of smartphones as they eat up time otherwise meant for social and physical activities.\n\nOur body and mind have fathomless capabilities yet to be metered. We haven\u2019t let them out completely, and moreover, too much dependency on technology calls for lesser use of our body and mind. Do make sure you take ample breaks from your time on phones, to talk with others, to think, to dream or at least to take that occasional deep breaths, and trust us, you\u2019ll feel much more better being alive. :)",
        "author_name": "Akshay Krishna",
        "slug": "smartphones-and-health"
    },
    {
        "layout": "post",
        "title": "A QuickStart Guide to Image Processing with OpenCV",
        "date": "2017-08-13T00:00:00",
        "author_github": "santosh-kodali",
        "image": "/assets/img/",
        "description": "Learn how to get started with OpenCV: A beautiful tool for a game changing field. ",
        "tags": [
            "OpenCV"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "santosh-kodali",
        "comments": true,
        "text": "[{% load static %}]()\n### Why learn Image Processing? \n\nThis is the question I was primarily asked when I first started exploring just exactly what Image Processing can offer. As time passed, I soon found the now seemingly obvious answer. Although the name, Image Processing, itself suggests manipulating an image for either aesthetic pleasure or requirement, Image Processing actually provides a gateway for the next revolution in engineering. As more and more people become accustomed to having human tasks being done by machines, image processing becomes crucial in training and programming these machines to flourish. Image processing not only provides the framework for Computer Vision but also gives way to Industrial Automation, Artificial Intelligence and much more. OpenCV is used for many Image and Video processing applications such as Facial Recognition, Object Detection, Photo Editing, Robotic Vision, Character Recognition and so much more!\n\n![Open CV Logo]({% static '/blog/assets/img/opencv/OpenCV_Logo.png' %})\nWhen I first started Image Processing, I was blown back at the abundance of information available online. One of the first steps I needed to take was to decide between the leading Image Processing tools, OpenCV (Open Source Computer Vision) and Matlab. Where Matlab is easier to setup and faster to debug, OpenCV usually runs faster due to its better optimization, has a much larger comprehensive open source library for computer vision, and many more functions. Not to forget, OpenCV is entirely free, whereas Matlab has a pretty pricing licensing involved.\n\n![Python Logo]({% static '/blog/assets/img/opencv/python-logo.jpg' %})\nNow without dwelling with the non-technical details, let us dive into the world of OpenCV! You can use OpenCV on Ubuntu, Windows, and macOS so no worries there. Installation can be a bit complicated but thankfully there are great guides out there for different Operating Systems. OpenCV is compatible with Python 2.7, Python 3.5, Java, C, and C++ so use whichever programming language you are most comfortable with. I would suggest learning in Python just because of the extensive support online for Python and OpenCV. For windows I usually prefer to work with Anaconda because of its versatility but it\u2019s up to you. You will also need Numpy for numerical operations and Matplotlib for viewing your images in an easy way.\n\n### Installation links :\n\n- [Ubuntu](http://www.pyimagesearch.com/2016/10/24/ubuntu-16-04-how-to-install-opencv/)\n- [Windows](http://docs.opencv.org/3.2.0/d5/de5/tutorial_py_setup_in_windows.html)\n- [Windows + Anaconda](http://mathalope.co.uk/2015/05/07/opencv-python-how-to-install-opencv-python-package-to-anaconda-windows/)\n- [MacOS](http://www.pyimagesearch.com/2016/12/19/install-opencv-3-on-macos-with-homebrew-the-easy-way/)\n\nNow once the installation is out of the way, we can get started on the fun stuff! It\u2019s important to note this article is a quick start guide to OpenCV, not a comprehensive tutorial. To get a further understanding of OpenCV I would suggest following a video tutorial such that on [https://pythonprogramming.net/loading-images-python-opencv-tutorial/](https://pythonprogramming.net/loading-images-python-opencv-tutorial/) or rent some books on OpenCV. The Video Tutorial Link Above is a very easy to understand and fun guide to OpenCV.\n\nWith OpenCV you can do with both Image and Video Processing. This is because a video is nothing but successive frames put together. The frames are then analyzed individually much like a single image. Most cameras nowadays usually record anywhere between 30 to 60 frames per second.\n\n![Gray]({% static '/blog/assets/img/opencv/gray.png' %})\n\nMost image processing operations in OpenCV is done on a simplified form of the actual image. Usually the image is converted to grayscale but we can also filter different colors from the original image or find gradients before sending the image to Processing. After extracting the information of this processed image, the information is then applied back to the original image or sent to your post processing program like a robot\u2019s motor for example.\n\nIn Image Processing we have many different operations we can do. This includes Color Filtering, Edge Detection, Feature Matching, Object Recognition, and so much more. \n\n### Documentation \n\nThere is very good Documentation online for any of your Image Processing Needs: \n\n- [http://docs.opencv.org/trunk/d2/d96/tutorial_py_table_of_contents_imgproc.html](http://docs.opencv.org/trunk/d2/d96/tutorial_py_table_of_contents_imgproc.html)\n- [http://docs.opencv.org/2.4/index.html](http://docs.opencv.org/2.4/index.html)\n\nWell I hope this article has sparked your interest in Image Processing! Hopefully you don\u2019t run into errors when installing OpenCV or any of the other required libraries. Have a fun time on your Image Processing Adventure and remember\n\n**\u201cComputers are good at following instructions, but not at reading your mind. \u201d - Donald Knuth**\n\n\nAll the Best!",
        "author_name": "Santosh Kodali",
        "slug": "opencv"
    },
    {
        "layout": "post",
        "title": "When Strokes are Six",
        "date": "2017-08-13T00:00:00",
        "author_github": "vinayakvenugopal27",
        "image": "/assets/img/",
        "description": "Ever heard about six stroke engines?? ",
        "tags": [
            "six stroke engines"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "vinayakvenugopal27",
        "comments": true,
        "text": "[{% load static %}]()\n### Man and his Car \n\nThe stories are different, yet the same . From the spark that taught him about fire and the wheels that took him to places, to the roads that he hit everyday, he has come a long way. The love and passion to be in one, to feel its heartbeat and power, to drive and tame them, he has always loved them. But rare are those who are interested in the technology that drives their beasts.\n\nIt is said that technology advances in rapid pace. Though it is right for a majority of areas, advancements in technology seldom has been helpful in improving engine efficiency.  And at a time where we seriously discuss about energy sources and their availability in near future, it is true that this matter require some serious attention. This is where the concept of 6 stroke engines come into play. \n\nEven though the notions of two-stroke engines and four stroke engines are common as their applications, the six-stroke engines has never gained popularity that it should have. Driven by overcoming the shortcomings of a normal four stroke engine, the six- stroke engines are essentially the beasts. \n\n### So what is a six-stroke engine? \nIt has nothing special, but two extra strokes than the conventional four stroke engine. \n\n#### So how does this work?\nThink about a normal four-stroke engine. The piston moves down to suck in air and fuel, then the fuel pushes up to compress, then the spark plug fires and the burning gas forces the piston down. Then the piston moves back up to push up the exhaust. The six-stroke engine adds two more strokes after the exhaust stroke. Water is injected, and as it turns into steam, it pushes the piston down. Then the piston comes back up to force out the steam. It's really simple.\n\nA six-stroke engine combines an internal combustion engine with a steam engine to turn some of the waste heat into power. The only catch is that you have to add a water tank to your car that's about the same size as the gas tank. The good news is that you can probably eliminate the radiator.\n\nNow\u2026\nThe question is: Why don\u2019t we see such cars in the street? Probably, because people like you and me are not trying harder. This is surely not the one to be left unexplored.",
        "author_name": "Vinayak V Gopal",
        "slug": "six-stroke-engine"
    },
    {
        "layout": "post",
        "title": "Hashing in Competitive Programming",
        "author_github": "somnathsarkar",
        "date": "2017-08-21T19:40:58",
        "image": "/assets/img/",
        "description": "A look at hashing and its role in Competitive Programming",
        "tags": [
            "Hashing",
            "C++",
            "Competitive Programming",
            "Data Structures"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "somnathsarkar",
        "use_math": true,
        "comments": true,
        "text": "[{% load static %}]()\n## Introduction\nAt it's most basic hashing is a technique to identify a particular value of a certain type among other values of the same type. To achieve this we typically make use of a hash function: a function that takes a value of the required type as input and generates a \"key\" that (hopefully) can uniquely identify this value. In this article we'll look at how this function works, how it's implemented in C++, and how to apply hashing to solve a Competitive Programming problem.\n\n## Hashing in C++\nC++ offers 2 containers in C++ that are often useful for hashing problems: map and unordered map. Both containers store elements in a \"mapped fashion\": we associate a key value to a mapped value, typically (but not necessarily) of a different data type. Out of these two the unordered map is the one that is typically implemented using a traditional hash function whereas the map represents a balanced binary search tree.\n\n### $std::map$\n\t1. Implemented with red-black tree.\n\t2. Elements are sorted.\n\t3. Small memory usage.\n\t4. O(log N) lookup time.\n\n### $std::unordered$_$map$\n\t1. Implemented with a hash function\n\t2. Elements are not sorted.\n\t3. Requires additional memory to keep the hash-table.\n\t4. Lookup is usually O(1), dependent on hash function.\n\n### Why use $std::map$ over $std::unordered$_$map$?\nA quick look at the comparison between the two containers gives us the impression that we should use $std::unordered$_$map$ in most cases (except those where we require a sorted traversal through all keys), and we would be right to do so normally. After all, $O(1)$ lookup and insertion is a lot more appealing than the $O(log N)$ operations of $std::map$. However, there are a few things to keep in mind. Namely that the worst-case runtime of unordered_map is $O(N)$ not $O(1)$. This is because of how it is implemented. As we'll see later, hash functions have a chance of \"colliding\"; that is, two distinct inputs to the hash function give the same output. This causes the unordered_map to store the associated values corresponding to these colliding keys in a list. In the worst case, when all keys collide, the lookup time for each key is $O(N)$.\n\nFor more:[Reference 1](http://codeforces.com/blog/entry/44731)\n[Reference 2](http://codeforces.com/blog/entry/4898)\n\n## A hash function\nLet's look at the case where we need to map a string to an integer: we need to take care of the distinct characters present in the string as well as their position in the string. A common strategy is to take the ASCII value of each character and multiply it by a prime p to the power of the index of the character. In order to ensure it fits in a C++ variable we typically take the hash mod a large prime M.\n\n\\begin{equation}\nF(R) = hash(S[0,R]) = (\\sum_{i=0}^{R}S_i * p^i)\\% MOD\n\\end{equation}\n\n## Looking at a problem\nLet's try and apply the hashing technique to a competitive programming problem\n[here](https://www.codechef.com/COOK82/problems/COOK82D). \nWe are given a graph with a list of edges from $1 ... M$. \nThe queries are of the form $[L, R]$. \nFor each query we look at the graph containing all the edges numbered from L to R. \nWe need to determine whether all the connected components of such a graph have Euler cycles. \nThe answer will then be \"YES\" if all vertex degrees are even, and \"NO\" otherwise. \nA na\u00efve solution would be to store the parity of each vertex of the graph for all edges from $1 ... X$, for all $X$ from $1$ to $M$. \nUsing these prefix parity values we can say the answer for a range L to R is \"YES\" when $pref[R] = pref[L-1]$ and \"NO\" otherwise. \nMemory use here is $O(N^2)$ and query time is $O(N)$. \nThis can be improved by storing each prefix parity value as a hash. \nIn the above expression $S_i = 0$ if degree of $i^{th}$ vertex is even and 1 otherwise. \nThis reduces memory use to $O(N)$ and query time to $O(1)$.",
        "author_name": "Somanth Sarkar",
        "slug": "hashing-in-competitive-programming"
    },
    {
        "layout": "post",
        "title": "Technology in Sports",
        "author_github": "abhishekthigale",
        "date": "2017-08-21T23:15:10",
        "image": "/assets/img/",
        "description": "Modern Technology in Sports",
        "tags": [
            "Sports"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "abhishekthigale",
        "comments": true,
        "text": "[{% load static %}]()\nThe world of sport is continually changing over the years, and the use of technology is just one of those areas that has made an impact on many sports.\u00a0It has changed the way players approach a particular sport. Various technologies are used to assist umpires or referees, while some of them are used for advanced coaching and enhanced experience for fans.\n\n## A digital coach\u2026\nIn the recently concluded Champions Trophy 2017, smart bats were used to obtain various data. Small sensors, fixed at the top of the handle, were used. \nThe bat sensor, powered by the Intel Curie technology, can be mounted on any cricket bat to generate data for every stroke that the batsman plays. With this technology, parameters like back-lift, bat speed from start of the shot till the end of bat swing and follow-through can be tracked for every stroke. This can be used for efficient coaching and improving the game level. After making smart bats, Intel is working on a future smart ball to obtain data for each delivery.\n\n## Hawkeye\n![Hawkeye]({% static '/blog/assets/img/technology-in-sports/w.jpg' %})\nHawkeye hit the screens in 2001. It is the technology used to track a ball's trajectory. Hawk-Eye uses technology originally used for brain surgery and missile tracking. It uses six or more computer-linked television cameras situated around the court. Six separate views are obtained and then combined together to produce an accurate 3D representation of the path of the ball. It is presently being used in international\u00a0cricket\u00a0and\u00a0tennis, and many other sports are also looking at making use of this technology. The review system through the use of Hawkeye helps in correcting many umpiring errors.\n\n## Data analysis\nData Analysis is changing the nature of sports science. Huge amount of data is collected through special field cameras for analysis. This analysis helps them to appropriately determine the skill set of players, uncover undervalued players, pinpoint aspects of team play that need to be worked on, and selecting the right players in a lineup for optimal performance. This is widely used to figure out the opponents\u2019 weaknesses and make suitable strategies. \n\n## Few other examples\nHot Spot\u00a0is an\u00a0infrared\u00a0imaging system used in\u00a0cricket\u00a0to determine whether the ball has struck the batsman, bat or pad. Hot Spot requires two\u00a0infra-red\u00a0cameras on opposite sides of the ground above the field of play that are continuously recording images. Any suspected snick or bat/pad event can be verified by examining the infrared image, which usually shows a bright spot where contact friction from the ball has elevated the local temperature.\u00a0\nSnick-o-Meter\u00a0is a very sensitive microphone located in one of the stumps, which can pick up the sound when the ball nicks the bat. This technology is currently used to in the review system to show if the ball did or did not actually hit the bat.\n\n## Future!\nOver the past few years media broadcasting has improved with advancement in camera technology. Technology is being used for deciding players\u2019 diet, proper clothing material for match comfort and the players\u2019 safety. \nResearch is going on to discover how technology can be used in sports. Various equipment designs for players safety, apps and websites for fans are being developed. Seeing the technology growth, one won\u2019t be surprised to see a robot-umpire in the future.",
        "author_name": "Abhishek Thigale",
        "slug": "technology-in-sports"
    },
    {
        "layout": "post",
        "title": "Airbus E-Fan",
        "author_github": "sachitkrishnan",
        "date": "2017-09-04T10:00:30",
        "image": "/assets/img/",
        "description": "A post explaining an environmental approach taken by Airbus",
        "tags": [
            "Aviation",
            "Environment"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "sachitkrishnan",
        "text": "[{% load static %}]()\n## The Airbus E-Fan Project\n\nFor over a hundred years - and certainly many to follow, the aircraft\nindustry, for the most part, has relied upon combustion based engines,\nranging from the long-forgotten piston-prop through the state of the art\nTurbofan. Yet, no matter what the degree of advancement is in the field\nof such engines, what\u2019s guaranteed is an ever-increasing toll of CO2 and\nother greenhouse gases.\n\nThe intuitive solution for sustainable development, as in nearly all\ndomains of power generation, especially in transportation, is\nelectricity. Presently, every mode of transport (save for aviation)\nemploys at least a certain level of technological know-how of the\nutilisation of electrical energy to power any system that would\notherwise use energy from combustion of fossil fuels.\n\nHowever, no such concrete technology in the aircraft domain is in\nexistence yet, due primarily to the commendable figures of power output\nand efficiencies of the contemporary combustion engines. But then,\nelectric engines were never invented with the aim of outperforming the\nexisting engines (though with the current trends of R&D, such a thought\ncould be realised), but to rather mimic them without using non-renewable\nresources and reducing greenhouse emissions in parallel.\n\nThe E-Fan Programme of Airbus is one such initiative that aims to\naccelerate developments towards environment-friendly aviation. The E-Fan\nwas initially designed specifically to utilise electric power. As a\nsecond step, E-Fan was transformed into an updated \u2018Plus\u2019 version with a\nhybrid configuration for longer flight endurance. E-Fan Plus \u2013 which\ndebuted during the summer of 2016 \u2013 incorporates an internal combustion\nengine as a range extender in addition to the aircraft\u2019s on-board\nlithium-ion batteries.\n\nThese two propulsion system configurations \u2013 and ongoing innovation \u2013\nhighlight the aircraft\u2019s role as a technology demonstrator that allows\nthe company to make important advances on its electric aircraft roadmap.\n\nThe E-Thrust concept with Rolls Royce Airbus has strategically partnered\nwith Rolls-Royce for the E-Thrust concept study. This research is\nfocused on delivering lower fuel consumption, fewer emissions and less\nnoise for future aircraft through a hybrid/electrical distributed\npropulsion system. Building on E-Fan\u2019s all-electric power system, the\nE-Thrust concept features electrically-driven fans distributed in\nclusters along the wing span and an advanced gas power unit to provide\nelectrical power for the fans and for recharging the energy storage\nsystem (potentially high-density batteries) depending on the phase of\nflight. Electrical energy management throughout the flight profile For\ntakeoff and climb, electrical power is generated from both the energy\nstorage system and gas power unit to provide peak power to the fans.\nDuring cruise, the gas power unit powers the fans and recharges the\nenergy storage system. In the aircraft\u2019s descent, the gas power unit is\nswitched off \u2013 with no power provided to the fans, and the energy\nstorage system producing the energy for on-board systems. In a second\nphase of descent, the fans will be used as \u201cwindmills\u201d to generate\nelectrical power to re-charge the energy storage system. Finally, for\nlanding the gas power unit is restarted, offering low levels of power\nfor propulsion as a safety measure during this phase of flight.",
        "author_name": "Sachit Krishnan",
        "slug": "airbus-e-fan"
    },
    {
        "layout": "post",
        "title": "Rock Paper Scissors",
        "author_github": "sssachin300",
        "date": "2017-09-04T10:00:30",
        "image": "/assets/img/",
        "description": "A deeper look into a popular childhood game",
        "tags": [
            "Game Theory",
            "Strategy"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "sssachin300",
        "text": "[{% load static %}]()\nIf you have ever played the game of rock, paper scissors I bet that the best strategy you had was trying to predict your opponent's move which is a pretty appealing idea. But given that it is nigh impossible to do that (unless, of course, you can read minds), is that really a sound strategy to adopt? If you think about it in a rational way there is no method to predict your opponent and basing your strategy on such an unpredictable thing is kind of dumb. I think the best approach to this problem is to make sure your opponent doesn't beat you more than half the time.  If both players play random moves, then both have an equal probability of winning. If there's a pattern in your moves, in a large number of matches your opponent can easily figure it out and increase his winning chances considerably. So the obvious thing to do is to randomize your moves. This, you can do, in a number of ways and I leave it to you to find them out: P\n\nNow the situation changes entirely when you introduce a third party in the game and all three of you play simultaneously. Try to think of a strategy which you can use here.\n\nOne strategy would be to play rock all the time and tell this to them. If the other two play rock, then it is a draw. If one of them plays scissors, then they lose to the rock. If they play paper, then there's always a chance that the other plays scissors and wins but in turn the other player would lose to the rock. This makes it obvious that there is no way for one to beat both you and the other person at the same time.\n\nThis is what **Game Theory** is all about. Popularized by Nobel laureate **John Nash** (notice the name of the article \ud83d\ude09) game theory deals with forming strategies to get the best possible moves you can make without ignoring the self-interests that human beings have. Despite its name it not only deals with games but with any form of strategic decision making including politics, defence, economics, trade, traffic control etc.\n\n## A look at a real world problem \n\nAs a real world example I present to you the following problem on traffic lanes.\n\nSuppose you are in a heavy traffic and want to change lanes (you cannot do that in my country India though, because we don't have many lanes \u2639). When do you switch lanes? When the lane beside you are faster than the current one? Which is a good approach in the long run?\n\n![Traffic]({% static '/blog/assets/img/rock-paper-scissors/traffic.jpg' %})\n\nLet us first make the simple assumption that a lane is faster only because it has fewer number of vehicles. Suppose you are impatient and switch to the fast lane. If there aren't many who think like you, then it is your lucky day. If not, the fast lane will get crowded and become the slow lane. If everyone thinks in this way and stays in the same lane, then the lanes maintain their speed and you have to waste your time. So, either way, you can only do a successful thing provided the others don't think of the same strategy as you.\n\nThere is, in fact, a way out of this pickle. Pick a random amount of time (say t). After time to check the lane speeds. If the lane beside you are faster, then switch. Else stay in your lane. Repeat the process after time t.\n\nMathematicians have shown that if everyone adopted this strategy we would achieve an equilibrium state with both lanes having the same speed. Though this may not suit your individual preference (you would rather have the entire lane or even all the lanes to yourself) this is the best we can do while being fair to everyone.\n\nThe problem here is, of course, that it requires everyone on the road to have read game theory.\n\n### References\n\n1. Read Rock, Paper, Scissors: Game Theory in Everyday Life by Len Fisher.\n2. [PittsBurgh Post-Gazette](http://www.post-gazette.com/opinion/Op-Ed/2013/02/03/The-Next-Page-Everyday-uses-for-game-theory-such-as-when-to-wash-the-dishes/stories/201302030375)\n3. [Reference 1](http://ncase.me/trust/)\n4. [Coursehero - Prisoner's Dilemma](https://www.coursehero.com/sg/microeconomics/the-prisoners-dilemma/#nash-equilibrium)",
        "author_name": "Sachin Sampath",
        "slug": "rock-paper-scissors"
    },
    {
        "layout": "post",
        "title": "Safety Issues in AI Systems",
        "author_github": "chennakeshava1998",
        "date": "2017-09-04T02:30:00",
        "image": "/assets/img/",
        "description": "Description of the causes of unexpected behaviour of AI models",
        "tags": [
            "ML",
            "AI",
            "Security AI"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "chennakeshava1998",
        "text": "[{% load static %}]()\nI am pretty sure the title will flood the reader\u2019s mind with images of Skynet and Matrix, but this post focuses on the shortcomings of AI technologies that are currently in vogue. It\u2019s much more productive to contemplate on the loopholes in technologies like Neural Networks, as we are still a far cry from building systems that are at least as intelligent as earthworms.\n\nAI has made several inroads into diverse fields such as NLP, Computer Vision, Autonomous Vehicles, Personal Assistants, and Game playing bots. AI has become truly ubiquitous. This magnifies the impact of security loopholes on the society. Accidents, in the context of AI, are defined as unintended or unexpected actions of the system, that might be harmful to human beings, and might cause irreversible damage to the environment. These may be caused due to reasons like incorrect objective function, errors in the implementation or the systems not being robust to outlier inputs. Many of ML problems are solved by designing algorithms that optimise a particular function - this function is known as the objective function. An objective function can be specified either as a cost functions(that gives us the error in the predictions, and hence has to be minimised) or as utility function(used to specify rewards for Reinforcement Learning agents, which try to maximise this reward). Another reason, that augments research in this area is that, the Value Alignment problem shares a major intersection of ideas with the security concerns.\n\nSo what exactly is the Value Alignment problem? Suppose that our system consists of n variables, and we define our objective / utility function in terms of k variables, where k is a subset of n. So, there are chances of the system optimising the utility function by setting the other variables to extreme values. This might cause a lot of permanent damage. Suppose we design genuine AGI(Artificial General Intelligence) for getting us medicines. Now, we wouldn't want it to imfringe upon others' rights when we want it to get medicines in the most optimal way right? Like, it shouldn't break the trafffic rules, it should stand in a queue, it must not steal the medicines,etc. The **paperclip maximiser** is a funny, but instructive example that demonstrates this problem.\n\nMost of the alignment problems initially show up as security issues itself. The value alignment problem isn\u2019t born out of the spooky fears of evil-intent and consciousness arising out of super-intelligent machines, but because of the following reasons:\n\n1. It\u2019s very hard to clearly define the cost/utility functions of agents which align perfectly with that of the human race.\n2. Any sufficiently intelligent AI will try to acquire more physical and computational resources, not to take over the world, but to succeed in its task more efficiently.\n\nLet\u2019s explore some of the ways in which AI systems can malfunction:\n\n## Incorrect Reward Functions \n\nWe are quite familiar with Sorcerer\u2019s Apprentice or King Midas\u2019 story - \u201cWe all get what we ask for, not what we want\u201d. For instance, consider the case when google photos tagged a person as Gorrilla - [Google apologises for Photos app's racist blunder](http://www.bbc.com/news/technology-33347866)\nThis was not because the AI system wanted to hurt the sentiments of particular section of society. It might be because, the cost function for misclassification of inanimate objects is the same as that of misclassifying people as animals/objects. On the contrary, greater cost should have been associated with misclassification of humans. This illustrates the unintended effects of faulty reward functions.\n\n## Negative side effects\n\n The AI system may try to maximise its reward function, without heeding to the other environment variables. For instance, consider the example of a robot used to clean office spaces: The robot must not break an expensive vase in it\u2019s way, and must search for an alternative path, although the cleaning process might be faster in the former approach.\n\n## Reward Hacking\n\n The system may find out a clever/easy way of increasing the reward, but might be against the spirit with which it was designed. Continuing with the same analogy, the robot might purposefully shut it\u2019s vision system so that it cannot \u2018see\u2019 any dirt, or it might hide the existing dirt with objects, so that the dirt is not visible to others. Take a look at this AI bot, that\u2019s designed to play the speedboat game of [CoastRunners](https://youtu.be/tlOIHko8ySg). Here, the boat that finishes first gets the maximum points. So, when this RL agent was given reward function of obtaining maximum points, it does not participate in the race, but rather tries to obtain higher points by perfecting it\u2019s timing of going around in the circle as shown. Indeed, the number of points obtained by the bot was 20% higher than an average human player, but this was not what the designer had in her mind. More on this by OpenAI - [Faulty Reward Functions](https://blog.openai.com/faulty-reward-functions/)\n\nHence many attempts have been made at evaluating a system\u2019s performance and solving the \u201creward engineering problem\u201d. For instance consider, Cooperative Inverse Reinforcement Learning(CIRL) - here, a robot observes a human being in an open environment and \u2018infers\u2019 the reward function by itself, rather than being explicitly specified. This is a paper on CIRL from researchers at BAIR - [Cooperative Inverse Reinforcement Learning](https://arxiv.org/abs/1606.03137) . OpenAI has also released [Universe](https://universe.openai.com/), which makes it easier to compare a machine\u2019s performance in a game with that of a human being.\n\n## Adversarial Examples\n\nAs the name suggests, we are trying to create inputs that bypass an ML algorithm. This is probably one of the segments that is receiving a lot of focus in the recent years. Neural Networks have been immensely popular for computer vision tasks, but of late they have been prone to a lot of adversarial loopholes. Initially, a ML algorithm will have correctly classified an input. But a very slight perturbation is added to the original input, which is so subtle that it is nearly imperceptible to humans, but this often \u201cfools\u201d the classifier and results in an incorrect output. Consider this,\n\n![Misclassification of Panda]({% static '/blog/assets/img/safety-issues-in-ai-systems/panda.png' %})\n\nOn the LHS, the image is classified as a panda with 57% confidence. Although the middle image appears as random noise, it\u2019s obtained by maximising the prediction error of the classifier, but this surprisingly results in the wrong classification as a \u201cgibbon\u201d. As we can observe, the difference between RHS and LHS images is quite imperceptible. It is true that, an attacker may not have access to directly input data into a classifier, so experiments were done to take input from camera devices, but similar results were obtained. Epsilon refers to the extent of perturbation added. In the below image, the image classifier is fooled into classifying the washing machine as a safe\n (34% probability) and as a loudspeaker(24% probability).\n\n![Misclassification of pictures from cameras]({% static '/blog/assets/img/safety-issues-in-ai-systems/washing_machine.png' %})\n\n\nFurther, it was believed that since Autonomous vehicles take many pictures from varied angles, perspectives and scales they would not be deceived very easily. But researchers were able to generate scale-invariant and transformation-invariant adversarial inputs also - [Synthesizing Robust Adversarial Examples](https://arxiv.org/abs/1707.07397). Scale invariance implies, even if all the dimensions of input are multiplied by a common factor, the output is still misclassified. Similarly, the results won't change much, even if tranforms(like geometric/morphological transforms) are applied onto these images. Hence, Adversarial Examples is one of the critical research areas in ML Security. The [cleverhans](http://www.cleverhans.io/) library is used to benchmark and evaluate the vulnerability of ML models against adversarial inputs.\n\nI think that Safety of ML models is a very exciting and crucial area in the coming years. It is quintessential to resolve these issues for the realization of products like Autonomous Driving Vehicles and Personal Robots. As AI systems make deeper inroads into our lives, ensuring the safety and reliability of these systems is of paramount importance. Please do put in your comments/suggestions regarding this article below.",
        "author_name": "Chenna Keshava B S",
        "slug": "safety-issues-in-ai-systems"
    },
    {
        "layout": "post",
        "title": "Water Memory, What A Memory",
        "author_github": "ayush9753",
        "date": "2017-09-11T10:00:00",
        "image": "/assets/img/",
        "description": "Know about this very new form of memory-the water memory concept and its significance.",
        "tags": [
            "Water Memory"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "ayush9753",
        "text": "[{% load static %}]()\nWhat would you preferably use to store your precious data - a hard disk, a pen-drive, a DVD; or perhaps WATER!\n\nScientists from Germany now believe that water has a memory, meaning that what once was seen as a simple commodity has been closely examined to reveal a scientific revelation, uncovering a mind-blowing truth.\n\n## Theory\n\nIndividual drops of water from the same sample were examined at incredibly high magnification, probably the first experiment of this kind and surprisingly scientists were able to physically see that each droplet of water has its own _individual microscopic pattern,_ distinguishable from the next and uniquely beautiful.\n\n_Different droplets possess varying_\n\n_physical structure under high resolution_\n\n## Experiments\n\nIn a scientific experiment, a group of students were made to obtain one drop of water from the same body, all at the same time. On examination, each of them produced different images.\n\nIn the second experiment, a real flower was placed into a body of water, and after a while droplet from the same were sampled and examined. The result produced a mesmerising pattern when hugely magnified, but all of the droplets of this water looked very similar. When the same experiment was one with a different species of flower, the magnified droplet looked completely different, thereby determining that a _particular flower is evident in each droplet of water._\n\n## Conclusion\n\nThrough this discovery which shows that water has a memory, according to scientists, a new perception of water can be formed. The German scientists believe that as water travels it picks up and stores information from all of the places that it has travelled through, which can thereby connect people to a lot of different places and sources of information depending on the journey that it has been on.\n\nThis has even been compared to the human body, of which each is incredibly unique and has an individual DNA unlike any other. Whilst the human body is made up of 70% water, conclusions could be drawn from these new discoveries that human tears _can hold a unique memory of an individual being_, through the body&#39;s store of water hosting a complete store of information that is linked to individual experience suggesting that everyone is globally connected by the water in the human body which travels through ongoing journeys, whereby information along the way is always stored.\n\n\n## Reference: \n* Jess Murray Truth Theory\n* This video will give you a brief insight into the basic of water - Memory Theory.\n[Youtube](https://www.youtube.com/watch?v=ILSyt_Hhbjg&feature=youtu.be)",
        "author_name": "Ayush G S",
        "slug": "water-memory-what-a-memory"
    },
    {
        "layout": "post",
        "title": "You don't really know Arduino",
        "author_github": "vibhoregit",
        "date": "2017-09-11T23:32:44",
        "image": "/assets/img/",
        "description": "Are you tired of the monotonic simplicity of Arduino? Well this article might add an extra dimension for you to explore.",
        "tags": [
            "Arduino"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "vibhoregit",
        "comments": true,
        "text": "[{% load static %}]()\nIs he kiddin? Well I am not (just be patient enough to reach the end). Arduino is a very good platform to start making **witty** electronics projects instead of dumb L.E.D blinking circuits (apologies to the 555) but after some time electronic hobbyists loose interest due to one or more of the following reasons:\n* It is too simple\n* It's performance is limited\n* It does not feels like embedded programming\n* There is a library for literally everything (sounds like an advantage?)\n  \nWell the amount of abstraction Arduino offers hides vital details of the micro-controller that powers the board. \"*It Kills the Spirit of Embedded Programming.*\" And thus people tend to move over to MSP430 and PIC development boards without appreciating the power of AVR controllers (Yup it's the family of controllers that power Arduino UNO, Mega, Nano, Mini and Duemilanove). And while people do use ATMEL's (now owned by Microchip) AVR Studio to program these boards, it's yet another IDE meant only for AVRs. Arduino on the other hand with proper libraries and compiler integration acts as a standalone IDE for micro-controllers by different manufacturers like the popular ESP8266. *Arduino does have advantages over its primitive alternatives.*\n\n### What's my point then?\n\nMy point is that Arduino is good for building projects quickly and hassle-free but it does not let's the user learn the details of the micro-controller. The user friedly libraries are __not so learner friendly__ because they make you lazy. Every one using Arduino be like \"*hmmm so I need to control a servo(or anything) well I can use timers in arduino but wait...*\"\n\n```c\n#include <servo.h> \n```\n\"*Done!*\"\n\nSo to learn Arduino one needs to stop using the user-friendly abstract fuctions of Arduino and get down a level deeper into the realm of Embedded C coding. And to one's comfort Arduino IDE supports it out of the box. (right after installation?)\n\n### The conservative face of Arduino\n\nNot many people might know but Arduino is **backward compatible**. For those who do not come from programming background (like myself) backward compatibility means support for primitive functions on which the newer abstract functions are based. For example the function cout and cin in C++ are the abstract functions while scanf and printf are the primitive ones. C++ however supports both so if performance is a major concern using printf (which is faster) instead of cout might save you a couple of micro-seconds. The analogy in embedded implies that instead of digitalWrite(8,HIGH), PORTB = 0b00000001 will do the exact same thing but a lot faster. \n \nAnother example is the widely used analogWrite() function in arduino that outputs a PWM signal with a rather low frequency of 980 Hz and 490 Hz which might be slow depending on the application. In such a case instead of using the rather good looking analogWrite(), set the clocks to the counter units manually to achieve frequencies as high as 200KHz. All this might sound very simple to a person with a lot of experience programming micro-controllers but where does a noob starts from? Don't worry, I got your back.\n\n### How and where to begin?\n\nTo start off how about reading some theory on the architecture? Sounds very interesting right? Well yeah it might not be the most entertaining part but it is indeed an essential part (especially the register view of peripherals). A basic image of the architecture in mind allows the programmer to use architectural exploits to make the code run faster. \n\n* Quality tutorials can be found on [avr-tutorials](http://www.avr-tutorials.com/) with examples. Added bonus is their assembly equivalent for every line of C code.\n\n* Ever feel lost? All good projects are complimented with ever helping communities. People at [AVRfreaks](http://www.avrfreaks.net/) will be glad to help you.\n\n* Although there will be examples on internet explaining how to manipulate registers to do certain tasks (like setting up ADC) it is recommended to figure that out using the [data sheet of the micro-controller used in Arduino.](http://www.atmel.com/products/microcontrollers/avr/megaavr.aspx) \n\n\n### An Example\nEnough with the blabbering, lets look at a simple example.\n\n```c\n#define BLINK_DELAY_MS 1000\nvoid setup()\n{\n //set pin 5 of PORTB to output\n DDRB |= _BV(DDB5);\n }\n \nvoid loop()\n{\n  //set pin 5 high to turn led on\n  PORTB |= _BV(PORTB5);\n  _delay_ms(BLINK_DELAY_MS);\n  //set pin 5 low to turn led off\n  PORTB &= ~_BV(PORTB5);\n  _delay_ms(BLINK_DELAY_MS);\n}\n```\n\nIf you copy-paste the above code to Arduino IDE and upload it, it will mimic the classic Blink LED  code. The catch is that the built in Blink example takes 928 bytes of program memory while the above code takes just 486 bytes. (Almost half the space!)\n\n![ATmega 328P pinmapping to Arduino UNO Pins](https://i.stack.imgur.com/NxqQs.jpg)\n\n### Comparison of performance\n\nStill not convinced about the boost in performance possible by coding using primitive methods? Julian Ilett did a nice [comparison video](https://www.youtube.com/watch?v=U7I0GkwW1yE) so you might want to check that out. \n\nIf you think that speed and space is the only argument then let me include power considerations. AVR micro-controllers come with a bunch of peripherals that may or maynot be used by your code. In a scenario where the entire setup runs on a battery you might want to shut down unused peripherals (like ADC or counter units) to save power and increase the running time. Kevin Darrah's [video](https://www.youtube.com/watch?v=urLSDi7SD8M) will help you appreciate the power saver modes the micro-controller offers.\n\nOn the already fantastic recipe of Arduino the icing is yet to come. You can code the __blocks that need to be fast__ for your project using primitive C codes and still use the abstract arduino functions for the __less time critical part__ of the code. For example, If you are working on a fast multiplexing application like an LED cube where data is sent to Arduino from PC, you would want to make sure that multiplexing is done at a very high speed (enough to fool human eyes at least) whereas you can still use the Serial.read() function to fetch data over serial port. \n\n>A word of advice: While programming in such a mixed style make sure the code doesn't create conflicts with itself (like trying digitalWrite() on a pin which is configured to read analog values).  \n\n### Ready, Steady, Go!\n\nCongratulations if you made it till the end of this blog. Hopefully your arduino sketches are more optimized and gives you that _sense of pride_. Now what are you waiting for? Find your arduino and maybe make an 8x8x8 multiplexed L.E.D cube or something! \n\n![Credits AVRfreaks](http://www.avrfreaks.net/sites/default/files/4CubeWave.gif)",
        "author_name": "Vibhore Jain",
        "slug": "you-dont-really-know-arduino"
    },
    {
        "layout": "post",
        "title": "BitTorrent Explained",
        "author_github": "mahirjain25",
        "date": "2017-09-18T10:00:30",
        "image": "/assets/img/",
        "description": "A post explaining the basic concepts of BitTorrent",
        "tags": [
            "Networking",
            "Torrents",
            "P2P"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "mahirjain25",
        "text": "[{% load static %}]()\n>\"Just torrent it!\"\n> \n>\"You can torrent any type of file, its all there on the internet...\"\n\nThese are just some of the common sentences one hears when it comes to downloading files from the internet. However, many people do not even know what BitTorrent is, much less know how it works.(It includes uploading files as well)\n\n## A simple introduction\n\n![BitTorrent Logo]({% static '/blog/assets/img/BitTorrent-Explained/pic1.png' %})\n\nBitTorrent is a communication protocol(set of rules) that is used for sharing all types of data and electronic files over the internet. This falls under the category of [peer-to-peer file sharing](https://en.wikipedia.org/wiki/Peer-to-peer_file_sharing).\n\nNow, to implement this protocol, we need:\n\n* A device connected to the internet\n\n* A program known as a **BitTorrent Client**(Example- \u03bcTorrent,BitComet,etc.)\n\n\n## How it works\n\n![Operation Explained]({% static '/blog/assets/img/BitTorrent-Explained/pic2.jpg' %})\n\nUsing BitTorrent is different from any other common download or upload. Usually, users download from a single host server, but this protocol allows users to become a host , joining several other such user-hosts in the \"swarm\", and upload as well as download files all at the same time. Consider the following simple example:\n\nSeveral thousand users wish to download a large video file. The user responsible for uploading this file initially, becomes the _seeder_. The users who wish to download this file are known as _leachers_.\n\nNow, each of the leachers begin to download segments/pieces of the file from the seeder. As they do so, they simultaneously begin to upload the pieces they receive to other users in need. Each of these pieces are downloaded non-sequentially, and this is where the client comes in. It **rearranges the pieces** so that they are in order, and **determines what pieces are yet to be recieved.**\n\nOnce the file is completely downloaded, the leacher becomes an additional seeder, and the greater the number of such transitions, the better the \"health\" of the file.\n\n\n## Why is this beneficial?\n\n* The data pieces are all of the same size. Hence, one can pause their download, and resume at any later point of time. There is **no loss in information.**\n\n* The **bandwidth requirement is significantly reduced**. This prevents an increase in internet traffic, and all users receive greater speed for data transfer.\n\n\n## Putting it all together\n\nIt is quite clear how using this protocol to transfer data is different than the orthodox method. The download in this case is in a random order, whereas it is sequential otherwise. There are security concerns that have been raised about the misuse of the technology, as it is harder to track down several different hosts rather than a single host. \n\nFor further reference, you can read up on [Wikipedia](https://en.wikipedia.org/wiki/BitTorrent#Technologies_built_on_BitTorrent) or watch this video on [YouTube](https://www.youtube.com/watch?v=urzQeD7ftbI)",
        "author_name": "Mahir Jain",
        "slug": "bit-torrent-explained"
    },
    {
        "layout": "post",
        "title": "Storing energy in molten salt",
        "author_github": "adilamrajiv",
        "date": "2017-09-18T10:00:00",
        "image": "/assets/img/",
        "description": "An article describing an interesting energy storage alternative",
        "tags": [
            "Energy",
            "Research",
            "Innovation"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "adilamrajiv",
        "text": "[{% load static %}]()\n## Introduction\n\n\nRenewable energy is the answer to sustainable development. It is the hot topic of our generation and even though it is in its rudimentary stage, it is the best solution in maintaining the balance between environment and our energy needs. We all know that a lot of research is going on in this field, whether it is the oft talked about and used solar and wind energy, or the slightly less used ocean thermal energy.\n\nBut, what do you do after you produce this energy? What if it is in excess to your current need? You need to store it because\u2026 Frankly, it would be a waste of resources otherwise. And this is the need of the hour.\n\nA lot of this storage is now being done on Lithium ion batteries but we\u2019re still trying to find new ways to store it which is more efficient, environment friendly and cost effective as well. One of these methods is storing energy in molten salt.\n\n## What is the molten salt technology?\n\nBasically, it stores energy in molten salts. Salt at the room temperature is brought to a higher temperature by heating and hence converting it into its molten state. The energy is stored as the latent heat of the salt. This energy being stored can be used as and when required.\n\nAlphabet\u2019s \u2018Project Malta\u2019 have used this concept to store energy from wind and solar. The set up consists of 4 large, insulated vats, 2 containing salt and the other 2 containing antifreeze. The electrical energy produced from these sources is used to create 2 separate streams of hot and cold air. These streams are used to respectively heat the salt and cool the antifreeze. When we need to use this energy, the 2 reservoirs can be used to produce hot and cold streams of air that leads to the formation of powerful gusts; this can then be used to run the turbine. The advantage of this method is that it is non-toxic and chemically inert.\n\nSolar Reserve also uses the same basic concept. Here, we have a tank with a salt which is a mixture of sodium and potassium nitrate. The heat energy from the sources is used to heat up the salt and then it is maintained at a higher temperature of around 560\u02daC. To use this energy, it is sent to a heat exchanger which produces steam at high temperature and pressure. This is then used to run a turbine.\n\nApart from this, a lot of research is also going on in this field all over the world. The University of South Australia recently won the ANSTO Eureka Prize for Innovative Use of Technology for their ground-breaking work in this field.",
        "author_name": "Adila M Rajiv",
        "slug": "storing-energy-in-salt"
    },
    {
        "layout": "post",
        "title": "What makes a Smart Home Smart",
        "author_github": "anirudhbh",
        "date": "2017-09-18T10:00:00",
        "image": "/assets/img/",
        "description": "Explore this new world of smart homes",
        "tags": [
            "Smart homes",
            "IoT"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "anirudh0707",
        "text": "[{% load static %}]()\n## Introduction\n\nWhen you leave your home have you been bothered by these little doubts like have I turned the water heater off? Did I set the security at home? Did I switch off the TV?\n\nWith the introduction of smart homes these will no longer be a bother to us anymore. Any electrical component, may it be a fan or your TV or even your coffee maker, every electrical appliance in our homes can be hooked up to the home network. This will basically allow us to control our entire house with our laptops or something as simple as our smart-phone. These smart homes are now very common and have now started to create a revolution.\n\n## What is the principle behind it?\n\nSo people generally ask themselves, what exactly makes these smart homes so smart?\n\nWell, it's due to the ability to connect multiple devices to your net and then wirelessly use them. This allows us to easily give commands to the system and control them.\n\nHow is this possible you ask?  It's all thanks to The  Internet of Things. This basically allows a person to interconnect multiple things to the internet and then use them to exchange data with one another and execute appropriate commands according to the data. You may find yourself what these \"things\" are. These \"things\" (also known as smart devices) could be anything ranging from sensors to even our daily appliances.\n\n## What actually happens in the network?\n\nWhat do you think happens when you send a command to one your appliances?\n\nLet's visualize this with an example.\n\nNow you want to turn one of your appliance, let's say your water heater. You will send a command through one of your transmitters ( phone, laptop or even one of your interfaces that is connected to the network) which will be later sent to the receivers with the help of the network. Two of the most used networks are Z-ware and ZigBee.\n\nThese transmitters will first issue a command to the system its giving the command. Then it will identify the corresponding device with its item number. Once confirmed it will send the main command, that is the OFF command which will alter the state of the appliance and thereby allowing you to control the working of the device.\n\n## How does this help Us?\n\nNow let's look at how these homes work and how they help us.\n\n![Smart Homes]({% static '/blog/assets/img/what-makes-a-smart-home-smart/Smart homes.jpg' %})\n\nWith the help of this picture we can get a pretty good picture as to how a smart house works. So basically in a smart house all the appliances are connected to the net and with the help of controllers like laptop, mobile phones and other interfaces we can controls all the appliances wirelessly. It is not only about controlling them wirelessly. They can be automated as well so that we need not bother with all the minor details in our day to day life.\n\nHaving doubts? Let us take an example.\n\nAll of us find it hard and very troublesome to go and buy grocery for our kitchen when they get over. Now imagine a situation where you need not do any of this. You can now automate your refrigerator to sense when your supplies are getting over and then directly place an order for new stock of supplies. This would save so much time and effort.\n\nThis is just the beginning. You would be able to control all the LEDs in the house, adjust their color and brightness as you would like it. You can even deactivate all your appliances except your security systems when you are leaving.\n\n![Control]({% static '/blog/assets/img/what-makes-a-smart-home-smart/Control.jpg' %})\n\nCombined with a smooth interface this system is really user friendly and really reduces the amount of work that we do and not only saves our time but also saves our other resources as well. All these factors make these smart homes really smart and a great place to live in.\n\nFor further references about Smart home systems and the principles\n\n1. [Smart Home](http://home.howstuffworks.com/smart-home1.htm)\n2. [Home Automation](https://en.wikipedia.org/wiki/Home\\_automation)",
        "author_name": "Anirudh B H",
        "slug": "what-makes-a-smart-home-smart"
    },
    {
        "layout": "post",
        "title": "The Challenger Disaster",
        "author_github": "nikhilgovindarajan",
        "date": "2017-09-24T10:00:30",
        "image": "/assets/img/",
        "description": "Examining a space exploration disaster",
        "tags": [
            "Space",
            "NASA",
            "Case Study"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "nikhilgovindarajan",
        "text": "[{% load static %}]()\n## Introduction\n\nOn January 28<sup>th</sup> 1986, the scheduled launch for NASA\u2019s space orbiter- the Challenger, took place as planned. But the launch was the only part of the mission that stuck to plan. 73 seconds after liftoff, the shuttle exploded and broke up, bringing a devastating end to the spacecraft\u2019s 10<sup>th</sup> mission, claiming all 7 lives onboard. All because of a tiny, rather overlooked aspect of a structure failing due to a very basic property of the given material.\n\nAfter extensive failure analysis, it was determined that the failure of 2 rubber O-rings caused the disaster.\n\nTo understand why the O-rings failed, the answer lies in a basic concept of polymer materials- the glass transition temperature. When a polymer is cooled below this temperature, it becomes hard and brittle, like glass. The temperature at which this happens is called glass transition temperature( T<sub>g</sub>). Each and every polymer material has a glass transition temperature that varies with its structure, substituent groups, molecular mass etc.\n\n## An analogy to make things clear\n\nA good way to understand why this transition occurs is to use an anology of snakes. Polymers exist as highly coiled chains. Consider a room full of snakes. Let each snake represent a polymer chain. We know that snakes are cold blooded. When it\u2019s warm, snakes move more, slither and slide over each other easier. The snakes move randomly. Now, when the temperature is reduced, snakes generally don\u2019t move much. They slow down and tend to stay still. They\u2019re still wrapped around, over and under each other, but there is no motion. Polymer chains work similarly. This lack of movement in the polymer chains causes it to become a brittle mass.\n\n## Scientific background to the problem\n\nThe O-Ring in the Challenger shuttle was made of nitrile rubber- a synthetic variant of natural rubber. The glass transition temperature of nitrile rubber used was around 1-2<sup>o</sup> C. Due to the cold ambient temperature along with the ice collected on the launch pad due to some unusual temperatures, the rubber became a brittle mass losing its ability to seal the fuel tanks properly. This allowed leakage of exhaust gases, which caused uncontrolled combustion between the liquid hydrogen and oxygen, destroying the fuel tanks and causing the shuttle to break apart.\n\nIt is interesting to note that the engineers involved in the design of the shuttle did warn their superiors about certain components of the shuttle failing at low temperatures but, for whatever reasons, these warnings were unheeded. This was also the first time NASA launched shuttles at such low temperatures. The coldest temperature of a previous launch was 11<sup>o</sup>C higher.\n\n## Impact of the disaster\n\n> Almost a billion US dollars and, more importantly, seven lives were lost because world renowned scientists and engineers forgot to keep in mind basic concepts of simple materials. Law suits were filed against both NASA and the company responsible for designing the rocket boosters and the tanks- Morton Thiokol.\n\nTo this day, this incident is a case study in many discussions of engineering safety and workplace ethics. This incident only goes on to prove the importance of materials used when designing structures.",
        "author_name": "Nikhil Govindarajan",
        "slug": "challenger-disaster"
    },
    {
        "layout": "post",
        "title": "Getting Started with GUIs",
        "author_github": "archit-p",
        "date": "2017-09-24T10:00:30",
        "image": "/assets/img/",
        "description": "Stepping into software development",
        "tags": [
            "GUI",
            "GTK+",
            "Gnome"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "archit-p",
        "text": "[{% load static %}]()\nWhile learning a programming language, there\u2019s often a question of how simple problem solving using code is going to help us create software we see around us. The fact is that software engineering requires a lot more skills as a programmer. And a good first step to creating software is to learn how to create graphical user interfaces, commonly referred to as GUIs. In this article, we\u2019ll learn how to get started with creating GUIs in C using a toolkit known as GTK+.\n\n## What are *GUIs*?\n\nAccording to Wikipedia, the graphical user interface, is a type of user interface that allows users to interact with electronic devices through graphical icons and visual indicators such as secondary notation, instead of text-based user interfaces, typed command labels or text navigation. GUIs are one way to make programs interactive. Almost every program we use has a GUI. Having one makes a program easier to understand for users.\n\n![Graphical User Interface (GUI) vs Command Line Interface (CLI)]({% static '/blog/assets/img/getting-started-with-gui/guivscli.jpg' %})\n\n*What are GUIs? : Graphical User Interface (GUI) vs Command Line Interface (CLI)*\n\n## Introduction to GTK+\n\nGTK+ or GIMP Toolkit is a toolkit for creating GUIs using widgets. Initially, development of GTK+ was started for use with the GIMP image manipulation program. Later, the toolkit was expanded for use with other software as well. It\u2019s currently one of the best toolkits for creating GUIs in C. And since it\u2019s free software, it can be freely used and softwares created using GTK+ can be freely distributed. GTK+ is based upon four libraries - *GLib*, *Pango*, *Cairo* and *ATK*. Each user interface created using GTK+ consists of widgets, since GTK+ is essentially a set of widgets.  \n\n## Getting Started with GTK+\n### Installing GTK+\nOpen up terminal and type the following command in. Enter your password when asked for.\n\n```shell\narchit@archit-len-510s ~ $ sudo apt-get install build-essential libgtk-3-dev\n```\n\n\nOnce you have these packages installed, it\u2019s time to start coding and creating our first GTK+ application.\n\n### Hello, world\nWe wish to create a simple program which opens up in a window, displays a button which says \"Hello, world\" and closes the window when clicked.\nThis we shall achieve using the given code.\n\nOpen up a text-editor and copy the following code into it, and save it as *helloworld.c*.\n\n```C\n#include <gtk/gtk.h>\n\nstatic void print_hello (GtkWidget *widget, gpointer data)\n{\n  g_print (\"Hello World\\n\");\n}\n\nstatic void activate (GtkApplication *app, gpointer user_data)\n{\n  GtkWidget *window;\n  GtkWidget *button;\n  GtkWidget *button_box;\n\n  window = gtk_application_window_new (app);\n  gtk_window_set_title (GTK_WINDOW (window), \"Window\");\n  gtk_window_set_default_size (GTK_WINDOW (window), 200, 200);\n\n  button_box = gtk_button_box_new (GTK_ORIENTATION_HORIZONTAL);\n  gtk_container_add (GTK_CONTAINER (window), button_box);\n\n  button = gtk_button_new_with_label (\"Hello World\");\n  g_signal_connect (button, \"clicked\", G_CALLBACK (print_hello), NULL);\n  g_signal_connect_swapped (button, \"clicked\", G_CALLBACK(gtk_widget_destroy), window);\n  gtk_container_add (GTK_CONTAINER (button_box), button);\n\n  gtk_widget_show_all (window);\n}\n\nint main (int argc, char **argv)\n{\n  GtkApplication *app;\n  int status;\n\n  app = gtk_application_new (\"org.gtk.example\", G_APPLICATION_FLAGS_NONE);\n  g_signal_connect (app, \"activate\", G_CALLBACK (activate), NULL);\n  status = g_application_run (G_APPLICATION (app), argc, argv);\n  g_object_unref (app);\n\n  return status;\n}\n```\nNow, in order to compile the code, we need to tell the compiler where to find the GTK+ header files and libraries. This is done with the *pkg-config* utility.\n\nThe following code demonstrates how *pkg-config* is used.\n\n```shell\narchit@archit-len-510s ~ $ pkg-config --cflags gtk+-3.0 -pthread\n-I/usr/include/gtk-3.0 -I/usr/lib64/gtk-3.0/include -I/usr/include/atk-1.0\n-I/usr/include/cairo -I/usr/include/pango-1.0 -I/usr/include/glib-2.0\n-I/usr/lib64/glib-2.0/include -I/usr/include/pixman-1 -I/usr/include/freetype2\n-I/usr/include/libpng12\n\narchit@archit-len-510s ~ $ pkg-config --libs gtk+-3.0 -pthread -lgtk-3 -lgdk-3\n-latk-1.0 -lgio-2.0 -lpangoft2-1.0 -lgdk_pixbuf-2.0 -lpangocairo-1.0\n-lcairo -lpango-1.0 -lfreetype -lfontconfig -lgobject-2.0 -lgmodule-2.0\n-lgthread-2.0 -lrt -lglib-2.0     \n```\nNow we are ready to compile our first program.\n```shell\narchit@archit-len-510s ~ $ gcc `pkg-config --cflags gtk+-3.0` -o\nhelloworld helloworld.c `pkg-config --libs gtk+-3.0`\n```\n\nNow the program can be run using the command,\n```shell\narchit@archit-lev-510s ~ $ ./helloworld\n```\n\n### Understanding the program\n\nGTK+ uses layouts to store the UI elements. First we create a widget for window using *gtk_application_window_new(app)*. Then in order to store the *button*, a *button_box* is used. *GtkButtonBox* is created for this purpose. This newly created button box is then added to the window widget. The button itself is then inialized using *gtk_button_new_with_label()* which returns a *GtkButton* to be stored inside *button*. Later *button* is added to *button_box*. Next we need to add functionality to close the app once the button is clicked. This is achieved using *g_signal_connect()* which is connected to *button*. After connecting *print_hello()*, another signal is connected to \"clicked\" state of the button. This calls the *gtk_widget_destroy()* and takes *window* as parameter. This destroys the window when the button is clicked. This completes the functionality we require for our simple applcation.\n\n![GIMP]({% static '/blog/assets/img/getting-started-with-gui/gimp.jpg' %})\n\n*Going Forward : GIMP - a popular Linux program made using GTK+*\n\n## Going Forward\nWe just created a simple program using GTK+. This has taught us how to respond to clicks and perform a function. And also that the basis for all UIs is a simple widget based structure. Understanding these two things can help us create usable programs. GTK+ is the toolkit used to code all of Ubuntu's GUI and most programs used on Linux and Windows. Hence it's capabilities shouldn't be underestimated. Learning the toolkit, can equip us to create complex UIs such as those seen on popular applications.",
        "author_name": "Archit Pandey",
        "slug": "getting-started-with-gui"
    },
    {
        "layout": "post",
        "title": "Quantum Computers and The Future of Computation",
        "author_github": "sampathkoti",
        "date": "2017-09-24T10:00:30",
        "image": "/assets/img/",
        "description": "A look at the future of computing",
        "tags": [
            "Quantum Computing"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "sampathkoti",
        "text": "[{% load static %}]()\n## Background:\n\nQuantum Mechanics a realm of physics where the tiny subatomic particles reside and whose physics is beyond the perception of the classical physical world.\n\n### 1] What makes this realm so weird and different in working?\n\nThe behaviour working of these particles   was resolved by proposing of a mathematical model governed by the principle of Superposition, entanglement and tunnelling.\n\nQuantum Computers makes use of these physical phenomena to do it calculation and this is where the speciality of the futuristic computing machines lies.\n\n![quantum computer]({% static '/blog/assets/img/quantum-computers/1.jpg' %})\n*The first ever made quantum computer*\n\n## Breaking The Myth of Superposition Theorem\n\nBefore dwelling into the topic, I would like to make one thing clear the superposition as taught by many is not a particle being at different position at a given time but it is the probability of the particle wave density being distributed at different positions in space time but does exists at only one found position at that given time.\n\n### Qubits\n\nA quantum computer maintains a sequence of qubits. A single qubit can represent a one, a zero, or any quantum superposition of those two qubit states a pair of qubits can be in any quantum superposition of 4 states, and three qubits in any superposition of 8 states. In general, a quantum computer with **N** qubits can be in an arbitrary superposition of up to 2^N{\\displaystyle 2^{n}}different states simultaneously.\n\nThe qubits can be made of quantities which obey the principles of Quantum Mechanics i.e. an electron spin or a proton spin can be considered as qubits where spin up represents a 0 state and spin down represents a 1 state or they can also be represented using polarised light meeting its applicability.\n\n![qubits]({% static '/blog/assets/img/quantum-computers/2.jpg' %})\n\n## What Makes Quantum Computers so Special?!\n\nThe ability of these machines to perform tasks in seconds or minutes that would require years and years or wait!!  Sometimes impossible to perform with even the so called present day super computers makes them special.\n\n### Speed Of Quantum Computers And Shor's Algorithm\n\nThe enhanced speed of quantum computers in relative to the classical computers is the way that the problems are being solved i.e. using the basic laws that govern the realm of Quantum Mechanics.\n\nI will try and give an analogy for the methods.\n\n### 1} The Football Analogy For Shor's methods\n\nSuppose say we need to calculate the height from which a soccer player needs to kick the ball with certain force at a fixed distance from the goal, so that he scores a goal.\n\n### What Would Our Classical Computer Basically Do?\n\nIt takes in the specified value of the force applied at specific angle and the distance from the goal. Each time the ball is placed at different values of height and checks whether it would reach the goal or not. So, say there are heights from 0-500 and assuming it does calculations only for integers (just to simplify), computing needs to be done for 500 cases and the sample out the right one to give the output, that means it takes 500 steps to complete the task give.\n\n*What Would Our Quantum Computers Do?*\n\nSay the same initial condition are given to the quantum computers, because of its speciality of handling properties like superposition, it would distribute the probability of football being at various places from 0-500 simultaneously and checks   whether it would reach the goal or not and give the output in just one step.\n\nThis is a simple case consideration, what if have 100000000 positions to check our classical computers would take a back seat, this is where the computational difference.\n\n### 2} The Maze Analogy Grover's Algorithm\n\nConsider a problem at hand is say the computer needs to find the way out\n\n*What Would Classical Computers Do?*\n\nPresent computer puts a person or any object at the centre and tries all different ways to come out of the maze until it finds the right path out, so it need to run these steps several times.\n\n*What Would Our Quantum Computers Do?*\n\nThe quantum computers would work on superposition it would consider the object as if its probability density is spread all over the 3D space and it would consider them as different state probabilities and send it different path to check whether that (ball) object would reach out or not, in this case the computation would also involve the property of entanglement and tunnelling to determine the final out put in a single step.\n\n![maze]({% static '/blog/assets/img/quantum-computers/3.jpg' %})\n\n*A] Shor's algorithm* - exponential speedup of factoring \u2013 Depends on fast quantum technique for finding the period of a periodic function\n\n*B] Grover's algorithm*  \u2013 quadratic speedup of search \u2013 works by gradually focusing an initially uniform superposition over all candidates into one concentrated on the designated element. Speedup arises from the fact that a linear growth of the amplitude of the desired element in the superposition causes a quadratic growth in the element's probability.\n\n\n![encryption]({% static '/blog/assets/img/quantum-computers/4.jpg' %})\n\n## Encryption : Is Quantum Computing A Threat To Encryption\n\nEncryption is the conversion of electronic data into another form, called ciphertext, which cannot be easily understood by anyone except authorized parties.\n\n## When Encryption Meets Quantum Computation\n\nCommunication security today is almost universally ensured by the use of RSA encryption. This method relies on the inaccessibility of large prime factors of a large composite number. The problem is an artificial one: The encrypter takes two (or more) large primes and multiplies them. The decrypter tries to work backwards from the product to the factors. It is hard work. The largest number factored so far RSA-640 had 193 decimal digits and took approximately 30 2.2GHz-Opteron-CPU years, over five months of calendar time. At that rate a 1024-bit number, the size currently recommended by a commercial, would take many years bit is short for binary digit each additional bit contributes a factor of 2 to the size of the calculation). The site adds For more security or if you are paranoid, use 2048 or even 4096 bits.\n\nA quantum computer of suitable size could factor these large numbers in a much shorter time. For a 1024-bit number, Shor's Algorithm requires on the order of 1024 cube, about one billion, operations. I do not have any information on how quickly quantum operations can be executed, but if each one took one second our factorization would last 34 years. If a quantum computer could run at the speed of todays electronic computers (100 million instructions per second and up) then factorization of the 1024-bit number would be a matter of seconds.\n\nThe most interesting quantum algorithm as far as cryptography is concerned is Shor's algorithm, called \"one of the major scientific achievements of the late 20th century\" by renowned complexity theorist Scott Aaronson.\n\nShor's algorithm brings an exponential speed-up for solving the factoring, discrete logarithm (DLP) and elliptic-curve discrete logarithm (ECDLP) problems that are widely used in cryptographic applications. Be it TLS, SSH or IPSec, most secure communication protocols rely one way or another on Diffie-Hellman key agreements (which rely on DLP or ECDLP's hardness), on digital signatures (DSA, ECDSA, or RSA-PSS signatures), or on public-key encryption (ElGamal, RSA-OAEP). Shor's algorithm thus potentially breaks all these algorithms and with them all the public-key cryptography mechanisms deployed on the Internet.\n\nSo is our today's information at threat!\n\nFor now, it would be a yes as present encryption techniques are vulnerable to the unmatched speed of quantum computers.!!!!\n\n## Quantum For A Quantum\n\nOf course, the threat isn't there yet. More importantly, people are beginning to pay attention, including the NSA. The threat to encryption posed by quantum computing isn't unsolvable. The same mechanism that makes it vulnerable can also turn it \"quantum computing-proof\" so to speak.\n\nThere is such a thing as quantum cryptography, which uses photon-based qubits to securely transmit information encoded into the quantum states of particles. This quantum communication makes it possible for the recipient to detect attempts to intercept incoming messages. And it isn't exactly new.\n\nIts applications include what's called the quantum key distribution(QKD). Basically, it uses quantum communication to share keys securely, which will be used to decrypt messages sent over conventional networks. Unfortunately, low bandwidth makes the system currently untenable, despite having been demonstrated to work in several cities.\n\nThis is just one possible work around. Other methods are being developed, including code-based cryptography and lattice-based cryptography. In any case, there's time to improve it. In the same way that quantum computing is still being refined, network infrastructure can be improved to allow for quantum secure cryptography.\n\n![cryptography]({% static '/blog/assets/img/quantum-computers/5.jpg' %})\n\n## For The Interested Ones (Resources)\n\n* [Shor's algorithm](https://www.youtube.com/watch?v=FA21Dj2l3Ac)\n\n* [Maze analogy](https://www.nature.com/articles/ncomms11682)\n\n* [Quantum crypto](http://www.popsci.com/what-is-quantum-cryptography)\n\n* [Crptography](https://www.youtube.com/watch?v=Kf9KjCKmDcU&amp;list=PL7AEDF86AABA1AA9A)\n\n* [IBM](https://www.youtube.com/watch?v=jf7D8snlsnQ&amp;feature=youtu.be)",
        "author_name": "Sampath Koti",
        "slug": "quantum-computers"
    },
    {
        "layout": "post",
        "title": "Biohacking",
        "author_github": "andydandyy",
        "date": "2017-10-02T21:00:30",
        "image": "/assets/img/",
        "description": "Fantasy Realized?",
        "tags": [
            "biology",
            "hacking",
            "biological hacking",
            "superhuman",
            "cyborgs"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "andydandyy",
        "text": "[{% load static %}]()\nEver fantasied becoming the Cyborg from DC comics universe? Well, brace yourself, because that dream is soon to come into reality.  \n\nIntroducing Biohacking, a fairly new practice that could bring major changes in human life as we know it. Biohacking can be defined as a \"social movement involving studying basic biology and exploiting it to make electronic implants made using easily available equipment\". In short, you make a cyborg out of yourself. As \"punk\" as it sounds, this practice caught the attention of media because of the possibilities it opens.  \n\n![RFID chip]({% static '/blog/assets/img/biohacking/1.jpg' %})\n\n**Ron Shigeta** runs Berkeley Biolabs, a biohacking site in Berkeley, California, where dozens of would-be biologists gather frequently to hack around. He says biohacking is \"a freedom to explore biology, kind of like you would explore good fiction.\" As for the hacking part,\n\n> \"Hacking is kind of like the freedom to sort of dig deep into something, just because you're interested in it. The whole idea of biohacking is that people feel entitled, they feel the ability to just follow their curiosity \u2014 where it should go \u2014 and really get to the bottom of something they want to understand.\"  \n\n**Amal Graafstra**, a _leading_ biohacker, sells a kit so that anyone can be a biohacker.  \n\n[Check his website out!](http://amal.net/)\n\nIt maybe Bluetooth implants allowing you to control the electrical devices around you, or headbands which allow you to boost your cognitive senses by sending electric impulses directly to your brain. Just imagine the possibilities. People with no vision can experience the beauty of the world, people with no hearing can experience sounds of nature, people with no olfactory senses can smell and taste. All this, in their own garage!  \n\n![MUSE headband]({% static '/blog/assets/img/biohacking/2.png' %})\n\n**Dave Asprey**, a nutritionist and the CEO of Bulletproof, tries to incorporate biohacking to improve people's senses. [Check his blog out!](https://blog.bulletproof.com/)\n\nNot only can you repair the human body, you can even modify it giving yourself special abilities. This \"social movement\" is bound to create Iron Mans and Cyborgs in the coming future :D.",
        "author_name": "Anand Pathak",
        "slug": "biohacking"
    },
    {
        "layout": "post",
        "title": "The Intriguing World of Chatbots",
        "author_github": "adilashique",
        "date": "2017-10-02T21:33:05",
        "image": "/assets/img/",
        "description": "A peek into the working of Chatbots.",
        "tags": [
            "Chatbot",
            "Chat",
            "Bots",
            "Language Processing",
            "IoT"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "adilashique",
        "text": "[{% load static %}]()\n\ufeff\n>\"Hey Siri, What\u2019s my schedule for today\u201d  \n>\u201cHey Cortana, when is my appointment with the dentist today\u201d\n\nThese are a few phrases that we use casually from day to day in order to make our life a tad bit easier.  In this time of rapid advances in AI and Machine learning Chatbots are one avenue that has been ushered into reality by this magical paradigm that has been growing by leaps and bounds in the past few years. But what if Chatbots can be a vital link in another field that is rapidly gaining traction, namely IoT.\n\n![chatbot]({% static '/blog/assets/img/chatbots/1.png' %})\n\n### What is a Chatbot?\n\nA Chatbot put in simple terms is a computer program that can have a conversation with a human, that is it simulates conversations with human users. Chabot\u2019s use a dialog system to have a conversation with a human.  \nThe first step is converting human input into an understandable context for the Chatbot. This is done through input recognizers and decoders, which can analyze speech, text and even gestures. The next step is applying *Natural Language Processing* to check the user input and analyze the meaning of what he/she has said or written. All the while the input is managed and processed by a dialog manager to ensure a proper exchange of information from and to the user.\n\n### Neural Networks\n\nNeural networks are one of the prime algorithms that are used by Chatbots to get as close as it can to mimicking a human while in conversation with a human participant. They consist of different layers for analyzing and learning data. Every hidden layer tries to detect patterns in the conversation. The more layers in a neural network, the more is learned and the more accurate the pattern detection is. Neural Networks learn and attribute weights to the connections between the different neurons each time the network processes data.  So the next time it encounters a similar dialogue it will be ready with answer instantly.\n\n### Chatbots and IoT\n\nApart from their existing use in mobile applications one interesting field of utilization would be in IoT based applications. Chatbots can act as an interface and thus improve the user experience. For example,  Amazon Echo is an example of integrating speech in your chatbot experience, it will allow you to order anything from the Amazon web shop. A new report, states that Amazon has sold near 3 million Amazon Echo smart speakers which supports the belief that chatbots are the future of IoT and retail, and it will be interesting to see what else retailers come up with in the coming years.",
        "author_name": "Adil Ashique",
        "slug": "chatbot"
    },
    {
        "layout": "post",
        "title": "Fingerprint Recognition",
        "author_github": "manan-sharma",
        "date": "2017-10-02T21:35:05",
        "image": "/assets/img/",
        "description": "Insight into the working of Fingerprint Recognition",
        "tags": [
            "fingerprint",
            "sensors",
            "fingerprint sensor"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "manan-sharma",
        "text": "[{% load static %}]()\nFingerprint recognition systems are practically everywhere today, from your smartphone, laptops, and are an integral part of the Aadhar system enabling us to authenticate our identity easily. Here's a look at how the whole system of fingerprint recognition works.  \n\nFingerprints are the tiny friction ridges on the ends of our fingers and thumbs make it easier to grip things. By making our fingers rougher, these ridges increase the force of friction between our hands and the objects we hold, making it harder to drop things. Unless you have accidents with your hands, your fingerprints remain the same throughout your life.\n\n### Why Fingerprints?\n\nFingerprints in are a brilliant way of telling people apart because fingerprints develop through an essentially random process according to information encoded in our DNA. Even the fingerprints of two identical twins are slightly different. Its is however possible that two people may have the same fingerprint, however the chances of this happening are 1 in 64 million. Such chances are really small to worry about, hence fingerprints are widely used to authenticate identity.\n\nOk so now does does an electronic device recognize fingerprints? It involves two components, the physical sensing(hardware) part, and the analysis and verification(software part).\n\n### The Sensor\n\nThe first part of this system involves recognizing and creating a digital version of your fingers. Three types of such systems are used:\n\n1. Optical Scanners  \nThey are the oldest method of capturing and comparing fingerprints. It relies on capturing an optimal image (basically a photograph), and uses algorithms to detect unique patterns on the surface, such as ridges or any unique marks, by analysing the lightest and darkest areas of image. These scanners are bulky(from the viewpoint of smartphones) as they pack a very high number of diodes per inch to capture details up close. However, the major disadvantage of such a scanner is that it be easily fooled with a 2D picture of good enough quality. So such systems are being slowly phased out, for better ones, like the capacitive scanners.\n![1]({% static '/blog/assets/img/finger-prints/1.jpg' %})\n\n2. Capacitive Scanners  \nThe most commonly found type of fingerprint scanner used today is the capacitive scanner. Again the name gives away the core component, the capacitor. Instead of creating a traditional image of a fingerprint, capacitive fingerprint scanners use arrays tiny capacitor circuits to collect data about a fingerprint. As capacitors can store electrical charge, connecting them up to conductive plates on the surface of the scanner allows them to be used to track the details of a fingerprint. The charge stored in the capacitor will be changed slightly when a finger's ridge is placed over the conductive plates, while an air gap will leave the charge at the capacitor relatively unchanged. An op-amp integrator circuit is used to track these changes, which can then be recorded by an analogue-to-digital converter. Once captured, this digital data can be analysed to look for distinctive and unique fingerprint attributes, which can be saved for a comparison at a later date. What is particularly smart about this design is that it is much tougher to fool than an optical scanner. The results can't be replicated with an image and is incredibly tough to fool with some sort of prosthetic, as different materials will record slightly different changes in charge at the capacitor. The only real security risks come from either hardware or software hacking.  \n![2]({% static '/blog/assets/img/finger-prints/2.jpg' %})\n3. Ultrasonic Scanners  \nThis is one of the latest fingerprint scanning technology, being actively developed by Qualcomm, and they are calling it Sense ID technology. The idea here is to capture the details of a fingerprint using a ultrasonic transmitter and a receiver. An ultrasonic pulse is transmitted against the finger that is placed over the scanner. Some of this pulse is absorbed and some of it is bounced back to the sensor, depending upon the ridges, pores and other details that are unique to each fingerprint. A sensor that can detect mechanical stress is used to calculate the intensity of returning ultrasonic pulse at different points on the scanner. Additional depth data can be captured if scanned for longer time duration.  The 3D nature of this capture technique makes it an even more secure alternative to capacitive scanners.\n\n [![Embed video](https://img.youtube.com/vi/FtKKZyYbZtw/0.jpg)](https://www.youtube.com/watch?v=FtKKZyYbZtw)\n\n### Fingerprint matching\n\nOkay, so now we have the fingerprint with us. We now need to extract unique information out of it.\n\nAccompanying the physical scanner is a dedicated IC that deals with interpreting the scanned data and transmitting this in a useful form to your smartphone's main processor. Different manufacturers use slightly different algorithms to identify key fingerprint characteristics, which can vary in speed and accuracy.\n\nTypically these algorithms look for where ridges and lines end, or where a ridge splits in two. Collectively, these and other distinctive features are called minutiae. If a scanned fingerprint matches several of these minutiae then it will be considered a match. Rather than comparing the whole fingerprint each time, comparing minutiae reduces the amount of processing power required to identify each fingerprint, helps avoid errors if the scanned fingerprint is smudged, and also allows the finger to placed off-centre or be identified with only a partial print.\n\nOn smartphones, the above information is kept safe on the device, away from the code that could compromise it. Most manufacturers allocate a secure area for keeping this secure data on a separate part of the processor that cannot be accessed by apps operating in the regular operating system environment. Any authentication request is handled by strong cryptographic protocols, such as FIDO(Fast IDentity Online) Alliance. They enable the requesting service to verify the identity, without the requesting service being able to actually read/process our fingerprint scan.\n\nFingerprint scanners have become quite a secure alternative to remembering countless passwords, and with the rollout of services like Samsung Pay, Apple Pay, these scanners are likely to become a more common and crucial security tool in the future.",
        "author_name": "Manan Sharma",
        "slug": "fingerprint-recognition"
    },
    {
        "layout": "post",
        "title": "Aircraft Engines",
        "author_github": "mithunraj",
        "date": "2017-10-08T04:55:00",
        "image": "/assets/img/",
        "description": "Jet Engines - The different types and how they work",
        "tags": [
            "Aircraft Engines"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "mirthunraj",
        "text": "[{% load static %}]()\nMany of you might have flown in an aircraft at least once in your lifetime and even if you haven't, you might have at least seen one in a picture. You might have noticed that there are different types of aircraft for different purposes.\n\nSome are small and can accommodate only 5 people while some are huge and can carry more than 600 people, some are used for dropping bombs while some others are used for rescuing people during disasters. \n\nOne of the most essential parts of any of these aircrafts is the engine. The engine provides the thrust required to take the aircraft to the sky. Now, all aircraft cannot have the same type and size of engines. \n\nThe size and type of the engine depend upon various factors, a major one being the uses of the aircraft. Let us now go through some of these different types in detail.\n\n### Reciprocating (Piston) Engines\n\n### Turboprop\n\nA turboprop consists of an intake, compressor, combustor, turbine, and a propelling nozzle. Air is drawn into the intake and compressed by the compressor. Fuel is then added to the compressed air in the combustor, where the fuel-air mixture is burnt. \n\nThe hot combustion gases expand through the turbine. Some of the power generated by the turbine is used to drive the compressor. The rest is transmitted to the propeller. Further expansion of the gases occurs in the propelling nozzle, where the gases exhaust to atmospheric pressure. The propelling nozzle provides a relatively small proportion of the thrust generated by a turboprop. \n\nThe major portion of the thrust is generated by the propeller. These are used in small aircraft used to transport a few passengers over short distances. The ATR 72-500 uses turboprop engines.\n\n### Turbojet\n\nA turbojet consists of a gas turbine with a propelling nozzle. The gas turbine has an air inlet, a compressor, a combustion chamber, and a turbine. \n\nThe compressed air from the compressor is heated by the fuel in the combustion chamber and then allowed to expand through the turbine. The turbine exhaust is then expanded in the propelling nozzle where it is accelerated to high speed to provide thrust. \n\nThese are not used anymore as turbofan engines are more efficient than these. The Concorde was a supersonic jet which used turbojet engines.\n\n### Turbofan\n\nEngines used in modern day airliners are of this type. They consist of a massive fan at the front which draws in the air. Most of this air passes bypasses the turbine and is directly sent to the nozzle to provide thrust. \n\nSome of the air drawn in by the fan is sent to the compressor which compresses the air after which combustion occurs. The exhaust is used to drive a turbine which powers the fan in the front and the compressors. The exhaust gasses are then sent through a nozzle to provide thrust. \n\nThe ratio of mass flow of air bypassing the engine core to the mass flow of air entering the engine core is called bypass ratio. Modern day airliners use engines with a high bypass ratio as they are more efficient.\n\n### Ramjet\n\nRamjets are engines that do not have a moving compressor. They use the forward motion of the engine to push the air through the compressor in the engine. Because of this they require some other form of thrust at lower speeds. Therefore they are not useful in slow moving aircraft. \n\nThe compressor is a geometrical feature of the engine. It has a lower area of cross section than the inlet. Therefore the air passing through that area gets compressed. After compression the fuel is injected and combustion occurs. The exhaust gasses are then directed into a nozzle which allows for expansion into the atmosphere and provides thrust. These are mainly used in missiles. The Brahmos missile jointly developed by India and Russia uses ramjet engines.\n\nMany more types of engines exist. The ones mentioned here are just to give a basic idea. Continuous research is being done to further improve the efficiency of the modern day engines and to develop new engines which are more efficient. We have come a long way from the early days of air travel when planes could carry very few people and required a lot of fuel. There is still a long way to go to improve the efficiency of engines and make planes lighter.",
        "author_name": "Mirthun Raj",
        "slug": "aircraft-engines"
    },
    {
        "layout": "post",
        "title": "Raspberry Pi Tutorial",
        "author_github": "varsh2506",
        "date": "2017-10-08T05:00:00",
        "image": "/assets/img/",
        "description": "This post shows how to setup Raspberry Pi and to use laptop as monitor",
        "tags": [
            "Raspberry Pi"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "varsh2506",
        "text": "[{% load static %}]()\nIf you have just started exploring microcontrollers and experimenting with development boards like Arduino and Raspberry Pi, you might have had struggles with configuring the board and interfacing them with the hardware that you used to build your little fascinating circuits. I too, faced some issues when I had to set up my Raspberry Pi B+ model. \n\nMost of the instructables available online give instructions for working with a separate monitor connected to RPi using a HDMI cable. However, to make things convenient, you can use your laptop display itself to work with your Rpi. This requires a few additional steps which I will be covering in the last section of this tutorial. So, this article is a step by step tutorial to help you kick start your Raspberry Pi and dive into the Internet of Things!\n\nUnlike an Arduino, you need to have an operating system to start working with your RPi. The most famously used OS for RPi is Raspbian, based on Debian, a Linux distribution.\n\n#### I - Steps to format SD card to flash the Raspbian image later\n\nA 16GB SD card should suffice for flashing the Raspbian image onto it.\n\n1) Download SD Formatter 4.0 for Windows from  [here](https://www.sdcard.org/downloads/formatter_4/eula_windows/index.html)\n\n2) Insert your SD card into the computer or laptop's SD card reader and make a note of the drive letter allocated to it, e.g. F:/\n\n3) In SD Formatter, select the drive letter for your SD card and format it.\n\n4) Under Option > choose Format Type as Full (Erase) > set Format Size Adjustment as ON \nto format the full space present in SD card if formatting the SD card for the first time. Otherwise, Quick format type should also be fine.\n\n==> Now we have a formatted SD card on to which the Raspbian image can be flashed later.\n\n#### II - Steps to flash Raspbian image on to SD card\n\n1) On your Windows laptop, go  [here](https://www.raspberrypi.org/downloads/)\n\n2) Click on Raspbian (so as to flash the Raspbian image on to SD card).\n\nNote: NOOBS can be used for installation if we have HDMI cable and display monitor.\n![Raspberry]({% static '/blog/assets/img/Raspberry-Pi-Tutorial/raspbian.png' %})\n\n3) Download RASPBIAN STRETCH WITH DESKTOP so that we can have a desktop to work on, if necessary.\nDownload Torrent (it is faster )\n\n4) Extract the files from the zip file downloaded in Step 3 to extract the Raspbian image (.img file).\n\n5) From [here](http://www.etcher.io/), download and install the Etcher SD card image utility.\n\n6) Run Etcher and select the Raspbian image you unzipped (.img file) on your computer or laptop.\n\n7) Select the SD card drive. Note that the software may have already selected the right drive\n\n8) Follow the instructions of Etcher to flash the Raspbian image on to SD card.\n\n9) Open the root folder in the SD card and create a file (SSH). Note that the filename should not have any extension.\n\nThis is required to make Raspberry Pi enable SSH so that we can connect later from PuTTY through SSH. Here, your laptop will later serve as the SSH client and your RPi will be the SSH server.\n\n==> Now the SD card contains the Raspbian image with SSH enabled.\n\nFollow the below mentioned steps if you are working in the comfort of your home network. If not, skip 3(a) and move ahead to 3(b).\n\n#### III(a) - Connecting the SD card to Raspberry Pi and setting up (home network)\n\n1) Take your Raspberry Pi and connect to the router (for example, DLink router) directly using a LAN cable.\n\n2) By typing **ipconfig** (in Windows) in your command prompt, find out the IP address of the machine you are using.\n\nCheck the Active Client Table of your router configuration. There will be an IP address allocated for your RPi. Note the IP address (e.g. 192.168.0.12).\n\n![ActiveClientTable]({% static '/blog/assets/img/Raspberry-Pi-Tutorial/ActiveClientTable.png' %})\n\n3) Using PuTTY, connect to the IP address, 192.168.0.12(this is just an example).\n\n4) It will get connected to Raspberry Pi and will ask for username/password. \n> Provide username as pi and password as raspberry (Pi is the default username and raspberry is the default password)\n\nWe can see the RaspberryPi prompt getting displayed and we are connected to the Raspberry Pi through SSH.\n\n![Putty]({% static '/blog/assets/img/Raspberry-Pi-Tutorial/putty.png' %})\n\n\n#### III(b) - Connecting the SD card to Raspberry Pi and setting up (to connect to mobile hotspot or any other WiFi network)\n\nConnecting to a WiFi network when you are on the go can be a hassle as finding the IP address of your RPi can be difficult. Here's a YouTube video if you are facing similar issues.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/xj3MPmJhAPU\" frameborder=\"0\" allowfullscreen></iframe>\n\n#### IV - Configuring WiFi for Raspberry Pi\n\n[Raspberry Pi Configuration Documentation](https://www.raspberrypi.org/documentation/configuration/wireless/wireless-cli.md)\n\n1) In the SSH terminal of PuTTY, give the command \n\n```\nsudo iwlist wlan0 scan\n```\n\nYou will be able to see the list of WiFi connections available.\n\n2) If you want to connect to a specific WiFi network, for example, rpi, give the following command so that it generates the WiFi configuration information which needs to be updated at the bottom of the file -  \n\n> /etc/wpa_supplicant/wpa_supplicant.conf\n\n```\nwpa_passphrase \"rpi\" \"wifipassword\"\n```\n\nThe output will be as follows:\n\n```apacheconf\nnetwork={\n\nssid = <rpi>\n\n#psk = <wifipassword>\n\npsk=c84f3033b09512fbd632a0510ba0cf0f9b8f7a8f8e3e075cdfa76b8036bab9e8\n\n}\n```\n\nNote: This is just an example. You need to specify your network name in place of rpi and the password for it in place of wifipassword.\n\n3) The above output needs to be copied to bottom of the file \n\n```\n/etc/wpa_supplicant/wpa_supplicant.conf \n```\n\nafter removing the line containing #psk for security purpose as we do not want to store the WiFi password in plain text.\n\n4) Command to open the file and copy the network settings at the bottom on the file -\n\n```\nsudo nano /etc/wpa_supplicant/wpa_supplicant.conf\n```\n\n5) Automatically, WiFi will be configured after sometime. We can also make sure it happens by issuing the following command\n\n```\nsudo wpa_cli reconfigure\n```\n\n6) You can verify whether it has successfully connected, using the command \n\n```\nifconfig wlan0\n```\n\nIf the inet addr field has an address beside it, the Raspberry Pi has connected to the network.\n \nFor example, the IP address may be 192.168.0.5. This is the IP address of Raspberry Pi through WiFi connection **rpi** (example).\n\nNow, if you are equipped with a HDMI cable, a monitor, a keyboard and a mouse, you are all set to go and discover the capabilities of the credit card sized computer that you possess. However, if you do not have a monitor, do not worry as the last step of this tutorial is aimed to eliminate that need. You can use your laptop display and remotely access your RPi using SSH. So, do not give up hopes after coming this far! One final step and you are done!\n\n#### V - Setting up the Raspberry Pi to connect to laptop display\n\n[DIY Hack - Connect RPI to Laptop's Display](https://diyhacking.com/connect-raspberry-pi-to-laptop-display/)\n\n1) To start VNC, enter the following command in the SSH terminal:\n\n```\nvncserver :1\n```\n\n2) In the client side laptop, Download VNC Viewer from [here](https://www.realvnc.com/download/viewer/)\n\n> Make sure to select Windows if installing VNC Viewer for Windows laptop\n\n3) Now click on VNC Viewer. A dialog box will be displayed. If the Raspberry Pi's IP address is say 192.168.0.5, in the **VNC Server** field, provide the value as [192.168.0.5:1](http://192.168.0.5:1/) and click OK. For password, enter Raspberry's username and password.\n\n==> Now the Raspberry Pi's display will be shown on the laptop display.\n\n![RaspberryPi]({% static '/blog/assets/img/Raspberry-Pi-Tutorial/view.png' %})\n\nAnd finally, you are done! Without further ado, enjoy exploring!",
        "author_name": "Varshini Srinivasan",
        "slug": "rpi-tutorial"
    },
    {
        "layout": "post",
        "title": "Search Engine Optimisation",
        "author_github": "pvgupta24",
        "date": "2017-10-08T05:05:00",
        "image": "/assets/img/",
        "description": "Basic Concepts of Search Engine Optimisation",
        "tags": [
            "SEO",
            "Google",
            "Web"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "pvgupta24",
        "text": "[{% load static %}]()\n> Guess how many websites come up each day.<br>\n Any ideas?\n\n> A survey by Netcraft in 2012 showed that total number of websites launched in 2012 were *51 million* i.e. ~140,000 per day.\n\nThis flood makes it necessary for your website to stand out from the rest. But , how can this be done ?\n\nThis is where **SEO i.e. Search Engine Optimisation** comes into play.\n\n![SEO and Ranking]({% static '/blog/assets/img/SEO/gif.gif' %})\n\nIn a world where **over 90% of online experiences start with a search engine,** showing up on the front page of Google Search can be the deciding factor for a business' growth.\n\n### What does SEO mean ?\nQuite simply, SEO is the term used for all the steps that can be used  to ensure the visibility of your website and its content on search engine results pages (SERPs). Visibility here means how high it appears in the SERP in the **Organic Results** i.e. the naturally appearing results, not the paid sections.\n\nThe methods vary from technical practices you can use behind the scenes on your website (on-page SE) to all the promotional (off-page SEO) approaches you can use to raise your site\u2019s visibility (link-building, social media marketing).\n\nHere's a short video that will give you a clear picture about SEO.\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/hF515-0Tduk\" frameborder=\"0\" allowfullscreen></iframe>\n\n\n### What do Search Engines look for ?\n\n1) How **Relevant** is the page w.r.t the user's search. Search Engines makes use of the keywords , user location , history, time period and many other factors to show the user results which is accurate and relevant.\n\n2) **Content Quality** is as important as the keywords. Websites publishing regular, fresh and helpful articles and videos are usually ranked higher.\n\n3) **User Experience** : Webpage should be easily navigable, should have internal linkings and should be searchable.\n\n4) It should be optimised to take less **Time to Load**.\nCheck out this [Google tool](https://developers.google.com/speed/pagespeed/) for checking your website's speed and fixes to make it faster.\n\n5) Webpage should be **Responsive**. Sites optimised for cross-device compatibility will have a higher rating.\nCheck if your website is mobile friendly using this [Google tool](https://search.google.com/test/mobile-friendly).\n\n6) The website should be **Trustworthy**. Proper certified websites are always preferred.\n\n7) Webpage having more **Backlinks** from popular websites rank higher. SE's give more weight to backlinks from trustworthy and popular sites to avoid bogus linkings.\n\n8) **Title** and **Meta** tags matter. These are used by Search Engines to filter out results.\n\n```html\n<html>\n<head>\n<title>Baseball Cards - Buy Cards, Baseball News, Card Prices</title>\n<meta name=\"description\" content=\"Brandon's Baseball Cards provides a\n large selection of vintage and  modern cards for sale.  We also offer \n daily baseball news and events in\">\n</head>\n</html>\n```\n\n![Tags]({% static '/blog/assets/img/SEO/titlemeta.png' %})\n\nUse separate description meta tags for each page.\n\n9) HTML tags used should have the required attributes (e.g alt attribute in img tag).\n\n10) **Simple to understand URL's** is another plus.\n\n11) **Simple website directory** makes it easier for GoogleBots to crawl your website.\n\nSEO is no longer an option in today's crowded online world.\nToday, making it to the first page of Google Search has become the deciding factor for a website's success.\n\n#### References\n- [Google SEO Starter Guide](https://static.googleusercontent.com/media/www.google.com/en//webmasters/docs/search-engine-optimization-starter-guide.pdf)\n\n- [Simple SEO Guide for Starters](https://blog.kissmetrics.com/simple-guide-to-seo/)\n\n- [Essentials for optimizing your site](https://searchenginewatch.com/sew/how-to/2259693/seo-basics-8-essentials-when-optimizing-your-site)",
        "author_name": "Praveen Gupta",
        "slug": "search-engine-optimisation"
    },
    {
        "layout": "post",
        "title": "Generative Adversarial Networks",
        "author_github": "mj10",
        "date": "2017-10-16T05:05:00",
        "image": "/assets/img/",
        "description": "A brief overview of GANs",
        "tags": [
            "Deep Learning",
            "Generative Adversarial Networks",
            "GANs",
            "Generative Models",
            "Machine Learning"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "mj10",
        "text": "[{% load static %}]()\nIn the last 2 years, **Generative Models** have been one of the most active areas of research in the field of Deep Learning. The paper on **Generative Adversarial Networks** (a.k.a GANs) published by **Ian Goodfellow** in 2014 triggered a new wave of research in the field of Generative Models. Today we'll explore what makes GANs so different and interesting. \n\n## Introduction\n\n![Ideal Generative Model]({% static '/blog/assets/img/GAN/ideal-gen-model.jpeg' %})\n\nThe main objective of a Generative Model is to learn the *probability distribution* from which the training data is sampled. Once the model learns the probability distribution, it can sample new data from it. For eg. If the model is trained on a sufficiently large dataset consisting of images of handwritten digits, we expect the trained model to then generate images of handwritten digits, which are not a part of the training data. Now, in reality, it is very hard to build models which can learn the *exact probability distribution*. So we usually try to build models that can just **estimate the distribution** or generate samples from the distribution **without explicitly learning the distribution** itself. \n\n## Generative Adversarial Networks \nEven before the introduction of GANs, there existed several Generative Models such as **Variational Autoencoders**, **Boltzmann Machines** and **Deep Belief Networks** among others. But GANs provide a totally new way of building Generative Models.\n\n### So, what is a GAN?\nThe key feature of a GAN is that the **generative model** is pitted against an *adversary*; a **discriminator model** which learns to determine whether a *sample is generated from the model or belongs to the actual data distrubution*. The generator works like a currency counterfeiter, who generates notes that are as close as possible to the original. The discriminator works as a bank that determines whether a given currency sample is fake or real. In other words, the generator tries to fool the discriminator, while the discriminator tries to prevent this from happening.\n\n![Generative Adeversarial Network]({% static '/blog/assets/img/GAN/GAN.png' %})\n\nMore concretely, the discriminator and generator can be represented by *two separate neural networks*, **G** and **D**. G takes *random noise* as input and outputs a sample from the learned probability distribution. D takes in a sample and outputs the *probability that the given sample is real*. So we would train the discriminator to *maximise the probability of correctly determining whether the given sample is real or fake*. Whereas for the generator we would like to *minimise the probability that the discriminator correctly guesses the sample being fake*. We can then determine losses for both G and D, and train them using backpropagation. \n\nThis is equivalent to G and D playing a **two-player minimax game**. In the ideal scenario, the generator would generate samples that are indistinguishable from the real data and the output of D would be *1/2*, that is the sample is equally likely to be either real or fake. \n\n__Value function V(G, D) for the minimax game__ \n\n![G and Minimax Game]({% static '/blog/assets/img/GAN/minimax-objective.jpeg' %})\n\n### Advantages and Disadvantages\n\nThis adversarial approach is very effective in building generative models. GANs have a number of computational as well as statistical advantages such as using only *simple backpropogation to obtain gradients*, *no need of inferences during training* among others. It also comes with its own set of drawbacks. One of them is the difficulty in training the model. **The Helvetica Scenario** occurs quite often during training, where the generator finds one sample which fools the discriminator, and then keeps on generating simple variations of the same sample without learning to generate other distinct samples.\n\n## _Fantastic GANs And Where To Find Them_\nAs mentioned earlier, GANs are one of the most active areas of research in Deep Learning. Hundreds of papers have been published since the original paper 2014, detailing various modifications and applications for the GAN framework. The applications range from Image to Image Translation to Image Style Transfer. Here are a few examples:\n\n\\> [**DCGAN**](https://arxiv.org/abs/1511.06434)\n![DCGAN]({% static '/blog/assets/img/GAN/dcgan.jpeg' %})\n\n\\> [**acGAN**](https://arxiv.org/abs/1702.01983)\n![acGAN]({% static '/blog/assets/img/GAN/acgan.jpeg' %})\n\n\\> [**DiscoGAN**](https://arxiv.org/abs/1703.05192)\n![DiscoGAN]({% static '/blog/assets/img/GAN/discogan.png' %})\n\nYou can find a complete list of various GANs at the [GAN Zoo](https://github.com/hindupuravinash/the-gan-zoo)\n\n## References\n1. [Generative Adversarial Networks, Ian Goodfellow et al., 2014](https://arxiv.org/abs/1406.2661)\n2. [Deep Generative Models](http://www.deeplearningbook.org/contents/generative_models.html)\n3. [GAN Tutorial, NIPS 2016](https://arxiv.org/abs/1701.00160)",
        "author_name": "Moksh Jain",
        "slug": "generative-adversarial-networks"
    },
    {
        "layout": "post",
        "title": "Robots Can Now See In 3D",
        "author_github": "sinchana-hegde",
        "date": "2017-10-16T05:05:00",
        "image": "/assets/img/",
        "description": "Exploring how a robot perceives depth",
        "tags": [
            "Robots",
            "Computer Vision"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "sinchana-hegde",
        "text": "[{% load static %}]()\nAutonomous robots can inspect nuclear power plants, clean up oil spills in the ocean, accompany fighter planes into combat and explore the surface of Mars.\n\nYet for all their talents, robots still can't make a cup of tea.\n\nThat's because tasks such as turning the stove on, fetching the kettle and finding the milk and sugar require perceptual abilities that, for most machines, are still a fantasy.\n\nAmong them is the ability to make sense of 3-D objects. While it's relatively straightforward for robots to \"see\" objects with cameras and other sensors, interpreting what they see, from a single glimpse, is more difficult.\n\nNow researchers have developed a new computer vision algorithm that gives a robot the ability to recognize three-dimensional objects and, at a glance, intuit items that are partially obscured or tipped over, without needing to view them from multiple angles. For instance, objects with flat square tops tend to have legs. If the robot can only see the square top, it may infer the legs. All pots are hollow in the middle. When the algorithm was being trained to recognize pots, it didn\u2019t spend time analyzing the hollow parts. Once it knew the object was a pot, it focused instead on the depth of the pot or the location of the handle. It sees the front half of a pot sitting on a counter and guesses there\u2019s a handle in the rear and that might be a good place to pick it up from.\n\n![Visualisation]({% static '/blog/assets/img/Robots-can-see-in-3D/visualisation.jpg' %})\n\nThe researchers trained their algorithm on a dataset of roughly 4,000 complete 3-D scans of common household objects: an assortment of bathtubs, beds, chairs, desks, dressers, monitors, nightstands, sofas, tables and toilets.\n\nEach 3-D scan was converted into tens of thousands of little cubes, or voxels, stacked on top of each other like LEGO blocks to make them easier to process.\n\nThe algorithm learned categories of objects by combing through examples of each one and figuring out how they vary and how they stay the same, using a version of a technique called probabilistic principal component analysis.\n\nResearchers have been teaching robots to recognize 3-D objects for a while now. What's new is the ability to both recognize something and fill in the blind spots in its field of vision, to reconstruct the parts it can't see.\n\nThat has the potential to be invaluable in a lot of robotic applications.",
        "author_name": "Sinchana Hegde",
        "slug": "robots-can-see-in-3d"
    },
    {
        "layout": "post",
        "title": "The Brain behind a Formula One Car",
        "author_github": "anirudhsundar",
        "date": "2017-10-16T05:05:00",
        "image": "/assets/img/",
        "description": "A look into the McLaren TAG-320",
        "tags": [
            "Formula One",
            "ECU",
            "TAG-320"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "anirudhsundar",
        "text": "[{% load static %}]()\n>The track belongs to the driver, the pits belong to the engineers\n\nFormula One cars are mean machines. They have been fine tuned for one purpose- Speed. In fact, they are the most efficient mode of transport after space shuttles.\n\nBe it aerodynamic performance, super-efficient engines or state of the art tyre management strategies, engineers have scaled new heights in their pursuit for speed.\n\n## The ECU:\nHowever, one aspect of the car that goes largely unnoticed is the electrical system. If the chassis and the engine form the arms and legs, **the Electronic Control Unit**, or **the ECU** forms the brain.\n\nIn the last ten years, these components have covered the equivalent of a trip to the moon and back and haven't missed a single beat.\n\nThe ECU basically controls the engine, the clutch, the differential, the Drag Reduction and Energy Recovery systems.The ECU aslo controls the ignition, the LED panel on the steering and fuel injection.\n\nAround **120 sensors** all over the car send information to the ECU, a quarter of which are from the engine and the gear box alone.\n\nFrom accelerometers to gyroscopes, thermocouples to pressure sensors, the ECU relays an enormous amount of data in real time to the pits for diagnostics and analysis.\n\n## Specs:\n\nThe unit used in modern F1 cars- the **McLaren TAG-320** is a powerful processing platform with a real time operating system based on **32-bit microprocessors**. Application code is autocoded from **MATLAB/SIMULINK** control modules.\n\nWith a mammoth processing power of **4000 MIPS** (million instructions per second) to **8GB** flash memory, this device boasts of powerful data logging and telemetry capabilities.\n\nOn the communications side, it comes with a wired gigabit ethernet interface and has a data relay speed of up to **10Mbps**.\n\nThe ECU has been designed to handle all sorts of weather conditions a driver may encounter on track and has a significant resistance to mechanical impact.\n\nThe units weigh in at **1.35 kilograms** and are either attached below the radiator or behind the steering. The components are tamper proof. In fact, other than the tyres, it is the only component to be exactly the same on all cars. Ultimately, the ECU forms an integral part of every modern Formula One car. It helps provide a whole new dimension to racing. Crunching the data received has enabled teams and drivers to analyze and optimize performance to entirely new levels.\n\n![The TAG-320]({% static '/blog/assets/img/the-brain-behind-a-formula-one-car/TAG.jpg' %})\n*The McLaren TAG-320 up close*\n\n**The next time someone says Formula One is all about the mech guys, tell them to think again!**\n\nIf you want to see more, check out the [McLaren Website](http://www.mclaren.com/appliedtechnologies/products/item/powertrain-control-unit-tag-320/)",
        "author_name": "Anirudh Sundar",
        "slug": "the-brain-behind-a-formula-one-car"
    },
    {
        "layout": "post",
        "title": "Augmented Reality",
        "date": "2017-10-23T13:51:02",
        "image": "/assets/img/",
        "description": "Introduction to Augmented Reality",
        "tags": [
            "Virtual Image",
            "Augmented Reality"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "anumehaagrawal",
        "comments": true,
        "text": "[{% load static %}]()\nWhether you have played \u201cPok\u00e9mon Go\u201d or not, you\u2019ve probably at least heard of the game. A special app on your smartphone or tablet makes it look like Pok\u00e9mon is right there in front of you\u00a0 while you search for and \u201ccapture\u201d the different characters.\u00a0 This game aquainted the world with this fast growing technology Augmented Reality. This new technology, blurs the line between what's real and what's computer-generated by enhancing what we see, hear, feel and smell.\n\n## How does Augmented Reality work?\n\nIn most augmented reality applications, a user will see both synthetic and natural light. This is done by overlaying projected images on top of a pair of see-through goggles or glasses, which allow the images and **interactive virtual objects** to layer on top of the user's view of the real world.\nFor augmented reality to work you need camera,accelerometer,rotation sensor and some math skills and GPS. Idea behind all of this is to use data from real word, data gathered by the sensors, and process it in the way computer can use.\n\n![Augmented1]({% static '/blog/assets/img/augmented-reality/Augmented1.jpg' %})\n\n## Make your own AR app !\n\nHave you ever dreamed of making your own AR app? But you don\u2019t know how to start? So, let\u2019s see how you can !\n\n### Vuforia\nVuforia is one of the most popular platforms to help you work with augmented reality development. The software implements the following functionalities: recognition of the different types of visual objects, text and environments recognition, VuMark (a combination of picture and QR-code). All plugins and functionalities of the platform are free to use but include the Vuforia watermarks.\n\n### EasyAR\nEasyAR is a free and easy to use alternative to Vuforia. It includes 3D Object Recognition ,Environment perception ,Cloud Recognition ,\nSmart Glass Solution and App Cloud Packaging. The library is completely free. To start your work with EasyAR, you only need to register the account and to generate the plugin\u2019s key of your Bundle ID. EasyAR is quite easy to integrate\n\n## Real World Examples of Social Media Apps Using Augmented Reality\n\n### Snapchat\n\n> The most popular and ephemeral messaging app is the first name in this high-tech world of Augmented Reality. Founders of this globally used social media app, namely Evan Spiegel, Bobby Murphy and Reggie Brown, aimed for more natural flow of interaction.\n\n### Facebook\n\n> Facebook believes that consumer-ready Augmented Reality glasses are bit far from everyday use, but instant and ready-to-use AR devices exist right in our pockets.\n\n![Facebook]({% static '/blog/assets/img/augmented-reality/Facebook.jpg' %})\n\n## Future of Augmented Reality\n\nAR may completely revamp how we navigate the world, both indoors and outdoors. Imagine seeing directions before you throughout Walmart to find the laundry detergent you were looking for. It can also help eCommerce shoppers try products at home before they purchase. \n\nIn the future, all stores will feature 3D models that can be seen in your very room with a click of a button. See how the lamp you want to buy would look in your living room through AR, or the queen size bed that you don\u2019t think will fit in your bedroom.",
        "author_name": "Anumeha Agrawal",
        "slug": "augmented-reality"
    },
    {
        "layout": "post",
        "title": "Charge your phone whenever you walk into the room",
        "date": "2017-10-23T13:51:02",
        "image": "/assets/img/",
        "description": "Turn your smart home into a giant battery with Wireless Charging.",
        "tags": [
            "transmitter",
            "reciever"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "sowjanyakandula",
        "comments": true,
        "text": "[{% load static %}]()\n## Charge your phone whenever you walk into the room\n\nOne quick and easy way to induce anxiety is to misplace your phone charger. You\u2019re stressed out just thinking about it right now, well what if instead of dealing with your wiry bundles and ports and mats and plugs and cords, you could just\u2026.walk into a room and automatically your mobile phones, laptops any other appliances get charged. However, mobile devices are not truly mobile if they require connection to power cord.\n\nWi-charge technology provides us finally a way to get rid of the power cord, efficiently and safely. The device itself looks something like a smoke detector on the ceiling. The product is FDA approved. Anything transmitted over air has to comply with safety and commercial regulations. As long as your phone, tablet, laptop, wireless speakers, etc. are in view of the Wi-charge, you will get the juice. It is the technology called \u201cline-of-sight\u201d. Power is delivered over distances using infrared beams of light. The system consists of two units:\n\n> **The transmitter unit** \u2013 Connected to power source (standard AC, DC or USB), delivers power to receivers anywhere in its vicinity. The transmitter emits focused, safe infrared beams that carry power to receivers.\n\n> **The receiver unit** \u2013 Captures the light from the transmitter and converts it to electrical power using a photo voltaic cell, just like the way that solar panels converts sunlight into electricity.\n\n![image1]({% static '/blog/assets/img/charge-your-phone/image1.jpeg' %})\n\n### Transmitter and recievers\n\nThe transmitter is a very smart device. Within its coverage area, it automatically identifies chargeable devices and discovers the power requirements. It delivers power to receivers based on their priority, power requirements, battery level and other parameters. Multiple clients can be charged simultaneously using an intelligent power- management algorithm embedded in the transmitter. Automatically, the transmitter directs a safe infrared beam to each receiver, establishing a virtual photonic power cord. No light is spread outside the beam; 100% of the power delivered from the transmitter to receiver.\n\nDepending on size and power requirements of the charging device, the tiny thumb size receiver is automatically illuminated by the invisible infrared beam. The receiver converts power from the beam to an electric charge which is used to power the device or recharge its battery. The tiny receiver may be embedded in the mobile device or plugged externally into an existing charging port.\n\n![image2]({% static '/blog/assets/img/charge-your-phone/image2.jpeg' %})\n\n### Power delivery\n\nEfficient power delivery via light necessitates a great deal of intelligence in the transmitter. The power of client identification and power delivery is performed automatically without any user intervention. For example, the mobile phone user simply enters the room and transmitter will automatically find it and begin charging it. The receiver is detectable by the transmitter even if device\u2019s battery is drained, and it will stop charging the device once the battery is full.\n\n### **Line of sight**\n\nPower flows in a straight, narrow line. If, for any reason, the line of sight between the transmitter and receiver is blocked, Transmission stops immediately. This ensures that power emission levels are always compliant with safety standards and international regulations, Once the line of sight is restored, charging restarts automatically. To ensure steady power delivery, each Wi-charge receiver is equipped with an energy storage buffer (capacitor or battery).\n\nThis can be used in commercial spaces, which considering the panic and stress of a dying phone while on-the-go, might be more helpful anyway.\n\nFor further more details about Wi-Charge and its working:-\n\n1. [IEEE Spectrum - Phone Charging by Infrared Lasers](https://spectrum.ieee.org/tech-talk/consumer-electronics/portable-devices/wicharge-promises-phone-charging-by-infrared-laser)\n\n2. [WI Charge - Technology](https://www.wi-charge.com/technology/)",
        "author_name": "Sowjanya Kandula",
        "slug": "charge-your-phone"
    },
    {
        "layout": "post",
        "title": "The Honeycomb Conjecture",
        "date": "2017-10-23T13:51:02",
        "image": "/assets/img/",
        "description": "Decoding the mathematics behind Honeycombs",
        "tags": [
            "Honeycomb Conjecture",
            "Hexagonal cells"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "rajatchandavar",
        "comments": true,
        "text": "[{% load static %}]()\nWhy do bees build **hexagonal structured** hives? Why not any other shape? Let us deliberate these questions which troubled many thinkers.\n\n![img1]({% static '/blog/assets/img/the-honeycomb-conjecture/img1.jpg' %})\n\nConsider prisms of different regular polygons and assume them to have same height and volume. This means that the one with the **least** perimeter will require the least material to build. We know that as we increase number of sides in the polygon, perimeter decreases. So, cylinder will use the least amount of material (circle can be considered as a polygon with infinite sides).\n\nBut cylinders are economical only if they stand alone. If placed together, they leave large gaps between them. Since no walls can be shared, they are wasteful of both space and material and the amount of material required to construct an array of cylinders doesn\u2019t change. However this is not the case in other polygons. \n> For ex. In an array of octagons 25% of material is saved (See diagram \u2013 to construct new octagon, 2 sides are already provided by the 3 octagons). \n\n![img2]({% static '/blog/assets/img/the-honeycomb-conjecture/img2.png' %})\n\nBy calculating similarly for all shapes, we see that Hexagons save the most material or in other words, a regular hexagonal grid or honeycomb is the best way to divide a surface into regions of equal area with the least total perimeter. This is the **Honeycomb Conjecture**.\n\nCollecting wax for construction of honeycomb requires lot of bee time and energy. So, economy in the use of wax is very important to the welfare of the bees. Hence, they use **hexagonal packing** in hives. \n\nBut this is not the only reason. Hexagon cells are also the best for mechanical stability. If we load a square cell, a load along a partition is not readily transferred to adjacent partitions. In a triangular cell, load is distributed but here the horizontal edges will be in compression (others in tension). This is a wrong way to load a thin plate since it buckles (see diagram). But, if we load a hexagon cell, load is transferred to several members and the load is transferred as tension. This makes maximum use of the thin wax walls.\n\n![img3]({% static '/blog/assets/img/the-honeycomb-conjecture/img3.jpg' %})",
        "author_name": "Rajat Chandavar",
        "slug": "the-honeycomb-conjecture"
    },
    {
        "layout": "post",
        "title": "Making Iot communications easy using mDNS",
        "date": "2017-10-26T00:00:00",
        "image": "/assets/img/",
        "description": "Forget IP addresses when in local network",
        "tags": [
            "Iot",
            "Networking"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "hrily",
        "comments": true,
        "text": "[{% load static %}]()\nA project in Iot often requires multiple devices communicating with each other. This is usually done through sending messages over sockets. To locate a device, usually addresses are used. This is good technique, but due to commom use of DHCP, the IP addresses change constantly. So one needs to keep track of these changing IP addresses.\n\nWhat if you don't need to remember the changing IP addresses? That's where mDNS comes into picture.\n\n# What is mDNS?\n\n[mDNS](https://en.wikipedia.org/wiki/Multicast_DNS) stands for Multicast Domain Name System. Apple needed to use readable machine names on a local network without a nameserver, so implemented multicast DNS in their [Bonjour](https://en.wikipedia.org/wiki/Bonjour_(software)). Multicast DNS uses a special domain called `local`. Host names are automatically registered in this domain.\n\nmDNS uses multicast address `224.0.0.251` and UDP as it's transport layer protocol on port `5353`.\n\n# How to use mDNS?\n\nI found about this cool feature recently. Most of the modern OSes support mDNS out of the box. In dedication to popular Iot device Raspberry Pi, I will tell you the usage of mDNS in linux. \n\nLinux uses [Avahi](https://www.avahi.org/) library for mDNS. To install Avahi on your linux/ubuntu system\n\n~~~~\n$ sudo apt-get install avahi nss-mdns\n~~~~\n\n<br>\nMost probably above packages are installed by default. Now start `avahi-daemon` which will handle mDNS functions.\n\n~~~~\n$ system-ctl start avahi-daemon\n$ system-ctl enable avahi-daemon\n~~~~\n\n<br>\nNow we'll need to configure our system to listen to mDNS. To do this, edit `/etc/nsswitch.conf` and modify the `host :` line as follows:\n\n`hosts:\tfiles mdns_minimal [NOTFOUND=return] dns mdns`\n\nIf you\u2019re using IPv4 only, then use mdns4_minimal and mdns4 instead.\n\nNow you are all set to go. You can check by resolving your own domain:\n\n~~~~\n$ dig +short <hostname>.local @224.0.0.251 -p 5353\n~~~~\n\n<br>\nHere the `<hostname>` is the system's hostname. If you don't know this, then execute `hostname` to get it. For example, my hostname is `hrily`, so following is the output of `hostname`.\n\n~~~~\n$ hostname\nhrily\n~~~~\n\n<br>\nSo when I resolved my hostname, I got:\n\n~~~~\n$ dig +short hrily.local @224.0.0.251 -p 5353\n10.50.47.112\n~~~~\n\n<br>\n<br>\n# Applications\n\nNow that we know how to use hostnames instead of IP addresses, we have a cool set of applications. Whenever you want to ssh to your RPi with hostname `myrpi`, you just need to do:\n\n~~~~\n$ ssh user@`dig +short myrpi.local @224.0.0.251 -p 5353`\n~~~~\n\n<br>\nThis is really good usage that I need constantly. When I'm in my cammpus network, but not in reach of my laptop and need to do some work on it, I just ssh into it using my phone using above command. I don't really need to know what IP address was assigned to my laptop. \n\n\n\nI hope you find mDNS useful. Untill then, this is _hrily_ signing off...",
        "author_name": "Hrishi Hiraskar",
        "slug": "using-mdns-in-iot"
    },
    {
        "layout": "post",
        "title": "Introduction to Docker",
        "author_github": "mahim23",
        "date": "2017-10-30T10:10:10",
        "image": "/assets/img/",
        "description": "A brief introduction to Docker",
        "tags": [
            "Docker",
            "Containers",
            "DevOps"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "mahim23",
        "text": "[{% load static %}]()\nDocker is probably the most talked-about infrastructure technology of the past few years. And why not? It makes managing and deploying applications so much easier without the overhead of a Virtual Machine. Here's how:\n\n**Docker** is a tool designed to make it easier to create, deploy, and run applications by using containers. **Containers** allow a developer to package up an application with all of the parts it needs, such as libraries and other dependencies, and ship it all out as one package. By doing so, thanks to the container the developer can rest assured that the application will run on **any other machine** regardless of any customized settings that machine might have that could differ from the machine used for writing and testing the code.\n\nIn a way, Docker is a bit like a virtual machine. But unlike a virtual machine, rather than creating a whole virtual operating system, Docker allows applications to use the same Linux kernel as the system that they're running on and only requires applications be shipped with things not already running on the host computer. This gives a significant performance boost and reduces the size of the application.\n\n## Containers\n\nAn image is a lightweight, stand-alone, executable package that includes everything needed to run a piece of software, including the code, a runtime, libraries, environment variables, and config files.\n\nA container is a runtime instance of an image\u2014what the image becomes in memory when actually executed. It runs completely isolated from the host environment by default, only accessing host files and ports if configured to do so.\n\nContainers run apps natively on the host machine\u2019s kernel. They have better performance characteristics than virtual machines that only get virtual access to host resources through a hypervisor. Containers can get native access, each one running in a discrete process, taking no more memory than any other executable.\n\n## Containers vs. Virtual Machines\n\n![Containers vs VM]({% static '/blog/assets/img/Docker/docker-vm-container.png' %})\n\n**Virtual Machines** run guest operating systems\u2014note the OS layer in each box. This is resource intensive and the resulting disk image and application state is an entanglement of OS settings, system-installed dependencies, OS security patches, and other easy-to-lose, hard-to-replicate ephemera.\n\n**Containers** can share a single kernel, and the only information that needs to be in a container image is the executable and its package dependencies, which never need to be installed on the host system. These processes run like native processes, and you can manage them individually by running commands like docker ps\u2014just like you would run ps on Linux to see active processes. Finally, because they contain all their dependencies, there is no configuration entanglement; a containerized app **\u201cruns anywhere\u201d**.\n\n## Who is Docker for?\n\nDocker is a tool that is designed to benefit both developers and system administrators, making it a part of many DevOps (developers + operations) toolchains. For *developers*, it means that they can focus on writing code without worrying about the system that it will ultimately be running on. It also allows them to get a head start by using one of thousands of programs already designed to run in a Docker container as a part of their application. For *operations*, Docker gives flexibility and potentially reduces the number of systems needed because of its small footprint and lower overhead.\n\n## In a nutshell, here's what Docker can do for you: \n\nIt can get more applications running on the same hardware than other technologies; it makes it easy for developers to quickly create ready-to-run containered applications; and it makes managing and deploying applications much easier. Put it all together and I can see why Docker is riding the hype cycle as fast as I can recall ever seeing an enterprise technology go.\n\n## Setup \n\nDocker is available on multiple platforms, on cloud and on-premises. It can be downloaded and installed by following the instructions [here](https://docs.docker.com/engine/installation/)\n\n> Installation on Windows and Mac OS have standalone install packages with some prerequisites for the OS.\nIf your system does not meet the requirements, you can install Docker Toolbox.\n\nYou should be able to run docker run hello-world and see a response like this:\n\n```sh\n$ docker run hello-world\n\nHello from Docker!\nThis message shows that your installation appears to be working correctly.\n\nTo generate this message, Docker took the following steps:\n...(snipped)...\n``` \n\n> Note: version 1.13 or higher is required",
        "author_name": "Mahim Agrawal",
        "slug": "introduction-to-docker"
    },
    {
        "layout": "post",
        "title": "The LIGO Experiment",
        "author_github": "meghbhalerao",
        "date": "2017-10-30T10:10:09",
        "image": "/assets/img/",
        "description": "Every flight begins with a fall",
        "tags": [
            "LIGO"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "meghbhalerao",
        "text": "[{% load static %}]()\nThe LIGO (Laser Interferometer Gravitational-Wave Observatory) Experiment \u2013 A completely different perspective of studying the universe.\n\n## Einstein\u2019s Prediction\n\n![Einstein\u2019s Prediction]({% static '/blog/assets/img/the-ligo-experiment/image1.jpeg' %}){: height=\"294px\" width=\"270px\"}\n\nOver a century ago, Albert Einstein proposed the idea of something called a Gravitational Wave which was a part of his General Theory of Relativity. The concept of Gravitational waves suggests that, the force of gravity is not an actual force but rather, a distortion in the fabric of spacetime. This was just an idea, as there was no experimental evidence to support Einstein\u2019s claim.\n\n## What is a gravitational wave?\n\nA gravitational wave is much like an ordinary ripple caused by any distortion in a medium. The only difference here is that the ripple is in the medium(fabric) of spacetime. A gravitational wave originates from an accelerating mass in spacetime. Some phenomena that could trigger a gravitational wave are two black holes rotating in a dead spiral, collision of neutron stars, or even a ball dropping off your hand. The gravitational waves compress space in one direction and expand it in the orthogonal direction.\n\n## Conceiving the idea for an experimental setup for detecting gravitational waves\n\nDuring the early 1970s an ambitious professor at the MIT named Rainer Weiss first proposed the idea of an experimental setup to detect gravitational waves. His idea was funded by the US national science foundation, and the very first version of the LIGO was setup in 1983. This very primitive version of the LIGO which had arms of length 1km failed to produce any concrete evidence of gravitational waves. It was not until 2014 that the advanced LIGO was made fully operational. Rainer Weiss recently won the Nobel prize for Physics in 2017.\n\n![Rainer Weiss]({% static '/blog/assets/img/the-ligo-experiment/image2.png' %}){: height=\"259px\" width=\"388px\"}\n\n## When were the first gravitational waves detected?\n\nOn the 14<sup>th</sup> of September, Rainer Weiss noticed something peculiar on the usual activity log of the LIGO. He instantly contacted the field engineers who were present at the observatory and Weiss was completely dumbfounded. Gravitational waves from the merger of two black holes, each of nearly 30 solar masses, 1.5 billion light years away were detected for a fraction of a second.\n\n## So, how does the LIGO exactly detect these \u201cGravitational waves\u201d?\n\nThe LIGO basically comprises of two 4km arms placed perpendicular to each other. The current LIGO setups are in two remote locations in the United States, one in the Washington state and the other three thousand kilometres away in Louisiana. A very coherent light from an extremely sensitive laser is produced which passes through a beam splitter, which splits the light into two perpendicular components. These two perpendicular components pass through the 4km arms of the LIGO which are highly depressurised (almost vacuum). We know that the ultimate law of physics states that the speed of light is unaltered irrespective of the expansion or compression of the space. The LIGO makes use of this ultimate law to detect and classify gravitational waves. Now, when the light is reflected from the mirrors placed at the extreme ends of the arms comes back to the beam splitter and then falls onto a sensor, there are two possible scenarios:\n\n1. The two light beams being 180 degrees out of phase will cancel out each other and nothing will be detected by the sensor \u2013 this will happen when the two light beams traverse the exact same path.\n\n2. The two light beams have a phase difference other than 180 degrees. In this case the two beams will not exactly cancel out each other hence leaving a difference in the amplitudes. This small difference in the amplitudes is detected by the sensor, giving information about the gravitational wave that struck the LIGO apparatus.\n\n![Image3]({% static '/blog/assets/img/the-ligo-experiment/image3.jpeg' %}){: height=\"296px\" width=\"441px\"}\n\n## The principle doesn\u2019t look that complicated, then why did it take so many years to get it operational?\n\nThe most important aspect of this observatory is the sensitivity of the measuring instruments that are used in the setup. The gravitational waves come from hundreds of millions of light years away and hence their intensity is very weak. The LIGO measures lengths of the order 10<sup>-21</sup> of a meter. This length is roughly the size of one thousandths of the diameter of a typical atomic nucleus. To get such sensitive measurements the LIGO uses the purest of laser light and mirrors of the highest quality. The mirrors weigh about ninety kilos each and are highly polished. The reason that the LIGO is placed in such isolated locations is to prevent any stray noise from interfering with the data. The LIGO also uses technology such as internal seismic isolation to prevent any stray noise from corrupting the results obtained. The two LIGO setups are placed so far off from each other so that the common portion of the response of the sensors can be filtered out thus eliminating the chance of any common noise. This makes the LIGO truly an engineering marvel.\n\n![Image4]({% static '/blog/assets/img/the-ligo-experiment/image4.png' %}){: height=\"238px\" width=\"392px\"}\n\n## What role did India play in the LIGO experiment?\n\nThe IUCAA (Inter University Centre for Astronomy and Astrophysics) at Pune played an important role in the research and development of the experiment. About 40 scientists from India participated in the experiment. India has taken a further step ahead and has decided on building a LIGO apparatus at a project cost of 1000 Cr.\n\n## How does the gravitational wave picture change our understanding of the cosmos?\n\nPrimarily, it confirms the predictions of Einstein\u2019s General Theory of Relativity that the universe is a fabric of spacetime and gravity is nothing but a distortion in space time. It also opens a whole new gateway to studying the universe. Gravitational waves can help us to understand the universe beyond the apparent. It could help us understand phenomena that isn\u2019t apparent through a telescope, like what happens at the core of two stars during their collision.",
        "author_name": "Megh Bhalerao",
        "slug": "the-ligo-experiment"
    },
    {
        "layout": "post",
        "title": "Wireless Power Transmission",
        "author_github": "vansjyo",
        "date": "2017-10-30T10:10:08",
        "image": "/assets/img/",
        "description": "Boon to the next generation, transmitting power directly without radioactive methods.",
        "tags": [
            "Power"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "vansjyo",
        "text": "[{% load static %}]()\nWe cannot imagine the world without electric power. Generally power is transmitted through wires, however there is an idea to eradicate the hazardous usage of electrical wires which involves a lot of confusion particularly in organizing them. Some of these devices might not even need their bulky batteries to operate.\n\nHere are the techniques of transmitting power without using wires with an efficiency of about 95% with non-radiative methods, due to which it does not affect the environment.\n\nIn any system of coupled resonators there often exists a so-called \u201cstrongly coupled\u201d regime of operation. If one ensures to operate in that regime in a given system, the energy transfer can be very efficient. Scientists have tried to develop methods of wireless power transmission that could cut the clutter or lead to clean sources of electricity. \n\nThe attempt to transmit power wirelessly is not new. A revolutionary attempt towards this was made by Nikola Tesla by inventing the wonder of physics, The Tesla Coil, was built with an aim to provide electricity without wires but it failed miserably due to many drawbacks which made it highly uneconomical.\n\n**Quick Physics:** The Tesla coil creates an electric field that pushes electrons through the light bulb. This is the same way the lights in your house work, except in your house, the electricty comes through a wire instead of through the air.\n\nA Tesla coil makes electricity, which is basically flow of charges. If a fluorescent light bulb is held near the coil, the electricity will then go through the light bulb to the ground, which makes it light up. \n\n## The modern Techniques developed so far\n\n### Short distance induction\n\nThese methods can reach at most a few centimetres. The action of an electrical transformer is the simplest instance of wireless energy transfer. The primary and secondary circuits of a transformer are electrically isolated from each other. The transfer of energy takes place by electromagnetic coupling through a process known as mutual induction. This method is known as **inductive coupling**.\n\n### Moderate distance\n\nSince a magnetic field spreads in all directions, making a larger one would waste a lot of energy. An efficient way to transfer power between coils separated by a few meters is that we extend the distance between the coils by adding resonance to the equation, i.e. the frequency at which an object naturally vibrates. It's easy to get objects to vibrate at their resonant frequency. This process is known as **Resonant inductive coupling**\n\n### Long-distance Wireless Power\n\nThe secret to the SHARP's(the unmanned plane's) long flight time was a large, ground-based microwave transmitter. The SHARP's circular flight path kept it in range of this transmitter. A large, disc-shaped rectifying antenna, or rectenna, just behind the plane's wings changed the microwave energy from the transmitter into direct-current (DC) electricity. Because of the microwaves' interaction with the rectenna, the SHARP had a constant power supply as long as it was in range of a functioning microwave array. \n\n## Conclusion\n\nThe crucial advantage of using the non-radiative field lies in the fact that most of the power is not picked up by the receiving coil, remains bound to the vicinity of the sending unit, instead of being radiated into the environment and lost. With such a design, power transfer for laptop-sized coils are more than sufficient to run a laptop that can be transferred over room-sized distances nearly omni-directionally and efficiently, irrespective of the geometry of the surrounding space, even when environmental objects **completely obstruct the line-of-sight** between the two coils. As long as the laptop is in a room equipped with a source of such wireless power, it would charge automatically, without having to be plugged in. In fact, it would not even need a battery to operate inside of such a room.\n\n## Resources\n\n[Wireless Power Transmission: An Innovative Idea](https://www.ripublication.com/ijepa/ijepav1n3_2.pdf)",
        "author_name": "Vanshika Gupta",
        "slug": "wireless-power-transmission"
    },
    {
        "layout": "post",
        "title": "The Beginning Of The End - Have The Machines Won?",
        "author_github": "smarty1palak",
        "date": "2017-11-04T23:32:44",
        "image": "/assets/img/",
        "description": "A Viewpoint on the immense growth of artificial intelligence in the recent times and its impact on the future.",
        "tags": [
            "Artificial Intelligence",
            "AI threat"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "smarty1palak",
        "text": "[{% load static %}]()\nWondering when artificially intelligent robots will take over the Earth? Wonder no more\u200a\u2014 they\u2019ve already done it! You\u2019re probably thinking \n\n> But wait. Wouldn\u2019t I have noticed if chrome-plated autonomous killing machines began to exterminate humans? \nNot necessarily.\n\nIt turns out evil robots in cyber-space are nowhere near as flashy as their science-fiction counterparts. We envisioned Cylons, Terminators, and Cyborgs but failed to imagine the much more immediate threat of helpful neural-network learning algorithms. I\u2019m talking about Alexa, Echo, Siri, Google Home, the Internet of Things, even social networking platforms masquerading as news sources. They control content. They learn what we like, what we crave, what we fear. They feed us content designed to keep us coming back for more. We are already dependent on them to an extent nobody watching Arnold Schwarzenegger gun down Terminators could have predicted. And yet, 20 years later Arnold was Governor of California, and a dozen years after that, Trump became President of the Free World.\n\n## Alpha Go Dominance\n\nLet's look at a recent advancement. The computer that stunned humanity by beating the best mortal players at a strategy board game requiring intuition has become even smarter.Even more startling, the updated version of AlphaGo is entirely self-taught\u2014a major step towards the rise of machines that achieve superhuman abilities > with no human input.Dubbed AlphaGo Zero, the Artificial Intelligence (AI) system learnt by itself, within days, to master the ancient Chinese board game known as \"Go\"\u2014said to be the most complex two-person challenge ever invented.\nIt came up with its own, novel moves to eclipse all the Go acumen humans have acquired over thousands of years.\nAfter just three days of self-training it was put to the ultimate test against AlphaGo, its forerunner which previously dethroned the top human champs.AlphaGo Zero won by 100 games to zero.\n\nAlphaGo Zero not only rediscovered the common patterns and openings that humans tend to play... it ultimately discarded them in preference for its own variants which humans don't even know about or play at the moment, said AlphaGo lead researcher David Silver. The 3,000-year-old Chinese game played with black and white stones on a board has more move configurations possible than there are atoms in the Universe. AlphaGo made world headlines with its shock 4-1 victory in March 2016 over 18-time Go champion Lee Se-Dol, one of the game's all-time masters. Lee's defeat showed that AI was progressing faster than widely thought, said experts at the time who called for rules to make sure powerful AI always remains completely under human control.\n \nThere are dangers which come with the creation of such powerful and omniscient technology, just as there are dangers with anything that is powerful. This does not mean we should assume the worst and make potentially detrimental decisions now based on that fear. Experts expressed similar concerns about quantum computers, and about lasers and nuclear weapons\u2014applications for that technology can be both harmful and helpful.Unlike its predecessors, AlphaGo Zero is no longer constrained by the limits of human knowledge. It might become creepily autonomous in the near future.\n\nThat sequence of events didn\u2019t take particle-beam weapons or militarized robots. It only required control of social media channels. Content-selecting algorithms combined with human intellectual laziness did the rest. Anybody with a commercial or political interest can and does leverage our smart content-delivery infrastructure for whatever axe they have to grind. Who can blame the Russians for piling on?\n\n## Conclusion\n\nWe want reduced choices, we want simplistic analysis, we want machines to think for us. Even though it presents an existential threat to our species, we like Facebook. We think it\u2019s news. We love the idea that tomorrow we could kick out all the foreigners and former coal miners could earn $50 an hour bolting tail fins onto Chevys once again. Our content-selecting robots happily reinforce that viewpoint.\n\n> The machines are here, and they have won.",
        "author_name": "Palak Singhal",
        "slug": "the-beginning-of-the-end-have-the-machines-won"
    },
    {
        "layout": "post",
        "title": "Magnetically repelling ice",
        "author_github": "sonalparab10",
        "date": "2017-11-04T23:32:44",
        "image": "/assets/img/",
        "description": "How is Ice used for Magnetic Purposes",
        "tags": [
            "Magnetism"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "sonalparab10",
        "text": "[{% load static %}]()\nPeople can be nervous fliers to begin with. Even when there isn\u2019t a cloud in the sky, there are few words that cause anxiety in some like, \u201cPrepare for takeoff.\u201d But add wintry conditions to the mix, add some ice, and the uneasy feeling can turn to fear.\n\n## How does ice cause a plane to crash? \n\nIce typically builds up when tiny cloud droplets impact and freeze on the leading edges, or front surfaces, of the plane. The ice alters airflow over the wing and tail, reducing the lift force that keeps the plane in the air, and potentially causing aerodynamic stall\u2014a condition that can lead to a temporary loss of control. For scheduled air carriers (including commercial passenger airlines) icing has been a contributing factor in 9.5 percent of fatal air carrier accidents.\n\nFor now, the creation is simply called a \u201cmagnetic slippery surface\u201d and is made of two components. The first is magnetic tape. \u201cThis tape can be put on any surface, whether polymer, metal, etc.\u201d he says. The second component is ferrofluid, which he says is basically a mixture of fluid and magnetic nanoparticles, the latter being on the order of 5 to 10 nanometers in diameter. The ultimate idea was that this combination would make sure ice never encounters a solid to cling to.\n\n## How it works? \n\nLooking at a water drop mixing with a solid, it showed a strong bond has adhesion strength on the order of 100,000 pascals. When ice builds up and has contact with a solid then it has a very solid bond and is very hard to detach. When cold weather sits on a fluid surface, this combination [of magnetic tape and ferrofluid] doesn\u2019t allow the ice to see the solid and it just floats on the surface. Adhesion strength with this invention is 2 pascals. Hadi Ghasemi, assistant professor of mechanical engineering at the University of Houston, who is working on this, says that this is under critical testing but its moving forward and has an optimistic future. One of his goals, he says, is for it to be used in spray form.",
        "author_name": "Sonal S. Parab",
        "slug": "magnetically-repelling-ice"
    },
    {
        "layout": "post",
        "title": "Smart Grid",
        "author_github": "Shama-Vasisht",
        "date": "2017-11-04T23:32:44",
        "image": "/assets/img/",
        "description": "How smart technology helps us in acheving our purpose.",
        "tags": [
            "Smart grid",
            "Electric grid"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Shama-Vasisht",
        "text": "[{% load static %}]()\n## Introduction\n\nHow many times have you felt guilty about excessive usage of electricity when you see your monthly bill? Imagine managing the electricity delivered to your house like you manage activities like personal banking from a computer!\n\nYou will no longer have to wait till the end of the month to know how many units you have consumed- you have \"Smart meters,\" and other mechanisms, will allow you to see how much electricity you use, when you use it, and its cost. It also comes with real-time pricing, which will allow you to save money by using less power when electricity is most expensive.\n\n## What is a Smart Grid?\n\nThe grid, is an electric grid which is a network of transmission lines, stations, transformers and more that deliver electricity from the power plant to your houses or establishments. When you connect any device to the switch, you are actually logging onto this grid. This grid is made \u2018smart\u2019 by establishing a two way communication between the customers and the supply, digitally. Technologies will work with the electrical grid to respond digitally to our quickly changing electric demand, at the consumer end.\n\n![Smart Grid]({% static '/blog/assets/img/smart-grid/smart-grid.gif' %})\n\n## How does it work?\n\n### Visualisation Technology \n\nIt will be able to explore the state of the grid at the national level and switch within seconds to explore specific details at the street level. It will provide rapid information about blackouts and power quality as well as insights into system operation for utilities. With a platform built on Google Earth, it can also take advantage of content generated by Google Earth\u2019s user community.\n\n### Advanced Metering Infrastructure \n\nIt is based on consumer friendly efficiency concepts like \u2018Prices to devices\u2019-Assuming that energy is priced on what it costs in near real-time ,price signals are relayed to \u201csmart\u201d home controllers or end-consumer devices .The devices, in turn, process the information based on consumers\u2019 wishes and power accordingly. So, the house or office responds to the occupants, rather than vice-versa. Unlike the traditional way, where technicians are sent to each consumer site monthly to record the data manually for the billing purpose, the smart meters in AMI provide real-time monitoring capability of electric loads remotely. The information on power usage can be collected periodically (e.g., every 15 minutes)\n\n### Phasor Measurement Units(PMU) \n\nThis samples voltage and current many times at a second at a given location to give instantaneous visibility into the power system.\n\n## How is it better than the existing grid?\n\n* It can complement the current electric grid system by including renewable energy resources, such as wind, solar, and biomass, which is environmentally cleaner as compared to the fossil fuels used in many bulk electric power generation facilities.\n\n* Lesser transmission loss due to decreased distance between the consumer and supply.\n\n* Amount of electricity consumed is intimated to the consumer periodically and hence he/she can decide to modify it and in the process save it too. The main challenge in implementing this on a larger scale ,would be the cost of rebuilding the entire electric grid, to make it smart and in the process, to handle the security issues that arise because of the networking ability of a smart grid.\n\n## References\n\nFor further reference about smart grid and its realization, visit\n\n[SmartGrid](https://energy.gov/oe/activities/technology-development/grid-modernization-and-smart-grid)",
        "author_name": "Shama G Vasisht",
        "slug": "smart-grid"
    },
    {
        "layout": "post",
        "title": "Design of Aircraft Windows",
        "author_github": "shashank-ravichandir",
        "date": "2017-11-12T17:05:21",
        "image": "/assets/img/",
        "description": "Its not as simple as it looks",
        "tags": [
            "Aeronautics"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "shashank-ravichandir",
        "text": "[{% load static %}]()\nEverything in an aircraft you see is for a reason. Nothing is there coincidentally. And one such thing is the shape and design of the windows.\n\nWith increase in commutation through flights, the aeroplanes started flying at higher altitudes, since lesser air density meant lesser drag which directly affected fuel efficiency. Also turbulence due to atmosphere is less. But some modifications had to be done to make the conditions safe for the passengers at high altitudes. So, a pressurized cabin was introduced into the flights. This pressurized cabin keeps the interior of the flight at a higher pressure compared to the low atmospheric pressure. This initiates an expansion of the flight due to which a stress field is formed.\n\nThe shape of the window plays an important role in the formation of this field. In square windows used before, when the stress field line flowed and encountered the window, it changed its path sharply around the corners increasing the stress at the corners significantly. This high stress caused the formation of cracks in the windows leading to three tragic crashes. Later investigation and analysis proved the reason for the cracks, leading to change the shape of windows to oval or circular where the stress field lines bend smoothly without resulting in high stress concentration.\n\nAnother curious thing about the aircraft design is a tiny hole located at the bottom of the windows. The window is actually made of three panes. There is a gap between the outer pane and the inner pane in which air is filled to maintain a steady temperature gradient to prevent condensation of vapour on the middle pane. Now, this air, due to high pressure might crack the window open. This is where the little hole comes into picture. The hole located on the outer pane balances out the pressure difference between the atmosphere and the air in between the panes.\n\nThe third pane? That is the inner pane which is just added as an extra safety measure so that the passengers cause no damage to the middle pane.\n\nSo, the next time you see *anything* in an aircraft, remember there\u2019s loads of engineering behind it.",
        "author_name": "Shashank Ravichandir",
        "slug": "design-of-aircraft-windows"
    },
    {
        "layout": "post",
        "title": "How QR code was invented",
        "author_github": "shreeramhegde",
        "date": "2017-11-12T16:37:21",
        "image": "/assets/img/",
        "description": "Ever wondered how QR Code is scanned by your mobile?",
        "tags": [
            "QR code"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "shreeramhegde",
        "text": "[{% load static %}]()\n## How QR code was invented\n\nDenso Wave a subsidiary of the Toyota Group are attributed with the creation of the QR Code back in 1994. Originally it was used to track parts in the vehicle manufacturing industry, but its use and scope has since grown tremendously. One of the main objectives when creating the QR Code was to come up with a new barcode that allowed more data to be encoded without increasing the size (dimensions / surface area) of existing barcodes. This was achieved by including a second dimension.\n\nThe QR Code was born.\n\n## Working of QR code\n\nThe conventional one dimensional barcodes used on virtually every consumer product are mechanically scanned. That is, they're read by physically bouncing a narrow beam of light onto the code, which can be interpreted using the pattern of light reflected off the white gaps between the lines.\n\n![QR]({% static '/blog/assets/img/qrCode/image1.jpeg' %})\n\nQR codes, on the other hand, can not only hold 100 times more data than 1D barcodes\u2014they can also be digitally scanned. The block of smaller black and white squares is read by a smart phone's image sensor, then interpreted by the system processor. The three large squares act as alignment targets, while the smaller square in the remaining corner acts to normalize the size and angle of the shot. The blue strips near the alignment squares contain formatting information, and the remaining yellow area is the actual data that's converted into binary code and checked for errors before being displayed. The encoded data can be interpreted as one of four primary modes\u2014numeric, alphanumeric, byte/binary, and Kanji. Other forms of data can also be displayed with the appropriate extensions.\n\nThe most common QR Code type is broken down in the following information identifiers:\n\n![QR]({% static '/blog/assets/img/qrCode/image2.png' %})\n\n**Version**\u00a0and\u00a0**format**\u00a0information are important for the scanning device to know what kind of data to expect. Meanwhile, the **data**\u00a0can be slightly smeared or missing and still be readable. This depends on the\u00a0error correction level being used when writing the code.\n\nAnd did you know that you can rotate QR Codes however you like? Upside down will still work! This is a courtesy of the position patterns (squares with dots in the middle) that allow the code to be read from any direction in 360 degrees.\n\nMeanwhile, the\u00a0**alignment**\u00a0patterns are used to assist in navigation of larger codes and the\u00a0**timing**\u00a0patterns are used to determine the size of modules. The\u00a0**quiet zone** requires a margin of at least 4-module worth.\n\n## How much can we store?\n\nWe can store up to 4,296 alphanumeric characters. One alphanumeric character is either a character from A to Z or a number from 0 to 9. With the need of special characters like ,.!/$\u00a7% or \u00f6\u00d6\u00e4\u00c4\u00fc\u00c4\u00df, the capacity shrinks down to 2.956 Bytes. But that\u2019s still plenty.",
        "author_name": "Shreeram Hegde",
        "slug": "how-qr-code-was-invented"
    },
    {
        "layout": "post",
        "title": "The Halting Problem",
        "author_github": "ShashankP19",
        "date": "2017-11-12T20:36:15",
        "image": "/assets/img/",
        "description": "Does there exist a program to check an infinite loop?",
        "tags": [
            "Complexity theory",
            "Logic",
            "Mathematics"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "ShashankP19",
        "text": "[{% load static %}]()\n## Introduction\n\nIf you have written computer codes, then you certainly would have fallen into the trap of infinite loops. Then you might have wondered if there is an algorithm to check if any given computer program runs into an infinite loop or not; in other words, to check if it halts or not. How wonderful it would be if such an algorithm exists! But alas, the truth is that there is no such algorithm, not because no one has found such an algorithm yet, but because there is no way of finding such an algorithm ever. \n \nThe **halting problem** is the problem of determining, from a description of an arbitrary computer program and an input, whether the program will finish running or continue to run forever. Now let me define two terms- **decidable** and **undecidable**. A problem is\u00a0decidable\u00a0if it has a solution. If there is no algorithm that solves the problem in a finite amount of time, the problem is\u00a0undecidable. \n\n![Halting-problem]({% static '/blog/assets/img/the-halting-problem/halting-problem.png' %})\n\n## A Little History\n\nAre all problems decidable? Given enough thought, can we always come up with a well-defined procedure that takes some input and answers a given question about it? At the start of the 20th century, the belief was that this was true. Mathematicians (there were no computer scientists back then!) believed that we would eventually discover tools that we could use to answer any question we wanted (provided we could express it in the language of logic).\n\nIn 1931, **Kurt G\u00f6del** shocked them all by proving that this was impossible. Using some very clever techniques, he showed that as soon as we devise a system that's sufficiently powerful and well-behaved to encompass mathematical reasoning, that system will necessarily contain a statement that we could never prove is true, even though it\u00a0is\u00a0true.\n\nA few years later, **Alan Turing** proved an analogous theorem in Computer science. He showed that there must exist undecidable problems, questions for which there is no definable solution. His proof relied on some of the same clever techniques used by G\u00f6del. The halting problem is one such undecidable problem\n\n![Turing]({% static '/blog/assets/img/the-halting-problem/turing.png' %})\n\n## Turing\u2019s Argument \n\nLets say I came up with an algorithm to solve the halting problem. Then I can write a computer program for this. For simplicity, wrap this program inside a black box. Let\u2019s call this black box - \u201cDoes it halt?\u201d. \u201cDoes it halt?\u201d takes a program as input and outputs \u201cyes\u201d if the input program halts and outputs \u201cno\u201d if the input program does not halt. \n\n![Does it halt]({% static '/blog/assets/img/the-halting-problem/img1.jpg' %})\n\nNow let me create another program that uses \u201cDoes it halt?\u201d as a subprogram. If the output of \u201cDoes it halt?\u201d subprogram is \u201cyes\u201d, then it  loops forever (like a while(1){}; in C language). If the output of \u201cDoes it halt?\u201d  subprogram is a \u201cno\u201d, then it halts and outputs \u201dhalted\u201d. Lets call this new program \u201cParadox\u201d. \n\n![Paradox]({% static '/blog/assets/img/the-halting-problem/img2.jpg' %})\n\nNow here is the paradox. What happens when I input \u201cParadox\u201d program to itself. Clearly one of the two things can happen:  either it runs forever or it stops and outputs \u201chalted\u201d depending on whether the program \u201cDoes it halt?\u201d outputs \u201cyes\u201d or \u201cno\u201d.\n\n* If \u201cParadox\u201d goes into an infinite loop, it is because \u201cDoes it halt?\u201d outputs \u201cyes\u201d. Since \u201cDoes it halt?\u201d outputs \u201cyes\u201d, we conclude that \u201cParadox\u201d halts. But this is a contradiction to the assumption that \u201cParadox\u201d goes into an infinite loop.\n\n* If \u201cParadox\u201d halts and outputs \u201chalted\u201d, it is because \u201cDoes it halt?\u201d outputs \u201cno\u201d. Since \u201cDoes it halt?\u201d outputs \u201cno\u201d, we conclude that \u201cParadox\u201d does not halt. But this is a contradiction to the assumption that \u201cParadox\u201d halts.\n\nWe have therefore led ourselves to a contradiction that \u201cParadox\u201d **halts if and only if it runs forever**. Since this is impossible, one of our assumptions must be invalid. By tracing our reasoning backwards, we find that it is therefore impossible to have written\u00a0\u201cDoes it halt?\u201d\u00a0in the first place. In other words, the **halting problem is undecidable**.\n\n## Equivalent problems in real life\n\nA lot of really practical problems are the halting problem in disguise. A solution to them solves the halting problem.\n\n1. You want a compiler that finds the fastest possible machine code for a given program?   Actually the halting problem.\n2. You have JavaScript, with some variables at a high security levels, and some at a low security level. You want to make sure that an attacker can't get at the high security information. This is also just the halting problem.\n3. You have a parser for your programming language. You change it, but you want to make sure it still parses all the programs it used to. Actually the halting problem.\n4. You have an anti-virus program, and you want to see if it ever executes a malicious instruction. Actually just the halting problem.\n\n## Cool Links\n* [The Freeze App Does Not Exist](https://thorehusfeldt.wordpress.com/2012/06/25/the-freeze-app-does-not-exist/)\n* [The Halting Problem](http://www.cgl.uwaterloo.ca/csk/halt/)\n* [Poetic proof of Halting problem](https://www.cs.rice.edu/~vardi/comp409/scooping.pdf)",
        "author_name": "Shashank P",
        "slug": "the-halting-problem"
    },
    {
        "layout": "post",
        "title": "Evacuated Tube Collectors",
        "author_github": "sanathbk",
        "date": "2017-11-20T00:00:30",
        "image": "/assets/img/",
        "description": "Extracting solar radiation in one of the more efficient ways",
        "tags": [
            "Energy",
            "Renewables",
            "Thermal Sciences"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "sanathbk",
        "comments": true,
        "text": "[{% load static %}]()\n## Introduction\n\nRenewable energy is the key for the planet\u2019s existence in the future. The fossil fuels are non-renewable and require finite resources, which are dwindling because of high cost and environmentally damaging retrieval techniques. Climate change is real. Global warming is real. It took some time to accept this hard fact, but now with the realization, production of clean energy has become the primary goal of countries across all continents.\n\nGreat amount of research is carried in improving the efficiency of renewable energy and making it inexpensive. The main objective is to minimize or eliminate the usage of electricity produced by burning of coal which leaves behind large amounts of CO2. Solar Powered houses is the solution to this problem. One of the key components of this being the solar water heater.\n\n## Need\n\nThe conventional solar water heaters used flat-plate collectors with water as the heat-transfer fluid. These type of collectors are less efficient. This is owing to the heat losses in the form of convection and radiation. The simple fundamental to increase the efficiency is to decrease the heat losses. We know that convection requires material medium. So,if there is no material medium, there won\u2019t be convection and thus, heat losses would be only due to radiation. This is the basic idea behind evacuated tube collectors. Solar water heating systems using vacuum tubes made of borosilicate glass with special coating to absorb the solar energy are called as Evacuated Tube Collector system (ETC Systems).\n\n## Working\n\nAir between the two glass tubes is evacuated. It results in high level vacuum which acts as insulation and minimizes heat loss from the inner tube. The black coating in the inner tube absorbs the solar energy and transfers it to the heat transfer fluid. \n\n![Evac details](https://www.homepower.com/sites/default/files/articles/ajax/docs/5_Evac-detail-shadows.jpg)\n\nAn evacuated tube collector can achieve high temperatures in the range of 350K-450K, and work very efficiently. Although, they are more expensive than the flat plate collectors, the return would be greater for ETCs.\nThere are a few different vacuum tube configurations, single wall tube, double wall tube, direct flow, heat pipe, and these differences can determine how the fluid is circulated around the solar hot water panel.\n\nIn heat pipe ETC, a heat pipe is connected to an absorber plate inside a vacuum sealed tube.\n \n![Heating](http://www.alternative-energy-tutorials.com/images/stories/heating/alt36.gif)\n\nThe heat pipe is hollow and is evacuated. Inside the heat pipe is small quantity of heat transfer fluid like alcohol with some additives. The heat pipe is connected to a manifold. Vacuum enables the liquid to boil at lower temperatures. The hot vapour rises and heats the heat pipe which heats the manifold. The fluid flowing through manifold picks up the heat. The connection between absorber and manifold is dry and thus can be easily setup and replaced.\n\nIn direct flow ETC, two pipes run through the vacuum sealed tube. \n\n![evactube](http://www.alternative-energy-tutorials.com/images/stories/heating/alt38.gif)\n\nThe fluid to be heated directly flows through these pipes and heats up. The arrangement is similar to flat-plate collectors, with the addition of evacuated tubes. Since the fluid flows into and out of each tube, it cannot be easily replaced.\n\nThe need was to minimize heat losses through convection and what better way can be, than to stop it completely from happening. The application of the simplest of concepts of eliminating convection by getting rid of the material medium itself has led to major improvements in this field as the efficiency has risen manifold.The understanding of fundamental sciences has provided one of the biggest leaps in the effort of making renewable energy efficient and inexpensive.",
        "author_name": "Sanath K.",
        "slug": "evacuated-tube-collector"
    },
    {
        "layout": "post",
        "title": "Machine Learning for Stock Market Investment",
        "author_github": "suraj44",
        "date": "2017-11-20T00:00:30",
        "image": "/assets/img/",
        "description": "An introduction to Machine Learning and how it can be applied to the stock market",
        "tags": [
            "Machine Learning",
            "Stock Market",
            "Support Vector Machine"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "suraj44",
        "text": "[{% load static %}]()\n> Arthur Lee Samuel, the man who coined the term \"machine learning\" defined it as the field of study that gives computers the ability to learn without being explicitly programmed. \n\nA good analogy to how it actually works would be teaching a child to play chess by locking him in a room with nothing but the rules of the game and a record of say, a hundred previous games (and of course, food and water). After a period of five days, the child would walk out of the room as a pretty good chess player. In our analogy, the child is the 'machine', the record of chess games would be the data (which the child is 'learning' from) and the rules of the game would be the algorithm.\n\n![overview](https://i.imgur.com/VG0it3w.png)\n\nNow, let's talk about how this can be applied to predicting the likelihood of a stock doing well.\n\n\n## 1. The Algorithm\n\nWe'll be using a *Support Vector Machine* (SVM) in this example. SVMs are based on the idea of finding a hyperplane that best divides a dataset into two classes, as shown in the image below. How do we find the right hyperplane? The distance between the hyperplane and the nearest data point from either set is known as the margin. The goal is to choose a hyperplane with the greatest possible margin between the hyperplane and any point within the training set, giving a greater chance of new data being classified correctly. However, if there isn't a clear margin between the support vectors like there is in the image below, things get tricky. To overcome this problem, we would use a kernel which would transform our support vectors to a different coordinate system which would make the classification problem easier. \n\n![SVM](http://blog.aylien.com/wp-content/uploads/2016/06/tumblr_inline_o9aa8dYRkB1u37g00_540.png) \n\n![Margins](http://blog.aylien.com/wp-content/uploads/2016/06/tumblr_inline_o9aa9nH3WQ1u37g00_540.png) \n\n![UseOfAKernel](http://blog.aylien.com/wp-content/uploads/2016/06/tumblr_inline_o9aabehtqP1u37g00_540.png)\n\n## 2. The Tools\n\nThere are a few challenges to our problem. Firstly, the stock market is closed on weekends and hence, there are no open/close prices for any stocks on those days which hampers our dataset. Hence, we would need to interpolate the data on these days to better our prediction. Secondly, the stock market is highly fluctuating and hence, we would need to use a technique called smoothing. Here's an example of how data looks before and after applying smoothing: \n\n![Smoothing](https://i.imgur.com/tJSnvVq.png)\n\nThe most important tool needed however for machine learning is the dataset itself. A good resource for data on the stock market is [Quandl](https://www.quandl.com/product/WIKIP/WIKI/PRICES-Quandl-End-Of-Day-Stocks-Info). [*Here's*](https://raw.githubusercontent.com/michaelgu95/machine-learning-stocks/master/YAHOO-INDEX_GSPC.csv) a dataset of ten years' data on SMP 500 Companies in the US and thirty five features to learn on. The SVM algorithm will try seeing if there is any correlation between these parameters and stock price.  We will be using the Python framework Scikit-learn which allows us to use Machine Learning tools almost as a black box, without having explicit knowledge on how these algorithms work.\n\n\n## 3. The Prestige\nHere's some code on preprocessing the dataset from Quandl that would transform the data into a form recognisable to the SVM:\n\n\n    def key_Stats(gather=[\"Total Debt/Equity\",'Trailing P/E','Price/Sales','Price/Book','Profit Margin', 'Parameters that we wil learn on']):\n\t    L = []\n\t    statspath = path + '/_KeyStats'\n\t    stock_list = [x[0] for x in os.walk(statspath)]\n     \t\n\t    sp500 = pd.DataFrame.from_csv(\"YAHOO-INDEX_GSPC.csv\")\n\t    ticker_list = []\n     \t\n\t    for each_dir in stock_list[1:]:\n     \t\teach_file = os.listdir(each_dir)\n     \t\t\n     \t\tticker = each_dir.split(\"/\")[6]\n     \t\tticker_list.append(ticker)\n     \t\tstarting_stock_value = False\n     \t\tstarting_sp500_value = False\n     \t\tif len(each_file)>0:\n     \t\t\tfor file in each_file:\n     \t\t\t\tdate_stamp = datetime.strptime(file,'%Y%m%d%H%M%S.html')\n     \t\t\t\tunix_time = time.mktime(date_stamp.timetuple())\n     \t\t\t\tfull_file_path = each_dir + '/' + file \n     \t\t\t\tsource = open(full_file_path, 'r').read()\n\t\t\t\t    value_list = []\n\t\t\t\t    for each_data in gather:\n\t\t\t\t\t    #Preprocessing for determining stock price\n\t\t\t\t\n\t\t\t\t    try:\n\t\t\t\t\t    stock_price = float(source.split('</small><big><b>')[1].split('</b></big>')[0])\n\t\t\t\t    if not(starting_stock_value):\n\t\t\t\t\t    starting_stock_value = stock_price\n\t\t\t\t    if not(starting_sp500_value):\n\t\t\t\t\t    starting_sp500_value= sp500_value\n\n\t\t\t\t    stock_p_change = (stock_price - starting_stock_value)/ starting_stock_value *100\n\t\t\t\t    sp500_p_change = (sp500_value - starting_sp500_value)/starting_sp500_value * 100\n\n\t\t\t\t    difference = (stock_p_change - sp500_p_change)\n\t\t\t\t    if difference > 0:\n\t\t\t\t\t    status = \"outperform\"\n\t\t\t\t    else:\n\t\t\t\t\t    status = \"underperform\"\n\t\t\t\t    if value_list.count(\"N/A\")>0:\n\t\t\t\t\t    pass\n\t\t\t\t    else:\n\t\t\t\t\t    d  = {...} #Dictionary of parameters for the stock being traversed \n\t\t\t\t\t    L.append(d) #Adding to the list \n\t\t\t\t\n \t\n\nAt this point, we would apply the smoothing technique on the data. The final step is just feeding the data to the algorithm and letting it work it's magic!\n\n\n    def analysis():\n        # Fit the SVC (exclude the last column).\n        X_train, y_train = build_data_set()\n        clf = svm.SVC(kernel=\"rbf\", C=8, gamma=0.3)\n        clf.fit(X_train[:-1], y_train[:-1])\n\n        # Now we get the actual data from which we want to generate predictions.\n        data = pd.DataFrame.from_csv(\"YAHOO-INDEX_GSPC.csv\")\n        data = data.replace(\"N/A\", 0).replace(\"NaN\", 0)\n\n        X_test = preprocessing.scale(np.array(data[FEATURES].values))\n        Z = data[\"Ticker\"].values.tolist()\n        invest_list = []\n\n        # If our SVM predicts outperformance, append that stock to an invest_list.\n        for i in range(len(X_test)):\n            p = clf.predict(X_test[i])[0]\n            if p:\n                invest_list.append(Z[i])\n\n        return invest_list\n\nWe're comparing the performance of each stock represented by  `stock_p_change` against how the market collectively is doing which is represented by `sp500_p_change`. If it outperforms the market, we would consider it as a stock that's doing well and assign it the parameter `outperform`. This is equivalent to a person 'winning' a game of chess as used in the analogy above and is what the SVM uses in its classification. \n\nWe're done! `invest_list` would represent which stocks are likely to do well as predicted by our SVM. With about 150 lines of code, we've managed to get a prediction accuracy of 63%.\n\n### Room for Improvement\nThe stock market isn't as dependant on its history as it is on current events and how companies are doing in real life. One way to increase our prediction accuracy would be to parse the headlines of newspaper articles relating to the companies we're interested in and use that as data in addition to the other parameters. [Scikit-learn](http://scikit-learn.org/stable/) really is meant for beginners just getting started on machine learning. The next step would be to move to a framework like [TensorFlow](https://www.tensorflow.org/) and either write your own code for applying a machine learning algorithm on your dataset or to use the built in functions of the framework which do offer better results.\n\n### Resources\n\nTo get started on Machine Learning, I would highly recommend watching the [MIT Open Courseware](https://www.youtube.com/watch?v=TjZBTDzGeGg&list=PLUl4u3cNGP63gFHB6xb-kVBiQHYe_4hSi) lectures. \n\nAlong with this material, getting started on small [projects](https://elitedatascience.com/machine-learning-projects-for-beginners) in Python is the only way to get the sense of achievement needed to stay driven. \n\nHappy learning!",
        "author_name": "Suraj Singh",
        "slug": "machine-learning-for-stock-market-investment"
    },
    {
        "layout": "post",
        "title": "Radio Frequency Identification",
        "date": "2017-11-20T00:30:02",
        "image": "/assets/img/",
        "description": "The Radio-frequency Identification technology is game changer.",
        "tags": [
            "RFID",
            "ADC"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "mehakarora",
        "comments": true,
        "text": "[{% load static %}]()\nRFID technology has been in circulation for many years now. You may have encountered it first-hand while borrowing library books, or while swiping for attendance in school. Recently, the government issued RFID chips to be installed in cars so that we can zoom through toll booths instead of waiting in long queues. RFID technology has also got a fair share of screen time. Remember Casino Royale? Her Majesty's Secret Service injects Bond with a glass-enclosed, rice-sized\u00a0RFID\u00a0chip, so that, in M\u2019s words, \u201cthey'll know where he is\u201d. Here is a little insight into the technology that is taking the world by storm.\n\n## What is RFID?\n\nRadio-frequency Identification is an Automated Data Collection (ADC) technology that uses radio-frequency waves to transfer data between a reader and an object with an RFID chip. It is used to track, categorise, and uniquely identify objects. The main advantage of RFID over other ADC technologies like barcode, Optical Character Readers etc., is that the object need not be in the line of sight of the reader. It uses tags which act as labels for the object to be identified. Tags generally have an integrated circuit for processing and storing information that modulates and demodulates the RF signals and antenna for receiving and transmitting the signals. Passive tags have a device that collects DC power from the reader. Active tags are powered by a locally connected battery, and have a much longer read range than passive tags. The reader, or interrogator, is a two-way radio transmitter-receiver which\nsends signals to the tags and reads their response.\n\n![RFID](https://i.imgur.com/UwZIYoE.png)\n\n## Real World Applications of RFID\n\n1.  Supply Chain Management, Manufacturing and Processing industry: RFID is used to take inventory and track manufactured products, tools and other assets.\n\n2.  Race Timing: Timing marathons and races are one of the most popular uses of RFID. The runners wear tags which are read by readers placed alongside the tracks. This is a huge advantage while timing marathons and other races with a large number of participants because participants don\u2019t need to start as batch.\n\n3.  Access Control: Certain areas require a higher level of security. From doors to parking lots,\u00a0[RFID access control tags](http://www.atlasrfidstore.com/access-control-rfid-tag-sample-pack-uhf-passive/)\u00a0restrict access to only those pre-approved.\n\n4.  Library systems: RFID tagged books improve the efficacy of the borrowing process. While barcodes require line of sight, RFID tags can be read from multiple angles which means the checkout and check-in process is significantly faster. Also, taking inventory of books on the shelf is dramatically faster.\n\n5.  Smart Groceries: RFID Tags are added to all grocery items. While leaving the store, the customer walks through an RFID interrogator which rings up the items in the cart. This greatly reduces the checkout wait time.\n\n6.  Smart Fridges: By recognising what is being put in and what is being taken out, smart fridges can make their own shopping lists and can even suggest recipes based on what is present in the fridge.\n\nLikewise, RFID can be used to make several things \u201csmart\u201d such as smart currency, for anti-counterfeiting and tracking, or smart airline tickets which indicate the location of the airport. In recent years, with the advent of the Internet of Things, the scope of RFID technology has increased by manifolds. However, as it is with most new, disruptive technologies, it can be abused in ways that have generated breach of privacy concerns, as tags can easily be used to track people instead of just objects. But if used smartly, this technology can go a long way in making the world around us smarter.",
        "author_name": "Mehak Arora",
        "slug": "rfid"
    },
    {
        "layout": "post",
        "title": "A step ahead in Reverse Engineering!",
        "author_github": "adwait1-G",
        "date": "2017-12-07T02:00:10",
        "image": "/assets/img/",
        "description": "A peek at how a computer can be fooled because of bad coding practices",
        "tags": [
            "CTF",
            "Ethical Hacking",
            "Security"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "adwait1-G",
        "comments": true,
        "text": "[{% load static %}]()\nHello to all the computer enthusiasts out there!\n\nIn this article,we will look into a simple C program intended to perform a particular task is MADE to perform a task it was not supposed to perform.\nSO,WE WILL BE TRYING TO FOOL THE COMPUTER!\n\n# Pre-requisites :\n1. A computer which runs Linux.     \n2. A curious mind which wants to know how stuff actually works!   \n\nWe will be using GDB(GNU Debugger) to understand the C program at the assembly level.    \n\nIf GDB is not installed in your box,you can type in this command to install it.    \n\n~~~~\nsudo apt-get install gdb   \n~~~~\n\nHere is the source code of the executable we will be dealing with. It is named overflow.c. You will know by the end of this article,Why it was named so.  \n![Image1]({% static '/blog/assets/img/A-step-ahead-in-Reverse-Engineering/source_code1.png' %})\n\n## What does this program do?\n\n**a.** In the main function,a printf() function is called to print the string \u201cBefore function call\u201d .  \n**b.** The main function simply calls another function \u201cprint_string()\u201d .   \n**c.** In the print_string() function , We have an uninitialised character array and WE HAVE ALLOCATED 30 BYTES OF SPACE for it.    \n**d.** In this function,Using gets() function , it asks us to enter a string.    \n**e.** It will simply print the string that we have entered.    \n**f.** After the \u201cprint_string\u201d function is executed completely,CONTROL IS TRANSFERED BACK TO THE main function.    \n**g.** At the end,there is another printf () statement which prints \u201cAfter function call\u201d.    \n\nThink for a moment why the 2 printf() statements are present before and\nafter the function call.\n\n## Compile the program(normally) using gcc :\n\n~~~~\ngcc overflow.c -o overflow   \n~~~~\n\nThis gives an executable named \"overflow\".   \n\n![Image2]({% static '/blog/assets/img/A-step-ahead-in-Reverse-Engineering/warning.png' %})\n\nIt is important to note the second warning(the overflow.c : (.text + 0x39))\n**It says \"the gets() function is dangerous and should not be used\".**\n\nYou would have used gets() before. **Have you given a thought of why this warning came up?**\n\n## Running the executable with random inputs and observing what happens.  \n\n![Image3]({% static '/blog/assets/img/A-step-ahead-in-Reverse-Engineering/random_input.png' %})\n\nLet us go step by step.\n\n**a.** The first 2 times it was executed,These are normal cases because the number of characters were less than 30.   \n**b.** The third time,The string length is greater than 30 but nothing **wierd** happened.   \n**c.** The last time it was executed,we get this \" **stack smashing detected** \" and the program is terminated right there. IF YOU NOTICE,The string \"After function call\" is not printed. This means BEFORE THE CONTROL GOT TRANSFERRED TO THE main function,THE PROGRAM WAS TERMINATED. Why did this happen? **One obvious guess is that our string length was way more than 30 bytes.** But what exactly happened??    \n**d.** To analyse this,we will have to compile our code in the following manner.   \n\n~~~~\ngcc overflow.c -o overflow -fno-stack-protector -zexecstack -g   \n~~~~\n\n**e.** One more thing,we will have to give random inputs like above to analyse\nthe behavior of the executable. So everytime typing \u201caaaaaaaaaaaaaa....\u201d\n100 a\u2019s for example is very cumbersome.WE HAVE A SOLUTION!   \n\nWe will use **python** to solve this problem.\nExecute the program in following manner :  __$ python -c \"print 'a' * 35\" | ./overflow__\n\n![Image4]({% static '/blog/assets/img/A-step-ahead-in-Reverse-Engineering/use_python.png' %})\n\nThe 35 can be replaced by any number you want as showed in the above screenshot.\n\n## Analyse the above screenshot   \n\n__a.__ Though the string storage capacity is 30 bytes,in the first case,the program is able to take 35 bytes.  \n__b.__ In the third case,something __wierd__ happened. It says \"__Illegal Instruction__\" and \"After function call\" was __not__ printed. So,control didn't get transferred to the main program.   \n__c.__ In the last case,again something __wierd__ happened. It said \"__segmentation fault\"__ and the program was __terminated right away__.   \n\nWe obviously have to investigate the wierd cases and know if we can do something about it.\n\n## Finally, it is time to fireup your debugger!    \n\n~~~~\n$ gdb -q overflow\n~~~~\n\n![Image5]({% static '/blog/assets/img/A-step-ahead-in-Reverse-Engineering/disass.png' %})\n\nFollow the instruction in the screenshot.\n\n__a. (gdb)set disassembly-flavor intel__ : This means there are other flavors also right?Research about them.   \n__b. (gdb)disass main__  : This dumped the assembly equivalent of the main function written in C.   \n__c. (gdb)disass__ print_string() : This dumped the assembly equivalent of the print_string() function.   \n\n## Analysis OF main() :    \n\n__a.__ In the main function,we had 3 main tasks 1.printf() , 2.function call 3.again a printf().   \n__b.__ We can easily figure out that tha main<+9> instruction is the printf(\"\\nBefore function call\\n\"), and main<+29> is printf(\"\\nAfter function call\\n\");   \n__c.__ What does main<+19> do? It says __<+19> call 0x40058f__ < print_string > . From this,it is clear that our \"print_string\" function is __called at <+19> instruction.(Address of this instruction is 0x400579)__.   \n\n## Analysis of print_string() :\n\n__a.__ push rbp\n       mov rbp,rsp\n       sub rsp,0x20\n\nWhat this does is,The system is making space(__in the stack__) to store our 30 bytes(string) .\n__b.__ print_string<+20> ,the __gets()__ is called.__(important!)__   \n__c.__ print_string<+32> ,the __puts()__ is called.   \n\nAfter the execution of print_string(), How does the computer know that control should __return__ ed back to main() again? __Notice that there is something called \"ret\"__ in print_string<+39> .\n\n(__NOTE : The 0x0000000000400566 is the starting point of main() IN MY COMPUTER. IT MIGHT BE DIFFERENT IN DIFFERENT COMPUTERS__).\n\n## Note our observation points and then run the program.\n\n__a.__ Let us stop before print_string() is called.   \n__b.__ Let us stop after print_string() is called and then go step by step.   \n\nWe can stop using the \"__break__\" instruction of gdb.\n\n![Image6]({% static '/blog/assets/img/A-step-ahead-in-Reverse-Engineering/break_points.png' %})\n\nFirst,we break at main() and then at print_string() .Now we run the program.__$(gdb)run__\n\n__c. (gdb)ni__-> means __next instruction__.Using \"ni\",we can analyse each instruction as we go.\n__AT 0X400579__,print_string() is present.This is __important__.   \n\nFinally,we have stopped at gets(a).This instruction is not yet executed.\n\n![Image6]({% static '/blog/assets/img/A-step-ahead-in-Reverse-Engineering/before_gets.png' %})\n\nIn the above screenshot,\n__d. (gdb) x/32xw $rsp__ shows us the memory space (the space in stack) that system has allocated to store the string we input. There is something that is highlighted in the screenshot. Can you guess what this could be??   \n\n__e. 0x0040057e__ -> Address of the instruction after \"call print_string() \" in the main() function . [Go back to the assembly code of main() and check] . This means,when print_string() function is called, __THIS \"0X0040057e\" or THE RETURN ADDRESS is stored.__   \n\n![Image7]({% static '/blog/assets/img/A-step-ahead-in-Reverse-Engineering/after_gets.png' %})\n\n__f.__ After a few \"ni\"s,gets(a) is executed and we input our string.\nThen again,check the stack by using __$(gdb) x/32xw $rsp__ . (refer the above screenshot).\n\n__g.__ We will run it again with nore 'a's and see what happens.   \n\n__The space where return address or 0x0040057e is supposed to be present has now become 0x00400061.__ 0x61 is the ascii equivalent of 'a'.So, __Our string this time tried to overwrite the return address.__\n\nThe computer now __thinks__ that the return address is __0x00400061__ ,but that is an __invalid address__ (Or an address which is not used in this particular program).\n\nCheck out the screenshot below .\n\n![Image8]({% static '/blog/assets/img/A-step-ahead-in-Reverse-Engineering/illegal_instruction.png' %})\n\nLet us see and check what happens . After a few \"ni\"s,The string is printed and then \"__Segmentation fault__\"Remember we had got the same error in the very beginning . (before we started analysing with gdb) .\n\nNow __we know__ why the error occurred. __The address \"0x00400061\" was not accessible(or not a valid address)__ and that's why we got an error. The control was __NOT__ transferred back to main() .\n\nThink about it once,__What if we overwrite the actual return address by a new VALID address where the new address has instructions that WE want to execute?__\n\nTHIS WAY,WE CAN EXECUTE WHICHEVER INSTRUCTIONS WE WANT.The statement we had at the beginning (trying to fool the computer) is slowly coming to our light right?\n\n__h.__ Ok,now that we know the trick,we can give any valid address and the instruction in that address will be executed.\n\nWhat if we give the address of \"print_string()\", will it be executed twice,__Will it print the inputed string twice?__ . Let us see..\n\n__Let us note that the address of the instruction which calls__ print_string() __is at 0x0000000000400579 .__\n\n![Image9]({% static '/blog/assets/img/A-step-ahead-in-Reverse-Engineering/final.png' %})\n\nInstead of using python,we will use another useful tool,the __printf__ .\n\n~~~~\nprintf \"aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa\\x00\\x79\\x05\\x40\"|./overflow\n~~~~\nThere are 39 a's and the address which calls print_string() in the reverse order.(WHY IS THAT??)\n\nAND WE HAVE ACHIEVED IT!!The print_string() function was executed twice silently(with no wierdness) and control was returned back to main() again silently.\n\n__(Note:The number of a's that I have put to reach the space where return address is stored may or may not be same as the number of a's you might have to put to reach it. AND AGAIN,Our addresses need not be same.)__\n\nSo,analyse the assembly code properly,make a note of all the important addresses and then proceed.\n\n\n## There are a few things that were left unexplained:\n\n__1.__ Why the source code was named \"overflow.c\" :What you have just done is a simple but an authentic example of what is known as \"BUFFER OVERFLOW\" . The a[30] is the buffer we had to store the string. When the strig length exceeded 30,the string is said to have overflown the buffer.   \n\n__2.__ When compiled normally,we could not have done all the analysis that we did now. The latest compilers keep a check on buffer sizes and protects us from buffer overflows.   \n\n__3.__ Check why we added -fno-stack-protector and -zexecstack while compiling the sourcecode.   \n\n__4.__ Why we typed in the address in the reverse order in the last part?   \n\nAnd a few more for you to research.   \n\nThere was one more thing. __Executing instruction that WE wanted.__ That is definitely possible but is beyond the scope of this article. READ ABOUT THE ACTUAL IMPLICATIONS OF A BUFFER OVERFLOW. It is very interesting!\n\nI know this is a lot of stuff to know at once.Go through the article several times,understand each and every bit.\n\nI hoped you enjoyed the article and learnt something new out of it. Any kind of suggestion,feedback or appreciation:P ,leave a comment below.",
        "author_name": "Adwaith V Gautham",
        "slug": "a-step-ahead-in-reverse-engineering"
    },
    {
        "layout": "post",
        "title": "BoT",
        "author_github": "hebbarashwin",
        "date": "2017-12-08T02:00:10",
        "image": "/assets/img/",
        "description": "How IoT devices are being used in cyber-attacks",
        "categories": [
            "Diode"
        ],
        "github_username": "hebbarashwin",
        "comments": true,
        "text": "[{% load static %}]()\n## Botnet of things\n\nThe Internet of Things aka IoT is one of the hot trending topics in the tech world right now. The IoT is basically a network of devices connected to the internet, each having a unique IP address. Today, the Internet of Things plays with the idea of connecting nearly every manufactured object to the Internet. It is being incorporated into smart homes, watches, cars, fitness trackers, power grids, thermostats, contact lenses, pacemakers and what not.\n\nWhile the Internet of Things brings a new horizon of opportunities, there are several barriers in respect to making IoT implementations successful. These include grave security and privacy concerns. Recent distributed denial of service (DDoS) attacks are forcing a shift in how we think about the Internet of Things (IoT). The debate on the security risks posed by widespread use of IoT devices has intensified after the cyber-attack on DNS provider Dyn in October 2016, which affected many high-profile online service providers like Twitter, Netflix, GitHub and Quora. This was carried out using a malware named \u2018Mirai\u2019, which used a \u201cbotnet of things\u201d.\n\n## What is a botnet? \n\nA botnet is a collection of internet-connected devices, which may include PCs, servers, mobile devices and internet of things devices that are infected and controlled by a common type of malware. The hacker instructs these botnets to certain things like overloading a website with a huge number of requests to effectively shut it down, to send spam, to make money out of clickbaits, and to further spread the malware itself. These botnets are pretty hard to track and bring down.\n\n![Image1]({% static '/blog/assets/img/BoT/image1.jpg' %})\n\n## Botnet of things\n\nUntil fairly recently, devices used as components of botnets were usually Windows PCs. The problem, for hackers, with PCs is that their defence is being strengthened every day. Antimalware tools are pretty good at detecting problems and evicting botnets. Hence, in recent times, \u201cThingbots\u201d, i.e. compromising IoT devices for this purpose is a growing trend. IoT devices currently are not really sophisticated in terms of security. The meteoric rise of the IoT industry has forced manufacturers to prioritise impressive features and cot efficiency, leaving security as an afterthought. Most of the cheaper IoT devices on the market are not designed with security in mind, and their hardware isn\u2019t capable of supporting advanced encryption techniques. One of the biggest contributors to the risk of future attack is the fact that many IoT devices have long-standing, widely known software vulnerabilities that make them vulnerable to exploit and control by remote attackers. Many of these devices still have their default passwords. These are very easy targets for hackers, and are being used in botnets. The device will still work normally, with a lesser bandwidth and might be a bit sluggish.\n\nAlthough the Dyn incident caused large-scale disruption, it is noteworthy that the attack involved only a few hundred thousand endpoints and a traffic rate of about 1.2 terabits per second. IT firm Gartner has predicted that there will be more than 20 billion devices connected to IoT by 2020. If the IoT remains vulnerable to attacks of this nature, the risk of similar attacks, on a much larger scale is imminent.  \n\nSome experts are not alarmed by this, as the Dyn attack required 100,000 infected devices and taking control of so many devices on a regular basis is borderline impossible. However, devices that have been part of the\u00a0Mirai botnet\u00a0will always be infected. If another attack was to take place, these devices could be called upon as part of the botnet army. Once a device has been infected, it is very difficult to get rid of the virus, making discarding the device the only viable solution.\u00a0\n\n## How to mitigate this problem?  \n\nIoT botnets will continue to rise. Right now they are an easy resource to harvest. IoT device manufacturers must act now to build in better security capabilities and controls. Security standards must be set by the government for all IoT devices. Until manufacturers step up their game and create better security practices, it's up to cyber security firms and the public to be more informed in their daily interactions with smart objects. Default passwords should be changed and the devices should be secured. As\u00a0IoT\u00a0botnets becoming a clear-and-present danger, it is only the combined efforts of everyone involved \u2014 including consumers, manufacturers and IT pros \u2014 that can stop the threat.\n\n## Resources\n\n[Botnet of things](https://www.technologyreview.com/s/603500/10-breakthrough-technologies-2017-botnets-of-things/)  \n[The Weaponisation of IoT: Rise of the Thingbots](https://securityintelligence.com/the-weaponization-of-iot-rise-of-the-thingbots/)  \n[Role of IoT in the Dyn Attack](https://www.kaspersky.com/blog/attack-on-dyn-explained/13325/)",
        "author_name": "Ashwin Hebbar",
        "slug": "bot"
    },
    {
        "layout": "post",
        "title": "Airbag System for two Wheeler vehicles",
        "author_github": "kiran-kumarg",
        "date": "2017-12-08T02:00:10",
        "image": "/assets/img/",
        "description": "Is bike safety a concern , this new technology is here to protect you",
        "categories": [
            "Piston"
        ],
        "github_username": "kiran-kumarg",
        "comments": true,
        "text": "[{% load static %}]()\n# Introduction\n\nThe Airbag system was first introduced in 4-wheelers . This paper gives information about the introduction of the airbag system can be used in the two wheelers(bikes). This paper shows the working, construction, installation of this system and problems that can occur in this system.\n\nThe concept of this airbag system is \u201cTo reduce the injuries to a rider when impacting with an opposing vehicle and/or opposing object in frontal collisions by absorbing rider kinetic energy and by reducing rider separation velocity from motorcycle in the forward direction.\u201d With the help of the ANGLE sensor with an angle indicator and sense of the collision and the large frequency vibration for to open the air bag.\n\n# Design with required instruments\n\nThe following instruments are used for the 2 wheeler Airbag system \u2014\n\n1. Air bag (leather material with grip technology)\n\n2. Chemical cylinders for releasing the air or gas\n\n3. Sensors (angle sensor and crash sensor)\n\n4. Fitting cage\n\n5. Inflator\n\n## Air bag (leather material with grip technology)\n\nAirbags are stretchable fabrics or other materials that are tightly packed in various locations throughout your vehicle. These bags are compressed and kept in a small area. When there is an accident, the airbags fill up with air very quickly to provide a cushioning system for the people on the motorcycle so that they are not thrown around in the event of a crash. While this does not necessarily prevent total injury or death, it can be very helpful in cushioning the passengers in many cases.\n\n## Chemical reaction behind opening of airbag\n\nActual opening of air bag is due to the chemical reaction occurred in between two chemical cylinders which are fit at the bottom of bike. When an external force or collision of two objects with bike occur then the chemicals in those two chemical cylinders mix with each other and the chemical reaction takes place inside the cylinder.\n\nIn 2-wheeler vehicle system, the air bags are fit in both sides of the bike. Figure shows the position of the air bag system to be fitted so that the total protection of the vehicle can be covered hence they pedestrian safety can be achieved.\n\n## Sensors (angle sensor and crash sensor)\n\nThe most important parts behind the success of the airbag system are the sensors. These are small pieces of electronics that are designed to tell when the vehicle has been damaged in an accident. They respond to several different sets of stimuli, including sudden stopping, increased pressure as pieces of the motorcycle are moved due to the force of the collision, and angle between road surface and tires of motorcycle. Different types of sensors measuring wheel speed, seat occupant status, brake pressure and impact, and other vehicle status indicators are monitored by the airbag control unit located in the front portion of the cabin.\n\n## Fitting cage\n\n![Image1]({% static '/blog/assets/img/THE_AIRBAG_SYSTEM_FOR_2-WHEELER_VEHICLE/image1.jpeg' %})\n\nFigure 1 - Modification for the airbag container\n\nFigure 1 shows that the modification is required for the storage of airbag. This cage is made up of metal strips with two openings for airbag.\n\n![Image2]({% static '/blog/assets/img/THE_AIRBAG_SYSTEM_FOR_2-WHEELER_VEHICLE/image2.jpeg' %})\n\nFigure 2 -Airbag opens up on the  right side of bike as the angle exceeds 30 degrees.\n\nFrom figure 2  The angle between road surface and motorcycle tyres exceeding 30 degrees is detected by using an angle sensor, hence as given above chemical reaction takes place and airbag opens up on right side of the motorcycle. In this figure 2 shows that the airbag contains Grips to the lower surface of it.\n\n![Image3]({% static '/blog/assets/img/THE_AIRBAG_SYSTEM_FOR_2-WHEELER_VEHICLE/image3.jpeg' %})\n\nFigure 3-airbag opens up in both directions when any object collapses to the\nmotorcycle.\n\nWhen any object collides with the motorcycle ,at that time the airbag opens up in both directions to protect the passenger who is riding.\n\n## Inflator\n\nOnce the control unit determines there is an accident, it sends a signal to the inflator system. The inflator sets off a chemical charge, producing an explosion of nitrogen gas, filling up the airbag. As the airbag fills up, it bursts through the paneling that contains it in order to protect you.\n\nAll this happens in an instant, usually within 25 or 50 milliseconds. That translates to almost 200 miles per hour. The airbag then will deflate itself on its own once it gets deployed.\n\n# How air bag system protects the human body parts ??\n\nThe provision of air bags on motorcycles is more complex than installation in cars, because the dynamics of a motorcycle crash are more difficult to predict . But we discussed the following points on accident.These characteristics leads to act with very short reaction time and fast inflation but only if the motorcycle is involved in the accident . Limitation of this system is that they work properly only under particular conditions , especially  the rider must remain on the motorcycle during the accident and the impact dynamics must lead him to hit exactly\nthe part of his vehicle protected by airbags.\n\nNormally, following kinds of bodily harms occur in the accident\n\n-Hand/leg Cracks or fractures\n\n-Head injury\n\n-Bleeding from body parts\n\n-Getting thrown from bike\n\n![Image4]({% static '/blog/assets/img/THE_AIRBAG_SYSTEM_FOR_2-WHEELER_VEHICLE/image4.jpeg' %})\n\nThe passenger can be protected from the above mentioned bodily harms as described as, the air bag system opens from the bottom to the left or right side of bike; hence we can avoid hand/leg cracks or fractures. The diameter of the air bag when opened is more than height of the bike and that of the rider, hence head injury is avoided. As the material used in the air bag construction is light weight strong leather the chances of bike skidding or slipping is diminished and external injuries like scratches are avoided. The chances of the rider being thrown from the bike are avoided as the leather belts are provided on both sides of the bike to secure the legs of the rider. The material used for manufacturing these belts is similar to those used for manufacturing seat belts of cars.\n\n## Conclusion\n\nBy providing  total safety to the motorcycle rider by implanting the airbags on both sides of the motorcycle as mention in this article, we will reduce the fatality rate by 20% to 30%. By using this technology there is not only a reduce in death rate but also gives the total protection to the rider as well as motorcycle.",
        "author_name": "Kiran Kumar G",
        "slug": "the-airbag-system-for-two-wheeler-vehicle"
    },
    {
        "layout": "post",
        "title": "The Future of Computing: Quantum Computing",
        "author_github": "RosaGeorge31",
        "date": "2017-12-18T12:19:03",
        "image": "/assets/img/",
        "description": "An insight on how Quantum Computing could change the world!",
        "tags": [
            "Quantum Computing",
            "Qubits"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "RosaGeorge31",
        "comments": true,
        "text": "[{% load static %}]()\nLately, you must been hearing a lot about quantum computing. There are loads of news stories about how it \u201ccould change the world\u201d and \u201copen new dimensions\u201d. Universities are hyping up their quantum microchip prototypes, demonstrations of quantum mechanical ideas in silicon, and other devices and theories. But how does it work? What does it do? Who\u2019s doing it? Read this article to find out!\n\nEven though they may be fragile today as well as they need to be kept at temperatures close to absolute zero, Quantum computers aren\u2019t much like the desktop PCs we\u2019re all so familiar with\u2014they\u2019re a whole new kind of machine, capable of calculations so complex, it\u2019s like **upgrading from black-and-white to a full color spectrum**.\n\nCurrently we\u2019re in the midst of entering a new era of quantum computing called the **NISQ era**, for Noisy, Intermediate-Scale quantum computer as John Preskill, a CalTech theoretical physicist calls it. Quantum computing is more or less in the era that classical computing was in the \u201850s, when room-sized hulks ran on vacuum tubes. But it could potentially revolutionize computing. \n\nLet\u2019s first understand the mathematical theory of quantum mechanics. \n\n### When one thing is two things at the same time\nIn our universe, we are used to a thing being one thing. A coin, for example, can be heads, or it can be tails. But if the coin followed the rules of quantum mechanics, the coin would be flipping midair.(Amazing right?) So until it lands and we look at it, we don\u2019t know if it\u2019s heads or tails. Effectively, it\u2019s both heads and tails at the same time. \n\nWe do know one thing about this coin. There is a probability for the flipping coin to be either heads or tails. So the coin isn\u2019t heads, it\u2019s not tails, it\u2019s\u2014for example\u2014the probability of 20% heads and 80% tails. Scientifically speaking, how can a physical thing be like this? How do we even begin describe it?\n\nThe most mind-boggling part of quantum mechanics is that for some reason, particles like electrons seem to act like waves, and light waves like particles. Particles have a wavelength. The most basic experiment demonstrating this fact is the double slit experiment:\n\n![Double Slit experiment]({% static '/blog/assets/img/Future-of-computing-Quantum-computing/double_slit_expt.jpg' %})\n\nIf you put a pair of parallel slits in a partition between a beam of particles and a wall, and put a detector on the wall to see what happens, a strange pattern of stripes appear. It\u2019s called an interference pattern.\n\nLike waves, the particles-waves that travel through one slit interfere with those that travel through the other slit. If the peak of the wave aligns with a trough, the particles cancel out and nothing shows up. If the peak aligns with another peak, the signal in the detector would be even brighter. (This interference pattern still exists even if you only send one electron at a time.) If we were to describe one of these wave-like particles (before they hit the wall) as a mathematical equation, it would look like the mathematical equation describing our coin (before it hits the ground and lands on heads or tails).\n\nThese equations can look kind of scary, like this:\n![Equation]({% static '/blog/assets/img/Future-of-computing-Quantum-computing/Equation.png' %})\n\nBut all you need to know is that this equation lists the particle\u2019s definite properties but doesn\u2019t say which one you\u2019ll get.  You can use this equation to find the probabilities of some of the particle\u2019s properties.\nAnd because this math involves complex numbers\u2014those containing the square root of _i_ \u2014it doesn\u2019t just describe the probability of a coin being heads or tails, it describes an advanced probability, which could include the way the face of the coin will be rotated.\n\nFrom all this crazy math, we get a couple of crazy things:\n\n__1.__ There is __*superposition*__\u2014the midair coin being heads and tails at the same time.\n\n__2.__ There is __*interference*__\u2014probability waves overlapping and canceling each other out.\n\n__3.__ There is __*entanglement*__, which is like if we tied a bunch of coins together, changing the probability of certain outcomes because they\u2019re, well, entangled now. \n\nThese three phenomenon are exploited by quantum computers to make whole new kinds of algorithms.\n\n> \u201cQuantum computers turn the rules of computers on their heads.\u201d\n\n## How a quantum computer works\n\nAs  Martin Laforest, Senior Manager of Scientific Outreach at the Institute for Quantum Computing at the University of Waterloo has said,\u201cIn some sense we\u2019ve been doing the same thing for 60 years. The rules we use to compute have not changed\u2014we\u2019re stuck with bits and bytes and logic operations,. But that is all about to change. Quantum computers turn the rules of computers on their heads.\u201d\n\nThe traditional common digital computing requires that the data be encoded into binary digits(bits), each of which is always in one of two definite states (0 or 1), whereas quantum computation use quantum bits or qubits, which can be in superpositions of states.\n\n![Bits]({% static '/blog/assets/img/Future-of-computing-Quantum-computing/Bits.png' %})\n\nThe qubit is like a regular bit, but it\u2019s both a zero and a one at the same time (before you look at it). It\u2019s that coin flipping in midair. A quantum computer is like flipping multiple coins at the same time\u2014except while these coins are flipping, they obey the rules of superposition, interference and entanglement.\n\nThe quantum computer first bestows the qubits with this quantum mechanical version of probability of what will happen once you actually peep the qubit. (Once you peep the qubit it becomes a defined bit.) Quantum mechanical computations are made by preparing the qubits (or adding weights to a coin before you flip it to manipulate the probability of the outcome), then interacting them together (or flipping a bunch of entangled coins at once) and then measuring them (which causes the coins to stop flipping and produces the final value). If done properly, all of this mid-air interaction should result in a best answer (the value) to whatever question you\u2019ve asked the computer.\n\nQuantum computing algorithms perform calculations by manipulating these qubits via the mathematics of quantum mechanics. At its core, that math is just probability combined with complex numbers and the linear algebra. Quantum computers computes a special version of probabilities\u2014not just heads vs. tails but also the orientation of the coin. So as you throw these coins up in the air, they collide with each other with their different sides and orientations, and this changes the probability of the side revealed by the outcome. Sometimes they bump into each other and cancel each other out, making certain outcomes less likely. Sometimes they push each other along, making certain outcomes more likely. All this is interference behavior and the idea in modeling a quantum computer is to choreograph a pattern of interference so that everything cancels out except for the required answer. To the observer, the answer just looks like the output of regular bits. The quantum mechanics happens in the background.\n\n## Quantum Vs Classical:\n\n* __Information processing__\u2014In a classical computer, bits are processed sequentially, which is similar to the way a person would solve a math problem by hand. In quantum computation, qubits are entangled together, so changing the state of one qubit influences the state of others regardless of their physical distance. This allows quantum computers to intrinsically converge on the right answer to a problem very quickly. \n* __Interpreting results__\u2014In classical computing, only specifically defined results are available, inherently limited by algorithm design. Quantum answers are probabilistic, meaning that because of superposition and entanglement, multiple possible answers are considered in a given computation. Problems are run multiple times, giving a sample of possible answers and increasing confidence in the best answer provided. \n* __Information representation__\u2014In classical computing, a computer runs on bits that have a value of either 0 or 1. Quantum bits, or \u201cqubits,\u201d are similar, in that for practical purposes, we read them as a value of 0 or 1, but they can also hold much more complex information, or even be negative values. \n\n## A Physical Orbit:\nRegular computers were made by scientists using transistors and electrical switches which stores the bits. Similarly, they need hardware that can store a quantum bit. The key to producing a quantum computer is finding a way to model a quantum system that people can actually control\u2014actually set the probabilities and orientations of those flipping coins. This can be done with atoms trapped by lasers, photons, and other systems. But right now, almost everyone in the industry who\u2019s presented a quantum computer has done so with superconductors\u2014ultra-cold pieces of specially-fabricated electronics.\nThey look like really small microchips. Except these microchips get placed into room-sized refrigerators cooled to temperatures just above absolute zero. These superconducting qubits stay quantum for a long time while performing quantum computing operations.\n![4 Qubit]({% static '/blog/assets/img/Future-of-computing-Quantum-computing/4-Qubit.png' %})\n\nActually performing calculations with these qubits can be a challenge. Regular computers have error correction, places where multiple bits perform the same function in case one of them fail. For quantum computers to do this, they need to have extra qubits built into their system specifically to check errors. But the nature of quantum mechanics makes actually doing this error correction more difficult than it does in classical computers. It could take around two thousand physical qubits working in tandem, in fact, to create one reliable \u201cerror-corrected\u201d qubit resistant to messing up. \n\nBetter qubits and further research continue to bring us closer to the threshold where we can construct few-qubit processors. Now we\u2019re at the junction where the theoretical demand versus the reality of experiments are converging together.\n\n## Who\u2019s working on it?\nUniversities, national labs, and companies like IBM, Google, Microsoft and Intel are pursuing qubits set-ups in logic circuits similar to regular bits, all with less than 20 qubits so far. Companies are simultaneously simulating quantum computers with classical computers, but around 50 qubits is seen as the limit\u2014IBM recently simulated 56 qubits, which took 4.5 terabytes of memory in a classical computer.\n\n### A quick look at the progress of the mentioned companies:\n\n1. **IBM** is taking a long-term approach, hoping to one day release a general-purpose quantum computer that classical computers rely on, when needed, through the cloud. \nIntel has just entered the race with their 17-qubit processor released in October.\n\n2. **Microsoft** is working on a different kind of \u201ctopological\u201d qubit based on the behaviors of many electrons, and described a similar long-term goal for quantum computing involving scalable hardware. \n\n3. **Google** has been rumored to unleash a quantum computer that will achieve \u201cquantum supremacy\u201d with 49 or 50 qubits. Quantum supremacy simply means finding one single algorithm for which a quantum computer always win, and for which a classical workaround can\u2019t be found to solve the same problem. In Google\u2019s case, the team will set up a quantum circuit with their qubits by entangling them (essentially, setting up a quantum link between them) and then allowing the system to evolve over time. At the end, how the qubits evolve is set by the rules of quantum mechanics, but the final measurement could take on different values with different probabilities. Figuring out the possible outcomes of the qubits, alongside the probability of measuring the outcomes, is so complex that the classical computer needs to simulate the quantum computer in order to do so, and might take weeks to do what the quantum computer can do in minutes.\n\n4. **IonQ**,a startup will use atoms trapped by lasers.\n\n5. **D-Wave**, a longtime quantum computing business uses a specialized system called quantum annealing to perform optimization problems. Rather than just a dozen to a few dozen qubits, D-Wave has announced a computer with 2,000 qubits and rather than rely on quantum logic circuits like the rest of the pack, their computer solves one type of problem\u2014optimization problems, like finding the best solution from a range of okay solutions, or finding the best taxi route from point A to point B staying as far as possible from other taxis. These kind of problems are potentially useful in finance.\n\n## The outlook\nEveryone agrees that we\u2019re far from seeing quantum computers used in everyday life\u2014there\u2019s a lot of excitement but we\u2019re still in the early days. There are still loads of challenges to overcome, like error correction. Then comes the related problem of transmitting quantum information between distant computers or storing quantum information long term in memory.\n\nWhile 2017 seems to be a year amidst a sort-of quantum boom, having a consumer-facing quantum computing product is still far off milestone. So no, you cannot own a quantum computer now. You hear about some benefits of quantum computing in the next few years, like biochemical advances, but other advantages could be 20 years down the line but it will definitely be worth the wait!\n\n## Some Cool Links to checkout!\n\n[Try Your Hand at Quantum Computing](http://research.ibm.com/ibm-q/quantum-card-test/)\n\n[IBM\u2019s 5 qubit quantum computer](https://arstechnica.com/science/2016/05/how-ibms-new-five-qubit-universal-quantum-computer-works/)\n\n[Want a quantum experience?](https://quantumexperience.ng.bluemix.net/qx/editor)",
        "author_name": "Rosa Anil George",
        "slug": "future-of-computing-quantum-computing"
    },
    {
        "layout": "post",
        "title": "The E INK",
        "author_github": "nightzoomer",
        "date": "2017-12-21T16:40:03",
        "image": "/assets/img/",
        "description": "An Apple iPad mini with retina display lasts 10 hours while kindle paperwhite lasts 8 weeks. Why the big difference?",
        "tags": [
            "E Ink"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "nightzoomer",
        "comments": true,
        "text": "[{% load static %}]()\nLet\u2019s start with talking about the screen you\u2019re currently glaring at to read this article. If you don\u2019t scroll down then the screen is pretty much static right? Not really. Behind the screen, all the pixels controlling what you see is getting multiplexed so fast that our eyes make it out as a constant one. Basically, a word is constantly consuming energy just to remain onscreen.\n\nIf you write a word on paper, does it need any more energy to remain on the paper? No. It remains there forever. This is the basic idea behind E Ink. Energy should only be used to \u2018write\u2019 the word. After that, no more energy should be consumed.\n\nThe notion of a low-power paper-like display had existed since the 1970\u2019s, originally conceived by researchers at Xerox PARC, but had never been realized. MIT undergrads J.D. Albert and Barret Comiskey came up with the idea of microencapsulated electrophoretic display to solve this problem. The initial idea was to create small spheres of half black and half white particles and somehow manipulate them to create images. Comiskey while experimenting could create tiny all white particles. Comiskey experimented with charging and encapsulating those all-white in microcapsules mixed in with a dark dye. The result was a system of microcapsules which could be applied to a surface and then could be charged independently to create black and white images.Once the orientation is attained, no energy is required to keep them in that fashion. Thus the image remains in that state forever.\n\n![E_Ink]({% static '/blog/assets/img/inkd/image1.png' %})\n\nThe above picture depicts how the pixels of E Ink display can be controlled using the potential difference. The MIT undergrads went on to create the E Ink Corporation in 1997 which created several new designs of the E Ink technology. Since then the technology has been improved tremendously. The first generation was called vizplex. Next came pearl and mobius. Pearl is used in almost all amazon kindle products except the paperwhite and oasis series. Pearl\u2019s major disadvantage was it\u2019s substrate made of thin glass which made it\u2019s structure fragile. This disadvantage was overcame by mobius which has a substrate of flexible plastic. The third generation E Ink called triton features a colour display which is easy to read in high light. The next updates carta and carta HD improved the resolution of the screen.\n\n![E_Ink]({% static '/blog/assets/img/inkd/image2.png' %})\n\nZoomed image of E Ink display. The black white sphere demarcation can be seen clearly.\n\nThat was all about how the E Ink was made and how it developed. Where is \u201creads like paper\u201d part? Well, this was achieved using the screen lighting properties. A general screen like the one you are using is backlit. Meaning it produces light of its own below the screen surface. \u2018Electronic paper\u2019 mimics the normal paper, it does not produce light, rather reflects it. This means that they are more comfortable to read and have wider viewing angles than most light-emitting displays. The contrast ratio in e-paper available as of 2008 approaches newspaper and newly developed displays are slightly better. An ideal e-paper display can be read in direct sunlight without image appearing to fade. Since it uses light reflected on it to display, thus you won\u2019t be able to see anything in a dark room (just like paper!). But have you seen the new kindle paperwhite? It boasts of it\u2019s readability even in dark with almost zero glare. Paperwhite uses an integrated lighting system. It\u2019s not a backlight. Its a small light shown onto the front screen which hits the screen and bounces back to you.\n\nNice right? Lots of innovations like these are happening in the world of E Ink. E Ink is slowly replacing LCD screens in lots of applications without us even noticing. The newly announced Advanced Colour E- Paper (ACeP) boasts of a display of 32000 colours. Currently they have a refresh rate of 2 seconds. Yes, the future is exciting. These durable easy to read screens are taking over the world from billboards to price tags to walls of your house. We have been imagining this future forever: the all screen worlds of *total recall, minority report, blade runner*, and other less dystopian movies.",
        "author_name": "Satyam Shivam",
        "slug": "e-ink"
    },
    {
        "layout": "post",
        "title": "The Rocker-Bogie Mechanism",
        "author_github": "nithinme",
        "date": "2017-12-21T15:40:03",
        "image": "/assets/img/",
        "description": "Insight into the mechanism that allowed rovers to move in space",
        "tags": [
            "Space",
            "Rovers",
            "Rocker Bogie"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "nithinme",
        "comments": true,
        "text": "[{% load static %}]()\n## Introduction\n\nEveryone might know what a rover is. It is a space exploration device designed to move across the surface of celestial bodies. Have you ever wondered how these rovers are capable of traversing the rough and rocky terrains of a planet? Different kinds of suspension systems are employed to help them achieve this task. Rocker-Bogie is one such suspensionsystem which is very popular and widely used. It is currently NASA\u2019s favoured design for wheeled mobile rovers. NASA\u2019s Curiosity rover uses this mechanism.\n\n## Design & Features\n\n<img src=\"/blog/assets/img/rocker-bogie/image1.jpg\" width=\"346\" height=\"174\" alt=\"Rocker Bogie\"/>\n\nThe largest link on each side is called rocker. It has two joints: one connected to the drive wheel and the other connected to the second link called Bogie. Bogie has drive wheel on each end.\n\nThe primary feature of rocker bogie is its simple and reliable design. It uses no springs or stub axles. It is thus capable of climbing over obstacles that are upto twice the size of wheel\u2019s diameter. It offers good stability along its travel by keeping all the six wheels on the ground.\n\nEach side of suspension moves independently allowing the rover to traverse terrains where the left and right rockers have to go over different sizes of obstacles. By proper design, the rover can be made to sustain tilt upto 50 degrees. Power is given to both front and back wheels enabling the rover to traverse obstacles without slip.\n\n<img src=\"/blog/assets/img/rocker-bogie/image2.png\" width=\"308\" height=\"173\" alt=\"Rocker Bogie\"/>\n\nOne major disadvantage is regarding the travel speed of the rover. At present, the speed is limited to about 10 cm/s to minimize dynamic shocks and damage upon impact.\n\nThere is still a lot to study and explore in this field. The design can be optimized to make it more cost effective and improve its strength and rigidity. One major concern would be to increase the travel speed so that it can be utilised to a greater extent.",
        "author_name": "Nithin M Eapen",
        "slug": "rocker-bogie"
    },
    {
        "layout": "post",
        "title": "How NASA discovered planets using Artificial Intelligence",
        "author_github": "gurupunskill",
        "date": "2017-12-21T16:44:03",
        "image": "/assets/img/",
        "description": "How deep learning was used to discover an exoplanet",
        "tags": [
            "Space",
            "Machine Learning",
            "Deep Learning"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "gurupunskill",
        "comments": true,
        "text": "[{% load static %}]()\nA Convolutional Neural Network.  \n**A Convolutional Neural Network and 90 minutes of training**  \n\nClearly, the hard part wasn't the training or the model but to get the _training set_.  \nBut what is the _data_? and where did we get it form?  \n\n>On December 14, 2017 NASA [announced](https://www.nasa.gov/press-release/artificial-intelligence-nasa-data-used-to-discover-eighth-planet-circling-distant-star) the discovery of an eigth planet in the *Kepler-90* system. *Kepler-90* is a Sun-like star 2545 light-years from Earth. The discovery was made using Artificial Intelligence.\n\n### Where did the data come from?  \n\nNASA has this huge camera up in space that has been sending us videos of _stars of interest_ every 29.4 minutes. That's an oversimplification of what the [Kepler Space Telescope](https://www.nasa.gov/mission_pages/kepler/main/index.html \"Kepler Mission Homepage\") does, but it's enough for the purpose and essence of this article. _Kepler_'s 94.6 megapixel<sup>[1]</sup> camera has a fixed field of view, which means it's been looking at the same spot in the sky since it launched in 2009.  \n\nThat's a _lot_ of data.  \n\n### What _is_ the data?\nTo understand the data, we need to first understand how NASA detect planets.   Planets don't emit light. So how does _Kepler_ even see planets that are lightyears away?   Planets don't emit light... but Stars do. _Kepler_ measures the brightness of stars over several years and since planets orbit around stars, they must cross the line of sight between kepler and the star periodically.  \n\n![Kepler Transit Graph]({% static \"/blog/assets/img/space-ml/kepler-graph.gif \"Kepler Transit Graph\"\" %})\n\nSo, _Kepler_ puts up a nice _brightness vs time_ graph and sends it back to us every 29 minutes. A _periodic dip_ in the graph implies that there must be a planet. NASA classified periodic dips that may be consistent with transiting planets as **Threshold Crossing Events** (TCE). The _Kepler_ team initially heterogenously put together TCEs by hand, but later on they designed an _Auto Vetter_ that automatically eliminates uninteresting candidates.  \n\nAnd this gives the raw unprocessed dataset: The _Autovetter Planet Candidate Catalogue_ hosted at the _[NASA Exoplanet Archive](http://exoplanetarchive.ipac.caltech.edu/)_. The light curves that were used to find the new planets are from the _[Mikulski Archive for Space Telescopes](http://archive.stsci.edu/)_. Each light curve in this dataset consists of integrated flux (brightness) measurements spaced at 29.4 minute intervals for upto 4 years<sup>[2]</sup>.  \n\n### Processing the dataset\n\nThe researchers at NASA then followed this up with _flattening_ the light curve by fitting a [basis spline](https://en.wikipedia.org/wiki/B-spline) (removes the low frequency variability) and then dividing it by the [best-fit spline](https://en.wikipedia.org/wiki/Curve_fitting) (connect the dots and smoothen them).  \n\nThey had to then _fold_ (to make it event centered<sup>[3]</sup>) and _bin_ the flattened curve to produce a 1D vector.  \n\nBinning and folding are two methods of making trends in messy data more apparent.  \n\n>Folding is useful when a source has periodic variability; the data is plotted in terms of phase, such that all the data are plotted together as a single period, in order to see what the repeated pattern of variability is. \n\nFolding is basically splitting the graph into blocks and then overlapping all the blocks on top of one another. This way we can notice repeated patterns more easily.  \n\n> Random variations from this pattern can be reduced by binning the data in time, which involves splitting the phase range into steps (bins) in which all the data are averaged, using a weighted mean.\n\nEven if we do fold the graph, it's still really messy since there may be random variations in the data. Binning can be thought as averaging the data out at a folded point in time to give 1 point.  \n\nOriginal Light Curve |  Folded Curve      |   Binned Curve\t |\n:-------------------:|:------------------:|:--------------------:|\n![Light Curve]({% static '/blog/assets/img/space-ml/lightcurve.png' %}) | ![Folded Curve]({% static '/blog/assets/img/space-ml/foldedcurve.png' %}) | ![Binned Curve]({% static '/blog/assets/img/space-ml/binnedcurve.png' %})|\n\nThe problem with binning the TCE dataset is that all the periods in our light curves are of different lengths. Different planets have different orbital lengths, different orbital periods and different distances from Earth. Hence, different periods of transits. So, how _did_ they bin these light curves to act as inputs?\n\nThey generated two separate _views_ of the same light curve:\n\n   1. A _global view_ which was generated by binning by a fraction of the TCE **period**. These views, the _global views_ of the light curves, were all binned to the same length, and each view represents the same number of points, on average, across all light curves. \n   \n   2. A _local view_ which was generated by binning by a fraction of the TCE **duration**. This results in a zoomed in view of the TCEs. It treats short and long TCEs equally and so might miss out on some information. It looks at only a part of the curve.\n\n[![Source]({% static '/blog/assets/img/space-ml/binning_example.png' %})](https://www.cfa.harvard.edu/~avanderb/kepler90i.pdf)\n\n### The dataset is now ready.\nThe researchers then passed these views as inputs into the their multilayered convolutional network and trained.  \n\n### Resources\n\nAlthough this post is not going to divulge into the details of the neural network, you can check it out on their research paper: [Identifying Exoplanets with Deep learning](https://www.cfa.harvard.edu/~avanderb/kepler90i.pdf).  \n\nIf you want to know more about binning, folding and how to apply them with numpy, go [here](https://www.southampton.ac.uk/~sdc1g08/BinningFolding.html).  \n\n<sup>[1] Kepler only sends back about 5% of the 94.6MP based on our _Stars of Interest_</sup>  \n<sup>[2] Roughly 70000 points </sup>  \n<sup>[3] There are 4 years of data and planets don't cross all the time. So, we need to remove the parts where they aren't crossing.</sup>",
        "author_name": "Gurupungav Narayanan",
        "slug": "space-ml"
    },
    {
        "layout": "post",
        "title": "Speed Printing",
        "author_github": "HarshitRanjan",
        "date": "2017-12-28T17:05:21",
        "image": "/assets/img/",
        "description": "Here's a breakdown of how the CLIP technology helps in printing faster than the conventional stereolithographic printers",
        "tags": [
            "Speed Printing",
            "Stereolithography"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "HarshitRanjan",
        "text": "[{% load static %}]()\n## Introduction\n3D printing technology is often used to construct highly complex objects of different kinds, properties and materials. Despite its numerous advantages, one major drawback of 3D printers is its traditionally slow speed. This is where it fails miserably, in its large scale applicability, against other major manufacturing processes like injection moulding. To overcome this major industry challenge, several 3D companies have come up with more updated and efficient technologies that guarantee quick speeds and utmost accuracy. One of the successful and modern technology is **CLIP (short for, continuous liquid interface production)**.\n\n> A typical 3D printing machine such as Stereolithography (SLA) can take several hours to print a 55mm diameter object and maybe several days to complete a larger object, while injection moulding takes minutes for the same.\n\nAdditive manufacturing startup **Carbon 3D** has developed an efficient technology that helps manufacturers cut their costs, waste less energy and materials while speeding up the time it takes to get from concept to product on the market. Some years back, it launched the M1 3D printer. The M1 used UV light in a process called stereolithography to *grow* objects from a pool of photopolymer resin which was not at all out of the box. But Carbon's technology allowed it to print in a steady stream rather than layer-by-layer, letting it churn out sophisticated objects up to 100 times faster than traditional stereolithography printers\n\nIn order to properly understand how the CLIP technology differs, we need to learn about the basic principles involved in Stereolithography. It's the process by which a uniquely designed 3D printing machine, called a stereolithograph apparatus (SLA) converts liquid plastic into solid objects layer-by-layer.\n\n### How Stereolithography works: \n\n**From CAD to STL:**\n\n> CAD files are digitalized representations of an object to be printed. \n\nThere are many different ways to 3D print an object. But nearly all of them utilize **computer aided design (CAD)** files. In 3D printing, CAD files are translated into **Standard Tessellation Language (STL)** that 3D printing machines can easily understand. Since additive manufacturing works by adding one layer of material on top of another, CAD models are broken up into layers before being printed in three dimensions.\n\n### Basic Working\n\nA SLA machine has four main parts: a tank that can be filled with **liquid plastic or photopolymer**, a **perforated platform** that is lowered into the tank, an **ultraviolet (UV) laser** and a **computer** controlling the platform and the laser. It starts with an excess of liquid plastic called as photopolymer, which is cured or hardened to form a solid object.  In the initial step of the SLA process, a thin layer of photopolymer is exposed above the perforated platform. The UV laser hits the perforated platform, \"painting\" the pattern of the object being printed. The UV-curable liquid hardens instantly when the UV laser touches it, forming the first layer of the 3D-printed object. Once the initial layer of the object has hardened, the platform is lowered, exposing a new surface layer of liquid polymer. The laser again traces a cross section of the object being printed, which instantly bonds to the hardened section beneath it. This process is repeated again and again until the entire object has been formed and is fully submerged in the tank. The platform is then raised to expose a three-dimensional object. After it is rinsed with a liquid solvent to free it of excess resin, the object is baked in an ultraviolet oven to further cure the plastic. \nSLA can go **top-down** (build platform facing up, descending while printing) or **bottom-up** (build platform facing down, ascending while printing). The bottom-up approach has the advantage that the object can be much larger than the vat itself, as the other approach requires the vat to contain the object. Bottom-up approaches also have an extra step where once a layer is done, the vat needs to 'rock' it off the resin (\"flexing and peeling the bottom of the vat away from the hardened photopolymer\" so that the build platform can be pulled up cleanly.\n\n### Continuous Liquid Interface Production\n\n![CLIP]({% static '/blog/assets/img/Speed-Printing/image1.jpg' %})\n\nCLIP, which stands for Continuous Liquid Interface Production, eliminates shortcomings of other 3D printing technology by emphasizing a *tunable photochemical process* over a traditional mechanical approach. The control over the solidification occurs by balancing oxygen and light to discriminately cure photo liquid resin. Continuous Liquid Interface Production technology uses components as shown in the image above such as\n\n1. BUILD PLATFORM\n2. UV CURABLE RESIN\n3. OXYGEN-PERMEABLE WINDOW\n4. DEAD ZONE \n5. PROJECTOR\n\n![CLIP]({% static '/blog/assets/img/Speed-Printing/image2.jpg' %})\n\n## How the CLIP Process Works\n\nCarbon's technology works through two phases. The first one, CLIP, consists of shaping the part by projecting a light through an oxygen-permeable window into a reservoir of UV-curable resin. As the UV images are projected in a sequence, the part solidifies and the build platform rises, allowing 3D objects to continuously grow without interruption. This is similar to SLA with a slight difference: it introduces oxygen into the liquid resin in order to control what is known as the *dead zone*. \n\n> The *dead zone* is a thin, liquid interface of uncured resin between the window and the printing part where the light passes through. \n\nDuring the development stage, images are fed into the system using a digital light projector via an oxygen permeable UV transparent screen. This process takes place beneath a liquid resin bath. \n\n![CLIP]({% static '/blog/assets/img/Speed-Printing/image3.jpg' %})\n\nThe second phase consists of thermal curing by baking the parts in a forced circulation oven to set off the properties of the materials. Through this process, a secondary chemical reaction sets off and causes the materials to adapt and strengthen, ultimately achieving desired engineering-grade mechanical properties.\n\n### Comparison:\n\nNow, as we are aware of the basic working of both the techniques, we can now understand what sets them apart in terms of speed. CLIP is much faster than bottom-up technologies that require a **peel** step between every layer as the cured layers stick to the resin container. That is, by far, the slowest step of SLA printing with most modern light sources. CLIP is a bottom-up technique that doesn't require a peel step, because the vat creates an oxygen layer over the window that keeps the resin from curing directly on the surface and sticking. In that way, it arguably performs more like a top-down printer than a bottom-up printer.",
        "author_name": "Harshit Ranjan",
        "slug": "speed-printing"
    },
    {
        "layout": "post",
        "title": "War of the Currents",
        "author_github": "aditya-834",
        "date": "2017-12-28T23:33:44",
        "image": "/assets/img/",
        "description": "The war that changed the world more than both the world wars.",
        "tags": [
            "Tesla",
            "Edison",
            "AC/DC"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "aditya-834",
        "comments": true,
        "text": "[{% load static %}]()\n![current]({% static '/blog/assets/img/War-of-the-currents/image1.png' %})\n**Thomas Edison**\n\n\n![current]({% static '/blog/assets/img/War-of-the-currents/image2.png' %})\n**Nikola Tesla**\n\nAt the end of the 19th century, the world witnessed one of the greatest scientific feuds of all times, which changed the course of the history.\n\nThree brilliant inventors, Thomas Edison, Nikola Tesla and George Westinghouse, battled over which electricity system\u2014direct current (DC) or alternating current (AC)\u2013would become standard. This would go on to be known as the 'War of the Currents'.\n\nBy 1880, Edison had created a model for the incandescent lamp that was commercially viable. But to make a profit out of the electric lights that he had invented, he realized that electricity would have to be brought to people's homes. Thus he began devising a system for generation and distribution of power. Edison's distribution system would supply 110 V DC voltage. In 1882, Edison opened the first ever electric power plant in Pearl Street, New York. The problem with Edison's system was that it could only deliver electricity within a half-mile radius from the plant.\n\n![current]({% static '/blog/assets/img/War-of-the-currents/image3.png' %})\n\n## The First Induction Motor\n\nTwo years later, A young Serbian engineer, Nikola Tesla immigrated to America and started working under Edison. He improved Edison's existing DC system in order to interest him in an AC motor that he had been developing. But Edison, a firm supporter of DC, said that AC has no future.\n\nTesla quit his job in 1885 and a few years later received a number of patents for his AC technology. In 1888, he sold his patents to industrialist George Westinghouse, whose Westinghouse Electric Company had quickly become an Edison competitor. Feeling threatened by this competition, Edison started a propaganda to malign George Westinghous and his AC System in the eyes of the public. As a part of this propaganda, Edison electrocuted a number of animals just to prove how dangerous high voltage AC is. Edison even suggesting electrocution as way of executing criminals and in 1890 convicted murderer William Kemmler became the first person to die on an electric chair which was powered by a Westinghouse generator. This marked one of the darkest events of the war.\n\nWith the advent of the alternating current transformer, high voltage AC produced by the generators could be stepped down to lower voltages within the safety standard. This was an important event in the War of The Currents. Now AC could be safely supplied at everyone's doorstep.\n\nThis made Edison's argument obsolete.\n\nDespite all his efforts to discredit AC, Edison failed. Westinghouse won the contract to supply electricity to the 1893 World\u2019s Fair in Chicago\u2014beating out rival General Electric, which was formed in 1892 by a merger involving Edison\u2019s company\u2014and the expo became a dazzling showcase for Tesla\u2019s AC system. Westinghouse also received an important contract to construct the AC generators for a hydro-electric power plant at Niagara Falls; in 1896, the plant started delivering electricity all the way to Buffalo, New York, 26 miles away. Thus the first electric grid came into being. The achievement was regarded as the unofficial end to the War of the Currents, and AC became dominant in the electric power industry.\n\nThe war pushed Westinghouse into bankruptcy. To save his friend, Tesla gave up all his perks that he had gained from his patents. As he grew old, the genius mind of Tesla took a toll on him and it drove him towards insanity. In his last days, Tesla lived in a hotel alone and without any cash. In 1943, Nikola Tesla died. Tesla was never given credit for his works and he ended penniless when he could have earned millions from his patents. Tesla's story is one of the most tragic story in the history of science.\n\nThe war of the currents changed the course of history. We cannot imagine our lives without electricity. Today, we reap the benefits of the pioneering works of Edison and Tesla. Although today AC is the standard, DC is gaining importance with the advent of digital electronics. So it appears the War of the Currents may not be over yet. But instead of continuing in a heated AC vs. DC battle, it looks like the two currents will end up working parallel to each other in a sort of hybrid armstice.",
        "author_name": "Aditya Rangarajan",
        "slug": "war-of-the-currents"
    },
    {
        "layout": "post",
        "title": "How do websites communicate securely?",
        "author_github": "nishanthebbar2011",
        "date": "2017-12-28T23:33:44",
        "image": "/assets/img/",
        "description": "How cryptography ensures secure communication",
        "tags": [
            "SSL",
            "Public Key Cryptography"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "nishanthebbar2011",
        "comments": true,
        "text": "[{% load static %}]()\nThe inherent need for human beings has always been to communicate and share information,albeit selectively. For this purpose, the information is to be coded in such a way that it can be deciphered only by the recipient, regardless of whether the recipient is right next to the sender or on the other side of the world. __*The communication channel is assumed to be plagued with adversaries.*__.These adversaries have access to this coded information and are constantly trying to pry.So, the basic problem has always been to: \n__*transfer secure information over such insecure channels.*__\n\nSo, how DO you communicate when everyone is an adversary and nobody is trustworthy?\nThe answer is simple.\n\n## Cryptography\n\n> \"Cryptography is the art and science of making a crypto system that is capable of providing information security.\"\n\nThe primary objective of cryptography is to provide basic security services like data integrity, confidentiality, user-authenticity and non-repudiation.\n\n\n## Crypto-systems\n\nGiven below is a simple model of a crypto-system\n\n\n![Crypto-system]({% static '/blog/assets/img/Public-key-crypto/cryptosystem.jpg' %})\n\n\n\nThere are two types of crypto-systems,based on how the encryption is carried out.\n\n### 1.Symmetric key encryption  \n\nHere, a single key is used for both encryption and decryption of the messages and such a key is established in advance by both the sender and the recipient.Thus,there has to exist \"trust\" between both sender and receiver, so as to not divulge the key.This was the most common method of encryption before the 1970's.\n\n\n![Symmetric key]({% static '/blog/assets/img/Public-key-crypto/symmetric_key_encryption.jpg' %})\n\n\nSuppose Alex wants to send a message to Carl using symmetric key encryption , then both of them will have to agree upon a key beforehand and use it to encrypt and decrypt the messages.\n\n### 2. Asymmetric key encryption/Public Key Cryptography \n\nHere, two different keys are used for both encryption and decryption purposes and there exists some mathematical relation between the keys and thus decryption is possible.One of them is the private key and the other one is the public key.It is assumed that the private key is kept secret by a particular individual.\n\nAn individual(recipient) makes one of his keys public.A sender can encrypt a message with the help of this public key and such an encrypted message is transmitted over the insecure channel. Only the recipient can decrypt this message with the help of his private key, since he alone possesses it. This is clearly a more elegant solution than symmetric key encryption, that relies on \"trust\". \n\n\n![Asymmetric key ]({% static '/blog/assets/img/Public-key-crypto/asymmetric_key_encryption.jpg' %})\n\n\n\nThe crux of this system is to ensure that it is computationally infeasible to deduce the private key from the public key and the encrypted message.Obviously, it is important to ensure that the public key actually belongs to the recipient. This is usually accomplished with the help of a trusted third party, that hands out digital certificates.These parties are called __*Certificate Authorities(CA's)*__.The public key is usually huge and is stored on such digital certificates whereas, the private key is stored on the system itself.\n\nAlso with public-key encryption systems, authenticity of digital systems or documents can also be ascertained with the help of Digital signatures. The content is signed with an individual's private key , and since the public key is easily available, the identity of the user is easily verified.\n\nSuppose Alex sends a message(Using Carl's public key) and a digitally signed document to Carl, using a public-key encryption system, these are the advantages,in a nutshell:\n\n__1.__ It is possible to ascertain Alex's identity from his public key and digital signature on the document.\n\n__2.__ Only Alex could have signed the document,since he alone possesses the private key. Therefore it is not plausible for him to deny   sending it.\n\n__3.__ Only Carl possesses the private key for decrypting the message. He,alone can decrypt the message.\n\n__4.__ When the signature is verified by Carl, it checks that the contents of the document or message match what was in there when the signature was applied. Thus, integrity of the data is preserved.\n\n\n\n## The SSL Protocol\n\n\n One of the major applications of public key cryptography is it's use in Secure Sockets Layer(SSL) protocol to ensure secure transactions   between web browsers and servers.The protocol uses a CA to identify one or both end of the transactions.This,in short, is how it works:\n\n__1.__ A browser requests a secure page (usually https://).\n\n__2.__ The web server sends its public key with its certificate.\n\n__3.__ The browser checks that the certificate was issued by a trusted party (usually a trusted root CA), that the certificate is still valid and that the certificate is related to the site contacted.\n\n__4.__ The browser then uses the public key, to encrypt a random symmetric encryption key and sends it to the server with the encrypted URL required as well as other encrypted http data.\n\n__5.__ The web server decrypts the symmetric encryption key using its private key and uses the symmetric key to decrypt the URL and htp data.\n\n__6.__ The web server sends back the requested html document and http data encrypted with the symmetric key.\n\n__7.__ The browser decrypts the http data and html document using the symmetric key and displays the information.\n\nThus, as long as the CA is trustworthy, it is possible for the web browsers and servers to communicate securely and that is how all the information that is continuously exchanged on the internet stays safe and secure.",
        "author_name": "Nishanth P Hebbar",
        "slug": "public-key-cryptography"
    },
    {
        "layout": "post",
        "title": "Bit Manipulation",
        "author_github": "Aj163",
        "date": "2018-01-12T11:15:53",
        "image": "/assets/img/",
        "description": "An introduction to Bit Manipulation and its use in Competitive Programming",
        "tags": [
            "Bit masks",
            "DP",
            "Competitive Programming",
            "C++"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "Aj163",
        "use_math": true,
        "comments": true,
        "text": "[{% load static %}]()\nAll data in computer programs is internally stored as bits, i.e., as numbers 0 and 1. It turns out that there are many uses for bit manipulation in competitive programming.\n\nIn programming, an n bit integer is internally stored as a binary number that consists of n bits. For example, the C++ type int is a 32-bit type, which means that every int number consists of 32 bits. For exmaple, the bit representation of the int number 42 is ```00000000000000000000000000101010```.\n\nThere are several bit operations supported by a typical ALU, some of them being AND, OR, NOT, XOR, and bit shifts\n\n#### Some basic applications of bit operations\n1. ```n&1 = 1``` implies n is odd, and ```n&1 = 0``` implies n is even. This executes much faster than the typical way to check if a number is odd (ie. ```n%2 = 1```).\n2. ```1<<k``` gives 2^k. This makes it easy to check if the kth bit of a number from the right is set or not, ie. ```n&(1<<k) == 1```. This can be used to easily convert a decimal number to binary as follows:\n    ```C++\n    for (int i = 31; i >= 0; i--){\n        if (n&(1<<i)) cout << \"1\";\n        else cout << \"0\";\n    }\n    ```\n3. It is also possible to modify single bits of numbers using similar ideas. For example, the formula ```x |= (1 << k)``` sets the k th bit of x to one, the formula ```x &= ~(1 << k)``` sets the k th bit of x to zero, and the formula ```x ^= (1 << k)``` inverts the k th bit of x.\n4. The formula ```x &= (x \u2212 1)``` sets the last one bit of x to zero, and the formula ```x &= \u2212 x``` sets all the one bits to zero, except for the last one bit. The formula ```x |= (x \u2212 1)``` inverts all the bits after the last one bit. Also note that a positive number x is a power of two exactly when ```x & (x \u2212 1) = 0```, and ```x&(-x)``` gives the highest power of 2 which divides x.\n\n#### Additional functions\nThe g++ compiler provides the following functions for counting bits:\n1. ```__builtin_clz(x)```: the number of zeros at the beginning of the number\n2. ```__builtin_ctz(x)```: the number of zeros at the end of the number\n3. ```__builtin_popcount(x)```: the number of ones in the number\n4. ```__builtin_parity(x)```: the parity (even or odd) of the number of ones\n\n## Bit Optimazations\n#### Counting subgrids problem\nConsider the following problem: Given an $N * N$ grid whose each square is either black (1) or white (0), calculate the number of subgrids whose all corners are black. For example, the grid\n\n![Bit Manipulation Grid]({% static '/blog/assets/img/Bit-Manipulation/grid1.png' %}) \n\ncontains two such subgrids:\n\n![Bit Manipulation Grid]({% static '/blog/assets/img/Bit-Manipulation/grid2.png' %}) \n\nThere is an $O(n^3)$ time algorithm for solving the problem: go through all $O(n^2)$ pairs of rows and for each pair $(a, b)$ calculate the number of columns that contain a black square in both rows in $O(n)$ time. The following code assumes that $color[y][x]$ denotes the color in row y and column x :\n```C++\nint count = 0;\nfor (int i = 0; i < n; i++) {\n    if (color[a][i] == 1 && color[b][i] == 1) count++;\n}\n```\nThen, those columns account for ```count*(count \u2212 1)/2``` subgrids with black corners, because we can choose any two of them to form a subgrid. To optimize this algorithm, we divide the grid into blocks of columns such that each block consists of N consecutive columns. Then, each row is stored as a list of N-bit numbers that describe the colors of the squares. Now we can process N columns at the same time using bit operations. In the following code, $color[y][k]$ represents a block of N colors as bits.\n```C++\nint count = 0;\nfor (int i = 0; i <= n/N; i++) {\n    count += __builtin_popcount(color[a][i]&color[b][i]);\n}\n```\nThe resulting algorithm works in $O(n^3/N)$ time.\n\nA comparison was done for a randomly generated grid of size 2500 \u00d7 2500 between the original and bit optimized implementation. While the original code took 29.6 seconds, the bit optimized version only took 3.1 seconds with N = 32 (int numbers) and 1.7 seconds with N = 64 (long long numbers).\n\n## Dynamic programming and bit masks\n#### Optimal selection problem\nConsider the following problem: We are given the prices of k products over n days, and we want to buy each product exactly once. However, we are allowed to buy at most one product in a day. What is the minimum total price? For example, consider the following scenario ( k = 3 and n = 8):\n![Bit Manipulation Optimal]({% static '/blog/assets/img/Bit-Manipulation/optimal1.png' %}) \n\nIn this scenario, the minimum total price is 5:\n![Bit Manipulation Optimal]({% static '/blog/assets/img/Bit-Manipulation/optimal2.png' %}) \n\nLet $price[x][d]$ denote the price of product x on day d. For example, in the above scenario $price[2][3]$ = $7$. Then, let ```total(S, d)``` denote the minimum total price to buy all products by day d, when a subset S has already been bought. Using this function, the solution to the problem is ```total({}, n \u2212 1)```.\n\nTo represent the subset S, we use **bit masks**. Consider a 32-bit integer N. If the kth bit (from the right) of N is set, then the kth product is a part of the subset, else not.\n\nTherefore on each day, we have a choice to buy one of the products which hasn't been bought before or buy no product. The base conditions would be:\n- If all products have been bought, then return 0\n- If we have exceeded the total number of days return INF.\n\nTherefore the C++ code is as follows:\n```C++\nint total(int mask, int day){\n    if(mask == ((1<<k) -1)) //All products have been bought.\n        return 0;\n    if(day == -1)\n        return INF;\n\n    int ans = INF;\n    for(int i=0; i<k; i++)\n        if(mask & (1<<i) == 0) //Product i hasn't been bought yet.\n            ans = min(ans, total(mask | (1<<i), day-1) + price[i][day]);\n\n    //Not buying anything\n    ans = min(ans, total(mask, day -1));\n\n    return ans;\n}\n```\nOptimizing the above code by memoizing:\n```C++\nint dp[1<<k][n]; //Initially all values set to -1\nint total(int mask, int day){\n    if(mask == ((1<<k) -1))\n        return 0;\n    if(day == -1)\n        return INF;\n\n    if(dp[mask][day] != -1)\n        return dp[mask][day];\n\n    int ans = INF;\n    for(int i=0; i<k; i++)\n        if(mask & (1<<i) == 0)\n            ans = min(ans, total(mask | (1<<i), day-1) + price[i][day]);\n\n    ans = min(ans, total(mask, day -1));\n    return dp[mask][day] = ans;\n}\n```\nThe above code runs in $O(nk2^k)$.\n\n#### Some dp+bitmask problems:\n- [Codeforces](http://codeforces.com) \n    1. [8C](http://codeforces.com/problemset/problem/8/C)\n    2. [71E](http://codeforces.com/problemset/problem/71/E)\n    3. [743E](http://codeforces.com/problemset/problem/743/E)\n- [Codechef](https://www.codechef.com)\n    1. [GEEK04](https://www.codechef.com/problems/GEEK04)\n    2. [TSHIRTS](https://www.codechef.com/problems/TSHIRTS)",
        "author_name": "Ashwin Joisa",
        "slug": "bit-manipulation"
    },
    {
        "layout": "post",
        "title": "Fluids and Satellites",
        "author_github": "krishnakv98",
        "date": "2018-01-12T11:05:53",
        "image": "/assets/img/",
        "description": "Importance of behavior of fluids in low gravity for satellites",
        "tags": [
            "Fluid Dynamics",
            "Satellites"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "krishnakv98",
        "text": "[{% load static %}]()\n*The increasingly large volumes of fluid that have to be carried aboard satellites designed for long operating lifetimes, but which often must also deliver high-accuracy pointing, mean that special attention has to be paid to the behavior of fluids in low gravity.*\n\n## Low gravity and fluid dynamics\n\nThere are several methods for analyzing the dynamic behavior of fluids under normal gravity conditions on Earth. A common assumption in such methods is that the free surface of the fluid is flat and at right angles to the direction in which gravity is acting, i.e. normal to the gravity vector. In this case, surface tension does not contribute to the fluid's dynamic behavior. However, this assumption is no longer valid if gravity is significantly reduced, as is the case for a satellite in orbit.\n\nAn indicator of the importance of the surface tension is the so-called 'Bond number', which is a measure of the relative magnitudes of the gravitational and capillary forces. It is proportional to the gravity level, the fluid density and the square of the characteristic length of the fluid's free surface, and is inversely proportional to the surface tension of the fluid. If the Bond number is much greater than 1, the surface- tension effects can be neglected and if it is much less than 1, the gravity forces can be neglected.\n\nFor example, the Bond number for water in a 2.7 mm- diameter tube subject to Earth's gravity is 1, and the fluid surface in such a tube is curved. If the tube diameter is much smaller, gravity forces have no significant influence on the fluid, and the water would not flow out when the tube is turned upside down.\n\nThe gravity level on an orbiting spacecraft is very low. Under such conditions, a fluid in a typical 1 m-diameter tank will have a Bond number of less than 1, which means that the free surface will be curved and surface tension has to be considered when modelling the fluid. In some cases, it might be possible to neglect gravity effects in the analysis, depending on the tank's diameter and the type of fluid that it holds. This would be true, for example, for a 0.6 m tank carrying nitrous oxide, for which the Bond number would be 0.1 under a \u2018Eureca-type\u2019 residual acceleration.\n\nTanks for microgravity applications often contain baffles and fluid-management devices and their effects need to be considered as a next step.\n\n## Fluid sloshing frequencies\n\nThe fluid sloshing modes of tanks subject to Earth's gravity exhibit frequencies that are usually far below the structural resonance frequencies. The first sloshing mode in a typical spacecraft tank has a frequency around 1 Hz. The frequency is approximately proportional to the square root of the gravity, and therefore decreases if the gravity level is reduced. If the Bond number becomes small, the frequency's dependence on the surface-tension increases. Analytical solutions are possible for simple problems of limited engineering value, but numerical methods are needed to model complex engineering problems.\n\nFluid sloshing mode frequencies in microgravity are generally very low, typically in the range 0.1 to 0.01 Hz. These low- frequency modes could interfere with the structural bending modes of large solar arrays, or with the satellite's Attitude and Orbit Control System (AOCS). As a result, such fluid effects in low gravity need to be determined as one of the more critical inputs when analyzing the dynamic control of satellites.\n\n## Analysis methods\n\nAppropriate analysis capabilities require facilities for the generation of a simplified dynamic model (few degrees of freedom) of the propellant fluid in low gravity for coupling with satellite mass and stiffness matrices as input to the satellite AOCS model.\n\nThe analysis is conducted in three steps. As a first step, the shape of the fluid's free surface is determined. Subsequently, the mass and stiffness matrices are evaluated. Finally, the fluid modes and frequencies are computed and simplified models are derived, assuming a rigid tank. At present, the analysis of the tank geometry is limited to cylindrical tanks with elliptical end caps, a spherical tank being a special case of such a geometry.\n\n## Free-surface shape determination\n\nAs mentioned earlier, the two shape extremes are the flat free surface obtained when the surface tension is negligible, and the spherical surface in zero gravity. Numerical analysis is necessary when gravity and surface tension are taken into account simultaneously, and in most cases a non-linear iteration process is required to derive the free-surface shape. The latter is discretized using finite elements, and this idealization is employed to describe the boundary of the fluid.\n\nFollowing figure shows the different shapes of the free surface at different Bond numbers for fluid contained in a cylindrical tank. It is possible in the analysis to define an offset angle between the tank's axis and gravity vector. The surface shapes displayed in the right-hand figure are for an offset angle of 5 degrees.\n![bionic]({% static '/blog/assets/img/fluids-and-satellites/1.gif' %})\n\nOnce the free surface and its contact line with the tank wall have been established, it is easy to determine the fluid boundary necessary to establish the fluid dynamics.\n\n## Generation of fluid mass and stiffness matrices\n\nTo account for the effect of the fluid on the satellite, the fluid mass and stiffness matrices are generated and implemented in the satellite mathematical model. In general, the latter is built up using the finite-element method (FEM). Similar finite-element representation of the fluid would involve a significant mesh- generation effort. Boundary-element techniques, however, facilitate the generation of the fluid matrices. A number of difficulties related to the usage of the boundary-element method (BEM) for this type of fluid processing have been solved. The combination of FEM and BEM techniques arrived at has been employed successfully to represent satellite propellant tanks, which have then been coupled into the satellite mathematical model for pointing-performance\nevaluations.\n\nAs an example of the analysis possibilities, following figure shows the first mode of vibration of the fluid in a satellite tank.\n![bionic]({% static '/blog/assets/img/fluids-and-satellites/2.gif' %})\nThe first fluid sloshing mode at 0.02 Hz\n\n## Derivation of dynamically equivalent simple models\n\nThe importance of each mode of vibration depends on the load generated at the tank interface with the satellite. The interface force depends on the effective modal mass matrix of the vibrational modes. It is possible to derive from the latter single-degree-of-freedom models which have equivalent dynamic behavior in terms of tank interface loads.\n\nMass-spring and pendulum models are two types of models which can be inserted directly into the satellite AOCS model\n![bionic]({% static '/blog/assets/img/fluids-and-satellites/3.gif' %})\nThe mass-spring and pendulum models\n\nThe simplified mass-spring models are determined by the vibrating mass and its location, the spring stiffness, and the residual mass and its location. The vibration characteristics of the pendulum model are determined by the pendulum length rather than the spring stiffness, and can be used for non-zero-gravity conditions.",
        "author_name": "KV Krishna",
        "slug": "fluids-and-satellites"
    },
    {
        "layout": "post",
        "title": "How is Technology Helping the Differently Abled People?",
        "author_github": "Shruti-Deshpande",
        "date": "2018-01-12T11:00:53",
        "image": "/assets/img/",
        "description": "Here are 10 assistive tech giving people encouragement to deal with disability",
        "tags": [
            "differently-abled",
            "assistive technology",
            "text scanners",
            "optical sensors",
            "bionic",
            "eye tracking system."
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Shruti-Deshpande",
        "text": "[{% load static %}]()\nThis year, the government of India dedicated the National Science Day to the theme \u201cScience and Technology for specially-abled people\u201d. The main aim was to encourage innovation and also to ease the difficulties of differently abled people. So here are 10 assistive technologies to assist them.\n\n## Devices for people with visual Impairment\n\n### DOT\n\nIt is the world\u2019s first Braille smart watch which is affordable compared to the regular e-Braille devices. Technically this device consists of 6 dots on four cells on the screen of the watch which forms 4 letters in Braille at any time. This tool helps the visually impaired to get access to messages, tweets and books.\n![bionic]({% static '/blog/assets/img/technology-for-differently-abled-people/image1.png' %})\n\n### FingerReader\n\nFingerReader is an index-finger wearable device which assists the visually impaired in reading as well as translating texts. The small camera on the FingerReader scans the text and gives real-time audio feedback of the word it detects. \n![bionic]({% static '/blog/assets/img/technology-for-differently-abled-people/image2.png' %})\n\n### Smart Glasses:\n\nA pair of glasses has been developed by the Oxford University to aid the people with limited vision which enables them to walk around unfamiliar places. The smart glasses consist of a video camera mounted on the frame of the glasses, a computer processing unit that is small enough to fit in a pocket and software that provides images of objects close-by to the see-through displays in the eyepieces of the glasses.  \n![bionic]({% static '/blog/assets/img/technology-for-differently-abled-people/image3.png' %})\n\n## Devices for people with hearing Impairment\n\n### UNI\n\nIt is a two way communication tool that relies upon gestures and speech technology. It works by detecting hand and finger gestures with its specialised camera and then converting into text to provide meaning to given sign language. It also has an additional feature of voice recognition software that will convert speech into text.\n![bionic]({% static '/blog/assets/img/technology-for-differently-abled-people/image4.png' %})\n\n### Transcense\n\nTranscense is a mobile app that accurately translates conversation in real time to text format. It works by catching conversations from the voices of different individuals and assigning them a colour bubble so the deaf person knows who said what. With the help of distributed microphone systems on each Smartphone the app catches the words of each individual and converts it into text.\n\n## Devices for people with speech Impairment\n\n### DynaVox EyeMax\n\nThis device gives the ability to participate in spoken communication using only eyes. The device consists of a sophisticated eye tracking system so that the users can interact with the on-screen keyboard to enter words and phrases which are then converted to spoken text using text-to-speech mechanism of the device.\n![bionic]({% static '/blog/assets/img/technology-for-differently-abled-people/image5.png' %})\n\n### Talkitt\n\nThis application helps people by translating unintelligible pronunciation into understandable speech. It works by learning the user\u2019s speech pattern first, creating a personal speech dictionary. Then it identifies and recognises the unintelligible pronunciations and translates it to speech which people can understand.\n\n## Technologies for others\n\n### HeadMouse Nano\n\nHeadMouse Nano replaces the standard computer mouse for people with limited hand movements. The HeadMouse has a wireless optical sensor mounted on top of the monitor screen which tracks and translates the natural movements of user\u2019s head into directly proportional mouse pointer movements.\n![bionic]({% static '/blog/assets/img/technology-for-differently-abled-people/image6.png' %})\n\n### Bionics\n\nAdvances in 3D printing and bio-electronics are allowing replacement of lost limbs with prosthetics, giving specially-abled person an extra functionality. Though the technology with bionic is still in its initial stages but few\u00a0bionic\u00a0items already exist, the best known being the cochlear implant, a device for deaf people.\n\n### dbGlove\n\nThis device helps the deaf blind people to communicate using tactical alphabets - pressing or pinching different parts of the hand represents different letters .The device consists of a sensor that converts these alphabet tracings into computer text and also trace the letters back onto the hand. \u00a0This will enable deaf blind people to operate computers and smart phones.\n![bionic]({% static '/blog/assets/img/technology-for-differently-abled-people/image7.png' %})",
        "author_name": "Shruti Deshpande",
        "slug": "how-is-technology-helping-the-differently-abled-people"
    },
    {
        "layout": "post",
        "title": "Electric Cars - The Future",
        "author_github": "sssachin300",
        "date": "2018-02-16T23:32:44",
        "image": "/assets/img/",
        "description": "Should you really buy electric cars? Are they really needed?",
        "tags": [
            "Electric Cars",
            "CAS/SPS"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "sssachin300",
        "text": "[{% load static %}]()\n***Should you really buy an electric car?***\n\nElectric vehicles are hitting the world\u2019s roads and with government support behind them these are hyped up to be the next big thing in the automobile industry . Combustion engine vehicles generate huge quantities of pollution, and developed economies create more than their fair share of this. The US transportation sector accounts for around 30 percent of US global warming emissions which is an order of magnitude more than most countries. In this blog I will tell you what I think on buying an electric car.\n\n## Pros\n\n1.  Quick and Quiet : It takes just one ride in a battery \u2013 powered card to understand the improved ride quality of an EV(Electric Vehicle) compared to a conventional fuel car . The drive is much smoother and you won\u2019t even realize at what speed you are moving at until you look at your speedometer . It makes most regular cars seem clunky and outdated. Acceleration is also smooth and very quick.\n\n2.  Cheaper to operate : Though the initial investment on electric cars may be higher , the cost to operate them is lower what with the higher efficiency of electric engines and the lower cost of electricity compared to conventional fuels such as petrol or diesel .\n\n3.  Emissions : If you are an Indian you would definitely have experienced the nauseating puff of smoke being thrown out by the vehicles in front you on the street .With EVs we can finally say goodbye to that and breathe in much fresher air on the streets .\n\n## Cons\n\n1. Limited range : Electric engines are the kings of limited mileage range .The Nissan Leaf has been claimed to go up to a 100 miles and the Chevorlet Volt 40 miles on all completely electric power .Although this may seem sufficient for daily commute you will have to part with your occasional road trips .\n\n## Variables\n\n1.  Cost : The initial investment for an electric car is definitely going to sting you more than a regular car because the technology involved in its manufacture is much higher than a regular car . But on the long run since electricity is usually cheaper than petrol/diesel the cost per mile for an electric car is lesser and the more you drive the more you save . This of course, depends on the fact that you are actually going to keep the same car for a long time which would leave you without the choice of buying a newer car after a few years . Once you buy it , you\u2019ll be stuck with it for years to come .\n\n2.  Refuelling time ; If you have an electric car you can refuel at home at night like you charge you cellphone and use it all day . But what if you have to travel a long distance or between two cities ? Currently we don\u2019t have enough charging stations available and even at those stations if these cars become popular you will have to wait in line for hours if you want to recharge . An even bigger nightmare is if your battery runs out halfway . If it were a petrol/diesel vehicle you could fill in a bottle at the nearest station and use it drove upto that station but what will you do with an electric car ? you will have to tow it all the way to a charging station which I would rather never do .\n\n3.  Pollution : I find it hard to believe that electric cars are very green and eco-friendly . Sure they don\u2019t emit smoke which you can see but where do we get the electricity to run them ? Fossil fuels . And how exactly are we going to get the batteries for the car? lithium mining and extraction . And what exactly are you going to do with dead batteries ? Shoot them into outer space ? Unless battery technology, renewable energy and recycling methods advance hugely we are not helping nature as much as we would like to believe . Good luck to you if you think you are going to save the Earth by buying an electric car .\n\nWell these are the most important things that you need to know if you are planning on buying an electric car .Though It\u2019s pretty clear what I think about that idea :P",
        "author_name": "Sachin Sampath",
        "slug": "electric-cars"
    },
    {
        "layout": "post",
        "title": "t-SNE",
        "author_github": "chennakeshava1998",
        "date": "2018-02-15T23:32:44",
        "image": "/assets/img/",
        "description": "Description of t-SNE algorithm",
        "tags": [
            "Data Visualization",
            "t-SNE",
            "ML",
            "Data Visualization"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "chennakeshava1998",
        "text": "[{% load static %}]()\nIt\u2019s manually impossible to understand the structure of a dataset with 100 dimensions. And it\u2019s fairly common to come across data having so many features. And the exploratory-analysis of data is a preliminary step, before proceeding to other aspects of data-wrangling and models. This is where t-SNE comes in.\n\n[t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding) is non-linear dimensionality reduction algorithm. It reduces each high-dimensional point and embeds them into a 2D/3D map. Additionally, it ensures that similar objects are nearby and dissimilar points are kept apart. \n\n## Uses of t-SNE\n\n- It's hard to visualize high-dimensional objects\n- Existing data-visualization techniques only enable visualization of __few variables at a time__.\n- Helps to find the local structures in data, and see if there are any clusters of points in the high-dimensional manifold.\n- Helps us decide if there are any distinct classes in the data, whether they are linearly or nonlinearly separable, etc.\n- It\u2019s used in a lot of applications cutting across Computer Security, Music Analysis, Cancer research, and in abstract cases like visualising the learnings of different layers of a Artificial Neural Network, etc.\n\nHere's a nice example of t-SNE's usage in last year's IEEE project(GoT chabot) https://gist.github.com/kaushiksk/ea0c0f25c082acb8604ad84466f85ca8\n\nIt\u2019s necessary to know about other dimensionality reduction techniques like Principal Component Analysis (PCA) to truly appreciate the distinctive nature of t-SNE. \n\n## PCA\n\n__PCA__ tries to retain the maximum information about high-dimensional data even after embedding it into a low dimension. It does this by finding out a few components in the higher dimension along which the maximum variation of the data is ensured. Hence it preserves the variation in the data even after reducing it to a lower dimension. Mathematically, PCA minimises the squared-error between points - essentially ensures that far-away points in the original dataset remain in the same way in the 2D map also. So PCA preserves large pairwise distances(global structure and properties), but these distances do not imply any meaningful property.\nBut it loses the the low-variance deviations between neighbours. t-SNE filled this gap by accounting for local properties, which have a lot of significance irrespective of the overall structure/topology of data.\n\nMoreover PCA is not very useful for unlabelled data. This is quite evident by looking at the visualizations produced by PCA and t-SNE on the MNIST dataset. \n\n![Output of PCA over the MNIST dataset]({% static '/blog/assets/img/t-SNE/pca_mnist.png' %})\n\nDistinct clusters are not produced by PCA. The distinctness is evident only because of the color-coding, so PCA is not very helpful for unlabelled data.\n\n![Output of t-SNE over the MNIST dataset]({% static '/blog/assets/img/t-SNE/tsne_mnist.jpeg' %})\n\nOn the other hand, the presence of clusters is striking with unlabelled-data using t-SNE. The bottom right, we can see the labelled version of the same. Hence, t-SNE finds a lot of utility with unlabelled data, over and above PCA.\n\n__Some Interesting properties of t-SNE:__\n\n* __Crowding problem:__ Semantically similar points in higher dimension will collapse onto one-another when brought down to a lower dimension. t-SNE solves this problem, unlike the previous dimensionality algorithms like PCA and SNE.\n\nSo the effectiveness/relevance of t-SNE is put into question if the required output is in a fairly higher dimension, where the possibility of crowding is low. On the other hand, PCA always gives the best components along which the variance of the data is maximised.\n\nHence, the results of t-SNE is much better if the degrees of freedom is increased, because the crowding problem is less severe. Consequently, visualising data in 3D rather than 2D gives a lot of gains in using t-SNE.\n\n- __t-SNE does not intentionally retain distances/densities:__ It only retains the probabilities. It adapts it's idea of distance to the local-variations of densities. So it automatically expands dense clusters (to prevent overcrowding) and expands sparse ones. This effect is expected due to the design of the algorithm. Relative sizes/distances between clusters in a t-SNE do not have any sensible interpretation. The same reason can be attributed to why we can\u2019t perform \u201cclustering upon the output of t-SNE\u201d using distance/density based algorithms like KNN/K-means.  But neighbourhood-graph based clustering approaches will produce the desired result. Hence t-SNE is best used for visualization purposes only.\n\n> Do not apply clustering upon the output of t-SNE\n\n- __t-SNE uses a non-convex objective function:__ The error/divergence is minimised using gradient descent(which is initialised randomly), so we might get different results for different iterations/initialisations.  t-SNE might have multiple minima which necessitates multiple runs, and places a question on the reproducibility of results. Practically, t-SNE is iterated until the results stabilise.\n\n> Multiple runs with the same parameters might not give the same result\n\n- __Perplexity__ is one of the parameters required by the algorithm. It dictates the number of effective of neighbours to be considered while calculating probabilities. So, the perplexity of a distribution over N items can never be higher than N (in this case, the distribution will be uniform). At low values of perplexity, the local variations might dominate, so usually values between 5 and 50 are chosen. We don't have a universal value of ideal perplexity that gives the best results for all types of datasets, so we will have to tune it as a hyper parameter. With increase in number and density of data points, perplexity should also increase. \n\n- t-SNE can generate __fake patterns__ at suboptimal perplexities. We may be on a wild-goose chase if we incorrectly expect clusters when there are actually none.\n\nConsider a gaussian distribution with 250 points in (-2, 0) and 500 points in (0, 2).\n\n![Original Distribution]({% static '/blog/assets/img/t-SNE/orig.png' %})\n\n\nUsing EM(Expectation Maximisation) clustering algorithm produces this result:\n\n![Clustering using EM]({% static '/blog/assets/img/t-SNE/EM.png' %})\n\nThe following are the visualizations produced by t-SNE:\n\n![Perplexity = 20]({% static '/blog/assets/img/t-SNE/20.png' %})\n\n\nIt\u2019s easy to conclude that there are 4 clusters here, while there are only two.\n\n![Perplexity = 40 (default)]({% static '/blog/assets/img/t-SNE/40.png' %})\n\n\nHere, there are too many clusters, and it\u2019s hard for any dim-reduction algorithm to find them.\n(Note: The color coding is only for our interpretation)\n\n![Perplexity = 80 (optimum)]({% static '/blog/assets/img/t-SNE/80.png' %})\n\n\nThis result is from the optimum perplexity of 80, but since perplexity is a global parameter that needs to be tuned and is not the same for every dataset.\n\nThe results are even more stark when these visualizations are obtained from a gaussian distribution as illustrated here. (https://distill.pub/2016/misread-tsne/)\n\n- __Time complexity__ of t-SNE is O(N^2), N = Number of Data points. (grows quadratically)\nSince t-SNE, doesn\u2019t scale well, PCA is first applied to the data (PCA has a better complexity) and number of features is reduced to a maximum of 50 dimensions. t-SNE is then applied onto the output of PCA. This step also ensures that t-SNE works with lesser noisy data.\n\nBut later implementations of t-SNE have better time complexity\n\n\nThere are certain cases t-SNE could be sub-optimal: \n\n* __Interpretability:__ t-SNE only maps the local l variables correctly, and the global trends in the data isn't accurately represented. But PCA's results can be explained very easily.\n\n* __Application to new data:__ t-SNE learns a non-parametric mapping - it does not learn an explicit function from the input space to the map. The embedding in t-SNE is learnt by directly moving the data from high-dimension to low dimension. So we can't apply t-SNE on the fly for an incoming test data point. But we can always apply t-SNE on the whole dataset again. Alternately, we can train a multivariable regressor to predict the position of an input point in the map or directly minimise the t-SNE loss (https://lvdmaaten.github.io/publications/papers/AISTATS_2009.pdf). \n\n* __Incomplete Data:__ t-SNE cannot handle incomplete data. A popular workaround is to use probabilistic PCA on the data, and then use t-SNE.\n\nFurther Reading:\n1. [t-SNE](https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding)\n2. [Visualizing Representations: Deep Learning and Human Beings Christopher Olah's blog, 2015](https://colah.github.io/posts/2015-01-Visualizing-Representations/)\n3. https://distill.pub/2016/misread-tsne/\n4. [Visualizing Data Using t-SNE](https://www.youtube.com/watch?v=RJVL80Gg3lA)\n\nDespite these misgivings, t-SNE might probably be the first tool a data scientist uses in the morning! Have fun exploring it. Do comment below for any clarifications or suggestions.",
        "author_name": "Chenna Keshava B S",
        "slug": "tsne"
    },
    {
        "layout": "post",
        "title": "Evolution of bicycle",
        "author_github": "adithyaprabhu98",
        "date": "2018-02-16T23:32:44",
        "image": "/assets/img/",
        "description": "Evolution of Cycle",
        "tags": [
            "Piston"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "adithyaprabhu98",
        "text": "[{% load static %}]()\nEver since a person began thinking of sources of transportation, maximum development until date has been in the field of bicycle.\n\nHere are a few of the designs discussed briefly, innovation in them and the reasons for their failure or upgradation.\n\nThe first known bicycle was the **velocipede**, a human powered vehicle which required the person riding it to actually push himself against the ground in order to move. No pedals, chain whatsoever, just 2 wheels, a frame and a seat.\n\n![Archetype of a Bicycle]({% static '/blog/assets/img/cycle/1.jpg' %})\n\nSlowly, came an era of multi-wheeled bicycles. Improved stability, increased load carrying capacity and improvement in design. But still the human power dependence was similar to that of a velocipede.\n\nThe first known mechanical bicycle which looks similar to a modern bicycle was invented by a Scottish blacksmith named, Kirkpatrick MacMillan. The cycle was still man powered but the power transmission was through crank and rods, similar to that in a steam engine.\n\n![Image result for kirk patrick first bicycle]({% static '/blog/assets/img/cycle/2.jpg' %})\n\nTurning while driving and cycling downhill were the main problems caused which are eliminated by the usage of free wheels in modern bicycles.\n\nAbout 50 years later came the first pedal driven bicycle. It was called the boneshaker, as it had wheels made of iron and practically, make the ride pretty uncomfortable.\n\nThe drawback of this? It had a fixed pedal to the front wheel, and the input given to it was directly used to propel the cycle. Absence of freewheel makes pedaling necessary and for smaller wheels, one had to pedal more rotations for moving the same distance. Thus the bicycles with huge driving wheel came into picture.\n\n![Related image]({% static '/blog/assets/img/cycle/3.jpg' %})\n\nThis was a great innovative idea, but this too had its own limitations\u2026\n\nWhat happened next?\n\nWhat was the reason for the change?\n\nStay tuned to know more!\n\nTo be continued\u2026",
        "author_name": "Aditya Prabhu",
        "slug": "evolution-of-bicycle"
    },
    {
        "layout": "post",
        "title": "Envision Project 1 - Professor Matching for Research Internships",
        "author_github": "anirudh0707",
        "date": "2018-05-05T23:32:44",
        "image": "/assets/img/",
        "description": "Matching students with professors based on similar interests for Research internships",
        "tags": [
            "Android",
            "Web",
            "Django"
        ],
        "categories": [
            "Envision"
        ],
        "github_username": "anirudh0707",
        "text": "[{% load static %}]()\n## Idea\nFinding research/internship opportunity under a professor\u2019s guidance is exceedingly difficult because the professors are very busy and students are not well aware of the professors who are ready to give internship opportunities. The professor has to shortlist the resumes and the student has to find out and send a resume to every professor offering a research opportunity. Obviously, this is a time consuming and tiresome process for both students and the professors.\n    \nWe came up with a idea which not only speeds up this process but also makes it hassle-free. The idea is to create a platform which comprises of students and professors and then, try to create a match from their preferences. The professor and student both login into this platform and set their preferences (for example professor sets his minimum cgpa as 8 while the student can set the cgpa he secured as 8.2). A recommender system works in the background which takes up each professor, one at a time and tries to create a match with the students in the platform. It then generates a list of students which match the criteria and the professor can just pick the students from the list.\n\n## Implementation\n\nThe front-end of the project comprises of an Android app and a Website. Android app is developed on Android Studio using libraries such as Volley and Gson and with Java as the primary language. The Web Frontend is built using the standard web technologies like HTML, CSS , JavaScript , libraries and Frameworks of JavaScript. Login and signup system is implemented using the OAuth standards via Google/Facebook.\n\nThe back-end of project used python as a primary programming environment and Django framework. It uses MySQL database. The algorithm tries to create an overlap in the student profile and professor requirements and make a ranked list of recommended candidates.\n\n## How did IEEE benefit us?\n\nIEEE envision project helped us to learn different programming languages and technologies. In the process we also learned how to use GitHub effectively. We also learned how to work as a team, how to devise a plan and how to go about developing such projects. Also, helped us in getting along with a better peer group with like minded people! Basically, IEEE mentors helped us whenever we had any problem, hence we could develop such a good platform even though we were first years\u2019.\n\n## Why should you take this up?\n\nTaking up this project in your first year is a good idea as it helps you in learning new stuff. It makes you do something productive. You would be learning so many new things while developing this project which makes it worth the time you spend on it. One will get to meet seniors and get to know about their project, hence a great exposure. Also\u2026 Club recruitments. :P\n\n## Team\n* Prajwal S Belagavi\n* Vaibhav G\n* Ranjana K\n* Nisarg S. Joshi\n\n![The Team]({% static '/blog/assets/img/prof-matching/team.jpg' %})\nThe Team\n\n## Mentors\n* Mahim Agrawal\n* Ashwin Joisa\n* Praveen Gupta\n* Moksh Jain\n\n## Images form the App\n\n![Login Screen]({% static '/blog/assets/img/prof-matching/login.jpg' %})\nLogin Screen\n\n![List of Students]({% static '/blog/assets/img/prof-matching/list-of-students.jpg' %})\nList of students who match a Professor\u2019s Criteria",
        "author_name": "Anirudh B H",
        "slug": "prof-matching-envision"
    },
    {
        "layout": "post",
        "title": "Envision Project 2: Smart Lighting Systems",
        "date": "2018-05-13T10:00:30",
        "image": "/assets/img/",
        "description": "",
        "author_github": "anirudh0707",
        "tags": [
            "IoT"
        ],
        "categories": [
            "Envision"
        ],
        "github_username": "anirudh0707",
        "text": "[{% load static %}]()\n## Idea\n\nWith the ongoing developments in the technological World, a lot of efforts have been invested in the field of automation to make everyone's life easier. As engineering students and the future of the World, it is our responsibility to lead the world into a better future via introducing technology which would reduce human effort as much as possible. Our project, **Smart Lighting System** is aimed at automating the process of lighting of homes in order to personalize the control over the system and to adapt itself to its surroundings for better experience. So basically we decided to introduce 3 main features in the system:\n1. Voice Control \n2. People Counter \n3. Adaptive Brightness\n\n## Implementation\n\nThe idea of a Smart Lighting System was conceived in the month of December 2017. Soon, a basic circuit was designed to give the project a start. After discussions, the team decided to implement the system using light dependent resistors (LDR), an LED, an Arduino board and Lasers as the major components. The team consisted of four first year students who were divided into two groups to take care of the software and hardware part of the project separately. The software part included setting up of a voice control interface to switch the light on or off. The hardware part included setting up the person counter and adaptive brightness feature of the project.\n\nAfter division of the team, the software group began their work to set up a voice control interface. The initial period (January-February) was spent in learning the basics of signal processing which would be needed throughout the project. Topics such as energy of signals, convolution and correlation were learnt during this time. The basic idea was to compare real time signals with a pre-recorded signal and compare the two signals to test for their similarity. If a certain threshold value was cleared, a suitable message was sent to the system to execute a certain function. All this was done using normalized correlation using Python by converting signals into arrays. The code was written in the month of March (till mid-march) and certain difficulties were faced due to the fact that these were relatively new topics to the students. Post writing the code, the team tested the code by using pre-recorded signals &quot;ON&quot; and &quot;OFF&quot; to switch the system on and off respectively. In order to cut-out background noise, low pass and high pass filters were also used to enhance the performance of the code.\n\n![Basic]({% static '/blog/assets/img/smart-lighting/Basic.jpg' %})\n\nThe hardware group implemented their part by using an Arduino board as the main component of the project. In order to count the number of people in the room, two lasers and two LDRs were used and the data received by the LDRs was forwarded to the Arduino board. An Arduino code was written in order to differentiate between a person entering and exiting the room and even a few unwanted situations were taken care of through the code. The adaptive brightness was executed using an LDR kept in isolation from the system. Based on the intensity of light falling on the isolated LDR, the brightness of the LED was made to vary inversely using an Arduino code. The two codes were integrated to work together in harmony. The components used included an LED, LDRs, lasers, jumper wires, a breadboard and an Arduino board.\n\n![Circuit]({% static '/blog/assets/img/smart-lighting/Circuit.jpg' %})\n\nAfter the basic work done by both the groups, it was time to integrate them both in order to make the system work as intended. The python and Arduino code were integrated using Py-serial. As a result, the team was able to complete a Smart Lighting System which was able to take in voice commands, switch itself on/off by itself according to the need and also reduce/increase its brightness according to the need. The project was exhibited by the team in the IEEE Envision Project Expo 2018 which was held in NITK, Surathkal.\n\n## **How did IEEE benefit us?**\n\nThroughout the project, we learnt the basics of signal processing, Python programming, Arduino programming and the working of components such as LDRs, LEDs and lasers. Teamwork was an essential quality of the teams work which was required for completion of project. The project gave us a lot to learn and we thank IEEE for providing them with an opportunity to give their ideas a proper shape and direction. On an individual basis, we learnt how to work as a team and cooperate with each other for proper implementation of project. Excellent guidance was provided at each and every step by the mentors to group members. They were available almost every time of the day for clearing doubts and helping members troubleshooting problems and never hesitated in clarifying any kind of doubts and problems. The project truly was successful only because of their consistent and sincere efforts and we, the group members highly appreciate and thank them for the same.\n\n## **The Team**\n\n- Abhi Kalra\n- Praveen Maiya\n- Rohan Garg\n- Iteesha V A\n\n![Team]({% static '/blog/assets/img/smart-lighting/Team.jpg' %})\n\n## **Mentors**\n\n- Manan Sharma\n- Anirudh B H",
        "author_name": "Anirudh B H",
        "slug": "smart-lighting-systems"
    },
    {
        "layout": "post",
        "title": "Amazon Go - Cashier Less Shopping",
        "date": "2018-05-16T10:00:30",
        "image": "/assets/img/",
        "description": "",
        "author_github": "anirudhsundar",
        "tags": [
            "IoT",
            "Spatial Imaging"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "anirudhsundar",
        "text": "[{% load static %}]()\nAmazon Go is Amazon\u2019s cashier-less shopping platform.\nContrary to Amazon\u2019s online website, the Go is a brick and mortar store- similar to your regular convenience store. What\u2019s unique about the Go is that you can pick up an item and just walk out of the store without having to wait in line to pay for it. The store sells ordinary grocery items like bread, dairy and packaged ready to eat meals.\n\n![Landing image]({% static '/blog/assets/img/amazon-go/1.jpeg' %})\n\n## How does it work?\nA customer has to hold up a QR code on the Amazon Go app onto scanning devices, similar to subway turnstiles. After that they just start shopping as they normally would.\nCameras mounted on the ceiling track each person and what item they pick up or put down and updates that on their cart. When done, he/she just has to walk out of the store and is billed only for the items picked up.\n\n## What\u2019s the technology behind it?\nNaturally, Amazon is pretty secretive about the technology used. However, what they have revealed is that it\u2019s a combination of Artificial Intelligence, Computer Vision, Deep Learning and data pulled up from multiple sensors that enables this cashier-less experience.\nIn Amazon\u2019s own words, \n>\u201cIt\u2019s similar to the same technology you\u2019d find in self driving cars. We\u2019re calling it \u2018Just Walk Out\u2019 technology\u201d\n\n## What exactly is this technology?\n\n#### Artificial Intelligence:\nAI is pretty much the rage these days and everyone\u2019s heard more than enough about it. \n\n#### Computer Vision\nComputer Vision however, is something that\u2019s a little vague. It deals with teaching computers to make educated guesses based on visual cues like images or video feeds. It aims at automating the human visual system.\n\n![Imaging technology in use]({% static '/blog/assets/img/amazon-go/2.png' %})\nComputer Vision deals with acquiring information from digital images, and then processing them to arrive at a digital output, like numeric or symbolic information in the form of decisions.\n\nUsing the horde of cameras on the ceiling, the software is able to uniquely classify and track each individual. In fact, the software is so good that it was able to uniquely distinguish between an army of customers in Pikachu costumes.\n\n![A view of the cameras]({% static '/blog/assets/img/amazon-go/3.jpg' %})\n*The Cameras*\n\n\nAfter that, the program follows each person around virtually and keeps track of what they pick up and put down. The program analyzes which shelves a customer interacts with to determine what item they chose. In order to aid the system, there are weight sensors on the shelves that help the program narrow down on an item.\n\n![The shelved with the items]({% static '/blog/assets/img/amazon-go/4.jpg' %})\n*The Shelves with items*\n\nIn addition to this, Amazon also keeps a track of each person\u2019s previous shopping history to zero-in on an item in the event of the program being confused. For example- if the program is unable to determine whether someone picked up a bottle of mustard or ketchup, it simply looks at the person\u2019s previous choices. If the person buys mustard more often than not, then that\u2019s what they\u2019re more likely to have picked up this time as well.\n\n### What are the drawbacks?\nLike any new software, Amazon Go has its fair share of problems.\nSome customers have been able to leave the store without being charged for certain items.\nThe program also has trouble when there are more than 20 people in the store at once.\nIt also runs into issues when there is a group of people shopping together- like families. There is the added issue of dealing with grabbers- kids that impulsively pick up items only to have their parents put it back immediately. It is still unknown as to how the system would deal with such issues.\n\nFurthermore, many people feel vulnerable about their shopping habits being monitored. A lot of people also feel uncomfortable being tracked by cameras while they shop. Another issue is that the technology facilitates impulse buying.\n\nIt is yet to be seen how Amazon will deal with these issues. However, the technology is still impressive and is completely unparalleled. It goes to show just how far Artificial Intelligence and Deep Learning will go on to influence our daily lives and how the future of retail shopping is shaping up.",
        "author_name": "Anirudh Sundar",
        "slug": "amazon-go"
    },
    {
        "layout": "post",
        "image": "/assets/img/",
        "description": "",
        "date": "2018-05-16T10:00:30",
        "title": "QEMU/KVM for Virtual Machines",
        "author_github": "archit-p",
        "tags": [
            "virtualization",
            "qemu"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "archit-p",
        "text": "[{% load static %}]()\nA \"virtual machine\" or \"VM\" is a virtual computer system. It is a tightly isolated software container with an operating system and application inside. Putting multiple VMs on a single computer enables several operating systems and applications to run on just one physical server, or \u201chost.\u201d\n\nA thin layer of software called a \u201chypervisor\u201d decouples the virtual machines from the host and dynamically allocates computing resources to each virtual machine as needed. \n\n## Introduction\n### QEMU and KVM\nQEMU or Quick Emulator is an open-source machine emulator and virtualizer. It can be used for full-system emulation, user-mode emulation as well as virtualization. We'll be using the full-system emulation capabilities here. \n\nKVM or Kernel-based Virtual Machine on the other hand is a virtualization infrastructure which turns Linux into a hypervisor. It is part of the Linux kernel mainline since 2007. KVM when activated along with Qemu, speeds up the emulation several times. \n\n![Virtualization architecture showing QEMU sitting between the Guest OS and KVM]({% static '/blog/assets/img/qemu-virtualization/kvm_qemu.png' %})  \n\nTo demonstrate QEMU's capabilities we'll create a Debian VM (for potential use as a SSH server)\n\n## Setting up the environment\n### Installing QEMU \nQEMU is available as a PPA for Ubuntu, which can be downloaded using\n```shell\n$ sudo apt-get install qemu\n```\nQEMU isn't one single program, rather it is a large set of tools and utilities each of which serve a different purposes. \n\n### Downloading the iso image\nWe need a Debian installation image before we can get started with QEMU. A simple wget command can download the image from the Debian's online repositories.\n```shell\n$ wget https://cdimage.debian.org/debian-cd/current/amd64/iso-cd/debian-9.2.1-amd64-netinst.iso\n```\n\n### Creating a virtual hard-disk\nThe first step in creating the VM is to create storage space to install the operating system on. We'll create something called a virtual hard drive (VHD). A virtual hard drive is a hard drive image with its own partition table, onto which operating systems can be installed. We can create one using the *qemu-img* utility which comes bundled with QEMU.\n```shell\n$ qemu-img create -f qcow2 debian.qcow2 6G\n```\nThe command takes the location and size of the VHD as arguments. The *-f* option specifies the format for the disk-image as *qcow2* - QEMU's proprietary format - and the last argument is for the size of the disk to be created. \n\n## Installing the ISO and Booting Up VM\n### Installation\nTo install the Debian iso image onto the virtual hard-drive we just created, we can use the QEMU utility for *x86_64* emulation. QEMU provides a variety of commands as *qemu-system-{system-architecture}* for use with different architectures. We're here emulating a 64-bit machine and hence make use of the *qemu-system-x86_64* command.\n```shell\n$ qemu-system-x86_64 -enable-kvm -hda debian.qcow2 -cdrom ../Downloads/debian-9.1.0-amd64-netinst.iso -net user -net nic -m 4G -boot d \n```\nThe command takes as input options - most of which are self-explanatory - and boots up the machine from the iso image mounted as a CD drive. From here on the time taken to install depends on the amount of ram allocated and whether the iso needs to download any packages.\n\nOnce the install is complete, QEMU reboots automatically. We can then close the window and go back to the terminal.\n\n### Booting up\nNow to boot up the virtual machine, we use\n```shell\n$ qemu-system-x86_64 -enable-kvm -hda debian.qcow2 -boot c -net user -net nic -m 2G\n```\nThe _-boot c_ option lets the machine boot up from the hard-drive image.\n\nQEMU also gives us a cool amount of options to tweak our machine to our liking. \n## Options in QEMU\n### Using SSH to login to the machine\nOne way to use the machine in headless mode is to SSH into it using a terminal on our host. For this we need to add _-redir tcp:5022::22_ to our machine startup command. What this does is that it redirects the port 22 of our guest machine into port 5022 of our host. Thus letting us use the machine via SSH. Another option which comes in use is _-display none_ which tells QEMU not to open up an emulator window, since we don\u2019t need it if we are logging into the machine using SSH. Now, to login using SSH, the usual procedure can be followed on the host machine.\n```shell\n$ ssh -p 5022 <user>@<hostname>\n$ <user>@<hostname>'s password:\n```\nWhere *user* and *hostname* are the username and hostname for the machine we created.\n\nTo power the machine down, we need to login as root using root's password, and then type in into the terminal\n```shell\n# poweroff\n```\n\n### Sharing Files Between Host and Guest\nViewing host's files on the guest is one problem that arises quite often while using virtual machines. QEMU solves this by providing an in-built Samba file server.\nIt can be started by adding _-smb {shared directory path}_ to the command for starting the VM. On the guest machine, *smbclient* needs to be installed in order to access the shared folders. On the guest machine run,\n```shell\n$ apt-get install smbclient\n$ smbclient //10.0.2.4/qemu/\n$ Enter user's password:\n```\nNow we can browse the shared directory. If we need to share files, we can copy them into the shared directory on the host, and then access them through the guest, using *smbcleint*.\n\n### Emulating specific CPUs\nWhile being used for system emulation QEMU can emulate specific CPUs as well. This can be achieved by using the _-cpu_ option. To bring up a list of CPUs QEMU can emulate, use _-cpu ?_. And then use that CPUs name in the command. Here I emulated an Intel i7. \n```shell\n$ qemu-system-x86_64 -m 4G -hda debian.qcow2 -enable-kvm -cpu Nehalem -net nic -net user -redir tcp:5022::22 -redir tcp:9080::80 -smb ~/c-prog\n```\nThis can come in especially useful when using the virtual machines for cross-compiling for a specific architecture.\n\n### Preventing Cursor Capture\nCursor capture happens when the guest machine prevents the cursor from being used on the host, i.e. the cursor becomes local to the guest machine. To release the cursor, we need to press *Alt + Ctrl*. Having to do this can be annoying at times. So, we can prevent the cursor from being captured by using the *-show-cursor* option. This works well when used along with the *-enable-kvm* option being enables.\nEnabling KVM speeds up emulation, and hence leads to a lower chance of the cursor being captured. Whereas *-show-cursor* ensures that the cursor is shown at all times, even if the guest machine doesn't have a GUI.\n\n## Conclusion\nWe've just created a simple VM running Debian. I aim to use it as an isolated environment for programming, and maybe learning a few networking concepts while I'm at it. Qemu is also available on Windows so this VM can be accessed from Windows as well, providing a consistent programming environment regardless of which host OS it's run on top of. Another interesting use case for QEMU VMs is to build updated Linux images, download and install packages and install an operating system on a SD card for raspberry pi or other such embedded boards so that you can directly get to working on the board without having to worry about network issues and having to connect to a monitor.",
        "author_name": "Archit Pandey",
        "slug": "qemu-virtualization"
    },
    {
        "layout": "post",
        "title": "Washable Electronics using Water repellant Graphene Circuits",
        "date": "2018-05-16T10:00:30",
        "image": "/assets/img/",
        "description": "",
        "author_github": "sonalparab10",
        "tags": [
            "graphene",
            "electronics"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "sonalparab10",
        "text": "[{% load static %}]()\n**Washable electronics? Water-repellent graphene circuits make it possible!**\n\n*New graphene printing technology can produce electronic circuits that are low-cost, flexible, highly conductive and water repellent.*\n\n![Landing image]({% static '/blog/assets/img/washable-electonics/1.jpg' %})\nThe nanotechnology \"would lend enormous value to self-cleaning wearable/washable electronics that are resistant to stains, or ice and biofilm formation,\" according to a recent paper describing the discovery.\n\nAccording to Jonathan Claussen, an Iowa State University assistant professor of mechanical engineering, an associate of the U.S. Department of Energy's Ames Laboratory, they\u2019re taking low-cost, inkjet-printed graphene and tuning it with a laser to make functional materials.\n\nThe research group used inkjet printing technology to create electric circuits on flexible materials. In this case, the ink is flakes of graphene -- the wonder material can be a great conductor of electricity and heat, plus it's strong, stable and biocompatible.\n\nThe printed flakes, however, aren't highly conductive and have to be processed to remove non-conductive binders and weld the flakes together, boosting conductivity and making them useful for electronics or sensors.\n\nTypically, post processing involves heat and chemicals, but here Claussen and his research group developed a rapid-pulse laser process that treats the graphene without damaging the printing surface -- even if it's paper.\n\nClaussen further added that they\u2019re micro-patterning the surface of the inkjet-printed graphene and the laser aligns the graphene flakes vertically like little pyramids stacking up, and that's what induces the hydrophobicity.\n\nThe energy density of the laser processing can be adjusted to tune the degree of hydrophobicity and conductivity of the printed graphene circuits.\nThe technology could also have applications in flexible electronics, washable sensors in textiles, microfluidic technologies, drag reduction, de-icing, electrochemical sensors and technology that uses graphene structures and electrical simulation to produce stem cells for nerve regeneration.\n\nThe graphene printing, processing and tuning technology is turning out to be very useful.  After all, \"electronics are being incorporated into everything.\"",
        "author_name": "Sonal S. Parab",
        "slug": "washable-electronics"
    },
    {
        "layout": "post",
        "title": "Envision Project 3: Regenerative Braking",
        "author_github": "salman-bhai",
        "date": "2018-05-20T23:32:44",
        "image": "/assets/img/",
        "description": "Using Regenerative Braking to generate electricity",
        "tags": [
            "Braking"
        ],
        "categories": [
            "Envision"
        ],
        "github_username": "salman-bhai",
        "comments": true,
        "text": "[{% load static %}]()\n## Idea:\n\n**Regenerative Braking** is an energy recovery mechanism which slows a vehicle or object by converting its kinetic energy into a form which can be either be used immediately or stored until needed. We have implemented this RBS in bicycles for generating electricity which can be used for different purposes like charging phones. For this we constructed a circuit to convert Mechanical energy to Electrical energy.\n\n![pic1]({% static '/blog/assets/img/regenerative-braking/pic1.jpeg' %})\n\n## Implementation\n\nRBS is based on the principle of electromagnetic induction. On the application of brakes, the gears on the brakes come in contact with the rim of the wheel. These gears are connected to a DC Dynamo. This dynamo converts mechanical energy to electrical energy which can be used directly or stored in a battery. We have used this energy directly to charge a phone.\n\n![pic2]({% static '/blog/assets/img/regenerative-braking/pic2.jpeg' %})\n![pic3]({% static '/blog/assets/img/regenerative-braking/pic3.jpeg' %})\n![pic4]({% static '/blog/assets/img/regenerative-braking/pic4.jpeg' %})\n\n## Mentors \n\nRBS project was under the supervision of assigned mentors Anand Pathak and Ayush Srivastava. Anand Pathak mentored the project by providing insights on the Mechanical structure and Casing. Ayush Srivastava mentored the design and building the electronic circuit. RBS was implemented under the ENVISION initiative by IEEE which funded for the same and assigned mentors who were IEEE Executive members.\n\n## Why should such activities be undertaken?\n\nRBS is one of those projects which does not require digital analysis, software simulation, high level technical knowledge and experience. Taking up such projects in the first year improves technical knowledge and experience, develops team spirit and management skills.\n\n![pic5]({% static '/blog/assets/img/regenerative-braking/pic5.jpeg' %})",
        "author_name": "Salman Shah",
        "slug": "regenerative-braking"
    },
    {
        "layout": "post",
        "title": "Alterego",
        "date": "2018-05-28T23:32:44",
        "image": "/assets/img/",
        "description": "What if we could just think of the question and a device could just Google it real quick and give the answer?",
        "tags": [
            "Alterego"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "sinchana-hegde",
        "comments": true,
        "text": "[{% load static %}]()\n# AlterEgo\n\nWe basically can't live without our cell phones, our digital devices. But at the moment, the use of those devices is very disruptive. Wouldn't it be rather nice if we could just think of the question and a device could just Google it real quick and give the answer? \nWell, we've reached the point where robots can now read your mind. Kind of.\nResearchers at MIT have developed a wearable gadget that transcribes and responds to conversations users have with themselves in their heads.\n![Example]({% static '/blog/assets/img/alterego/alterego.jpg' %})\n\nComprised of a wearable device and a small computing system, AlterEgo enables users to \"silently converse\" with a computing device without talking or making any recognisable movements.\nIt uses electrodes to pick up neuromuscular signals in the user's jaw and face that are triggered by internal verbalisations.\nThese signals are then sent to a machine-learning system that has been trained to associate certain signals with certain words.\n\nA pair of bone-conduction headphones included in the device transmits vibrations through the bones of the face to the inner ear. As the headphones don't obstruct the ear canal, this means the system can pass on information to the user without interrupting a conversation or interfering with the user's aural experience.\nThe motivation for this was to build an IA device \u2014 an intelligence-augmentation device, a computing platform that's more internal, that melds human and machine in some ways and that feels like an internal extension of our own cognition.\nIf the user were to internally question, \"what is the time,\" the AlterEgo headset would register this, and feed the answer back to them through bone conduction.\n\nThis means the user doesn't have to look at a screen and type in words to find the answer to their question, or to control a device.",
        "author_name": "Sinchana Hegde",
        "slug": "alterego"
    },
    {
        "layout": "post",
        "title": "Market Basket Analysis",
        "date": "2018-05-28T23:32:44",
        "image": "/assets/img/",
        "description": "Market Basket Analysis in retail market",
        "tags": [
            "Data mining",
            "Apriori"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "anumehaagrawal",
        "comments": true,
        "text": "[{% load static %}]()\n# Market Basket Analysis\n\nEver wondered how Flipkart knows what items to suggest to you before and after you make a purchase? The is an aspect of analysis created specifically to promote sales called market basket analysis (MBA).  In the retail industry, MBA refers to an unsupervised data mining technique that discovers co-occurrence relationships among customers\u2019 purchase activities. The technique is based on the theory that if you buy a certain group of items, you are likely to buy another group of items. For example: while at McDonald\u2019s, if you buy sandwiches and cookies, you are more likely to buy a drink than someone who did not buy a sandwich.\n\n![Example]({% static '/blog/assets/img/market-basket/mba.png' %})\n\n\nThis technique  involves finding a relationship between purchased products by discovering links using simple rules, known as the rules of association. Market basket analysis can effectively present product offers, create more effective promotions and develop more efficient marketing campaigns.  The quality of rules determines several measures like support,confidence and lift which are important from business point . \n\n\n### Apriori Algorithm\n\nIn data mining, Apriori is a classic algorithm for learning association rules. Apriori is designed to operate on databases containing transactions (for example, collections of items bought by customers, or details of a website frequentation). . The whole point of the algorithm (and data mining, in general) is to extract useful information from large amounts of data. For example, the information that a customer who purchases a keyboard also tends to buy a mouse at the same time is acquired from the association rule below:\n\n#### Support\nThe percentage of task-relevant data transactions for which the pattern is true.\nSupport (Keyboard -> Mouse) =  ![Support]({% static '/blog/assets/img/market-basket/support.png' %})\n\n#### Confidence\nThe measure of certainty or trustworthiness associated with each discovered pattern.\nConfidence (Keyboard -> Mouse) = ![Confidence]({% static '/blog/assets/img/market-basket/conf.png' %})\n\nMBA is being used in all aspects of business from small scale retailing to large scale marketing .Its being used extensively in the food industry as well .Suppose that the sales report in a confectionary shop shows that on the first Monday of the month of 1,000 customers, 200 bought cupcakes, and from those who bought cupcakes , 50 bought chocolates as well . Here we see a rule that if someone buys cupcakes, also buys chocolates, which support is 5% (50/1000), and the confidence is 25% (50/200). \n\n### How can you use MBA for your own store !\n\n1. Store Organisation:\nBased on the insights from market basket analysis you can organize your store to increase revenues. Items that go along with each other should be placed near each other to help consumers notice them.\n2. Spam Messages:\nBy using data from MBA you can suggest the next best product which a customer is likely to buy. Hence you will help your customers with fruitful suggestions instead of annoying them with marketing blasts.\n4. E-Commerce Websites:\nIn case of e-commerce businesses, website content placement is very important. MBA can be used to predict what a customer will buy next based on the items the customer has viewed . This will not only increase revenue but also bring in more customers \n5. Recommendation Engines:\nRecommendation engines are already used by some popular companies like Netflix, Amazon, Facebook, etc. If you want to create an effective recommendation system for your company then you will also need market basket analysis to efficiently maintain one.",
        "author_name": "Anumeha Agrawal",
        "slug": "market-basket"
    },
    {
        "layout": "post",
        "title": "Why 50Hz ?",
        "date": "2018-05-28T23:32:44",
        "image": "/assets/img/",
        "description": "Why does Power Supply Systems run only at 50 or 60 Hz and not any other frequency?",
        "tags": [
            "electric power grid",
            "50Hz frequency"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "meghbhalerao",
        "comments": true,
        "text": "[{% load static %}]()\n##  Why do most of the countries use a 50/60 Hz, 110/220 V power supply and not any othervalue?\n\nThe answer to this question is not definite but quite a few technical and historical reasons can be found to support this which date back to the very beginning of the development of the electrical power system.\n\n## What is supply frequency and why is it an important parameter in the electrical grid?\n\n![Example]({% static '/blog/assets/img/why-50hz/image001.jpg' %})\n\nThe electrical power grid consists of a massive network of generators, transmission lines, transformers, substations and a variety of loads drawing real and reactive power. The grid frequency is decided by the speed at which the synchronous generators at various thermal, hydro and nuclear power stations operate (f = N*P/120, P = no. of poles in the machine). \n\nIn turn the frequency determines the size, efficiency and power producing capacity of the machine. The frequency also decides the inductive reactance of the transmission line, hence the power losses in the line. One thing that must be kept in mind is that the frequency is a global parameter of the grid and must be kept almost exactly at the given value with a variation of at most 0.2Hz. If disturbed, the entire grid may fail to cause a blackout.\n\n## So why do some countries use 50 Hz while others use 60?\n\nMost of the North & South Americas, parts of Japan and Europe operate at 60Hz while the rest of the world operates at 50Hz. There isnt much of a difference between the two numbers but one may be curious to know what is the reason for the difference. \n\nThe first ever alternating current experiment was conducted by an inventor named **William Stanley** in a small town called Great Barrington in Massachusetts, funded by the Westinghouse Electric Company. At that time in the US used Arc Lighting systems, on which calculations showed that they operated with the best efficiency at 60Hz. Also, back then the US used the duodecimal numbering system which made calculations easier at a 60Hz frequency. Simultaneously in Europe, the AEG (German acronym for General Electric Company) started to build power stations with generators producing 50Hz to compete with the western system of 60Hz. They started manufacturing electrical equipment which operated at 50Hz.\n\n![Westinghouse]({% static '/blog/assets/img/why-50hz/image003.png' %})\n\n## When and how did the 50Hz frequency come to the Indian subcontinent?\n\nBy the beginning of the 20<sup>th</sup> century 50Hz had become standardized across all of Europe. At that time, as we all know, India was colonized by the British. India's first small hydroelectric power plant of 130kW capacity was set up in Darjeeling in 1897. After 5 years India's first major power plant near Shivanasamudram falls in Karnataka, of 4.5MW was commissioned to provide power to the Kolar Gold mines located about 150km away. Both of these plants had generators operating at 50Hz.\n\n![Why 50 Hz]({% static '/blog/assets/img/why-50hz/image005.jpg' %})\n\n## Why do aviation systems operate at a 400Hz frequency, which is about 8 times the frequency of mainland power systems?\n\nThe major tradeoff that an aviation engineer has to keep in mind is between efficiency and size. Modern day aircrafts have highly sophisticated communication systems, complex avionics, and a lot of passenger entertainment systems which increase the power requirement of the aircraft. The primary requirement of an airplane is fuel efficiency. When an AC generator is operated at a higher frequency the number of windings that are needed to produce the same amount of voltage is reduced drastically. This modification brings down the weight of the machine which in turn saves vast amount of fuel. The drawback of high frequency is large amounts of reactive power losses in the system, but as mentioned earlier, our primary requirement is not reducing the losses but to reduce the weight and size. Hence, it can be concluded that the frequency of the system has to be chosen to satisfy the end needs.\n\n![img7]({% static '/blog/assets/img/why-50hz/image007.jpg' %})\n\n## Why is the supply voltage 110V in some countries while 220V in India and others?\n\nOnce again, we go back to the late 19<sup>th</sup> century during the time when Thomas Alva Edison invented the first incandescent lamp. He set up the first ever DC power supply line on Pearl Street, Manhattan. A voltage of 110V was supplied from the sending end to feed the incandescent lamp loads which were designed to run at 100V. The extra 10V was provided at the supply end to account for the transmission line losses. \n\nSince then always an extra voltage of 10% with respect to the desired receiving end voltage is supplied (That is why we see transformers of rating such as 11kV, 110kV, 220kV) etc. Since then the US has followed the practice of 110V live wire voltage with respect to the neutral. In European countries, a 220V line voltage was preferred due to the increased transmission efficiency even though it is more dangerous.\n\n## What are the recent trends that help the modern power system provide clean and efficient power?\n\n![img9]({% static '/blog/assets/img/why-50hz/image009.jpg' %})\n\nThe latest emerging technology is the smart grid technology. This technology aims at integrating control systems with the grid to provide a more detailed information about power demand to the engineers in various regions of the grid. It also aims at using Artificial Intelligence to solve power system related issues and also automatic rerouting of power from generating station to homes in case of any environmental damage caused to the transmission line. The smart grid also aims at merging renewables such as wind and solar energy to the existing grid. This technology will help in saving this precious resource that all of us take for granted.",
        "author_name": "Megh Bhalerao",
        "slug": "why-50hz"
    },
    {
        "layout": "post",
        "title": "Charge Crowding",
        "author_github": "sampathkoti",
        "date": "2018-06-05T23:32:44",
        "description": "The smallest entity 'charge' can cause the greatest disasters in electronic devices",
        "tags": [
            "CAS/SPS",
            "electronic devices"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "sampathkoti",
        "text": "[{% load static %}]()\n# Charge crowding\n\n## Electrostatic Discharge\n\nElectrostatic Discharge, or ESD, is a single-event, rapid transfer of electrostatic charge between two objects, usually resulting when two objects at different potentials come into direct contact with each other.  ESD can also occur when a high electrostatic field develops between two objects in close proximity.  ESD is one of the major causes of device failures in the semiconductor industry\nElectrostatic charge build-up occurs as a result of an imbalance of electrons on the surface of a material.  Such a charge build-up develops an electric field that has measurable effects on other objects at a distance.  The process of electron transfer as a result of two objects coming into contact with each other and then separating is known as 'triboelectric charging'.  \n     \nThis charging process results in one object gaining electrons on its surface, and therefore becoming negatively charged, and another object losing electrons from its surface, and therefore becoming positively charged. A person can get triboelectrically charged in a number of ways, even by just walking across a room. The tendencies of various materials to charge up either positively or negatively are shown in a Triboelectric Series.      \nThere are three (3) predominant ESD models for IC's: 1) the Human Body Model (HBM); 2) the Charged Device Model (CDM); and 3) the Machine Model (MM).  The HBM simulates the ESD event when a person charged either to a positive or negative potential touches an IC that is at another potential.  The CDM simulates the ESD event wherein a device charges to a certain potential, and then gets into contact with a conductive surface at a different potential.  The MM simulates the ESD event that occurs when a part of an equipment or tool comes into contact with a device at a different potential. HBM and CDM are considered to be more 'real world' models than the MM. .\n\n\n## ESD Control With Ionization \n\n### Introduction\n\nElectrostatic charge attracts contamination to surfaces of semiconductor wafers and flat panel displays, as well as to critical surfaces of process tools. Electrostatic discharge (ESD) damages electronic components as well as the tools used to manufacture them (for example, semiconductor photomasks).  \n \nThe ESD sensitivity of semiconductor components is routinely decreasing with each succeeding semiconductor generation. While typical damage thresholds for semiconductors may range from 50 volts to over 10,000 volts. Technology trends in all areas of electronics production will make existing static problems worse. \n\n### Static charge control with ionisation\n\nStatic charge generation is unavoidable. Triboelectric charging occurs through the contact and separation of materials. A charged surface can also induce an electrostatic potential (voltage) on conductive objects close to the static charge without any actual contact.  Avoiding charge generation and transfer throughout the electronics manufacturing process is essentially impossible, but with a comprehensive program, it can be controlled. Every static control program starts with extensive grounding of all conductive and static dissipative materials including personnel.  \n \nNon-conductors are required in process areas (most often, they are the product itself). Since charged non-conductors will not consistently lose their charge by grounding, methods of neutralization are needed.  Ionization is the most used technology for the neutralization of static charge on non-conductors and conductors that are isolated from ground. Room ionizers provide neutralization over large areas, worksurface ionizers target smaller production areas, and point-of-use ionizers are included in production equipment. Ionizers used in electronics production areas must follow the same general guidelines for material compatibility as other equipment in the area.\n\n\n### Challenges\n\nMost current ionization techniques used in electronics production are electronic, employing corona ionization. Efficient designs, providing sufficient ion densities for fast neutralization, use the AC, steady-state DC, and pulsed DC technologies currently available.  Increasing future requirements for low levels of ionizer balance require the use of sophisticated methods of monitoring and feedback control. We have already learned in disk drive production that there is a practical limit of about 3-5 volts in control of corona ionizer balance. This has produced a shift to the use of alpha or soft x-ray ionizers. Physics dictates that these ionizers produce a continuous balance of positive and negative ions without feedback and control methods. They will find increasing uses in applications requiring low levels of ionizer balance, particularly in industries shifting to automated production techniques.  Maintenance of these ionizers involves the periodic replacement of their active elements and compliance with the applicable government regulations.  \n \nMeasuring methods of ionization may need to be changed to better represent the range of sizes of ESD sensitive components. Static charge issues for a one-meter square of glass (flat panel display) are very different than for a GMR head, and neither is adequately represented by measurements made with a CPM. Yield analysis needs to establish requirements for ionizer balance, as well as the applicability of existing ionizer technologies (particularly AC ionizers) in handling extremely static-sensitive components.  \n \nClose attention must also be paid to turbulence generation by the ionizer.  Cleanrooms use laminar airflow to prevent particle deposition on critical surfaces.  The use of ionizing blowers or other ionizers placed in the airflow path means that turbulence could compromise the laminarity of the airflow.  Therefore, bar ionizers, ionizing blowers or other types of ionizers must be carefully selected to minimize this effect.  The cleanroom compromises between contamination control and ESD control must be well understood.  \n \nThe following are some recommendations that will assist manufacturers of ionizer products in updating equipment specifications:   \n \n1. Establish the \u201creal\u201d damage thresholds of electronic devices with a well-defined testing program, including simulation methods that reflect the ESD hazards that devices will encounter. For example, the increasing use of automation tends toward CDM and MM testing rather than HBM.  Estimate how sensitive devices will be in the future.\n2. Establish ionizer neutralization time and balance specifications for standard airflow rates (both laminar and turbulent) and operating distances. Relate this information, if possible, to production yield or defect rates. \n\n## Conclusion\n\nIonization continues to be an essential element in a static control program. As with other elements in the program, control of the operating parameters will become more critical with technological change. Product design to maximize ESD damage thresholds may not be able to keep up with this rapid change, putting more dependence on ionizers and other static control methods to protect products. In all areas we are looking at rapidly approaching technology barriers. Surmounting these barriers means, \u201cstatic control is not an option\u201d. Ionizers and ESD control methods will need to keep pace with rapid technology change or they will become the barriers to future production.",
        "author_name": "Sampath Koti",
        "slug": "charge-crowding"
    },
    {
        "layout": "post",
        "title": "Computational Approaches of Shock Waves",
        "author_github": "shashank-ravichandir",
        "date": "2018-06-05T23:32:44",
        "description": "CFD approaches for solving involving shock waves",
        "tags": [
            "Piston",
            "Shock Waves",
            "CFD"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "shashank-ravichandir",
        "text": "[{% load static %}]()\nIn this article I would like to discuss the two different CFD approaches while\nsolving for flows involving shock waves.\n\nIn flow fields involving shock waves there is a sharp discontinuous change in\nthe flow variables like pressure, density, temperature, etc. across the shock.\nThe first approach is to have the shock wave appear naturally in the\ncomputational domain as a direct solution of the governing equations. This is\ncalled the *shock capturing method.* The second approach is to explicitly\nintroduce the shock wave into the flow and then solve the governing equations\nfor the rest of the domain (between the shock and some boundary) to determine\nthe effect of the shock on the flow field. This is called the *shock fitting\nmethod.*\n\nThe shock capturing method is used for complex flows for which we do not know\nthe location or number of shock waves. This approach also does not require any\nspecial treatment of the shock within the algorithm used and hence simplifies\nprogramming. The disadvantage of this approach is that the shock wave obtained\nis smeared over a finite number of grid cells and rarely resembles the shape of\nthe physical shock wave and hence the precise location of shock discontinuity\nand the shock thickness are uncertain within the mesh elements\u2019 size.\n\nIn contrast in the shock fitting method, the shock is treated as a discontinuity\nand its location is defined numerically. However prior knowledge of the shock\u2019s\nlocation and the number of shocks is required which is a major disadvantage\nwhile dealing with complex flows.\n\nHence a combination of these two methods is generally used whenever possible. A\nshock capturing approach is used to predict the approximate location of shocks\nand then these shocks are fit midway during computation. Another combination is\nto fit shocks explicitly in those parts where we know they occur and to employ a\nshock capturing method for the remainder of the flow field.",
        "author_name": "Shashank Ravichandir",
        "slug": "computational-approaches-of-shock-waves"
    },
    {
        "layout": "post",
        "title": "Diving into and Exploiting the Heap",
        "author_github": "suraj44",
        "date": "2018-06-05T23:32:44",
        "description": "Understanding the heap segment of a process and analysing one of its vulnerabilities",
        "tags": [
            "heap",
            "exploitation",
            "memory",
            "allocation"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "suraj44",
        "text": "[{% load static %}]()\nAt the physical level, computer memory consists of a large number of flip flops. Each flip flop consists of a few transistors, and is capable of storing one bit. Individual flip flops are addressable by a unique identifier, so we can read and overwrite them. Thus, conceptually, we can think of all of our computer\u2019s memory as just one giant array of bits that we can read and write. \n\nSince as humans, we are not that good at doing all of our thinking and arithmetic in bits, we group them into larger groups, which together can be used to represent numbers. 8 bits are called 1 byte; beyond bytes, there are words (which are sometimes 16, sometimes 32 bits). Thus, more often, we conceptually regard our computer\u2019s memory as one large (size 2<sup>32</sup> or so) array of bytes. \n\n## What is the Heap and How Big is it?\nWhen you compile a program, you're generating an executable called a program. A program is said to be 'executing' when it has been loaded into main memory and has different resources allocated to it. When a process is under execution, it has its own [address space](https://github.com/angrave/SystemProgramming/wiki/Processes,-Part-1:-Introduction#process-contents) that looks something like this: \n![haha](https://camo.githubusercontent.com/dc11307dfd6164f6021c6089eee9e12518554fdd/68747470733a2f2f692e696d6775722e636f6d2f706c364b3563462e706e67)\nA heap is a part of this address space.\n\nThe heap is part of the process memory and it does not have a fixed size. Heap memory allocation is performed by the C library when you call `malloc` (`calloc`, `realloc`) and `free`. This means that the program can 'request' and 'release' memory from the heap segment whenever it requires. Also, this memory is global, i.e. it can be accessed and modified from anywhere within a program and is not localized to the function where it is allocated. This is accomplished using 'pointers' to reference dynamically allocated memory which in turn leads to a small [_degradation_](https://en.wikipedia.org/wiki/Thrashing_(computer_science)) in performance as compared to using local variables (on the stack).\n\nMemory in the heap is allocated in 'chunks'. The structure of a chunk is something like this:\n\n\n```\nAllocated chunk:\n\nchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Size of previous chunk, if unallocated (P clear)  |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Size of chunk, in bytes                     |A|M|P|\n      mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             User data starts here...                          .\n            .                                                               .\n            .             (malloc_usable_size() bytes)                      .\n            .                                                               |\nnextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             (size of chunk, but used for application data)    |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Size of next chunk, in bytes                |A|0|1|\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n\n\nFree chunk:\n\n\nchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Size of previous chunk, if unallocated (P clear)  |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n    'head:' |             Size of chunk, in bytes                     |A|0|P|\n      mem-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Forward pointer to next chunk in list             |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Back pointer to previous chunk in list            |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Unused space (may be 0 bytes long)                .\n            .                                                               .\n            .                                                               |\nnextchunk-> +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n    'foot:' |             Size of chunk, in bytes                           |\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n            |             Size of next chunk, in bytes                |A|0|0|\n            +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n\n```\n\nA **bin** is list (singly or doubly linked list) of free chunks. They're [categorized](https://sourceware.org/glibc/wiki/MallocInternals) based on the size of chunks they can hold. \n\n## Exploiting the Heap\n\n### Double Free ()\nFreeing a resource more than once can lead to a memory leak. The data structure implemented by the allocator gets corrupter and can be exploited by the attacker. Since *glibc* has a protective measure against the `double free()` exploitation, we will free another chunk it between. By doing so, we can return the same chunk by two different `malloc()`s. If the attacker is in control of one of the pointers, they can modify the memory leading to different attacks, including injecting the address of a malicious program which could be executed. \n\nConsider the following code: \n\n```\na = malloc(10); //0xb16010\n\nb = malloc(10); //0xb16030\n\nc = malloc(10); //0xb16060\n\nfree(a);\nfree(b); //To circumvent glibc's check\nfree(a); // DOUBLE FREE!\n\nx = malloc(10); //0xb16010\n\ne = malloc(10); //0xb16030\n\nz = malloc(10); //0xb16010 - Same address as 'x' !\n```\n\n### How did that happen?\n\nThe state of the bin through the code:\n1.  'a' freed: \n> head -> a -> tail\n2.  'b' freed: \n> head -> b -> a -> tail\n3.  'a' freed: \n> head -> a -> b -> a -> tail\n4.  malloc() request for 'x' \n> head -> b -> a -> tail\n5.  malloc() request for 'y' \n> head -> a -> tail\n6.  malloc() request for 'z'\n> head -> tail\n\nNow that 'x' and 'y' point to the same memory location, any change in one will reflect in the other.\nNote: This exploitation only works when the size of the chunks allocated are in the fastbin list.  \n\nThis was an extremely simple exploitation of the heap but you can read up on some of the more sophisticated ones: \n1. [House of Spirit](https://gbmaster.wordpress.com/2015/07/21/x86-exploitation-101-house-of-spirit-friendly-stack-overflow/)\n2. [House of Einherjar](https://github.com/st4g3r/House-of-Einherjar-CB2016) \n\n\n\n\n\n\n### Resources\n\n1. [Understanding the heap by breaking it](https://www.blackhat.com/presentations/bh-usa-07/Ferguson/Whitepaper/bh-usa-07-ferguson-WP.pdf)\n2. [Heap exploitation](https://legacy.gitbook.com/book/dhavalkapil/heap-exploitation/details)\n3. [Systems Programming](https://github.com/angrave/SystemProgramming/wiki)\n4. [A Step Ahead in Reverse Engineering!](https://ieeenitk.org/blog//a-step-ahead-in-reverse-engineering/)\n5. [Your first heap exploit by GaTech](https://tc.gtisc.gatech.edu/cs6265/2016/l/lab10-heap/README-tut.txt)",
        "author_name": "Suraj Singh",
        "slug": "heap_exploitation"
    },
    {
        "layout": "post",
        "image": "/assets/img/",
        "description": "",
        "date": "2018-06-10T10:00:30",
        "title": "Chain of Things",
        "author_github": "mehakarora",
        "tags": [
            "blockchain",
            "IoT"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "mehakarora",
        "text": "[{% load static %}]()\n> \u201cThe Internet will disappear. There will be so many IP addresses, so many devices, sensors, things that you are wearing, things that you are interacting with, that you won't even sense it. It will be part of your presence all the time.\u201d\n\nThese words, spoken in the World Economic Forum in 2015 by Google\u2019s then Chairman Eric Schmidt, are quickly proving to be true. From smart homes to smart citites, the **Internet of Things** is everywhere. The number of connected devices has grown by a whopping 31% over the last year. However, all this data is present on a very hackable internet, and this poses an ever increasing security and privacy threat.\n\n## What is Blockchain?\n\nWhile the Internet of Things is taking the tech world by storm, another revolutionary technology has started a new era in the financial world: Blockchain. Bitcoin, one of the first cryptocurriencies, built on a Blockchain platform, made it possible to have peer-to-peer online electronic cash transactions without any intermediaries.\n\n### Understanding it through Money Transactions\n\nHistorically, when it comes to transacting money or anything of value, people and businesses have relied heavily on intermediaries like banks and governments to ensure trust and certainty in the transaction by various practices like authentication and record-keeping. Because digital assets like money, stocks & intellectual property, are essentially files, they are incredibly easy to reproduce. This creates what\u2019s known as the double spending problem (the act of spending the same unit of value more than once) which until now has prevented the peer to peer transfer of digital assets. This is where blockchain comes into picture.\n\nSimply put, a blockchain is a type of *distributed ledger or decentralized database* that keeps continuously updated digital records of who owns what. Rather than having a central administrator like a traditional database a distributed ledger has a network of replicated databases, synchronized via the internet and visible to anyone within the network.  \n\nWhen a digital transaction is carried out, it is grouped together in a cryptographically protected block with other transactions that have occurred in the last 10 minutes and sent out to the entire network. *Miners* (computers in the network with high levels of computing power) then compete to validate the transactions by solving complex coded problems.The first miner to solve the problems and validate the block receives a reward. (In the Bitcoin Blockchain network, for example, a miner would receive Bitcoins).\n\nThe validated block of transactions is then timestamped and added to a chain in a linear, chronological order. New blocks of validated transactions are linked to older blocks, making a chain of blocks that show every transaction made in the history of that blockchain.The entire chain is continually updated so that every ledger in the network is the same, giving each member the ability to prove who owns what at any given time.\n\nThis brings unprecedented security benefits. Hacking attacks that commonly impact large centralized intermediaries like banks would be virtually impossible to pull off on the blockchain. For example\u200a\u2014\u200aif someone wanted to hack into a particular block in a blockchain, a hacker would not only need to hack into that specific block, but all of the proceeding blocks going back the entire history of that blockchain. And they would need to do it on every ledger in the network, which could be millions, simultaneously.\n\n## A Marriage of the Titans: Blockchain and IoT\n\nUsing blockchain to increase the reliabilty and security of Internet of Things is going to be the next big thing in the industry. Consider tech giants like Google or Amazon. They have the resources to employ teams of security analysts whose job is to detect flaws in their softwares before any malicious third party does. Thier software is updates through regular security patches. Not let us imagine a company providing a city-wide webcam surveillance service. Unlike tech giants like Google, or Amazon, this company does not have the resources to provide such security. The ability to prevent future cyberattacks like that depends on creating a way companies like this can quickly, easily and cheaply issue software updates to customers when flaws are discovered.\n\nAlso there is the issue of connectivity. IoT devices need to be connected to the network to function. If your Amazon Echo goes offline for a while, there is no real issue. But say a medical Bot goes offline, or starts installing an update in the middle of a surgery, there\u2019s going to be a problem.\n\n### Blockchain Solution\n\nA blockchain solution would be available for device makers to use in place of creating their own software update infrastructure the way the tech giants have. These smaller companies would have to program their products to check in with a blockchain system periodically to see if there was new software. Then they would securely upload their updates as they developed them. Each device would have a strong cryptographic identity, to ensure the manufacturer is communicating with the right device. As a result, device makers and their customers would know the equipment would efficiently keep its security up to date. Also, even if connectivity is lost in one area, other areas are still connected to the internet.\n\n\nMany companies are now investing in a Blockchain-IoT future. IoT desperately needs the blockchain\u2019s features while the blockchain is hungry for the big-time opportunity that IoT represents. Together, it seems like this is going to be the next big thing.\n\n\n\n## References:\n\n1. 'Using Blockchain to secure the Internet of Things', Nir Kshetri,Professor of Management, University of North Carolina \u2013 Greensboro",
        "author_name": "Mehak Arora",
        "slug": "chain-of-things"
    },
    {
        "layout": "post",
        "image": "\\assets\\img\\earthing-systems\\",
        "description": "",
        "date": "2018-06-17T10:00:30",
        "title": "Earthing Systems",
        "author_github": "shreeramhegde",
        "tags": [
            "heavy electronics"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "shreeramhegde",
        "text": "[{% load static %}]()\nMany appliances such as an electric heater, a refrigerator, a CPU etc. have metallic body (casing).If with use the insulation of a wire breaks or melts , the live wire may touch the casing. If we happen to touch the casing carrying a current we may receive a severe shock.\n\n![image001]({% static '/blog/assets/img/earthing-systems/image001.jpg' %})\n\nTo avoid this, metal casing of all electrical appliances are earthed.\n\n![image002]({% static '/blog/assets/img/earthing-systems/image002.jpg' %})\n\n## What is an earthing system?\n\nThe connection between electrical appliances and devices with the earth plate or electrode through a thick wire of low resistance to provide safety is known as \u2018Earthing\u2019 or Grounding.\n\nThe main objective of the Earthing system is to provide an alternative path for dangerous currents to flow so that accidents due to electric shock and damage to the equipment can be avoided.\n\n## Importance of earthing\n\n* The earthing protects the personnel from the short circuit current.\n* The earthing provides the easiest path to the flow of short circuit current even after the failure of the insulation.\n* The earthing protects the apparatus and personnel from the high voltage surges and lightning discharge.\n\n## Types of earthing\n\n### Neutral Earthing \n\nIn neutral earthing, the neutral of the system is directly connected to earth by the help of the GI wire.  The neutral earthing is provided in the generator, transformer, motor etc.\n\n### Equipment Earthing\n\nSuch type of earthing is provided to the electrical equipment. The non-current carrying part of the equipment like their metallic frame is connected to the earth by the help of the conducting wire.",
        "author_name": "Shreeram Hegde",
        "slug": "earthing-systems"
    },
    {
        "layout": "post",
        "title": "Nanomotors",
        "author_github": "sanathbk",
        "date": "2018-06-18T16:25:03",
        "image": "/assets/img/",
        "description": "An interesting take on nanomotors and their biomedical applications",
        "tags": [
            "Nanotechnology",
            "Nano motors",
            "Motors",
            "ATP",
            "Biology"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "sanathbk",
        "comments": true,
        "text": "[{% load static %}]()\n\ufeffNanotechnology was regarded to be the technology of the future. Well, the future has come. Its no more the technology of the future. Scientists and engineers have become increasingly good at designing and engineering materials down at the level of atoms and molecules. Nanotechnology is the manipulation of matter at nanoscale which is about 1-100 nanometres. Its well known that everything in this universe is made out of atoms and it\u2019s the atoms and the arrangement of these atoms that dictate the characteristics. Two wheels and a handle, although are said to make a bicycle, it\u2019s called a bicycle only if they are assembled the right way. Otherwise, they are just parts and the functionality of bicycle can never be achieved. Atoms are just like the wheels, except they are smaller, however the idea remains the same. The arrangement of atoms decides how strong or how weak a substance is, or if the substance is transparent etc.  \n\nHere, I would be talking about the bottom-up approach. In this approach we try and control the atoms and their assembly in order to achieve the desired characteristics. Imagine a scattered pile cups in front of you and you are wearing the boxing gloves. You can move the cups around but you cannot control an individual cup. If you are asked to pick a cup at a time and arrange, it would be extremely hard or even impossible. Nanotechnology allows us to take the boxing gloves off, thus making it easier to pick one cup at a time and arrange in the desired manner. Nanotechnology has opened gates for research in every possible field. One such field is Biology.  \n\nThe human body is made up of millions and millions of cells and each cell is made of several cell organelles. Each of which performs specific task. The energy required to perform these tasks is supplied by mitochondria, the powerhouse of the cell. Apart from the energy, there is cargo that needs to be transported to different parts of the cell. These activities are made possible by the nanomotors or the biological motors in the cell.  \n\nBio-molecular motors like kinesins, myosins and ATP synthase have been created by nature and these are essential for the functioning of the cell and also regulate specific functions like inter-cellular cargo transportation and macroscopic muscle contraction. These are self-propelled and composed of proteins and are of the size within the nanoscale. Hence, these are called biological nanomotors. These motors are highly efficient and its evident by their relentless performance throughout the lifecycle of a cell.  \n\nATP synthase or ATPase is a rotary motor and is responsible for catalytic synthesis of ATP molecules, accompanying rotary motion. Kinesins are motors that are responsible for transport of inter-cellular cargo and also the mitosis. Kinesins carry the cargo on their heads and walk on microtubules. The walking action is similar to that of the walking action of human beings. Kinesins have two leg like structures, and alternatively place each one at the front. The impressive performance of these motors stimulates the transition towards the artificial system.  \n\nThe combination of bottom-up, self-controllable assembly and the design inspired by the natural biological motors has made it possible for the development of self-propelled synthetic nanomotors. The improvements in nanotechnology permits realization of autonomously synthetic motors with engineering features. These assembled nanomotors are expected to be extremely useful in the future and hold potential promise for targeted drug delivery, biodetoxification, cleaning wounds, blood clots and parasites.  \n\nThe properties of the nanomotors can be conveniently variedby assembling different components such as polymer, nanoparticles, proteins, inorganic or organic functional molecules, vesicles etc. Therefore, controlled self-assembly of the motors enables achievement of functionalities like self-propulsion, regulated motion and controlled transportation by assembling corresponding components.  \n\nThe development of these synthetic, self-propelling nanomotors is one of the most exciting yet challenging fields in nanotechnology. Controlling the motion of the motors is one of the challenges. The solution to this is also inspired by the biological model of chemotaxis and phototaxis. Now, immense control on the motion of these motors is achieved. The capability and the sophistication of the motors is continuously growing but so are the challenges associated with them and their applications in the areas of biomedicines. One of the major problems being biodegradability and biocompatibility as these motors are mainly manufactured from metals. Not just that, these motors must be efficient enough to function in different biological media.  \n\nIn spite of these major challenges, with continuous innovation and development of controlled assembled self-propelling nanomotors will have a profound impact on future biomedical applications.",
        "author_name": "Sanath K.",
        "slug": "nanomotors"
    },
    {
        "layout": "post",
        "title": "Evolving Strings with Genetic Algorithms",
        "author_github": "gurupunskill",
        "date": "2018-06-19T16:44:03",
        "image": "/assets/img/",
        "description": "A short introduction to Neuroevolution",
        "tags": [
            "Machine Learning",
            "Genetic Algorithms",
            "String",
            "Genes",
            "Chromosome"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "gurupunskill",
        "comments": true,
        "text": "[{% load static %}]()\n## Introduction\n\nA genetic algorithm is a search heuristic inspired by Darwin\u2019s theory of natural evolution. A genetic algorithm reflects the process of natural selection: the fittest survive and breed.  \n\nThe process of natural selection begins by selecting the fittest individuals from a population. The cross to produce offsprings that have characterestics of both parents. If both parents have good fitness, then their child is also likely to inherit their characterestics and have better fitness.\n\nIn this introduction, we will be trying to match a string from scratch.  \n\n## Environment and Population\n\nFirst, we need to define an environment that would determine the fitness of an individual. Here, our environment would be the String that we're trying to create.  \n\nSo, a string S of some length with alphanumeric characters.  \n```\n\tString = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n```  \n   \nAny string of any length is an individual in this environment.  \n   \n```\n\tIndividual = \"LAKSKDJLKMCasdkjAKSDjkj\"\n```\nOur initial population would we filled with multiple such individuals with random letters and numbers.  \n\n## Fitness\n\nThe fitness of an individual is proportional to how close it is to the defined string S for this environment.  \n  \n\n```\n\tIndividual = \"ABCDEFGHIJKLMNOPQRSTUVWXYS\"  \n\tFitness(Individual) = (Number of characters in Individual same as String)  \n\t\t\t      / (Number of characters in String)  \n\t\t\t\t\t\t= 25/26\n\t\t\t\t\t\t= 0.962\n```\n\n\nThe python code below implements the fitness function we've defined.  \n\n  \n```python\t\n    def evaluate_fitness(actual, new):\n        together = itertools.zip_longest(actual, new, fillvalue=None)\n        cost = len([a for a,b in together if a == b])\n        cost /= max(len(actual), len(new))\n        return cost\n```\n  \n  \n### Breeding and Mutation\nAs per Natural Selection laws, the fittest individuals should survive (along with some random unfit undividuals) and breed to create the next generation of individuals.  \n\nGiven two parents, We can define breeding to select a character from the two parents at random. That is, We would choose the `i`th character of `Parent 1` or the `i`th character of `Parent 2` at random for the `i`th character of the `Child`.  \n\nTo simulate mutation, given a `0 < mutation rate < 1`  if a uniformly generated pseudorandom floating point number is greater than this defined `mutation rate`, then an alphanumeric character is randomly chosen to be in that position `i` for the child. We can also used the same `mutation rate` to randomly add or remove an extra character in the child.  \n\n```python\n    def make_child(parent_1, parent_2):\n        longer_parent  = list(parent_1 if len(parent_1) > len(parent_2) else parent_2)\n        shorter_parent = list(parent_2 if len(parent_1) > len(parent_2) else parent_1)\n    \n        child = longer_parent.copy()\n        n = len(shorter_parent)\n        i = 0\n        while i < n:\n            child[i] = random.choice(characters) if random.uniform(0, 1) > mutation_rate else child[i]\n            \n            if (i < len(shorter_parent)):\n                child[i] = shorter_parent[i] if random.uniform(0, 1) > 0.5 else longer_parent[i]\n            \n            if(len(child) < length_max):\n                if (random.uniform(0,1) > mutation_rate):\n                    child.insert(i, random.choice(characters))\n                    n+=1\n                \n            if (random.uniform(0,1) > mutation_rate):\n                child.remove(child[i])\n                n-=1\n            i+=1\n        return \"\".join(child)\n```\n  \n  \n### Wrapping it all up\n  \nAfter concretely defining our environment, to genrate a random population and choosing the best indiviuals and breeding them to create the next generation is trivial.  \n\nCheck out the full notebook [here](https://github.com/gurupunskill/string-matcher/)",
        "author_name": "Gurupungav Narayanan",
        "slug": "string-genetic"
    },
    {
        "layout": "post",
        "title": "Quantum Supremacy: Is it hyped?",
        "author_github": "ShashankP19",
        "date": "2018-06-23T10:00:00",
        "image": "/assets/img/",
        "description": "This article talks about advantages of a quantum computer and challenges faced in building them",
        "tags": [
            "Quantum Computing"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "ShashankP19",
        "text": "[{% load static %}]()\nThe term \u201cquantum computer\u201d has been buzzing around in recent years. But what is a quantum computer? Have they been built? How are they different from a classical computer? What kind of problems do they solve? Do I need to be an expert in physics to understand them?(Not really!).  These may be some of the questions arising in your mind when you hear someone say \"quantum computer\". In this article, I will address these questions by giving a basic understanding of quantum computer and what makes it so special.\n\n## Fundamentals\n\nAll the electronic devices that we use in our daily lives are classical. They work on streams of classical bits. These classical bits are represented mathematically in binary digits, 0 and 1. The basic building blocks of a quantum computer, however, is something known as a **qubit**. A classical bit is either 0 or it is 1, it cannot be both at the same time. But a qubit can be in states 0 and 1 simultaneously. This property of being in multiple states simultaneously is called **superposition**. \n\n![Difference between classical bit and qubit ([image credit](http://qoqms.phys.strath.ac.uk/research_qc.html))](/blog/assets/img/quantum-supremacy/qubit.png)\n\nIn a classical computer, a bit is represented physically as two distinct states of a system. It could be two distinct levels of voltage or current, two stable states of a flip-flop, two directions of magnetizations, etc. A qubit can also be represented physically. Qubit is not just a theoretical concept. Some of the physical realizations of a qubit are using **polarization of photon** and **electron spin**, there are many more ways. In fact each quantum computer that is being built today uses a different type of physical representation.\n\nThe process of finding out the value of a bit is called **measurement** of the bit. Classical measurement is very obvious, you simply find out the value of the bit. If the bit turns out to be 0 then you know that it is 0 for sure, if it is 1 then you know that it is 1 for sure. There is no problem with classical measurement. However, this is not the case with qubit measurement. Measuring a qubit forces the qubit to take the state of 0 or 1, but not both. Hence the **superposition is lost** once a qubit is measured. Upon measuring, the qubit is said to have \u201c**collapsed**\u201d to either state 0 or 1. If you measure this same qubit for the second time it gives the same result as first measurement. You can say that measurement process has converted the qubit to a classical bit. There is something even more interesting. A qubit in superpostion has some probabilities of being in state 0 or 1. When this qubit is measured it collapses to 0 or 1 based on these probabilities. The only way of finding these probabilities is to measure infinite copies of the same qubit and counting how many times it gives 0 and 1.   To better understand measurement, think about a coin toss as analogy. When the coin is in air, it is in superposition of two states \u2013 head and tail. Probability of getting head or tail depends on the inherent property of the coin. Here,  a coin is said to be \u201cmeasured\u201d when it falls flat and shows either head or tail. \u201cSuperposition\u201d of the coin is lost when it is \u201cmeasured\u201d.\n\n![Measurement of qubit ([image credit](https://www.clerro.com/mobile/guide/580/quantum-computing-explained))](/blog/assets/img/quantum-supremacy/measurement.png)\n\n## What makes it special\n\n#### Parallelism:\nLets go beyond a single qubit to multiple qubits. If there are two qubits then there are four states (00, 01, 10, 11) in superposition. So a two qubit state can represent all possible 2-bit states (which are 4 in number). A 100 qubit state can represent all possible 100-bit states, which is 2<sup>100</sup> states, at once. That is a very very very huge number! A quantum computer can parallelly compute all these states. It is this power of parallel computation that greatly sets it apart from a classical computer. Parallel computation in classical computers requires that number of hardware components be in the order of (N). While for quantum computers, the number of qubits required is of order (log(N)). \n\nThis property of parallelism can be used to solve problems that classical computers cannot solve. For example, the problem of finding factors of an integer is a hard problem. Currently, there exists no classical algorithm that can solve the factorization problem in polynomial time.  1994, a mathematician named Peter Shor devised a **quantum algorithm** to solve the factorization problem in polynomial time (polynomial in log N, which is the size of the input). This algorithm is called **Shor\u2019s algorithm**.  Another problem is the search problem \u2013 search for an element among N elements. The best classical algorithm has complexity - order of N. However, there exists a quantum algorithm named **Grover\u2019s algorithm** which can solve the search problem in order of <span>&#8730;</span>N. \n\n#### Cryptography\n Quantum computers play an important in cryptography. The security of existing cryptographic algorithms depend on the hardness of problems like integer factorization and discrete-log problem. But quantum computers can solve these problems in polynomial time and hence can break systems like RSA encryption, Diffie-Hellman key exchange protocol easily. This is scary! However, on the brighter side, quantum computers open up a new area of cryptography where private keys can be exchanged with **perfect secrecy**. Using the property of superposition, it is easy to generate **truely random keys** and it is also possible to send these keys across a public channel with perfect secrecy. Perfect secrecy is the ultimate goal of cryptography which can be fulfilled by a quantum computer.\n\n#### Simulations \n The most important advantage of a quantum computer is that it can be used to **simulate quantum mechanical experiments**. No classical computer can simulate these experiments with 100% accuracy. Remember, the whole world is quantum, not classical! Hence quantum computers will help in the progress of scientific research and greater understanding of our universe.\n\n## Challenges \n\nBuilding a quantum computer is certainly not easy. The field of quantum computation is still in its infancy, just like how classical computers were a few decades ago. There are many challenges to overcome before one can build a **true quantum computer** that can be made available for general use. Quantum computers are extremely **sensitive to the environment**. This is because interaction with the environment causes the state of qubits to collapse. Isolating quantum computers is a very difficult engineering problem. Just like classical gates, there are **quantum gates**. The implementation of these quantum gates physically are likely to cause **errors**. The registers storing qubits may have already been affected by the environment before the computation even begins.  Also greater the number of qubits in quantum computer, greater is the \u201c**noise**\u201d present in it, which leads to incorrect computations.\n\nWe take copying data for granted in classical computer. However, in quantum computation copying unknown data is not possible. In fact there is a theorem to prove it called **no-cloning theorem**. So any specific unknown state cannot be determined. This means that unless the state is specifically created, we have limited ability to control it.\n\n## Race for supremacy\n\nHumans have never given up when it comes to overcoming challenges. This applies to quantum computation as well.  Extremely great computation power to solve unsolved problems is the driving motivation for ongoing research and engineering in this field. Hence there is lot of competition among companies to build a quantum computer that surpasses the computation capabilities of classical computers.  This potential ability of a quantum computer to solve problems that classical computers cannot solve is called **quantum supremacy**. The term \u201cquantum supremacy\u201d was coined by John Preskill, a theoretical physicist at the California Institute of Technology. \n\nCompanies like IBM, D-Wave, Google, Intel, Microsoft are the major players in this field now. D-Wave is the first company to sell quantum computers commercially. In January 2017, D-Wave released a 2048-bit quantum computer named **D-Wave 2000Q**. But this is not a true quantum computer. \"The Holy Grail of quantum computing to build a **'universal' quantum computer**\u2014one that can solve any computational problem\u2014but at a vastly higher speed that today's computers,\" Hilton told EE Times. \"That's the reason some people say we don't have a 'real' quantum computer\u2014because D-Wave's is not a 'universal' computer.\"  \n\n![Google's Bristlecone ([image credit](https://ai.googleblog.com/2018/03/a-preview-of-bristlecone-googles-new.html))](/blog/assets/img/quantum-supremacy/bristlecone.png)\n\nGoogle has been claiming recently that it would achieve quantum supremacy in the near future. Google\u2019s latest quantum processing chip with 72 qubits, called **Bristlecone**, has been predicted to achieve quantum supremacy. There are still a lot of tests to be conducted though, to claim this achievement. IBM has also been investing a lot in building a universal quantum computer and boasts of a 50 qubit processor. IBM has created an online platform, named IBM Q Experience, which allows access to their [5-bit quantum computer for free](https://quantumexperience.ng.bluemix.net/qx/experience) and 16-qubit computer for some cost. They have good documentation to help you get started with using them.      \n\nBut should quantum supremacy be the main objective of building a quantum computer. Does the concept of quantum supremacy make sense? \n\u201cYou\u2019ll struggle to find any [researcher] who likes the term \u2018quantum supremacy,\u2019\u201d says Simon Benjamin, a quantum expert at Oxford University. \u201cIt\u2019s very catchy, but it\u2019s a bit confusing and oversells what quantum computers will be able to do.\u201d \n\u201cI don\u2019t believe that quantum supremacy represents a magical milestone that we will reach and declare victory,\u201d says theorist Jay Gambetta at IBM. \u201cI see these \u2018supremacy\u2019 experiments more as a set of benchmarking experiments to help develop quantum devices.\u201d\nSome researchers have suggested that there should be focus on \u201c**quantum advantage**\u201d instead. Quantum advantage refers to the specific tasks that only quantum computers can do.  Other terms used for this are \u201c**quantum ascendancy**\u201d and \u201c**quantum inimitability**\u201d. \nEven John Preskill, who coined the term \u201cquantum supremacy\u201d published a paper in January 2018 published a paper in which he said quantum computing was about to enter a phase he called NISQ, or \u201cnoisy intermediate stage quantum,\u201d where machines will have 50 to a few hundred qubits. \u201c\u2018Noisy,\u2019\u201d he wrote, \u201cmeans that we\u2019ll have imperfect control over those qubits; the noise will place serious limitations on what quantum devices can achieve in the near term.\u201d Preskill said he\u2019s still convinced quantum computers will have a transformative effect on society, but that transformation, he concedes, \u201cmay still be decades away.\u201d\n\n## Resources\n\n* [Use IBM's 5-qubit processor for free](https://quantumexperience.ng.bluemix.net/qx/experience)\n\n* [Why D-Wave quantum computers have not yet brought quantum supremacy?!](https://www.linkedin.com/pulse/why-d-wave-quantum-computers-have-yet-brought-supremacy-kaiser)\n\n* [Quantum Computing Is Going to Change the World. Here\u2019s What This Means for You](https://futurism.com/quantum-computing-qa/)",
        "author_name": "Shashank P",
        "slug": "quantum-supremacy"
    },
    {
        "layout": "post",
        "title": "BIO(logy and electro)NICS",
        "author_github": "col-aatresh",
        "date": "2018-09-08T19:16:00",
        "image": "/assets/img/",
        "tags": [
            "Biology",
            "Electronics",
            "Bionics"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "col-aatresh",
        "text": "[{% load static %}]()\nThe living cell can be considered as an extraordinary piece of engineering, where each of the components of the cell work in precise coordination with utmost efficiency. Each cell generates its own energy which it uses to carry out specific tasks, and in the process, produces waste products which are removed from the cell. We are yet to understand the working of DNA and RNA, which are chains of particular bio molecules arranged in a certain order. This preprogrammed nature of life that allows for replication of molecular strands is what grabs the attention of many scientists today, and its true working is yet to be understood.\n\n![Cell](http://www.nature.com/scitable/content/ne0000/ne0000/ne0000/ne0000/14704902/U1CP1-5_ProkvsEukCell_ksm.jpg \"Cell\")\n\nOver millions of years, human ingenuity and creativity has led to the invention of numerous devices that have changed the way we live today. The evolution of man made technology has reached a parallel to the evolution of cells, and a stage  has come where their convergence can mutually benefit each other. This is where bionics comes into play.\n\nBionics is a field of engineering, where mechanical and electronic systems are integrated with biological systems in order to achieve a particular biological advantage. You can think of it as a machine and an organism working together, so that this machine is beneficial to the organism in some way. This field is very rich, and has a lot of scope and application in our world.\nOne such example, is where it can be used to help mentally and physically challenged people by fixing their biological problems using electronic and mechanical means. \n\n![Bionic Hand](https://img.purch.com/w/553/aHR0cDovL3d3dy5saXZlc2NpZW5jZS5jb20vaW1hZ2VzL2kvMDAwLzAwNy8wODEvb3JpZ2luYWwvMDgwNDIyLXByb3N0aGV0aWMtaGFuZC0wMi5qcGc= \"bionic hand\")\n\nBionics has reached a pretty impressive stage today, where amputees are fitted with bionic prosthetic limbs, and those with certain brain diseases are fitted with brain implants. These are complex systems that mimic a living organ and appear to be one from the brain\u2019s perspective. They allow for basic limb movements, and newer advances have even given a sensation of touch to it. This will involve mechanical systems such as special joints and structures, motors to mimic muscle actions and electronic systems that are used to establish a communication link between the prosthetic and the brain, and allow it to be controlled and coordinated. The device will receive signals from the brain, which will biologically travel through a network of nerves as electrical impulses, and upon reaching the target site, special decoders are used to decode the information stored in the impules and then carry out the desired task.\n\n![Brain Implant](https://img.purch.com/w/553/aHR0cDovL3d3dy5saXZlc2NpZW5jZS5jb20vaW1hZ2VzL2kvMDAwLzAwNi84MzUvb3JpZ2luYWwvYnJhaW4tcmVnaW9ucy0xMDA0MTQtMDIuanBn \"brain implant\")\n\nDevelopments and study in neuroscience have led to collaborative work between doctors and engineers, wherein problems related to memory loss and organ control have been tackled using special bionic brain implants. A notable example of such an implant is a computer chip that can mimic the working of the hippocampus, which is a part of the brain that deals with short term memory. This is especially useful for people suffering from Alzheimer's disease where the patient has short term memory loss.\nFaulty organs can be rectified using bionics, and one of the best examples of such an invention is the pacemaker. Artificial bionic kidneys and pancreas have been created and are currently under clinical trials.\n\nA very important branch of bionics on the nano-scale are nanobots. These are robots whose size ranges between **100nm** and **1nm**. Nanobots can be considered as machine versions of bacteria and viruses, which help in bringing about changes at the atomic and molecular level and in carrying out breakthroughs in medical science. They are built to work in harsh chemical environments and are usually powered by the chemicals in which they exists or by an on-board battery.\nIt is said that Nanobots will bring in a new age of medical diagnosis and treatment, allowing doctors to carry out medical procedures on a cellular level which has much better efficiency and effectiveness. They can be used for applications such as closing open wounds, causing blood clots, rebuilding ruptured arteries and veins, and traversing through the body for diagnosis of a particular disease.\n\nIt must be noted that a cell is an extremely stable unit of life, and any modification of it must be done carefully. Unmonitored use of technology in bionics has the potential of causing more harm than good. However, if allowed to grow on the right path, this field has solutions to majority of the problems that life suffers from today.",
        "author_name": "Anirudh Ashok Aatresh",
        "description": "",
        "slug": "biology-and-electronics"
    },
    {
        "layout": "post",
        "title": "Proof-of-work as a consensus mechanism in blockchains",
        "author_github": "NJnisarg",
        "date": "2018-09-08T10:00:00",
        "image": "/assets/img",
        "description": "This blogpost is about the major and one of the most ground-breaking consensus mechanisms called \"proof of work\" adopted in the earliest blockchain.",
        "tags": [
            "Blockchain, Consensus, Distributed systems"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "NJnisarg",
        "text": "[{% load static %}]()\n*Disclaimer: This article is not an introduction to blockchain, it does try to cover some basics of blockchain, but it\u2019s focus is more on the proof-of-work as an approach to solving consensus problem.*\n\n## Terminologies and breakdown\n\nThe title of the article contains \u201cconsensus mechanisms in blockchain\u201d, which sounds like a title with heavy words. So let's try to first breakdown this title and understand each term separately.\n- **Consensus** : \u201cConsensus\u201d in simple terms means reaching an agreement amongst a set of parties about a specific thing like an event or a data value. Consensus is collaborative and cooperative in nature by the parties seeking agreement. So one word mapping, *consensus = global agreement*\n- A **Consensus mechanism** is an algorithm or approach through which a multi-party system can reach consensus/*agreement*.\n- **Blockchain** : Now coming to the system under discussion today which is blockchain. Blockchain has really been in a huge buzz for a long time and mostly because of its application in the implementation of the bitcoin network which as its creators call it \u201cA peer to peer electronic cash network\u201d. Blockchain as a piece of tech is being studied standalone now in a neutral manner to leverage its strengths that have been used in the bitcoin network. In simple terms blockchain is kind of chain of blocks or a ledger(of blocks) that can keep a record of transactions of any sort, whether it is financial transactions( bitcoin network being the example) or transactions about ownership of assets, documents or even storing the state of computer code(used in \u201cprogrammable\u201d blockchains like ethereum). This ledger allows us to keep a history of all the transactions that were carried out. In simple terms a blockchain = A ledger/chain of blocks that represent a set of transactions/events and thus record a history of all transactions.\n\n## Distributed nature of blockchain\nNow let\u2019s take a step back, if this ledger were just any ledger that a person/computer system can keep a \u201clocal\u201d copy of, then it is not very interesting, the real use of this ledger comes when it becomes \u201cdistributed\u201d in nature, i.e, all the parties involved in recording the history of transactions/events participate in maintaining a shared global copy of the ledger that they all should agree to. Again when we say a global copy, there is no one copy uploaded on a centralized server to which each person reads and writes, but rather each party in the network holds one local copy that they try to keep in sync with the rest of the network and if majority of people have the same copy of ledger then we consider that most people are in \u201cconsensus\u201d with the history of transactions/events or in other words most people have a common \u201cglobal view\u201d of the ledger. Maybe for now this seems a simple task, let\u2019s explore the workings of such a distributed ledger system and what sort of problems it can encounter trying to reach the consensus about the state of ledger.\n\n## Byzantine's Generals and Consensus problem\n\n![Byzantine's problem]({% static '/blog/assets/img/POW-consensus-blockchain/byzantine_problem.png' %})\n\nLet\u2019s first setup a very simplified version of a problem called \u201cByzantine\u2019s Generals Problem\u201d. This is a classic problem modeled in computer networks and distributed systems. The problem is as follows:\n- There is a group of generals each commanding a portion of Byzantine\u2019s army, encircling a city they are planning to attack. In simplest form the generals must decide upon either to \u201cattack\u201d or \u201cretreat\u201d.\n- The generals are separated by sizeable distance and hence can communicate only via some messengers.\n- Some of the generals have to send message to other generals about their view on \u201cattack\u201d or \u201cretreat\u201d and then all of them in the end have to agree somehow to either attack or retreat.\n- The attack can succeed only if a majority of generals attack at the same time. Else it has high chances of failure. Hence it is crucial to attain agreement on the issue.\n- This sets up a series of problem. This situation has an issue of trust and consensus. First problem,whatever information the messenger has given is it untampered or not. Second, also after receiving the information, whatever decision a general has made is it in agreement with all other or at least a majority of generals or not. How to know what is the global view of all the generals about attacking the city. The problem can be made complicated by having traitors in the system.\n\n\nThe above problem can be modelled in the context of blockchain:\n- The generals map to the nodes in the blockchain\u2019s network\n- The messengers map to the network of communication\n- The problem of agreeing to an attack strategy maps to the problem of having a consensus on the global state of transactions of the network.\n\nThis problem can be solved in blockchain through various consensus mechanism of which we will focus on something called \u201cproof-of-work\u201d.\nThis consensus mechanism was first outlined in a paper by Satoshi Nakamoto(who is still not known) and in a list of emails in his conversations with his peers.\nWe will first look at what is involved in proof-of-work and then how it solves the consensus problem.\n\n## Proof-of-work: The consensus machine behind blockchain\n- This is the meat of the article, so *buckle* up.\n- In a blockchain network, any transaction/event is broadcasted to all the nodes of the network.\n- The nodes take up these transactions. They form a block of data containing the transaction details, the hash of the previous block(parent) and have to add something called \u201cnonce\u201d and generate a hash value.\n- The difficult part is the hash value to be generated has to follow a *certain pattern.* In case of bitcoin blockchain, the hash should have specific number of leading zeroes in the value. More the number of leading zeroes more intensive the task is and less the zeroes less intensive it is. Thus they can adjust nonce value to find this hash.\n- The way to look for the right nonce is simply brute force which is a very computationally intensive task\n- A set of nodes that received the transaction start \u201cmining\u201d a block whose hash value will be accepted by the blockchain.\n- More the compute power higher is the chance of getting the work done. Hence if nodes form a pool and a majority of nodes start working on a block, they will get the required hash first. This shows that majority of people (sort of) stand behind that particular transaction and \u201cagree\u201d on it. Here we start seeing the beginnings of reaching agreement/consensus on the event that has occured.\n- But if multiple transcations take place at same time, any node can receive any transaction. The key here is whichever transcation is received by \u201cmajority\u201d of nodes has a higher chance of forming a valid block first and is being agreed upon by a majority of nodes.\n- Thus in blockchain network, the nodes show their vote/agreement via the compute power. The more the compute power behind an event/transcation, more nodes agree on it.\n- But the next question is how is the local copy of the ledger that is formed by nodes update to be in sync with what the other nodes have.\n- Whenever a block is successfully mined, it is broadcasted to the network. The nodes who are honest have to accept this mined node after verifiying it and add it to their local copy. Thus this synchronizes the local ledger copies. Also if multiple blocks are broadcasted then the blocks that form a \u201clarger chain\u201d are to be accepted. This is also called the longest chain rule. The larger chain bascially shows that it has more compute power backing its formation, thus majority agreeing to it.\n- Now if we consider a set faulty nodes who are trying to introduce false blocks, then they need more compute power than rest of the network to form a longer chain by racing against the network. If majority of nodes are honest this is not possible.\n- Also if a faulty node tries to change the transcations before, they have to \u201cre-mine\u201d the blocks ahead since they are cryptographically linked by the hashes of previous blocks and the cost of re-mining is very large, it is next to impossible to change the order of events or even the events itself.\n- Thus blockchain network works by reaching a consensus on the set of events/transactions and the order of those events as well by the nodes casting their votes in terms of compute power. Here the nodes need not trust each other to agree, but just follow the rules of work and they can automatically reach a consensus.\n\n## Conclusion and Ending remarks\nThus proof of work is a consensus mechanism where the nodes vote to back a transaction/record in the ledger by providing their compute power as a vote. The more the votes or \u201ccompute power\u201d behind a particular copy of blockchain/ledger, means more parties \u201cagree\u201d on that particular ledger and history of transactions. Thus all the parties can always agree and trust the other nodes by possessing the longest proof-of-work chain and can know for sure that their copy is in sync with the common global view of the ledger.\n\nThe proof-of-work chain is how different synchronization, distributed database\nand global view problems can be solved.\n\nBlockchains can have other consensus mechanisms as well, like Proof-of-stake, Federated-Byzantine-Fault-Tolerance, Proof-of-activity and so on. But proof-of-work is the earliest of the mechanisms trying to solve the Byzantine Agreement and develop a Byzantine fault tolerant system. It certainly has drawbacks but is one of the most robust mechanisms and many mechanisms still draw the concepts from proof-of-work.\n## Proof of work outlined by Satoshi Nakamoto\n> The proof-of-work chain is a solution to the Byzantine Generals' Problem.  I'll \n> try to rephrase it in that context.\n>\n> A number of Byzantine Generals each have a computer and want to attack the \n> King's wi-fi by brute forcing the password, which they've learned is a certain \n> number of characters in length.  Once they stimulate the network to generate a \n> packet, they must crack the password within a limited time to break in and \n> erase the logs, otherwise they will be discovered and get in trouble.  They \n> only have enough CPU power to crack it fast enough if a majority of them attack \n> at the same time.\n>\n> They don't particularly care when the attack will be, just that they all agree. \n> It has been decided that anyone who feels like it will announce a time, and \n> whatever time is heard first will be the official attack time.  The problem is \n> that the network is not instantaneous, and if two generals announce different \n> attack times at close to the same time, some may hear one first and others hear \n> the other first.\n>\n> They use a proof-of-work chain to solve the problem.  Once each general \n> receives whatever attack time he hears first, he sets his computer to solve an \n> extremely difficult proof-of-work problem that includes the attack time in its \n> hash.  The proof-of-work is so difficult, it's expected to take 10 minutes of \n> them all working at once before one of them finds a solution.  Once one of the \n> generals finds a proof-of-work, he broadcasts it to the network, and everyone \n> changes their current proof-of-work computation to include that proof-of-work \n> in the hash they're working on.  If anyone was working on a different attack \n> time, they switch to this one, because its proof-of-work chain is now longer.\n>\n> After two hours, one attack time should be hashed by a chain of 12 \n> proofs-of-work.  Every general, just by verifying the difficulty of the \n> proof-of-work chain, can estimate how much parallel CPU power per hour was \n> expended on it and see that it must have required the majority of the computers \n> to produce that much proof-of-work in the allotted time.  They had to all have \n> seen it because the proof-of-work is proof that they worked on it.  If the CPU \n> power exhibited by the proof-of-work chain is sufficient to crack the password, \n> they can safely attack at the agreed time.\n>\n> The proof-of-work chain is how all the synchronisation, distributed database\n\n## Interesting links and Furthur Reading\nHere is the link to the mails that very exchanged between \u201csatoshi nakamoto\u201d and his peers discussing the idea of \u201cbitcoin and blockchain\u201d:\n- [Mail exchanges about bitcoin and blockchain](https://www.mail-archive.com/cryptography@metzdowd.com/msg09959.html)\n\nHere is the link to the original paper published:\n- [Bitcoin Paper](https://bitcoin.org/bitcoin.pdf)",
        "author_name": "Nisarg Joshi",
        "slug": "pow-consensus-blockchain"
    },
    {
        "layout": "post",
        "title": "THE NEW BIOMETRIC - BRAINPRINT",
        "author_github": "pjk99",
        "date": "2018-09-08T18:34:00",
        "image": "/assets/img",
        "tags": [
            "biometrics",
            "brainprint"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "pjk99",
        "text": "[{% load static %}]()\nSince the conception of computers, mobile phones, tablets and most importantly, Internet, securing our devices and data has become crucial. Personal data ranging from shopping wishlists to bank details are either stored in PCs or are available online, all protected by passwords - a rather impotent sentinel, as it turns out. There have been several methods used by computers as well as the different websites to authenticate its user.\n\nThe first passwords were simple and easily stored since sophisticated hacking networks did not exist yet. One such example is, Cryptographer Robert Morris, who created the infamous Morris worm, developed a one-way encryption function for his UNIX operating system, known as \"hashing\", which translated a password into a numerical value. The actual password was therefore not stored in the computer system, making the information less readily accessible to hackers. Recently, the two-step authentication has brought in development by adding an extra layer of protection by requiring verification from two different sources. But this too is not foolproof, but a challenge to intrepid hackers. Therefore, a more promising solution can be multi-factor authentication which demands more than one authentication method, one of which can be biometric authentication like fingerprint, facial recognition, etc. But the multi-factor authentication calls for extra efforts from the user.\n\nRecently, researchers at Binghamton University have discovered that, with a bit of training, your computer can identify you based on the way your brain reacts to certain words. This could be another type of biometric authentication. This means that instead of a password you could simply listen to a few words and unlock your secret files. The idea is for a person to authenticate their identity with electroencephalogram (EEG) readings. You simply have to sit down with the brain scanner on. The user's response is measured via an EEG headset. EEG signatures are unique and are more complex than a standard password, making them difficult to hack. For this, it is required to make an algorithm which can be educated about the user's reaction for particular character. This can be made by giving the user to type a few characters and learning and registering the responses of the user.\n\nResearchers at the University of Washington have demonstrated another way to extract private information using an EEG headset. They created games that subliminally flashed up images such as bank logos and noted when a person's brain waves registered recognition.\n\n> If someone's fingerprint is stolen, that person can't just grow a new finger to replace the compromised fingerprint \u2014 the fingerprint for that person is compromised forever. Fingerprints are 'non-cancellable.' Brainprints, on the other hand, are potentially cancellable. So, in the unlikely event that attackers were actually able to steal a brainprint from an authorized user, the authorized user could then 'reset' their brainprint, \n- said Sarah Laszlo, an assistant professor of psychology and linguistics involved in the project.\n\nAlthough this method is accurate upto 94% as of now, it still has confounding fallacy like, the effect of drugs(opioids, caffeine and alcohol) on the neural systems, and hence, the brain waves. This manipulation of brain waves due to the external factors comes out as a challenge. John Chuang at the University of California, Berkeley, last year published research into the impact of exercise on EEG authentication and found that accuracy degrades immediately after a workout (though it quickly recovered). He suggests that other factors such as hunger, stress or fatigue could also reduce reliability. To overcome this, if accuracy under different conditions were required, it could be possible to collect multiple brainwave \"templates\" for a user by separately mapping their EEG signature when drunk, tired and so on. It is possible to tweak the EEG data analysis using machine learning to improve the results for participants who were inebriated, as suggested by another researcher Tommy Chin and Peter Muller, a graduate student.\n\nNevertheless, given that it is difficult to copy another person's exact thought process, the technology is certainly advantageous. Considering the advancement in the technology, we will likely see uptake of biometric applications based on brainprints soon \u2013 especially as part of multi-factor system for enhanced authentication.",
        "author_name": "Preeti J Kalloli",
        "description": "",
        "slug": "the-new-biometric-brainprint"
    },
    {
        "layout": "post",
        "title": "Cloud Computing",
        "author_github": "SaurabhAgarwala",
        "date": "2018-09-15T23:30:33",
        "image": "/assets/img/",
        "description": "What is cloud computing? Why is it so popular? Why is it crucial to the future of our societies?",
        "tags": [
            "Cloud Computing",
            "AWS"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "SaurabhAgarwala",
        "comments": true,
        "text": "[{% load static %}]()\n> You are probably using cloud computing right now, even if you don\u2019t realise it. If you use an online service to send an email, edit documents, watch movies or TV, listen to music, play games or store pictures and other files, it is likely that cloud computing is making it all possible behind the scenes. \n\nThe first cloud computing services are barely a decade old, but already a variety of organisations, from tiny start-ups to global corporations, government agencies to non-profits, are embracing the technology for all sorts of reasons.\n\n## What is Cloud Computing?\n\n![Cloud Comp]({% static '/blog/assets/img/cloud-computing/Cloud_Comp.png' %})\n\nCloud computing is the on-demand delivery of compute power, database storage, applications, and other IT resources through a cloud services platform. It is made available to the user via the Internet with pay-as-you-go pricing. Whether the users are running applications that share photos to millions of mobile users or they are supporting the critical operations of their business, a cloud services platform provides rapid access to flexible and low-cost IT resources which is available in just a few seconds. This allows enterprises, start-ups, small and medium-sized businesses, and customers in the public sector to access the building blocks they need to respond quickly to changing business requirements.\n\nWith cloud computing, users don\u2019t need to make large upfront investments in hardware and spend a lot of time on the heavy lifting of managing that hardware. Instead, with cloud computing they can provision exactly the right type and size of computing resources they need to power their newest bright idea or operate their organization\u2019s IT department. This means the users don't need to buy their own hardware, instead, the Cloud Computing Service provider will buy the network-connected hardware required for the user\u2019s application, and maintain it for them. The users can access as many resources as they need, almost instantly, and only pay for what they use.\n\n### How Cloud Computing Works? \n\nCloud computing services all work a little differently, depending on the provider. But many provide a friendly, browser-based dashboard that makes it easier for IT professionals and developers to order resources and manage their accounts\n\n## Advantages of Cloud Computing:\n\n**Replacing up-front capital infrastructure expense for variable expense:**\n\nInstead of having to invest heavily in data centres and servers before a user know how they are going to use them, they can pay only when they consume computing resources, and pay only for how much they consume. Hence, with the cloud, businesses no longer need to plan for and procure servers and other IT infrastructure weeks or months in advance. Instead, they can instantly spin up hundreds or thousands of servers in minutes and deliver results faster.\n\n**Benefit from massive economies of scale:**\n\nBy using cloud computing, users can achieve a lower variable cost than what they can get on their own. As usage from hundreds of thousands of customers is aggregated in the cloud, service providers can achieve higher economies of scale, which translates into lower pay as-you-go prices for the users.\n\n**Eliminates the process of guessing about capacity required:**\n\nCloud computing eliminates guessing on the infrastructure capacity needs. When a user makes a capacity decision prior to deploying an application, they often end up either sitting on expensive idle resources or dealing with limited capacity. With cloud computing, these problems go away. They can access as much or as little capacity as they need, and scale up and down as required with only a few minutes\u2019 notice.\n\n**Increased speed and agility:**\n\nIn a cloud computing environment, new IT resources are only a click away, which means that organizations can reduce the time required to make those resources available to their developers from weeks to just minutes. This result in a dramatic increase in agility for the organization, since the cost and time it takes to experiment and develop is significantly lower.\n\n**Eliminates workload of running and maintaining data centers:**\n\nCloud Computing lets organizations focus on projects that differentiate their business, and not on the infrastructure. Cloud computing lets them focus on their own customers, rather than on the heavy lifting of racking, stacking, and powering servers.\n\n**Allows going global in minutes:**\n\nOrganizations can easily deploy their application in multiple regions around the world with just a few clicks. This means they can provide lower latency and a better experience for the customers at minimal cost.\n\n![Cloud Computing]({% static '/blog/assets/img/cloud-computing/Cloud_Computing.jpg' %})\n\n\n## Categories of Cloud Computing:\n\nNow all this sounds like a very broad domain, so cloud computing can be essentially divided into 3 categories:\n\n**1. Infrastructure as a Service (IaaS):** Here the users are provided access to the basic hardware and they are free to customize it to their use case. For ex, they are provided access to the machine and they can choose whatever Operating System they would like and configure it\u2019s every system related detail.\n\n**2. Platform as a Service (PaaS):** This removes the need for the user to manage the underlying infrastructure (usually hardware and operating systems) and allows them to focus only on the deployment and management of their applications.\n\n**3. Software as a Service (SaaS):** With a SaaS offering, the user need not have to think about how the service is maintained or how the underlying infrastructure is managed; they only need to think about how they will use that particular piece of software. Hence, this provides them with a completed product that is run and managed by the service provider. Eg: Gmail, Youtube.\n\n> These three types of services are sometimes called the cloud computing stack, because they build on top of one another. Knowing what they are and how they are different makes it easier for an organization to accomplish their business goals.\n\n## Types of Cloud Deployment:\n\nNot all clouds are the same. There are three different ways to deploy cloud computing resources:\n\n**Public Cloud:** With a public cloud, all hardware, software and other supporting infrastructure is owned and managed by the cloud provider. The users access these services and manage their account using a web browser.\n\n**Private Cloud:** A private cloud refers to cloud computing resources used exclusively by a single business or organisation in which the services and infrastructure are maintained on a private network. A private cloud can be physically located on the company\u2019s on-site datacenter.\n\n**Hybrid Cloud:** Hybrid clouds combine public and private clouds, bound together by technology that allows data and applications to be shared between them. By allowing data and applications to move between private and public clouds, hybrid cloud gives businesses greater flexibility and more deployment options.\n\n## Why is Cloud Computing the Future?\n\nAlthough the cloud is far from a new idea, its true capabilities are only now beginning to be realized. It is predicted that Cloud will provide the digital infrastructure of tomorrow\u2019s cities, where an estimated\u00a06 billion of the world\u2019s population will live by 2045. The\u00a0cloud\u00a0will support emerging technologies such as artificial intelligence and help them to adapt to new platforms such as mobile. Therefore, it can be well said that Cloud Computing will be one of the technological shift into which the world will leap towards in the near future.\n\n### Resources:\n\n1. [Overview of AWS](https://d1.awsstatic.com/whitepapers/aws-overview.pdf)\n\n2. [Cloud Computing: A Beginner's Guide](https://azure.microsoft.com/en-in/overview/what-is-cloud-computing/)",
        "author_name": "Saurabh Agarwala",
        "slug": "cloud-computing"
    },
    {
        "layout": "post",
        "title": "Nanoparticle-Wonder Liquid",
        "author_github": "kulcod",
        "date": "2018-09-15T23:30:33",
        "image": "/assets/img/",
        "description": "Exploring the potential of a nanoparticle-wonder liquid.",
        "tags": [
            "nanotechnology",
            "nanomedicine",
            "magnetoserum",
            "panacea"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "kulcod",
        "comments": true,
        "text": "[{% load static %}]()\n## Magneto Serum - A potential panacea\n\nThough the name sounds very cool and straight out of a science-fiction franchise , what I'm referring to is actually called a ferro-fluid , but make no mistake , this liquid's abilities are very near sci-fi.\n\nA ferro-fluid is basically a colloidal sol , made up of 3 components which are, nano-sized particles of a ferromagnetic substance like haematite or magnetite, a liquid dispersion medium and a surfactant ( citric acid for example) which prevents the nanoparticles from clumping together and depositing .\n\nWhat's special about this fluid is that it gets strongly magnetized in the presence of a magnetic field , because of the ferromagnetic nanoparticles of course. This liquid can be moved around without using a fluid pump of any sort but by a magnetic field alone.\n\n![ferro-fluid](https://upload.wikimedia.org/wikipedia/commons/2/21/Ferrofluid_Magnet_under_glass_edit.jpg \"ferro-fluid\")\n\n_Ferro-fluid in presence of a magnetic field_\n\n![lines of force](https://xmdemo.files.wordpress.com/2014/03/0magnetfeld_eines_stabmagneten_006.jpg \"lines of force\")\n\n_The magnetic lines of force around a bar magnet_\n\n_As you can see , the shape sustained by the ferro-fluid is due to the nanoparticles aligning themselves with the magnetic lines of force and due to surface tension of the liquid(dispersion medium)._\n\n\nIt was first developed by **Steve Papell** of NASA in 1963 with a goal of transporting liquid fuel in zero gravity and vacuum using magnets , though it wasn't realized due to some practical limitations, ferro-fluids are very efficient in some applications in the present, like they're being used as a replacement for dampers in speakers making them energy efficient and able to produce better sound quality. They're also being used for reducing friction in certain mechanical arrangements, also this liquid maybe used to harvest energy from vibrations more efficiently than the solids currently being used, however the most ambitious and attention grabbing application of this weird looking thing is in medicine.\n\n## **Drug Pilot**\n\nFerro fluids can be driven by magnetic fields , it is this property which makes it possible to use it as a carrier of a drug to administer it to a specific target organ and thereby reduces the chances of the drug acting on non target organs which might cause some side effects.\n\n## **Cure for cancer ?**\n\nThis is the coolest and most ambitious idea of all regarding ferro fluids . The current cancer treatment methods have a drawback that there is a chance of the tumor growing back once it's gone ,because often only the cells on the outer side of the tumor are eliminated and not the ones in its core , whereas in the case of ferro fluids as the particles are tiny enough to pass in and out of the cells' membranes and as the fluid's movement can be driven by a magnetic field , the idea as suggested by **Thomas Webster** , Director of Nanomedicine laboratory Northwestern University , is to get this liquid into the targeted cells at the core of the tumor by appropriately applying a magnetic field and heating it up by doing the same, till a certain temperature at which the cancerous cell dies,  also at that temperature a normal cell endures no damage ( which is around 42-430 celcius) .\n\nResearch is being done on this and it has been only used on mice until now. This might be yet another example of how nanoscience is bringing about a revolution in modern technology.\n\nThis creepy looking material maybe having marvelous potential , but watching it is surely a feast for the eyes :)\n\n![magnetoserum](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQpLSWrBMl2_JKdWmj4mAYUr-pZvyfCZS_eMtiO-qKLNm_wBPtN)\n\nCheck it out yourself :\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/5APHa7vscoI\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>\n\nReferences :\n\n1. [https://en.wikipedia.org/wiki/Ferrofluid#Mechanical_engineering](https://en.wikipedia.org/wiki/Ferrofluid#Mechanical_engineering)\n2. [http://sustainable-nano.com/2016/08/30/magnetic-nanoparticles-cancer-treatment/](http://sustainable-nano.com/2016/08/30/magnetic-nanoparticles-cancer-treatment/)",
        "author_name": "Rohit Manohar Kulkarni",
        "slug": "nanoparticle-wonder-liquid"
    },
    {
        "layout": "post",
        "title": "Biomicry: Creating A Truly Sustainable Environment",
        "author_github": "KushagraRawat",
        "date": "2018-09-15T16:30:33",
        "image": "/assets/img/",
        "description": "What is biomicry and where do we see it in real life.",
        "tags": [
            "Biomicry"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "KushagraRawat",
        "comments": true,
        "text": "[{% load static %}]()\n## What is something that is common between the bullet train , self cooling houses , super strong adhesives and turbine blades?\n\nWell it turns out they are all inspired by nature is profound ways to optimise favourable\ncharacteristics and reduce unfavourable one. Yes! All these products have incorporated within\nthemselves ideas inspired from nature to solve tricky problems in our world by a process called\nas biomimicry . Interested to know more about Biomimicry , stick on.\n\n## What is biomimicry/biomimetic?\n\nBiomimicry is a process in which systems , products , designs are modeled based on the\n/adapted from the nature in order to solve problems in real life. Biomimicry can be of two types - form and process.\n\nForm biomimicry is the copying of the form/shape of some of the efficient shapes in the nature.\n\nFor example: the nose shape of the Shinkansen bullet train , the honeycomb structure and so\non. Process biomimicry is copying of the process employed by the nature in making processes best suited for a given problem . for example : the communication technique between ants, the transmigration of birds in nature and so on.\n\n## A History of Biomimicry\n\nOne of the early examples of would-be biomimicry was the study of birds to enable human flight. Although never successful in creating a \"flying machine\", Leonardo Da Vinci was a keen observer of the anatomy and flight of birds, and made numerous notes and sketches on his observations as well as sketches of \"flying machines\". The Wright Brothers, who succeeded in flying the first heavier-than-air aircraft in 1903, allegedly derived inspiration from observations of pigeons in flight.\n\nThe more recent development in the field of biomimicry has taken place in the mid 20th century with Jack E Steele coining the term bionic in the 1960 and described it as \"the science of systems which have some function copied from nature, or which represent characteristics of natural systems or their analogues\" .\n\nBiomimicry was popularised by scientist and author Janine Benyus in her 1997 book : **Biomimicry: Innovation Inspired by Nature** .\n\n## Applications of Biomimicry\n\n### 1.The Shinkansen Bullet train and biomimicry\n\nOne of the most successful design from biomimicry has been the design of the shinkansen\nbullet trains in Japan . Japan is a country which has a very old and developed technical\nexpwertise in the field of bullet trains. But in 1989 the designers of the Sinkansen bullet train were faced with a major problem . The noise that were produced by these trains tavelling at high speeds was defening and could be heard significantly even at distances upto half a kilometer!.\n\nWith a high population density in the city areas this became a serious problem for the Shinkansen bullet train authorities. The answer to this problem was finally found out by a designer by the name Eiji Nakatsu who also happened to be a birdwatcher. How did birdwatching help him solve the problem . Well the nose of the engine of the bullet train had to be redesigned in the shape resembling the shape of the beak of a kingfisher bird, which are able to dive into waters to catch their prey without making a splash. \n\nInterestingly this design of the engine incresed the eficiency by upto 20 % by reducing the air drag and reduced the noise produces to under 70 DB considered acceptable in residential areas .\n\n![Bullet Train]({% static '/blog/assets/img/biomicry-creating-a-truly-sustainable-environment/bullet_train.jpg' %})\n\n## **THE VELCRO**\nSwiss electrical engineer George de Mestral invented his first touch fastener when, in 1941, he\nwent for a walk in the woods and wondered why Burdock seeds clung to his coat and dog. He\ndiscovered it could be turned into something useful. He patented it in 1955 and subsequently\nrefined and developed its practical manufacture until its commercial introduction in the late\n1950s.\n\n## **SELF CLEANING SURFACES**\nSelf-cleaning surfaces are a class of materials with the inherent ability to remove any debris or bacteria from their surfaces in a variety of ways. The self-cleaning functionality of these surfaces are commonly inspired by natural phenomena observed in lotus leaves , shark skin , gecko feet, and water striders to name a few. All these surfaces have shown exceptional ability to self clean which has inspired many to conduct several researches on it .\n\n**SELF COOLING BUILDINGS**\nGlobal warming is leading to a large scale heat up of our planet , the summers are becoming continuously hotter. People are coming up with better and more efficient cooling technologies.\n\nBut at the best our cooling technologies require lot of energy which ultimately comes from burning the fossil fuels which contributes to global warming. This is seen as a major problem in the coming times.Is there a solution to this problem ? \n\nWell, yes and it already exists in Zimbabwe .In 1996 a Zimbabwean architect Mick Pearce constructed the world\u2019s first ever self cooling building - The Eastgate Centre in Harare. The building is inspired from the cooling mechanism in a termite mound . A series of intricate passages and pores across the building ensure that proper ventilation takes place within the building keeping the temperature inside 2-3 degrees lesser than its surroundings. The surface area of the building exposed to the surrounding is increased increasing the heat transfer to the surroundings and highly thermal absorbent materials are used to absorb maximum heat and keep the inside cool . The energy usage by the building is almost 25% lesser than those of other building around . \n\nThis is truly an amazing build and a real answer to sustainable architecture all inspired by nature folks!\n\n![termite mound]({% static '/blog/assets/img/biomicry-creating-a-truly-sustainable-environment/index.jpg' %})\n\n**The Eastgate**\n![eastgate]({% static '/blog/assets/img/biomicry-creating-a-truly-sustainable-environment/eastgate.jpg' %})\n\n### Sources:\n\n1. [http://www.ktransit.com/mainline/asia/japan/shinkansen/japan_bullet-tokaido.htm](http://www.ktransit.com/mainline/asia/japan/shinkansen/japan_bullet-tokaido.htm)\n2. [https://www.seat61.com/Japan.htm](https://www.seat61.com/Japan.htm)\n3. [https://en.wikipedia.org/wiki/Shinkansen](https://en.wikipedia.org/wiki/Shinkansen)\n4. [https://www.youtube.com/watch?v=iMtXqTmfta0](https://www.youtube.com/watch?v=iMtXqTmfta0)\n5. [https://en.wikipedia.org/wiki/Self-cleaning_surfaces](https://en.wikipedia.org/wiki/Self-cleaning_surfaces)\n6. [https://en.wikipedia.org/wiki/Eastgate_Centre,_Harare](https://en.wikipedia.org/wiki/Eastgate_Centre,_Harare)",
        "author_name": "KushagraRawat",
        "slug": "biomicry-creating-a-truly-sustainable-environment"
    },
    {
        "layout": "post",
        "title": "Segment Trees",
        "author_github": "shanthanu9",
        "date": "2018-10-06T22:55:00",
        "image": "/assets/img/",
        "description": "This article gives an introduction to segment trees and it\u2019s applications in competitive programming",
        "tags": [
            "Competitive Programming",
            "Segment Trees"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "shanthanu9",
        "text": "[{% load static %}]()\n## Introduction\n\n> Segment trees are extensivly used in competitive programming for problems that invlove range queries and range updates. \n\nFor example, given an array A of N elements, answer Q queries (of two types):\n\n1 L R : output the sum of elements between L and R  \n2 x I   : A[I] = x\n\nA similar version of the above problem for minimum(i.e. to find the minimum between L and R) is famously known as Range Minimum Query. These problems can be easily solved with segment trees.\n\n## Naive Approach\n\nA naive approach would be to answer queries of type 1 by running in a loop from L to R and finding the required sum and type 2 by performing updates on the array. \n\n```cpp\nfor(int i = 0; i < Q; i++) {\n\tint type;\n\tcin>>type;\n\tif(type == 1) {\n\t\tsum = 0;\n\t\tcin>>L>>R;\t\t\n\t\tfor(int j = L; j <= R; j++) {\n\t\t\tsum += A[i];\n\t\t}\n\t\tcout<<sum<<endl;\n\t}\n\telse {\n\t\tcin>>I>>x;\n\t\tA[I] = x;\n\t}\n}\n```\n\nTherefore, range queries (queries of type 1) takes O(N) time and point updates (queries of type 2) take O(1) time. This algorihtm works well when there are large number of point updates compared to range queries.\n\nBut what if there there are a large number of range queries, the above algorithm can take O(Q*N) time. For such problems, Q and N have an upperbound of 10<sup>5</sup> usually in contests (Hence won't pass). \n\nCan we improve this algorithm? Yes, we can by using Segment Trees.\n\n## Structure of Segment Tree\n\nSo, what is a Segment Tree?\n\nWe compute and store the sum of the array A[0 \u2013 N-1]. Then we divide the array into two halves and compute the sum of each half, i.e, sum of A[0 \u2013 N/2] and A[N/2+1 \u2013 N-1]. We continue this process again for each half untill an array with single element is reached.\n\n**Note on convention**: From now on, A[L - R] will represent the sub-array of A between L and R (both inclusive).\n\nIn other words, we call a procedure to find the sum of elements of A[0 \u2013 N-1], split the current segment into half and call the procedure for the both the halves. We can view this setup as a binary tree. The root node being sum A[0 \u2013 N-1] and its left and right child being left and right halves of A[0 \u2013 N-1].\n\nFor example, for A = {2,3,1,9,4,3,7,8}\n\n![Construction of Segment Tree]({% static '/blog/assets/img/segment-tree/construction.png' %})\n\nEach node corresponds to sum of given segment. Its called \u201cSegment Tree\u201d because it can be easily visualised as a tree.\n\nNote that the height of the tree is O(logn) since moving from any parent node to chlid node, the size decreases by approximately by half.\n\nNumber of nodes in the tree is   1 + 2 + 4 + ... + 2<sup>ceil(logn)</sup> = 2<sup>ceil(logn)+1</sup> < 4n = O(n) . Thus we will need O(n) extra space to implement segment trees.\n\nNote that segment trees are full binary tree(every node has two or zero child nodes). (prove it!)\n\n## Construction\n\nA Segment Tree can be efficiently implemented as follows:\n\nWe start at the bottom level, the leaf vertices. A vertex is a leaf vertex, if its corresponding segment covers only one value. Therefore we can simply copy the values of the elements a[i]. On the basis of these values, we can compute the sums of the previous level. And on the basis of those, we can compute the sums of the previous, and repeat the procedure until we reach the root vertex. \n\nIt is convenient to describe this operation recursively: we start the construction at the root vertex and the construction procedure, if called on a not-leaf vertex, first recursively constructs the two child vertices, and then sums up the computed sums of these children. If it is called on a leaf vertex, it simply uses the value of the array.\n\nThe time complexity of construction in O(n) because each edge is returns just one value.\n\n**Note**:  \nHere, segment trees are stored in an array T(1 - indexed). We call a node i if it's corresponding sum is stored at T[i]. We reresent the tree in T by  \n1. Storing sum corresponding to root node at index 1, i.e, at  T[1]. (node i )\n2. Store the sum of corresponding to node child nodes of _i_ at _2i_ ans _2i+1_\n3. Given a node i, it's parent is i/2 (integer division). \n\nFollowing is the implementation for construction of Segment Tree:\n\n```cpp\n//Max size of given array\nconst int N = 1e5;\n//Segment Tree is represnted\nint a[N], t[4*N]; \n//a is the array\n//t is the segment tree\n\n//fuction to build the tree build(1, 0, n-1)\n//[start, end] start and end both inclusive\nvoid build(int node, int start, int end) {\n\tif(start == end) {\n\t\tt[node] = a[start];\n\t}\n\telse {\n\t\tint mid = (start+end)/2;\n\t\tbuild(2*node, start, mid);\n\t\tbuild(2*node+1, mid+1, end);\n\t\tt[node] = t[2*node]+t[2*node+1];\n\t}\n}\n```\nbuild(1, 0, n-1) builds the tree.(assuming n is the size of array a)\n\nBut how does constructing a segment tree help in finding range queries?\n\n## Range Queries\n\nAfter the constructing the Segment Tree, it is possible to answer sum queries of the form A[L \u2013 R] in O(logn) time.\n\nWe compute sum queries by using the pre-computed values of segment sums. The idea is any given segment of values L and R, sum A[L \u2013 R] can be represented as sum of pre-computed sub segments.\n\nThis can be done by traversing the segment tree.\n\nFollowing the algorithm we use for range query: \n\n> Range_Query(node, L, R)\n>\n> (Let start and end be the extremes of the segment for which sum is stored in \u201cnode\u201d)\t  \n>    1. If start >= L and end <= R, then sum of \u201cnode\u201d contains a part of the required sum. So we return sum stored in node. \n>    2. If start > R or end < L, then it means the required sum A[L \u2013 R] is not present in that node or it\u2019s child nodes(if it exists). In this case, return 0.\n>    3. Else a part of the required sum A[L \u2013 R] is present in the node. So, we need to how much \u201cnode\u201d contributes to A[L \u2013 R]. For this, we recursively call Range_Query for left and right child. i.e. return Range_Query(left child of node, L, R) + Range_Query(right child of node, L, R)\n\nWe get the required output by calling Range_Query(root, L, R).\n\nThe procedure is illustrated in the following image. The array a=[1,3,\u22122,8,\u22127] is used, and here we want to compute the sum of A[2 - 4]. The colored vertices will be visited, and we will use the precomputed values of the green vertices. This gives us the result \u22122+1=\u22121.\n\n![range query]({% static '/blog/assets/img/segment-tree/range.png' %})\n\nFollowing is the implementation for range queries:\n\n```cpp\nint query(int node, int start, int end, int l, int r) {\n\tif(l > end || r < start) {\n\t\treturn 0;\n\t}\n\telse if(l <= start && end <= r) {\n\t\treturn t[node];\n\t}\n\tint mid = (start+end)/2;\n\treturn query(2*node, start, mid, l, r) + query(2*node+1, mid+1, end, l, r);\n}\n```\n\nIn conclusion the query works by dividing the input segment into several sub-segments for which all the sums are already precomputed and stored in the tree. And if we stop partitioning whenever the query segment coincides with the vertex segment, then we only need O(logn) such segments, which gives the effectiveness of the Segment Tree.\n\n## Point Updates:\n\nThis query is easier than the sum query. Each level of a Segment Tree forms a partition of the array. Therefore an element A[i] only contributes to one segment from each level. Thus only O(logn) vertices need to be updated.\n\nIt is easy to see, that the update request can be implemented using a recursive function. The function gets passed the current tree vertex, and it recursively calls itself with one of the two child vertices (the one that contains A[i] in its segment), and after that recomputes its sum value, similar how it is done in the build method (that is as the sum of its two children).\n\nHere is a visualization using thearray a=[1,3,\u22122,8,\u22127]. Here we perform the update a[2]=3. The green vertices are the vertices that we visit and update.\n\n![point update]({% static '/blog/assets/img/segment-tree/point.png' %})\n\nFollowing is the implementation for point updates:\n\n```cpp\nvoid update(int node, int start, int end, int idx, int val) {\n\tif(start == end) {\n\t\ta[idx] = t[node] = val;\n\t}\n\telse {\n\t\tint mid = (start+end)/2;\n\t\tif(idx>=start && idx<=mid) {\n\t\t\tupdate(2*node, start, mid, idx, val);\n\t\t}\n\t\telse {\n\t\t\tupdate(2*node+1, mid+1, end, idx, val);\n\t\t}\n\t\tt[node] = t[2*node]+t[2*node+1];\n\t}\n}\n```\n\n## Note on implementation:\n\nYou can use the following functions as helper fuctions(for ease of use):\n\n```cpp\nvoid build() {\n\tbuild(1, 0, n-1);\n}\n//change idx value to val, i.e, a[idx] = val\nvoid update(int idx, int val) {\n\tupdate(1, 0, n-1, idx, val);\n}\n//[l,r] both inclusive\nint query(int l, int r) {\n\treturn query(1, 0, n-1, l, r);\n}\n```\n\n**Bonus**: Try to implement range minimum and maximum query with point updates.\n\n## More examples:\n\n> 1. Given an array of N numbers and Q queries, each query consists of L and R. We need to write a program that prints the number of occurrence of the smallest element in the range L-R. \n\nThis is similar to range query except that it asks for number of minimum elements in the given range.\n\nHere, at each node of the segment tree, minimum and count of minimum element is stored. Try implementing the solution to this problem. \n\nThe solution is similar to sum queries problem but with different conditions for merging solutions.  By merging, I mean the step of updating the parent node value(or gathering query result) from its child nodes value(which occurs in construction and range queries).\n\n**Hint**: \nUse struct for each node to store minimum and count\n\n1. min(left_subtree) < min(right_subtree): \nnode.min=min(left_subtree), node.count = left_subtree.count\n\n2. min(left_subtree) > min(right_subtree): \nnode.min=min(right_subtree), node.count=right_subtree.count\n\n3. min(left_subtree) = min(right_subtree): \nnode.min=min(left_subtree) or min(right_subtree), node.count=left_subtree.count + right_subtree.count\n\nImplementation: [https://www.geeksforgeeks.org/count-number-of-smallest-elements-in-given-range/](https://www.geeksforgeeks.org/count-number-of-smallest-elements-in-given-range/)\n\n> 2. Given N numbers and Q queries, each query consists of L and R. Task is to write a program which prints the count of numbers which divides all numbers in the given range L-R.\n\nThis is a slightly harder problem. The point to note for such problems is that everything boils down to the merging step. If we are able to define contents of each node in the segment tree and are able to get a way to merge two child nodes to return required answer(or update parent), then we are done.\n\nHere, note that the number that divides all elements in a segment should be the minimum. If such a number exists, then it should be the GCD of all numbers in the segment.\n\nSo the count of the number of minimums in range L-R, given that minimum is equal to the gcd of that range will be our answer to every query. The problem boils down to finding the GCD, MINIMUM and count of MINIMUM for every range using Segment trees. On every node of the tree, three values are stored. \nOn querying for a given range, if the gcd and minimum of the given range are equal, countMINIMUM is returned as the answer. If they are unequal, 0 is returned as the answer.\n\n## Implemenation: \n* [https://www.geeksforgeeks.org/count-elements-which-divide-all-numbers-in-range-l-r/](https://www.geeksforgeeks.org/count-elements-which-divide-all-numbers-in-range-l-r/)\n\n\n**Bonus**: Check out two dimensional segment trees if you are interested: [https://www.geeksforgeeks.org/two-dimensional-segment-tree-sub-matrix-sum/](https://www.geeksforgeeks.org/two-dimensional-segment-tree-sub-matrix-sum/)\n\n## Epilogue\n\nThis article is supposed to be an introduction to Segment Trees. If you feel comfortable with the contents of the article, then do check out lazy propagation on segment trees. (this allows you to perform range queries and range updates on the segment tree)\n\n## References:  \n\n* [https://cp-algorithms.com/data_structures/segment_tree.html](https://cp-algorithms.com/data_structures/segment_tree.html)  \n\n* [https://www.hackerearth.com/practice/data-structures/advanced-data-structures/segment-trees/tutorial/](https://www.hackerearth.com/practice/data-structures/advanced-data-structures/segment-trees/tutorial/)  \n\n* [https://visualgo.net/en/segmenttree](https://visualgo.net/en/segmenttree)",
        "author_name": "Shantanu S Rai",
        "slug": "segment-tree"
    },
    {
        "layout": "post",
        "title": "Avionics",
        "author_github": "ChethanM",
        "date": "2018-10-07T14:27:00",
        "image": "/assets/img/",
        "description": "Understanding Avionics and other flight control mechanisms.",
        "tags": [
            "Drone",
            "Hobbycraft",
            "Avionics",
            "Flight Control"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "ChethanM",
        "text": "[{% load static %}]()\nThe merging of the words aviation and electronics, Avionics are all of the electronic devices and systems which perform individualized functions, used on any aircraft. Whether it is a military standard drone or a small hobby craft, it contains many electronic components which together achieve controlled flight. In this blog I\u2019m going to talk mainly on the electronics aspect of the equipment used in small scale aircraft and drones.\n\n\n## Accelerometer\n\n![Accelerometer]({% static '/blog/assets/img/avionics/Accelerometer.jpg' %})   \n\n![Accelometer IC]({% static '/blog/assets/img/avionics/AccelerometerIC.jpeg' %})\n  \nAn accelerometer is a transducer that is used to measure the physical or measurable acceleration that is made by an object. The most commonly used device is the piezoelectric accelerometer. It makes use of a piezoelectric quartz crystal. Due to the special self-generating property, the crystal produces a voltage that is proportional to the accelerative force. The basic arrangement is shown in the figure above. When movement occurs in one direction the amplitude of voltage signal generated gives the amount of acceleration. Coupled with two more setups in two other axes\u2019 give complete 3dimentional acceleration. Well known algorithms exist to calculate the speed increase or displacement based on this data. This type of device is called dynamic accelerometer and the movement of the object can be foreseen using it.\n\nNowadays, a new type of accelerometer called the Micro Electro-Mechanical System (MEMS) Accelerometer is being used as it is simple, reliable and highly cost effective.\n\n## Gyroscope\n\nA gyroscope is a device that is used to measure angular velocity. It is similar to accelerometer in working. More precisely called Vibration gyro sensors sense angular velocity from the Coriolis force applied to a vibrating element. the accuracy with which angular velocity is measured differs significantly depending on element material and structural differences. Here is an example of how gyroscope using Epson 's double-T structure crystal element works\n    \n1. Normally, a drive arm vibrates in a certain direction.  \n\n![Drive arm vibration]({% static '/blog/assets/img/avionics/DriveArmVibration.gif' %})\n\n2. Direction of rotation  \n\n![rotation]({% static '/blog/assets/img/avionics/rotation.gif' %})\n \n3. When the gyro is rotated, the Coriolis force acts on the drive arms, producing vertical vibration  \n\n![vertical vibration]({% static '/blog/assets/img/avionics/VerticalVibration.gif' %})\n\n4. The stationary part bends due to vertical drive  arm vibration, producing a sensing motion in the sensing arms.   \n\n![sensing motion]({% static '/blog/assets/img/avionics/SensingMotion.gif' %})\n\n5. The motion of a pair of sensing arms produces a potential difference from which angular velocity is sensed. The angular velocity is converted to, and output as, an electrical signal.  \n![Electrical Signal]({% static '/blog/assets/img/avionics/ElectricalSignal.jpg' %})\t\t\t\t\n\n## Servo Motor\n\n![Working of Servo]({% static '/blog/assets/img/avionics/WorkingOfServo.png' %})  \n\n![MicroServo]({% static '/blog/assets/img/avionics/microservo.jpg' %})  \n\nServo motors are specially designed motors to be used in control applications of drones. They are used for precise position and speed control at high torques. It consists of a suitable motor, position sensor and a sophisticated controller. Servomechanism is basically a closed-loop system, consisting of a controlled device, controller, output sensor and feedback system. \n\nMechanical position of the shaft can be sensed by using a potentiometer, which is coupled with the motor shaft through gears. The current position of the shaft is converted into electrical signal by the potentiometer, and the compared with the command input signal. Command input is given according to the required position of the shaft. If the feedback signal differs from the given input, an error signal is generated. This error signal is then amplified and applied as the input to the motor, which causes the motor to rotate. And when the shaft reaches to the required position, error signal becomes zero, and hence the motor stays standstill holding the position. \n\nIn modern servo motors, electronic encoders (Gray code) or sensors are used to sense the position of the shaft.\n\n### Electronic Speed Controller\n\n![ESC]({% static '/blog/assets/img/avionics/esc.png' %})  \n\nESCs are used to vary the speed of main motor that is used to provide the thrust for the aircraft. They work on the principle of Pulse width modulation. The control of motor speed is obtained by switching the power to the motor on and off in various ratios e.g. maximum throttle is permanently on, half throttle is on half time, off half time etc. This switching on and off is done many times a second. The speed at which the switching takes place has a large effect on overall efficiency. \n\nEarly speed controls used what is known as \"frame rate\" switching, which means that they switched approximately 50 times a second, the same rate as frames of information are delivered over the radio. Most modern ESCs switch at a much higher rate which makes them much more efficient i.e. they lose less power as heat in the controller. Switching rates around 3000 Hz (times a second) are about optimum. Anywhere between 1000 Hz and 5000Hz is acceptable. \n\nModern ESCs also contain a BEC inbuilt in them. A battery eliminator circuit (BEC) is an electronic circuit designed to deliver electrical power to other circuitry without the need for multiple batteries. A BEC-equipped ESC meant for airplane use often incorporates a low-voltage-cut off (LVC) circuit which can sense the voltage drop caused when the battery has little charge left in it. It then cuts the power to the 'drive' motor in order to provide the 'steering' servo(s) with enough power to be able to bring the model safely back to the operator. The power to the propeller would be cut but the operation of the control surfaces would be maintained in order to perform a dead-stick landing. Without this feature, all control would be lost when the battery expired, probably resulting in the destruction of the model. R/C BECs in their simplest form use a linear fixed voltage regulator.\n\n### Transmitter and Receiver\n\nIn order to control a drone remotely, you must be able to communicate with it wirelessly. This is when circuits which convert electrical signals into radio signals and vice versa are used in avionics.  \n\n![Radio frequency transreciever]({% static '/blog/assets/img/avionics/RadioFrequencyTransreciever.jpg' %})   \n\nThey are usually frequency modulated signal generators and demodulators. While controlling your drone it may so happen that another person\u2019s signal may interfere with you. To avoid situations such as your drone being controlled by someone else\u2019s remote control, devices use a unique identification code to identify a transmission on one particular radio frequency as the transmission it wants to receive. To do this, transmitters and receivers are paired using an RFID or a \u201cradio frequency identification.\u201d All information broadcast over RFID is prefixed with an RFID so that the receiver knows that the information it is picking up is for it. Along with this RSSI method is used to detect the distance of drone from controller and if it is going out of range.\n\nApart from this various circuits like power distribution boards and FPVs exists which give additional functionality to the drone. The field has advanced so much that autonomous intercontinental flight also is possible for a small-scale drone. \n\n### References\n1. [http://www.instrumentationtoday.com/accelerometer/2011/08/](http://www.instrumentationtoday.com/accelerometer/2011/08/)  \n\n2. [https://www.quora.com/What-the-function-of-gyroscopes-in-airplane](https://www.quora.com/What-the-function-of-gyroscopes-in-airplane)  \n\n3. [https://www.electricaleasy.com/2015/01/how-does-servo-motor-work.html](https://www.electricaleasy.com/2015/01/how-does-servo-motor-work.html)  \n\n4. [https://www.dummies.com/consumer-electronics/drones/understanding-how-your-drone-is-controlled/](https://www.dummies.com/consumer-electronics/drones/understanding-how-your-drone-is-controlled/)  \n\n5. [https://uavcoach.com/infographic-drones-work/](https://uavcoach.com/infographic-drones-work/)  \n\n6. [https://www.dimensionengineering.com/info/bec](https://www.dimensionengineering.com/info/bec)  \n\n7. [https://www.quora.com/How-does-ESC-electronic-speed-control-work](https://www.quora.com/How-does-ESC-electronic-speed-control-work)",
        "author_name": "Chethan M",
        "slug": "avionics"
    },
    {
        "layout": "post",
        "title": "Phase Change Material",
        "author_github": "AnuragPK",
        "date": "2018-10-07T20:35:00",
        "image": "/assets/img/",
        "description": "The storage device of the future",
        "tags": [
            "Storage Device",
            "Material"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "AnuragPK",
        "text": "[{% load static %}]()\nA Phase Change Material (PCM) is a substance with a high heat of fusion which, on melting and solidifying at a certain temperature, is capable of storing and releasing large amounts of energy. In the most basic terms, a PCM is a substance which can store and release large amounts of energy, typically in the form of heat. Although phase changes can occur among any combination of the three phases of a substance - gas, liquid, or solid - the most commercially viable transition is between the liquid and solid phases. \n\n## Working of a PCM \nA PCM in its\u2019 solid phase absorbs heat as the external temperature rises. The temperature of the PCM will mirror the external temperature until the PCM's melt point is reached. When the external temperature reaches the melt point of the PCM, the PCM will begin to melt, i.e. \"change phase\". During the phase change process, the PCM will absorb large amounts of heat with almost no change in temperature. During this time period, the PCM is providing a cooling effect. The reverse cycle occurs as the external temperature cools. The PCM, now in its' liquid phase, can release the heat it absorbed as the external temperature decreases. During this time period, the PCM solidifies and provides a warming effect. \n\n![Phase Change Material]({% static '/blog/assets/img/phase-change-material/pcm.jpg' %})\n\n## Advantages of using PCM \nNo electricity or power of any kind is necessary to create a real reduction or increase in temperature. In addition, PCMs have the advantage that they can be used over and over again. Also, unlike the ice storage system, however, the PCM systems can be used with any conventional water chiller both for a new or alternatively retrofit application. The positive temperature phase change allows centrifugal and absorption chillers as well as the conventional reciprocating and screw chiller systems or even lower ambient conditions utilizing a cooling tower or dry cooler for charging the TES system.\n\n \n## Usage of Phase Change Alloys for Memory Applications \n\nPCM has been successfully used in optical memory devices such as DVD-RAM since the 1990s and recently commercial production of electronic non-volatile phase-change random memory (PC-RAM) has been launched by two of the world\u2019s leading memory makers Samsung and Micron. \n\nThe basic idea of PCM is to utilize the property contrast between the crystalline and amorphous phases. When a material is cooled down slowly, a crystalline phase is formed, when it is cooled down rapidly, an amorphous (glassy) phase is formed. In the phase-change process, a short intense pulse melts the material that is subsequently transformed into the amorphous phase. A longer pulse of lower intensity reverts the material to the crystalline phase. The underlaying phase-change mechanism is generally believed to have a purely thermal origin making use of either Joule heating in electrical memories or the heat released during non-radiative recombination in optical memories. \n\n# References: \n1. [https://www.youtube.com/watch?v=BG2w4wb4Q5w&t=122s](https://www.youtube.com/watch?v=BG2w4wb4Q5w&t=122s)  \n2. [https://en.m.wikipedia.org/wiki/Phase-change_material](https://en.m.wikipedia.org/wiki/Phase-change_material)\n3. [https://engineering.jhu.edu/materials/research-projects/phase-change-alloy-for-memory-applications/#.W7eoMWgzZPZ](https://engineering.jhu.edu/materials/research-projects/phase-change-alloy-for-memory-applications/#.W7eoMWgzZPZ)\n4. [https://www.nature.com/articles/srep13698#ref2](https://www.nature.com/articles/srep13698#ref2)\n5. [http://microteklabs.com/how-do-pcms-work.html](http://microteklabs.com/how-do-pcms-work.html)",
        "author_name": "Anurag P K",
        "slug": "phase-change-material"
    },
    {
        "layout": "post",
        "title": "Automate The Industries",
        "author_github": "NisargBShah",
        "date": "2018-10-13T23:32:44",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Piston",
            "Automation",
            "Industries",
            "Robots",
            "Manufacturing"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "NisargBShah",
        "text": "[{% load static %}]()\n# Automate The Industries\n\nIndustrial Automation refers to the use of sensors, robots , actuators in various industrial processes so that we get much better efficiency and the working labour required is low. Although the initial cost of setup of an automated industry is high, but the industrialists will surely be in profit if they apply the automation method as they can produce more no of goods in a limited time and also the quality of their products will increase as the human error in production is minimised.\n\nHere are some of the industrial processes in which we can use automation:-\n\n 1. Welding:- Welding is one of the most important process in every manufacturing industry.Lets take an example of the automobile industry. Chassis - the supporting frame of a car needs a lot of welding to be done. We can't make the whole chassis of a car by just the casting process as it's too intricate. What we do is cast the individual parts of the chassis and then weld them together to get final chassis. Similarly there are many other industrial manufacturing activities that requires welding. Hence a lot of industries can use welding Robots instead of humans doing the welding process.This is because if Industries use Welding Robots , then the defects that arise due to welding will decrease and hence the quality of product can be enhanced.\n![Image1]({% static '/blog/assets/img/Automate-The-Industries/Image1.jpg' %})\n\n2. Assembling Robots:-\nAssembling is the most important phase of a manufacturing process and it requires skilled labourers as various parts must be assembled as per the design. Robots can simplify this task as the final design can be loaded into robot's memory and based on the design , the robot can be coded to perform various assembling tasks.\n![Image2]({% static '/blog/assets/img/Automate-The-Industries/Image2.jpg' %})\n\nThe image shows a robot involved in automation of car assembly process\n\n3. CNC machines:- CNC machines are the automatic versions of traditional machines on which we perform metal removal operations. The tool motion can be controlled through various G codes and M codes in a CNC machine and we can obtain the desired final workpiece.\n![Image3]({% static '/blog/assets/img/Automate-The-Industries/Image3.jpg' %})\n\nThe image shows robot used in packaging.\n\n![Image4]({% static '/blog/assets/img/Automate-The-Industries/Image4.jpg' %})\n\nRobotic Vision being employed for product testing Basically we must adopt Flexible Manufacturing System (FMS)for maximising the output of Industries. By using this method , we can effective maintain the flow of material through FML(Flexible Manufacturing Line)that's going to undergo a certain process in FMC's(Flexible Manufacturing Cells).\n\nSome more places where the concept of automation can be applied:-\n\n1. Effluent Treatment Plants(ETPs):- Every chemical Industry produces a lots of waste material called effluents which must be treated before dumping them into industries. These effluents are treated in effluent treatment plants. Even after treating the effluents through ETPs in first pass, there can still be presence of unwanted materials which can affect the river bodies in which this effluents are dumped. Also the temperature at which these effluents are thrown to the water bodies is a factor because aquatic animals cannot resist temperature difference of water. Water Quality sensors,temperature sensors can be used to check the water quality at the outlet of ETPs and if they don't meet the required criteria for the disposal, these can be sent back to the ETPs through other pipes automatically by operating valves and servo motors to control it. This can be a small step towards protection of environment.\n\n2. Thermal Power Plants:- In thermal Power plants the majority of accidents are caused because of bursting of boilers mostly because there were some impurities in the water that was poured into it due to which excess temperature was required to heat the water in it and the material that was used to build the boiler was not able to withstand such high temperature. Automation can be used here by making use of temperature sensors ( the one which are able to measure high temperatures).If the temperature measured by this sensors go beyond 0.8 times the max temperature that can be withstanded by the boiler material , the heat supply to the boiler must be automatically cut off. By this we can ensure lesser accidents in Thermal Power Plants.",
        "author_name": "Nisarg B Shah",
        "slug": "automate-the-industries"
    },
    {
        "layout": "post",
        "title": "Varistor",
        "author_github": "PrithviPatil",
        "date": "2018-10-13T23:32:44",
        "image": "/assets/img/",
        "description": "Understanding Varistor, it\u2019s characteristics and working",
        "tags": [
            "IEEE NITK",
            "Diode",
            "resistors",
            "capacitors",
            "zener diode",
            "voltage transients",
            "regulators"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "PrithviPatil",
        "text": "[{% load static %}]()\n**Varistor**\n\nThe name itself might have given you some idea about what it is, varistor stands for variable resistor\n\n**What is a varistor?**\n\nVaristor is a varying resistor whose resistance depends on applied voltage,one of the special features of varistor is that it possesses non ohmic characteristics, which defines themselves as a non linear type of resistors\n\nIt has two semiconductor elements fixed in a particular fashion (i.e. anti-parallel to each other) as to provide same characteristics for both directions of traversing current\n\nIts composition might have provided you with the insight that as it is made of semiconductor, it's resistance will decrease as voltage across it increases\n\nIts symbol is\n\n ![Image1]({% static '/blog/assets/img/Varistor/1.png' %})\n\nOne of the major applications of varistors is its ability to suppress voltage transients in the cut\n\nVoltage transient - short duration surges of electrical energy that are result of sudden release of energy that are previously stored or induced by other means, such as heavy inductive loads or lightning\n\n**Waveform of AC Transient**\n\n![Image2]({% static '/blog/assets/img/Varistor/2.jpg' %})\n\nThe above characteristics might actually lead you to think that varistor behaves like a potentiometer or a rheostat\n\nTwo characteristics that makes varistor a different entity is\n\ni) resistance cannot be changed manually\n\nii) resistance decreases with increase in voltage\n\n**Varistor vs voltage graph**\n\n ![Image3]({% static '/blog/assets/img/Varistor/3.jpg' %})\n\n**Working of a varistor**\n\nAs I have already told that its characteristics are similar to that of Zener diode, its V-I characteristics are also similar to that Zener diode\n\n**V-I characteristics of varistor**\n\n ![image4]({% static '/blog/assets/img/Varistor/4.png' %})\n\nAs the varistor operates in first and third quadrant, it makes it a suitable device to connect it in a circuit with AC or DC source\n\nAs the voltage increases above the clamping voltage, there is an abrupt increase in current\n\nThis helps during voltage transients, as when the circuit experiences high transient voltage, the voltage across the varistor increases, to a value greater than its clamping voltage, which in turn increases the current and acts as a conductor, it basically acts like a self-regulator at the event of voltage transient keeping the voltage input in check\n\nSteep nonlinear curve indicates excessive currents can be passed through varistor over narrow range of voltage\n\n**Capacitance of varistor**\n\nA varistor in non-conducting state acts like a capacitor more than a resistor\n\nNon-conducting state-the voltage applied across the varistor is less than the clamping voltage\n\nSince the semiconductor body of the Varistor acts like an insulator during its insulating state, it can be regarded as the dielectric material, while the two terminals can be regarded as the two electrodes\n\n**Types of Varistor**\n\nSilicon carbide varistor\n\nMetal oxide varistor",
        "author_name": "Prithvi Patil",
        "slug": "varistor"
    },
    {
        "layout": "post",
        "title": "Why Machine Learning Matters ?",
        "author_github": "siddhartha15",
        "date": "2018-10-13T19:05:44",
        "image": "/assets/img/",
        "description": "A primer on Machine Learning and Introduction to Supervised Learning",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Machine Learning",
            "Artificial Intelligence"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "siddhartha15",
        "text": "[{% load static %}]()\n**Hello everyone !**\n\nThis article is for people who want a primer on *Machine Learning* and anyone who is curious about how Machines think.If you are already familiar with ML, this may not be the right place for you.\n\n## Why Machine Learning matters ?\n**Artificial intelligence** will shape our future more powerfully than any other innovation this century.\nThe rate of acceleration is already astounding. After a couple of AI winters and periods of false hope over the past four decades, rapid advances in data storage and computer processing power have dramatically changed the game in recent years.\n\nIn 2015, *Google* trained a *conversational agent (AI)* that could not only convincingly interact with humans as a tech support helpdesk, but also discuss morality, express opinions, and answer general facts-based questions.\nThe same year, *DeepMind* developed an agent that *surpassed human-level performance* at *49 Atari games*, receiving only the pixels and game score as inputs. Soon after, in 2016, *DeepMind* obsoleted their own achievement by releasing a new *state-of-the-art* gameplay method called **A3C**.\n\nMeanwhile, **AlphaGo** defeated one of the best human players at Go\u200a\u2014\u200aan extraordinary achievement in a game dominated by humans for two decades after machines first conquered chess. Many masters could not fathom how it would be possible for a machine to grasp the full nuance and complexity of this ancient Chinese war strategy game, with its 10\u00b9\u2077\u2070 possible board positions (there are only 10\u2078\u2070atoms in the universe). \n\nArtificial intelligence is the study of agents that perceive the world around them, form plans, and make decisions to achieve their goals. Its foundations include *mathematics, logic, philosophy, probability, linguistics, neuroscience,* and *decision theory*. Many fields fall under the umbrella of AI, such as *computer vision*, *robotics*, *machine learning*, and *natural language processing*.\n\n**Machine learning is a subfield of artificial intelligence.**\n\n![Image 1]({% static '/blog/assets/img/Why-Machine-Learning-Matters/MLimage_1.png' %})\n\n**Strong AI** will change our world forever; to understand how, studying *Machine Learning* is a good place to start ...\n## Machine Learning covers a lot\nMachine Learning is **self-explanatory**. Instead of you telling the machine what decisions and rules to make, you **teach** it. A machine that **learns**. So that leaves the methods of teaching and learning pretty wide open. So what can you teach a machine, and what can it learn?\n\n![Image 2]({% static '/blog/assets/img/Why-Machine-Learning-Matters/MLimage_2.png' %})\n\n>**Classification** algorithms can be taught to split existing data into classes, like say names of animals.Then when you give it new data, it will tell you which class it belongs to, like say this is a cat and not a dog.\n\n>**Regression** algorithms basically try to learn the function of a dataset,by predicting future data based on past data.\n\n>**Unsupervised Learning** can be used if you\u2019ve got lots of data and you can\u2019t make sense of it,so you teach the machine to try and make sense of it instead.\n\n>**Reinforcement Learning** is how to beat every human on Earth in games like GO and Chess,or drive autonomous cars and drones. And a lot more.\n\nSo let us talk about **Supervised Learning** now.\n\n>How much money will we make by spending more dollars on digital advertising? \n>Will this loan applicant pay back the loan or not? \n>What\u2019s going to happen to the stock market tomorrow?\n\nIn supervised learning problems, we start with a **data set** containing training examples with associated correct **labels**. For example, when learning to classify handwritten digits, a supervised learning algorithm takes thousands of pictures of handwritten digits along with **labels** containing the correct number each image represents. The algorithm will then learn the **relationship** between the images and their associated numbers, and apply that learned relationship to **classify** completely new images (without labels) that the machine hasn\u2019t seen before. This is how you\u2019re able to deposit a check by taking a picture with your phone!\n\nTo illustrate how supervised learning works, let\u2019s examine the problem of **predicting annual income** based on the number of years of higher education someone has completed. Expressed more formally, we\u2019d like to build a model that approximates the relationship **f** between the number of years of higher education **X** and corresponding annual income **Y**.\n\n![Image 3]({% static '/blog/assets/img/Why-Machine-Learning-Matters/MLimage_3.png' %})\n\n**X** (input) = years of higher education\n\n**Y** (output) = annual income\n\n**f** = function describing the relationship between **X** and **Y**\n\n**\u03f5** (epsilon) = random error term (positive or negative) with mean zero \n\nIn supervised learning, the machine attempts to learn the relationship between income and education from scratch, by running **labeled training data** through a **learning algorithm**. This **learned function** can be used to estimate the income of people whose income **Y** is unknown, as long as we have years of education **X** as inputs. In other words, we can apply our model to the **unlabeled test data** to estimate **Y**.\n\nThe **goal** of supervised learning is to predict **Y** as accurately as possible when given new examples where **X** is known and **Y** is unknown.\n\nTherefore **predicting annual income** comes under **classic regression** problem. Your input data **X** includes all relevant information about individuals in the **data set** that can be used to predict income, such as years of education, years of work experience, job title, or zip code. These attributes are called **features**, which can be *numerical* (e.g. years of work experience) or *categorical* (e.g. job title or field of study).\n\nYou\u2019ll want as many training observations as possible relating these features to the target output **Y**, so that your model can learn the relationship **f** between **X** and **Y**.\n\nThe data is split into a *training data set* and a *test data set*. The training set has labels, so your model can learn from these labeled examples. The test set does not have labels, i.e. you don\u2019t yet know the value you\u2019re trying to predict. It\u2019s important that your model can generalize to situations it hasn\u2019t encountered before so that it can perform well on the test data.\n\n**Regression**\n\n**Y = f(X) + \u03f5**, where **X** = (x1, x2\u2026xn)\n\n**Training:** machine learns **f** from labeled training data\n\n**Test:** machine predicts **Y** from unlabeled testing data\n\n![IMAGE4]({% static '/blog/assets/img/Why-Machine-Learning-Matters/MLimage_4.png' %})\n\n## So how do we solve these problems? \n\nHow do we build models that make accurate, useful predictions in the real world? We do so by using supervised learning algorithms.\nThe above problem of predicting annual income can be solved using [Linear regression](https://towardsdatascience.com/linear-regression-simplified-ordinary-least-square-vs-gradient-descent-48145de2cf76) method and [gradient descent](https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/) algorithm.\n\nDo read on supervised learning algorithms for better uderstanding and you can take a well known and free course - [MACHINE LEARNING by ANDREW NG](https://www.coursera.org/learn/machine-learning) on coursera.\n\nFeel free to contact me for any queries, complaints or suggestions.",
        "author_name": "Siddhartha M",
        "slug": "why-machine-learning-matters"
    },
    {
        "layout": "post",
        "title": "Light backscatter application",
        "author_github": "RohitKhoja",
        "date": "2018-10-20T19:30:00",
        "image": "/assets/img/",
        "description": "caculating fat content in milk using light backscatter application",
        "tags": [
            "light backscatter",
            "fat content in milk",
            "gerber method",
            "coagulating milk"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "RohitKhoja",
        "text": "[{% load static %}]()\n# Measure of fat content in milk with LED and LDR\n\n**India&#39;s milk production** is estimated to have increased by 6.6 per cent  during the last four year. Price of milk is based on the  quantity of FAT  present in milk . so we need machine which can  measures milk parameters like SNF (Solid but Not FAT), FAT, CLR, with less cost.\n\nThe important thing is that machine should be cheaper and only based on electricity .then it is very helpful for the remote area also.\n\n**Coming to the point photometric based sensor or light backscatter application technique is able to fulfil the remote area condition(which was proposed by IJIRCCE).**\n\n**How is it better then other method ?**\n\n**first have a look on primary method -**\n\n**1)Gerber method :**\n\n The specific gravity of fat is 0.9 and that of acid milk mixtures is 1.43. This situation promotes complete separation of fat when proper centrifugal force is applied.\n\nDue to application of centrifugal force lighter substances (Butter fat) are thrown towards centre and rest of serum portion that is heavier is thrown towards the pheriphen.Addition of amyl alcohol helps for separation of fat from the milk acid mixture and also prevents the charging of fat and sugar by the H2SO4.\n\n**Apparatus:**\n\n- Milk sample bottle.\n\n- 10 ml automatic tilt measure for H2SO4.\n\n- 1 ml automatic tilt measurefor amyl\n\n- 10.75 ml capacity milk pipette.\n\n- Dairy floating thermometer.\n\n**Procedure:**\n\n1. Put the clean and dry butyrometer in a butyrometer stand (Camlab part no. 1162195) with open mouth upwards.\n\n2. Run 10 ml of sulphuric acid with the tilt measure in the butyrometer.\n\n3. Pipette out 10.75 ml of milk sample gently by the side of butyrometer, whose temperature is  about 15-21 degrees C.\n\n4. Pour 1 ml. of amyl alcohol with tilt measure.\n\n5. Stopper the butyrometer with the help of lock stopper using regulating pin/guiding pin.\n\n6. The tube is well (mixed) shaken till mahogany red colour is obtained. Keep the butyrometer in hot water bath till it attains 15-21 degrees C and the butyrometer are placed in the centrifuge at 1100 ram for 4 minutes.\n\n7. Take out the butyrometer in an upright position with the stopper end down wards.\n\n8. Keep the butyrometer in hot water bath at (65 degrees C) for some time.\n\n9. Note the reading. Reading should be taken from bottom of the fat column to lower border of meniscus on the scale.\n\n**2) Soxhlet extraction principle :**\n\nGenerally this principal  is used in industries and it is also costly.\n\n**Now let&#39;s discuss about light backscatter application and it&#39;s working-\\&gt;**\n\n**photometric based sensor or light backscatter application :**\n\nIn this application the scattering of a beam of light by the fat globules present in the homogenized milk is the principle of this application . The amount of light scattered by the milk sample is a measure of the fat content in the milk.\n\nA high intensity LED is used as a light source. The light beam is made to pass through the sample solution contained in the test tube. A LDR is placed exactly on the opposite side of the test tube to detect the amount of light passing through the test tube un-scattered. To obtain maximum sensitivity the test tube is covered in wooden shield which has opening only for LED and LDR to pass through.\n\n**METHOD:**\n\nA sensor with two optical fibers, as shown in Figure 1, was used to monitor the coagulation step. An LED light source directs light into an optical fiber that terminates in the milk and an adjacent fiber separated by a distance r1 receives the light backscatter. The uniqueness of this light backscatter configuration is that the light must traverse through a finite distance of fluid (free of specular reflectance) and be backscattered by particulates to the receiving fiber. This configuration was used to measure the change in light backscatter during milk coagulation.\n\n![Figure 1]({% static \"/blog/assets/img/light-backscatter-application/1.jpg \"Figure 1\"\" %})\n\n**Figure 1: Schematic of a single waveband backscatter sensor used for cutting time determination in cheese processing**.\n\nNext figure shows the fiber optic measurement of intensities at two radial distances, r1 and r2, from the emitting fiber. The sensor employed three optical fibers: one that delivered light to the scattering media and two others that measured light intensity (I1 and I2) at spatially separated points r1 and r2, respectively (\u223c1 mm apart).\n\n![Figure 2]({% static \"/blog/assets/img/light-backscatter-application/2.jpg \"Figure 2\"\" %})\n\n**Figure 2. Schematic of the optical fiber configuration tested for measuring composition showing a LED focused on a fiber and two receiving fibers at radial distance r1 and r2 focused onto detectors with outputs F1 and F2.**\n\nA relatively simple empirical correlation between the distribution of backscattered light intensity and the particle concentration was utilized by adapting a widely used diffusion approximation equation presented by Bolt and ten Bosch [1]:\n\n                         I(r) = I0 exp(- \u03b2Cr) /r^m         equation(1);\n\nwhere: I0 = apparent intensity at radial center line of emitting fiber\n\nI(r) = Light intensity as a function of radial distance from the emitting fiber\n\n\u03b2 = specific backscatter light coefficient\n\nC = concentration of particulates\n\nm = exponent relating light diffusion in the radial direction\n\n r = radial distance of the receiving fiber (centerline to centerline), mm.\n\nThe backscatter light coefficient, \u03b2, is based on the ability of the sample to scatter light and depends on the optical and radiative properties of the particles in the sample. The value of m depends on whether the detector is placed in the intermediate area (m = \u00bd) or the diffusion area (m = 2). The diffusion area is defined as the area in which sufficient multiple scatterings have taken place, so that the diffusion approximation is valid.\n\nIn the development of a sensor, the use of signal ratios has the advantage of normalizing the resulting response. This isolates the signal ratio from changes in light intensity and some changes to the physical system (optics, mechanical connections, etc.). For the fully developed diffusion area, the ratio of the intensities at two radial distances (r1 and r2) using Equation (1) reduces to the following equation:\n\n                 I(r1)/I(r2) =   (r2/r1)^m  exp(K(r2-r1))     equation(2)\n\nwhere r1 and r2 are radial distances for fiber 1 and fiber 2, respectively, and K is a constant for a specific fat content. This equation predicts an increasing signal ratio with increasing milk fat. Light scattering is dominant for high concentrations of fat and the widely used diffusion approximation is valid for this case.\n\nA consistent light backscatter pattern (reflectance ratio \u2013 ratio of light backscatter signal to initial signal at time zero) was found and is shown in Figure 3. The reflectance ratio is initially relatively steady, traverses through a sigmoidal increase and then gradually increases at a decreasing rate. The first chemical reaction during the enzymatic coagulation of milk for cheese making is the reaction of chymosin and casein. This enzymatic reaction phase destabilizes the colloidal nature of casein micelle but does not significantly affect light backscatter. This reaction proceeds as a first order chemical reaction when the substrate concentration (casein) is in excess supply in relation to the enzyme concentration. The second phase is the aggregation phase of the destabilized casein micelle into flocks which grow and consume the entire casein particulate system. It is during this phase when larger particles are formed and the reflectance of light increases rapidly. The aggregation reaction follows a second order chemical reaction. The third phase is the gel forming phase which results from a micro consolidation of protein strands that result in stronger protein strands and an increasing space within the protein matrix. This phase follows a first order chemical reaction. These three chemical reactions overlap with the resulting reflectance profile as shown in Figure 3. The reflectance of milk at 880 nm typically increases 15% to 40% during coagulation providing a strong response signal.\n\nThe goal of signal analysis is the extraction of information which will allow accurate prediction of the cutting time from the profile. A successful cutting time prediction technology was developed by utilizing the strong correlation between two time events; tmax and tcut. The correlation, however, needed to be corrected for protein content to develop a precise prediction. The following cutting time prediction equation was determined to predict the cutting time with an approximate precision of \u00b1 1 min:\n\n             tcut = \u03b2 tmax (protein correction)               Equation 3\n\n![Figure 3]({% static \"/blog/assets/img/light-backscatter-application/3.jpg \"Figure 3\"\" %})\n\n**Figure 3. A typical light backscatter profile, R, for coagulating milk is shown with the first derivative, R&#39;, the second derivative, R&#39;&#39;, and the time-based parameter, tmax**\n\nwhere \u03b2 = a calibration constant (usually determined by the cheese maker) which typically varies from 1.3 to 2.5. Equation 3 requires only tmax. (a time-based parameter) and protein content to make a cutting time prediction. The time-based parameter, tmax to be an exact measurement proportional to the enzymatic reaction rate. The change in reflectance does represent product changes; however, numerous attempts at extracting consistent information from reflectance changes have proved fruitless. Figure 4 shows observed and predicted cutting time for a broad based test of conditions typically encountered in cheese making.\n\n![Figure 4]({% static \"/blog/assets/img/light-backscatter-application/4.jpg \"Figure 4\"\" %})\n\nFigure 4. Measured and predicted cutting time using the cutting time prediction equation Eqn. 3\n\nThe light backscatter was measured during the culture of cottage cheese. A typical reflectance profile is shown in Figure 5.\n\nA cutting time prediction equation was determined for cottage cheese culture using time-based parameters and a multiple linear regression. The profile is more complicated in that there are two maxima in the first derivative. The second noteworthy difference is that light backscatter increases 200% during the process providing a very strong signal.\n\n![Figure 5]({% static \"/blog/assets/img/light-backscatter-application/5.jpg \"Figure 5\"\" %})\n\n**Figure 5. A typical light backscatter profile, R, for cottage cheese culture showing the first derivative, R&#39;, the second derivative, R&#39;&#39;, and the time-based parameters**.\n\nThe ratio of light intensity response, (I2/I1), measured with two detectors spaced at different radial distances was referred to as signal ratio and was found as predicted by Equation 2 to increase with fat content. The signal ratio for homogenized whipping cream diluted in water is shown in Fig. 6.\n\nA single waveband backscatter with two detectors spaced at different radial distances that measures optical extinction has several applications in the dairy industry where an inexpensive sensor is required to monitor and control product composition. A light extinction sensor is a response-based sensor normalized by dividing two signals. Further testing to optimize light wavelengths and fiber spacing is required before a light extinction sensor can be developed with the accuracy and precision required by the dairy industry. From experience, it is known that the attenuation of light varies significantly between products. Thus the determination of optimum fiber spacing and wavelength is required for different products.\n\n![Figure 6]({% static \"/blog/assets/img/light-backscatter-application/6.jpg \"Figure 6\"\" %})\n\n**Figure 6. Light backscatter signal ratio as a function of homogenized whipping cream (36% milk fat) at 25 C for light at 470 nm.**\n\n**EXPERIMENTAL RESULTS :**\n\nThe  results of fat percentage present in 20 ml milk sample calculated using Gerber&#39;s method , dairy method and light backscatter method are as follows -\n\nMethod | Buffalo Milk | cow  \n--- | --- | ---  \nGERBER'S Method | 6.9% | 5.00%  \nDairy Machine | 6.85% | 4.90%  \nLight backscatter | 7.00% | 4.95%\n\n![Figure 7]({% static \"/blog/assets/img/light-backscatter-application/7.jpg \"Figure 7\"\" %})\n\nCircuit shown in image is simple and cheaper so this process is in favour of farmer and as well as dairy industry and as the results from all three methods are nearly same. So it is best in compare to other .\n\n**References :**\n\n1) [https://camblab.info/wp/index.php/total-fat-analysis-in-milk-using-the-gerber-method/](https://camblab.info/wp/index.php/total-fat-analysis-in-milk-using-the-gerber-method/)  \n2) [https://mrclab.com/default.aspx](https://mrclab.com/default.aspx)  \n3) [http://ijircce.com/](http://ijircce.com/)",
        "author_name": "Rohit Khoja",
        "slug": "light-backscatter-application"
    },
    {
        "layout": "post",
        "title": "Vortices",
        "author_github": "DarrenFernandes",
        "date": "2018-10-20T21:20:00",
        "image": "/assets/img/",
        "description": "A brief understanding of vortices and their uses",
        "tags": [
            "aerodynamics",
            "vortex",
            "formula 1",
            "aviation",
            "flight",
            "airflow",
            "fluids"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "DarrenFernandes",
        "text": "[{% load static %}]()\n# Vortices\n\n>\u201c_When everything seems to be going against you, remember that an airplane takes off against the wind, not with it_\u201d \u2013 Henry Ford\n\nThis article has been written especially for those who are interested in aerodynamics and also for people who are curious to know how small modifications in design help make an airplane or a car more aerodynamically efficient. The content is from an aerodynamic point of view, in an airflow.  \n\n![Fig.1]({% static \"/blog/assets/img/vortices/1.jpg \"Fig.1\"\" %})  \n\nA Vortex (Fig. 1) is nothing but a region in a fluid which revolves about a certain axis. These vortices often disturb an airflow and induce a lot of drag. Wingtip vortices in an airplane are due to the tendency of air to flow from a higher pressure region to lower pressure region, which induce drag. These vortices might seem to be unnecessary and drag inducing when it comes to vehicle performance. The good news is, these vortices have a good side and have made things easy in so many ways.\nIn what way are these vortices helpful?  \n\n![Fig.2]({% static \"/blog/assets/img/vortices/2.PNG \"Fig.2\"\" %})\n\nA vortex by nature helps keep the air stuck to the surface it is flowing around (Fig. 2). It also acts as a boundary between airflows. Let us consider an F1 car. There are regions such as the intake vents and under a spoiler where smooth airflow is required. By generating vortices in front of these regions or before the airflow splits up, the airflow can be directed smoothly into the required region. This also reduces drag as the airflow sticks to the vehicle body.\n\n![Fig.3]({% static \"/blog/assets/img/vortices/3.PNG \"Fig.3\"\" %})\n\nAir has a natural tendency to flow over a car as shown in Fig. 3. This flow is highly inefficient which induces drag behind the car and also involves the turbulence generated by the high-speed tires. By introducing vortices in specific regions, we can create a boundary between the airflow regions and keep them contained as shown in Fig. 4.\n\n![Fig.4]({% static \"/blog/assets/img/vortices/4.PNG \"Fig.4\"\" %})\n\nThis clever trick is also used in airplanes. In supersonic jets such as the Concorde and Tupolev Tu 144, the airflow separates from the wing surface easily at high speeds. Hence they use vortices to keep the airflow stuck to the wings. (Fig. 5).\n\n![Fig.5]({% static \"/blog/assets/img/vortices/5.JPG \"Fig.5\"\" %})\n\nEconomically speaking, in order to shorten the runway, the aircrafts need to have higher lift at lower speeds. This can be achieved by either increasing the surface area of the wing (increases drag and weight) or increasing the lift coefficient by generating vortices (increase in drag only). It is obvious that generating vortices is more economical.\n\nHow to generate such Vortices? If you look at the front wing of an F1 car (Fig. 6) you can find slit like structures called \u2018Vortex generators\u2019. Air flows over these vortex generators creating high pressure at the top and low pressure at the bottom (Fig. 7). As a result, air leaks around the tips from the top to the bottom creating a spiral motion of air carried by the airflow creating vortices.  \n\n![Fig.6]({% static \"/blog/assets/img/vortices/6.PNG \"Fig.6\" \" %})  \n![Fig.7]({% static \"/blog/assets/img/vortices/7.PNG \"Fig.7\"\" %})  \n![Fig.8]({% static \"/blog/assets/img/vortices/8.jpg \"Fig.8\"\" %})",
        "author_name": "Darren Charles Fernandes",
        "slug": "vortices"
    },
    {
        "layout": "post",
        "title": "Why 'you' need GPU",
        "author_github": "rohit-mp",
        "date": "2018-10-20T18:30:00",
        "image": "/assets/img/",
        "description": "What more can you do with a GPU other than play games?",
        "tags": [
            "GPU",
            "parallel computing"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "rohit-mp",
        "text": "[{% load static %}]()\n# Why you need GPU\n\nYou've probably wanted to buy a laptop/desktop with a GPU to play those awesome high graphics games that your friends keep talking about, and yes your parents wouldv'e probably said \"You're not going to get that. You need to study now\". But is that all the use there is for a GPU?  \n\n### GPU vs CPU  \n\nYeah we all know what they stand for, so let's skip that and get a little technical.  \n\n![GPUvsCPU]({% static '/blog/assets/img/why-you-need-gpu/cpuvsgpu.jpg' %})\n\nTo put it in one line, we can say that if the CPU is being used for computations, then the GPU is a computational warehouse. What I mean is that while a normal CPU has 4-16 cores which can each run a few threads at once, a GPU has around 1000-4000 cores with each core capable oh handling a certain number of threads.  \nYou might want to take a look at this link if you aren't sure about what I just said  [https://www.howtogeek.com/194756/cpu-basics-multiple-cpus-cores-and-hyper-threading-explained/](https://www.howtogeek.com/194756/cpu-basics-multiple-cpus-cores-and-hyper-threading-explained/)\n\n![flops]({% static '/blog/assets/img/why-you-need-gpu/flops.png' %})\n\nNow yes, this means that the graphics in a game will render faster with a GPU than without and everything appears fast and smooth, but it also has other implications. It literally means that the GPU can process more infomation that a CPU in the same time.  \n\nNow does this mean that a GPU is the same as many CPUs put together? This will answer your doubt  \n[https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/)  \nwhich brings us to the point of this article - GPU computing\n\n### GPU Computing  \n\nNow how a GPU can process more information per second might be a doubt to few of you. It is all because of the concept of parallelism. With so many cores in a GPU and with each core capable of running numerous threads, _each thread runs parallely_ and is independent of the other. To understand the significance of this, let us take the example of running errands. Let us say we have 10 items to buy and each item is available in a different store. The CPU version of running these errands would be that a single person would go to each of the stores one by one and buy all the required items whereas the GPU version would be that there are ten people and each person would go to a store and buy the item. Get the idea?  \n\nA simple practical problem where this can be used is calculating the sum of two large arrays. We can have the CPU compute the sum of respective array elements one by one or we can make the GPU compute the sum of the elements parallelly. Yes GPU can make executing a lot more efficient in some cases and in some, the presence of the GPU makes no difference (as in suppose the same store had all 10 items).  \n\nA major practical applications some of you might come across is using GPU for training [neural networks](http://pages.cs.wisc.edu/~bolo/shipyard/neural/local.html)  \n\n![waiting for my neural network to train]({% static '/blog/assets/img/why-you-need-gpu/nntrain.jpeg' %})\n\nYeah we're gonna use GPUs for this.  \n\nThe end goal of all of this is to get high performance and for that, NVIDIA provides a parallel programming or heterogenous computing model called [CUDA](https://www.nvidia.in/object/cuda-parallel-computing-in.html) which help developers create more efficient applications or achieve high performance in their task.  \n\nAlso, note that this doesn't mean everything can be made better with a GPU. Considering different factors such as cost, increase in performance as compared to CPUs, sometimes GPUs are not a good idea. So I hope that you have a better understanding of GPUs now and that gaming isn't the only thing that comes to your mind when you hear GPU.",
        "author_name": "Rohit M P",
        "slug": "why-you-need-gpu"
    },
    {
        "layout": "post",
        "title": "Coronary Stents",
        "author_github": "AravindG",
        "date": "2018-10-27T21:42:00",
        "image": "/assets/img/",
        "description": "What are they? How do they work? Why are they so convenient for angioplasty?",
        "tags": [
            "heart",
            "disease",
            "treatment",
            "surgeries"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "AravindG",
        "text": "[{% load static %}]()\n## How coronary stents  work?\nA Brief Breakdown On One Of The Most Frequented Surgeries.\n\nEver wondered how exactly stents are used during or after an angioplasty? Why did this method replace bypass surgeries for minor heart attacks ? Well, read on to comprehend.\n\n## What is coronary artery disease (CAD)?\nCAD happens when a waxy substance called plaque builds up on the inner walls of the coronary arteries. This causes the arteries to harden and narrow, decreasing blood flow to the heart.\nAs a result , the heart  doesnt getthe blood, oxygen and nutrients it needs, which can lead to chest pain or a heart attack.\n## What are stents and how do they work?\n\n![Fig.1]({% static '/blog/assets/img/coronary-stents/Fig1.jpg' %})\n\nCoronary stents are small, wire, mesh tubes that help widen a clogged artery and restore adequate blood flow to the heart. during the procedure, the stent is placed over a thin, long tube with a balloon tip called a catheter and inserted into an artery in the thigh or arm.\nonce the stent reaches the clogged artery, the balloon is inflated to expand the stent. when the stent reaches the desired size to widen the clogged artery, it is deflated and the balloon is removed.  \n\n![Fig.2]({% static '/blog/assets/img/coronary-stents/Fig2.jpg' %})  \n\nThe stent will stay in place permanently to help prop open the artery and decreases its chance of narrowing agin. over time, the inner lining of the artery will grow over the surface of the stent, making it a permanent part of the artery.\n\n## Materials used\nStainless steel is the most widely used material.  \n\n![Fig.3]({% static '/blog/assets/img/coronary-stents/Fig3.jpg' %})  \n\nBut in balloon expandable stents, cobalt alloy is used.It is composed of cobalt, chromium and other metal elements . compared to stainless steel , cobalt alloy is stronger and has a higher radiocapcity. these properties make thinner struts (framework) possible. thinner struts make the device more flexible and reduce the stents cross-sectional diameter.\nNitinol (fig 4) is another alloy used , which is composed mostly of nickel-titanium as well as cobalt and other metal elements.  Nitinol alloy is the material most commonly used in self-expanding stents.\nNitinol stents have 2 unique merits: super elasticity and shape-memory properties.  \n\n![Fig.4]({% static '/blog/assets/img/coronary-stents/Fig4.jpg' %})  \n\n## Advantages\nIn certain patients, stents reduce the re-narrowing that sometimes occurs after balloon angioplasty or other procedures that use catheters.\npatients who have angioplasty and stents recover from these procedures much faster than patients who have coronary artery bypass surgery. They have much less discomfort too.\n\n## Can stented arteries reclose?\nIn over a third of patients who have  had angioplasty without a stent , the artery that was opened begins to become narrowed again within months of the procedure. Stents help prevent this by the use of drugs along with the them\n\nYou can watch the following video for a better visualization.  \n[https://youtu.be/t-zCBKRg7Cs](https://youtu.be/t-zCBKRg7Cs)",
        "author_name": "Aravind G",
        "slug": "coronary-stents"
    },
    {
        "layout": "post",
        "title": "Electronic Aspirin",
        "author_github": "DeekshaMS",
        "date": "2018-10-27T21:33:00",
        "image": "/assets/img/",
        "description": "Exploring the application of neuro-stimulator in treating chronic headaches'",
        "tags": [
            "electronics",
            "neurostimulators",
            "remote control",
            "SPG"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "DeekshaMS",
        "text": "[{% load static %}]()\n# ELECTRONIC ASPIRIN\n\nFor people who suffer from migraines, cluster headaches and other causes of chronic, excruciating head or facial pain, the &quot;take two aspirins and call me in the morning&quot; method is useless. Doctors have long associated the most severe, chronic forms of headache with the sphenopalatine ganglion (SPG), a facial nerve bundle, but haven&#39;t yet found a treatment that works on the SPG long-term.\n\nWell, there is technology for the rescue: **The Electronic Aspirin**.\n\nIt is a patient-powered tool for blocking SPG signals at the first sign of a headache.\n\n**Working** :\n\nIt consists of two components. A nerve-stimulated implant is placed in the patient's upper gum near the area where the patient experiences the most pain. The procedure to input the **neuro-stimulator** device is fairly simple and is conducted with minimal invasiveness and has minimal side effects.\n\nThe implant is attached to the Sphenopalantine Ganglion (SPG), a collection of nerves deep in the bone cavity of the mid-face area. The permanently embedded implant has a pointed tip which connects with the SPG bundle of nerves. When a patient senses the first sign of a headache, he/she the places a hand held remote device on the cheek area nearest the implant device. When the remote is pressed, a slight electrical charge stimulates nerve cells, which works to block the neurotransmitter signals causing pain.\n\n![image]({% static \"/blog/assets/img/electronic-aspirin/image.jpg \"neuro-stimulator\"\" %})\n\nThe Central Nervous System (CNS) is composed of the brain, brain stem, and spinal cord. The transmission mechanism that drives these communication pathways is separated into two types of signals, namely electrical signals for long pathways through neurons and chemical signals through short pathways between neurons. The shape and biology of neurons are well-adapted to generating, processing, and propagating these signals.\n\n**Neuro-stimulators** are devices dedicated to eliciting action potentials in targeted neurons.\n\nDifferent types of neuro-stimulators:\n\n1. The BION microstimulator\n2. An Ultrasound-Powered Neurostimulator\n3. CMOS-Circuit Neurostimulator\n4. Capacitor-Based Neurostimulators\n5. 16-Channel Neural Stimulator IC\n\nWhen the remote is pressed the electrical signals from the neuro-stimulator effects the neuron of SPG, causing certain blocks in the nervous network. This stops the release of pain causing neuro-transmitters.\n\nThis process does not effect the other functions.The patient has complete control over the remote. Also, the implanted neuro-stimulator can be removed once the frequency and magnitude of migraine attack reduces.\n\n**Possible Side effects:**\n\nWhile there is no threatening risk attached to the use of electronic aspirin, some of the reported symptoms include lower blood pressure, throat numbness, or nausea. However, in rare cases, the symptoms can be a nasal infection, nasal bleeding and a slight increase in headache.\n\nNonetheless, electronic aspirin is an effective solution to facial and head pains. It helps the sufferers to improve their lifestyle and this small invention can help millions in the incorporation of medicine into innovative technology.",
        "author_name": "Deeksha M S",
        "slug": "electronic-aspirin"
    },
    {
        "layout": "post",
        "title": "Web 3.0: The upcoming Web revolution",
        "author_github": "govardhangdg",
        "date": "2018-10-27T21:30:00",
        "image": "/assets/img/",
        "description": "Introduction to Web 3.0",
        "tags": [
            "Web 3.0",
            "Ethereum",
            "Blockchains"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "govardhangdg",
        "comments": true,
        "text": "[{% load static %}]()\n## Web 3.0?? What in the world is that???!!!\nThere is a lot of buzz around web 3.0 and the changes it can bring in the world of tech. But only a few know what it actually is...Before hopping into web 3.0 let us know a little about its predecessors. \n\nThe _Web 1.0_ , which started the whole damn internet and the  _Web 2.0_ that we all use and love so much. Web 1.0 was just a set of _static websites_ . It comprised of a  load of websites with no interactive information. But as the internet got faster and faster, web wasn't only restricted to viewing content.  People started participating. Interactive content started flourishing on the servers worldwide. This is the web we all know and love spending so much time on. This is the age of the  Facebook, Twitter, Youtube...\n\n>_**So, now that we all are enjoying the web 2.0, why don't we let it stay as it is?? Why do we have to disturb this amazingly fun world???**_\n\n**Now, there is a catch. Web 2.0 is not all goodies as one might think.**\n\nThe web 2.0 makes the rich richer and poor poorer. It hinders the flow of money which is needed for a better economy. The consumers' data is being collected and used the way the huge companies want, without the consumers being paid for their data and also they don't have any control over how the data is being used. (Eg: [PRISM](https://en.wikipedia.org/wiki/PRISM_(surveillance_program))) So, *think twice before you ask Alexa anything :p.*\n\nWeb 2.0 removes the opportunities for the budding startups to compete with the tech giants like Google, Microsoft etc. who have the best human and computational resources in the world. Web 2.0 promotes the monopoly in a field.\n![Companies that control internet]({% static '/blog/assets/img/web-3-0/monoploy.jpg' %})\n\n![Data Breach]({% static '/blog/assets/img/web-3-0/data_breach.png' %})\n\n![Data Ownership]({% static '/blog/assets/img/web-3-0/data_own.jpg' %})\n### Web 3.0 to the rescue.\nWeb 3.0 removes all these problems of web 2.0 and also provides much more...\nWeb 3.0 gives:\n1. **No central point of failure.** : Servers are the place where all the data are stored and this makes it very easy for the bad forces to do the misdoings. This is also a problem during natural calamities or anything where the failure in the server makes the whole network go down\n2. **Ownership of data**: the end users have much more control over their data. In web 2.0 the companies have complete control over the users' data and may choose to use it however they want. This also is lucrative as the ad-agencies give them handsome amount of money for the users' data\n3. **Data Permanence**: the data can be stored permanently on the web as the data is actually stored in the network(ipfs link) instead of a single server.\n4. **Good economy**: Web 3.0 allows for the better rotation of money. It provides tokens which can be used for the monetization\n5. **Censorship Resistant**: As the storage is not in a server but in the network , it gets very hard to stop the data from being accessed by people\n\n#### So what are required for building Web 3.0 stuff??\nThe main technology that made the Web 3.0 possible is the **blockchain**. Blockchain which is mainly known in the context of the cryptocurrencies, can also be used to build decentralized applications or the *dApps*. **Ethereum** is the blockchain which promotes the dApps. Ethereum provides *smart contracts* which can be used to build dapps. Ethereum provides the developers with a bunch of technology,called the web 3.0 stack, but that's a discussion for another blog post\n\n![Ethereum Stack]({% static '/blog/assets/img/web-3-0/eth_stack.png' %})\n\n![Ethereum Stack detailed view]({% static '/blog/assets/img/web-3-0/eth_stack2.png' %})\n\nLike any emerging technology is still being refined. For access to the decentralized web, people will only need a seed. This will be a single asset which enables the interaction with dApps and other services. Individuals will still use a web browser to access the internet, and visually it will be Web 2.0 user-friendly.On the surface, the learning curve from 2.0 to 3.0 will be gentle. But behind the scenes, the framework connecting users with digital services are markedly different. Transactions are signed and verified manually, to prevent platforms from stealing away personal information.  \n\n**Web 3.0 alternatives to Web 2.0 things**\n* Instead of Google Drive or Dropbox, we have services like Storj, Siacoin, Filecoin or [IPFS](https://ipfs.io/) technology to distribute and store files.\n* Instead of Skype we have platforms like Experty.io.\n* Instead of WhatsApp and Wechat we have [Status](https://status.im/), that uses the [*Whisper*](https://github.com/ethereum/wiki/wiki/Whisper) protocol in the Ethereum stack\n* Instead of operating systems such as iOS and Android, frameworks such as Essentia.one and EOS provide a gateway to the new web.\n* Akasha or [Steemit](https://steemit.com/) will play the role of Facebook, the Brave browser will serve as Chrome and Ethlance can take over from Upwork.\n\n![Web 3.0 alternatives]({% static '/blog/assets/img/web-3-0/alt.png' %})",
        "author_name": "Govardhan G D",
        "slug": "web-3-0"
    },
    {
        "layout": "post",
        "title": "Drive By Wire",
        "author_github": "SmitRambhiya",
        "date": "2018-11-03T00:00:00",
        "image": "/assets/img/",
        "tags": [
            "Drive by wire",
            "Steer by wire",
            "Autonomous vehicle"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "SmitRambhiya",
        "comments": true,
        "text": "[{% load static %}]()\n# Drive-by-wire\nDrive by wire is a catch all term that can refer to a number of electronic systems that take either augment or completely replace traditional mechanical controls.Instead of using cables, hydraulic pressure and other ways of providing a driver with direct, physical control over the speed or direction of a vehicle, drive-by-wire technology uses electronic controls to activate the brakes, controls the steering, and operate other systems.Although it's well-established in the airline industry, drive-by-wire has been slow in its introduction to the car.\n![fig1]({% static '/blog/assets/img/by_wire/fig1.jpg' %})\n# Throttle by wire\nA typical Electronic Throttle Control(ETC) system consists of three major components: \n(i) an accelerator pedal module (ideally with two or more independent sensors), \n(ii) a throttle valve that can be opened and closed by an electric motor and \n(iii) a powertrain or engine control module (PCM or ECM)\nThe ECM is a type of electronic control unit (ECU), which is an embedded system that employs software to determine the required throttle position by calculations from data measured by other sensors, including the accelerator pedal position sensors, engine speed sensor, vehicle speed sensor, and cruise control switches. The electric motor is then used to open the throttle valve to the desired angle via a closed-loop control algorithm within the ECM\n\n# Brake by wire\nOnce the driver inputs a brake command to the system via a human-machine interface - HMI (e.g. the brake pedal), four independent brake commands are generated by the ECU based on high level brake functions such as anti-lock braking system (ABS) or vehicle stability control(VSC). These command signals are sent to the four electric calipers (e-calipers) via a communication network. This technology is widely used on all hybrid and battery electric vehicles. \n \n# Shift by wire\nShift by wire is the system by which the transmission modes are engaged/changed in an automobile through electronic controls without any mechanical linkage between the gear shifting lever and the transmission. This eliminates routing space required for housing the mechanical linkages between the shifter and the transmission and provides effortless shifting through the press of a button or through knobs. Companies like BMW, Audi, Mercedes, Jaguar, Volvo have adopted this type of transmission system.\n![fig2a]({% static '/blog/assets/img/by_wire/fig2a.jpg' %})\n![fig2b]({% static '/blog/assets/img/by_wire/fig2b.jpg' %})\n![fig2c]({% static '/blog/assets/img/by_wire/fig2c.jpg' %})\n# Steer by wire\nThe first production vehicle to implement this was the Infiniti Q50. When the driver turns the steering wheel, sensors relay the input to control units. The control units then assess this input, along with other key information such as vehicle speed and yaw rate, before delivering suitable control signals to the system that physically actuates the steering rack.\nWhile all of this is being carried out, the electronics filter out unwanted feedback from the front wheels and deliver relevant signals to a force generator at the steering wheel. This provides the driver with the appropriate resistance and feedback.\nIn most cases, the steer-by-wire rack is driven by an electric motor - although some set-ups may use multiple motors, affording an extra degree of redundancy.\nThe cycle of monitoring inputs and commanding a response is extremely quick; in the case of Infiniti's system, it is claimed to be capable of making up to 1,000 adjustments per second.\n![fig3]({% static '/blog/assets/img/by_wire/fig3.jpg' %}) \n# Advantages \n- Drive-by-wire is a key enabling technology for autonomous vehicle development.\n- Eliminating mechanical linkages can provide savings in weight.\n- Reduces fuel consumption.\n- Stretches out the time between service visits for things like mechanical maintenance and other adjustments\n\nConsidering all the advantages and the current situation, Market researchers have predicted that the global drive-by-wire market will likely become worth US$59.5 billion revenue by 2025 from US$18.22 billion in 2016 by clocking a robust 15.0% Compound Annual Growth Rate(CAGR) during the period between 2017 and 2025.",
        "author_name": "Smit Rambhiya",
        "description": "",
        "slug": "by-wire"
    },
    {
        "layout": "post",
        "title": "KMP Algorithm",
        "author_github": "Chans321",
        "date": "2018-11-03T00:00:00",
        "image": "/assets/img/",
        "description": "An introduction to KMP algorithm",
        "tags": [
            "KMP",
            "Strings",
            "Competitive Programming",
            "C++"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "Chans321",
        "use_math": true,
        "comments": true,
        "text": "[{% load static %}]()\nI'd like to talk about an algorithm that I believe should be in any Software Engineer's toolkit. That is, the KMP algorithm named after its three inventors: Knuth-Morris-Pratt. The algorithm was designed to solve efficiently a very common problem. That is the problem of finding whether or not one string is a substring of a different string.\n\n### Brute Force\n\nLet's start first with the original problem. We would like to implement the function: int  strstr(const string &Str, const string& Pattern); \nThe brute force solution of this function entails two loops, one over Str and one over pattern. Let's see it:\n\n![KMP brute force]({% static '/blog/assets/img/kmp/p1.png' %})\n\nWhat is the runtime complexity of this algorithm?\n\nSince we are running in two loops and for each i we are trying all j's, we have a **worst-case complexity of O(NM)** where N is the size of Str and M is the size of Pat. Can you spot where the inefficiency comes from?  The problem is that if we can't find a match we have to go back (with i) and start over from where we left off when we started the match. Why are we doing this? let's look at two examples:\n\n1. str = \"abcbcefg\" and pat = \"bce\".  In this case we start the match at i = 1 and j = 0, we continue to i = 2 and j = 1, but then at i = 3 and j = 2 we have a mismatch. Our algorithm will detect this mismatch and will set \"i\" to be 2. But what's the point? for i = 2 we already know that there will be no match. In fact, based on strwe should not be setting \"i\" to a previous index, but we could just as well continue advancing i.\n\n2. str = \"abababc\" pat = \"ababc\". In this case, we will start matching at i = 0 and will reach i = 4 when we detect a mismatch. We will then return to i = 1 and start again. When we reach i = 2 we will detect that the strings match. The observation is, that we should not have even bothered returning to i = 1 we could have set \"i\"to 2. But, if we look even closer we may notice that after we matched str = \"abab\" and pat = \"abab\" we already have one pair of \"ab\" in pat which we know that can be used for future matches.\n\n### Let's try and make our code efficient!\nThe two examples are a great way to explain how KMP works and why it is so efficient. KMP starts off by building a prefix table for pat. This prefix table will be used to indicate for each index \"i\" how much we have matched thus far and use this information to jump back only by the amount needed.\n\nLets see how the prefix table will look like for different patterns:\n\n1. pat = \"ababc\" will result in the table: {0,0,1,2,0}\n2. pat = \"ississi\" will result in the table: {0,0,0,1,2,3,4}\n3. pat = \"abcd\" will result in the table: {0,0,0,0}\n\nNotice what is happening here. When we detect a prefix that we have seen (such as the second \"ab\" in \"ababc\") the numbers in the table indicate the size of that prefix. So if we start matching \"abababc\" with \"ababc\" for i = 4 and j = 4 we see that 'a' is different from 'c', and hence we have a mismatch. Using the table above we see that the table at index j - 1 = 3 has the value 2. We can have \"j\" jump back two indexes from 4 to 2, and continue our match from there. We don't have to touch \"i\" anymore, we just modify \"j\" based on the maximal size of the prefix that we have seen thus far.\n\nLet us see the code to generate the Prefix table:\n\n![KMP Prefix table]({% static '/blog/assets/img/kmp/p2.png' %})\n\nWe allocate the prefix table to be the size of \"Pat\". We use two indexes to fill the table. The first is \"i\", it will indicate the end of the current prefix. The second is \"j\" which will be used to find the next occurance of the prefix. Notice that \"j\" starts from index 1. As we are traversing we check to see whether Pat[i] == Pat[j]. If they are, then we know that there is a prefix of size i + 1 that we have encountered at index \"j\". We update the table at index \"j\" to indicate this prefix, and update \"i\" as well. If they are not the same, then we have two cases: First, if i == 0, it means that we have not found any prefix thus far and we just increment \"j\". However, if  i > 0, we keep \"j\" where it is and make \"i\" go back to the index where the prefix  has started and compare it again with \"j\" .\n\n### Complete code\n\nWe are now ready to see the full algorithm and how it uses the prefix table:\n\n![KMP code]({% static '/blog/assets/img/kmp/p3.png' %})\n\nThe runtime complexity of this new version in the worst case will be **O(N)**.",
        "author_name": "TSS Chandana",
        "slug": "kmp-algorithm"
    },
    {
        "layout": "post",
        "title": "Supercapacitors",
        "author_github": "NamanAgarwal",
        "date": "2018-11-03T00:00:00",
        "image": "/assets/img/",
        "description": "A peep into the future of power storage",
        "tags": [
            "Supercapacitors",
            "Energy Storage",
            "Futurology",
            "Batteries",
            "Renewables"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "NamanAgarwal",
        "comments": true,
        "text": "[{% load static %}]()\n## Introduction\n\nA supercapacitor is a specially designed capacitor which has an enormous capacitance. While capacitors used in everyday life tend to have values in the nano or microfarad range, supercapacitors can have values of upto 12,000 F depending on their application.\n\n![img1]({% static '/blog/assets/img/supercap/img1.jpeg' %})\n\n## Construction\n\nLike an ordinary capacitor, a supercapacitor has two plates that are separated. The plates are made from metal coated with a porous substance such as powdery, activated charcoal, which effectively gives them a bigger area for storing much more charge.\n\nWhat about the separator between the plates? In an ordinary capacitor, the plates are separated by a relatively thick dielectric made from something like mica , a thin plastic film, or even simply air. When the capacitor is charged, positive charges form on one plate and negative charges on the other, creating an electric field between them. The field polarizes the dielectric, so its molecules line up in the opposite direction to the field and reduce its strength. That means the plates can store more charge at a given voltage.\n\nIn a supercapacitor, there is no dielectric as such. Instead, both plates are soaked in an electrolyte and separated by a very thin insulator (which might be made of carbon, paper, or plastic). When the plates are charged up, an opposite charge forms on either side of the separator, creating what's called an electric double-layer, maybe just one molecule thick.\n\nWe all know of the standard capacitance equation : c=eA/d\n\nSupercapacitors drastically increase the area **A** and the value of **e**, while reducing the distance **d** to facilitate such high capacitance values.\n\n\n\n## Supercapacitors vs batteries\n\nThe big advantage of a supercapacitor is that it can store and release energy almost instantly, much more quickly than a battery. That's because a supercapacitor works by building up static electric charges on solids, while a battery relies on charges being produced slowly through chemical reactions, often involving liquids.\n\nBatteries have a higher energy density (they store more energy per unit mass) but supercapacitors have a higher power density (they can release energy more quickly). That makes supercapacitors particularly suitable for storing and releasing large amounts of power relatively quickly, but batteries are still king for storing large amounts of energy over long periods of time.\n\nSince supercapacitors work electrostatically, rather than through reversible chemical reactions, they can theoretically be charged and discharged any number of times (specification sheets for commercial supercapacitors suggest you can cycle them perhaps a million times). They have little or no internal resistance, which means they store and release energy without using much energy and work at very close to 100 percent efficiency (97-98 percent is typical).\n\n![img2]({% static '/blog/assets/img/supercap/img2.jpeg' %})\n\n## Applications\n\n### Energy recovery\n\nA primary challenge of all transport is reducing energy consumption and reducing emissions. Recovery of braking energy helps with both. This requires components that can quickly store and release energy over long times with a high cycle rate. Supercapacitors fulfill these requirements and are therefore used in a lot of applications in all kinds of transportation.\n\n### Consumer electronics\n\nSupercapacitors are frequently designed into these products for memory protection. Internal back-up power is another common application. The supercapacitor can be used either as a battery replacement or as a short-term redundant backup supply. Most recently, Samsung started using supercapacitors in the S Pen which comes along with the Galaxy Note 9. The supercapacitor enables the S Pen to be used for almost 30 minutes, with a mere 40 seconds of charge.\n\n### Transportation\n\nIn early 2005 Shanghai tested a new form of electric bus called capabus that runs without power lines using large onboard supercapacitors that partially recharge whenever the bus is at a stop (under so-called electric umbrellas), and fully charge in the terminus. In 2006, two commercial bus routes began to use the capabuses; one of them is route 11 in Shanghai. It was estimated that the supercapacitor bus was cheaper than a lithium-ion battery bus, and one of its buses had one-tenth the energy cost of a diesel bus with lifetime fuel savings of $200,000.\n\n## Conclusion\n\nThe field of supercapacitors is one that needs a significant amount of research to develop further and be economically feasible to manufacture at a large scale. It should, however see great innovations with the introduction of materials such as graphene. The future sure does look exciting as it does offer a considerable amount of application potential, especially in the fields of renewable energy and electric/hybrid vehicles.\n\nEven **Elon Musk** once said, \"If I were to make a prediction, I'd think there's a good chance that it is not batteries, but supercapacitors.\"",
        "author_name": "Naman Agarwal",
        "slug": "supercap"
    },
    {
        "layout": "post",
        "title": "Demystifying Deconvolutions",
        "author_github": "deepakkavoor",
        "date": "2018-11-10T00:00:00",
        "image": "/assets/img/",
        "description": "An introduction to Deconvolutions",
        "tags": [
            "Deep Learning",
            "Convolution",
            "Up-sampling",
            "Neural Networks"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "deepakkavoor",
        "comments": true,
        "text": "[{% load static %}]()\n# Demystifying Deconvolutions\n\nThis article mainly discusses about ***Up-sampling with Deconvolution***. If you've heard about the term Deconvolution, and are willing to learn more about it, please read on.\n\nThe contents of the this article is as follows:\n\n* The need for Up-sampling\n* Convolution Operation\n* Going backward\n* Convolution Matrix\n* Transposed Convolution Matrix\n\nDeconvolution is also known as **Transposed Convolution** and **Fractionally-strided Convolution**.\n\n\n## The need for Up-sampling\n\nWhen we use neural networks to process or generate images, there is a decrease in the size of the intermediate images in the subsequent layers.\n\n![fig1]({% static '/blog/assets/img/deconv/p1.png' %})\n\nSometimes, when an output result having the size same as the input image is to be generated, using the learned parameters, up-sampling is useful. This technique is used to up-sample images from low-resolution to high-resolution.\n\n## Convolution Operation\n\nI will use a simple example here to discuss about the convolution operation.\n\nSuppose we have a 4x4 matrix, and we wish to convert it to a smaller 2x2 matrix. This can be accomplished by using a 3x3 \"**kernel**\" as shown below.\n\n![fig2]({% static '/blog/assets/img/deconv/p2.png' %})\n\nThe convolution operation calculates the sum of the element-wise multiplication between the input matrix and kernel matrix. Since we can slide the kernel twice through the input matrix, the resulting output will be of size 2x2.\n\n![fig3]({% static '/blog/assets/img/deconv/p3.png' %})\n\n**One important point of such convolution operation is that the positional connectivity exists between the input values and the output values.**\n\nFor example, the top left values in the input matrix (4, 5, 8, 1, 8, 8, 3, 6, 6) affect the top left element in the output matrix (122). This spatial relationship is essential; in images, any set of features on the top-left corner must be showcased in the top-left corner of the resulting smaller image (in the form of learnable parameters).\n\nMore concretely, the 3x3 kernel is used to connect the 9 values in the input matrix to 1 value in the output matrix. **A convolution operation forms a many-to-one relationship.** Let\u2019s keep this in mind as we need it later on.\n\n## Going backward\n\nNow, suppose we want to go the other direction. We want to associate 1 value in a matrix to 9 values in another matrix. **It\u2019s a one-to-many relationship.** This is like going backward of convolution operation, and it is the core idea of **transposed convolution**.\n\nFor example, we could up-sample a 2x2 matrix to a 4x4 matrix, maintaining the 1-to-9 relationship.\n\n![fig4]({% static '/blog/assets/img/deconv/p4.png' %})\n\nTo talk about how such an operation can be performed, we need to understand the **convolution matrix** and the **transposed convolution matrix**.\n\n## Convolution Matrix\n\nWe can view the process of converting a 4x4 matrix to a 2x2 matrix, in another way.\nWe can express a convolution operation using a matrix. It is nothing but a kernel matrix rearranged so that we can use a matrix multiplication to conduct convolution operations.\n\n![fig5]({% static '/blog/assets/img/deconv/p5.png' %})\n\nWe rearrange the 3x3 kernel into a 4x16 matrix as below:\n\n![fig6]({% static '/blog/assets/img/deconv/p6.png' %})\n\nEach row defines a convolution operation, separated by one or more zeros.\n\nThe first step is to flatten the input matrix of size 4x4 into a column vector (16x1).\n\n![fig7]({% static '/blog/assets/img/deconv/p7.png' %})\n\nWe can matrix-multiply the 4x16 convolution matrix with the 1x16 input matrix (16 dimensional column vector).\n\n![fig8]({% static '/blog/assets/img/deconv/p8.png' %})\n\nThe output 4x1 matrix can be reshaped into a 2x2 matrix which gives us the same result as before.\n\n![fig9]({% static '/blog/assets/img/deconv/p9.png' %})\n\nIn short, a convolution matrix is nothing but rearranged kernel weights, and a convolution operation can be expressed using the convolution matrix.\n\n## Transposed Convolution Matrix\n\nWe want to go from a smaller 2x2 matrix to a larger 4x4 matrix. **But there is one very important thing to consider; we want to maintain the 1-to-9 relationship**.\n\nLet us reshape the 2x2 input matrix to a 4x1 column vector as before. If we have a 16x4 matrix with us, we could perform the operation\n\n(16x4)  x (4x1) = (16x1)\n\nto get a 16x1 column vector, which we could then resize to the desired 4x4 output.\n\nIn order to get this 16x4 matrix, we use the convolution matrix used earlier, whose dimensions were 4x16. If the convolution matrix is denoted by **C**, then its transpose **C<sup>T</sup>** has the shape (16x4).\n\nThrough careful observation, you can convince yourself that the transposed matrix **C<sup>T</sup>** connects 1 value in the input to 9 values in the output.\n\n![fig10]({% static '/blog/assets/img/deconv/p10.png' %})\n\nThe output can be reshaped into 4x4.\n\n![fig11]({% static '/blog/assets/img/deconv/p11.png' %})\n\n**Note:** The actual weight values in the matrix does not have to come from the original convolution matrix. They are learnable. What is important is that the weight layout is transposed from that of the convolution matrix (4x16 to 16x4).\n\n## References:\n\n**A guide to convolution arithmetic for deep learning**\n\n[https://arxiv.org/abs/1603.07285](https://arxiv.org/abs/1603.07285)\n\n**Convolution Arithmetic Tutorial**\n\n[http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)\n\n**Up-sampling with Transposed Convolution**\n\n[https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0](https://towardsdatascience.com/up-sampling-with-transposed-convolution-9ae4f2df52d0)\n\n**Convolutional Neural Networks**\n\n[http://cs231n.github.io/convolutional-networks/](http://cs231n.github.io/convolutional-networks/)\n\n\n**An Introduction to different Types of Convolutions in Deep Learning**\n\n[https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d](https://towardsdatascience.com/types-of-convolutions-in-deep-learning-717013397f4d)",
        "author_name": "Deepak K",
        "slug": "deconv"
    },
    {
        "layout": "post",
        "title": "MEMS",
        "author_github": "avinashkumar",
        "date": "2018-11-10T00:00:00",
        "image": "/assets/img/",
        "description": "Micro Electro Mechanical Systems",
        "tags": [
            "MEMS"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "avinashkumar",
        "comments": true,
        "text": "[{% load static %}]()\n# MEMS (Micro Electro Mechanical Systems)\nAn introduction to field of micro systems and their applications.\n\n>**\"There is plenty of room at the bottom.\"**\n> - Richard Feynman \n\nThis article deals with the emerging field of micro-electromechanical systems, or MEMS. MEMS is a process technology used to create tiny integrated devices or systems that combine mechanical and electrical components.\nThese devices (or systems) can sense, control and actuate on the micro scale, and generate effects on the macro scale.\nMEMS has been identified as one of the most promising technologies for the 21st Century and has the potential to revolutionize both industrial and consumer products by combining silicon-based microelectronics with micromachining technology. Its techniques and microsystem-based devices have the potential to dramatically effect of all our lives and the way we live.\n\n![fig1]({% static '/blog/assets/img/mems/fig1.jpg' %})\n\nWhile the device electronics are fabricated using \u2018computerchip\u2019 IC technology, the micromechanical components are fabricated by sophisticated manipulations of silicon and other\nsubstrates using micromachining processes.In the most general form, MEMS consist of mechanical\nmicrostructures, microsensors, micro actuators andmicroelectronics, all integrated onto the same silicon chip.Microsensors detect changes in the system\u2019s environment by measuring mechanical, thermal, magnetic, chemical or electromagnetic information or phenomena. Microelectronics process this information and signal the micro actuators to react and create some form of changes to the environment.\n\n![fig2]({% static '/blog/assets/img/mems/fig2.jpg' %})\n\nHowever, MEMS is not just about the miniaturization of mechanical components or making things out of silicon (in fact,the term MEMS is misleading as many micromachined devices\nare not mechanical in any sense). MEMS is a manufacturing technology; a paradigm for designing and creating complex mechanical devices and systems as well as their integrated\nelectronics using batch fabrication techniques.Over the past few years some highly innovative products have emerged from bio MEMS companies for revolutionary applications that support major societal issues including DNA sequencing, drug discovery, and water and environmental monitoring.\n\n![fig3]({% static '/blog/assets/img/mems/fig3.jpg' %})",
        "author_name": "Avinash Kumar",
        "slug": "mems"
    },
    {
        "layout": "post",
        "title": "Organic Photovoltaics",
        "author_github": "IteeshaAshwath",
        "date": "2018-11-10T00:00:00",
        "image": "/assets/img/",
        "description": "Can OPV be the first step back to our roots? The path from Silicon world back to the organics?",
        "tags": [
            "Organic Photovoltaics",
            "Excitons",
            "Photo-active layer"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "IteeshaAshwath",
        "comments": true,
        "text": "[{% load static %}]()\n# Organic Photovoltaics\n\n## Introduction :\n\nPhotovoltaics is the process of directly converting sunlight into electricity without any heat engine to interfere. Photovoltaic devices are rugged and simple in design requiring very little maintenance and their biggest advantage being their construction as stand-alone systems to give outputs from microwatts to megawatts. Hence they are used for a vast array of applications resulting in the increase in demand for photovoltaics. Recently, ORGANIC PHOTOVOLTAICS (OPV), has been the subject of active research as this technology has the potential to spawn a new generation of low-cost, solar-powered products with thin and flexible form factors.\n\n## Solar Technology :\n\nSolar technologies are currently dominated by wafer-size single-junction solar cells based on crystalline silicon that are assembled into large area modules. However, other semiconductor materials and devices are under active investigation in order to further reduce the cost of produced electricity by increasing the power conversion efficiency, reducing the amount of absorbing material needed, and lowering the assembly cost of modules. Thin-film photovoltaic technologies, referred to as second-generation photovoltaics, are based on inorganic semiconductor materials that are more absorbing than crystalline silicon and can be processed directly onto large area substrates. Despite the laboratory demonstration of cells with high efficiencies (19+ACU- for CIGS and 16+ACU- for CdTe), the controlled manufacturing of second-generation cells remains a challenge and their commercial use is growing but not as widespread yet.\n\n## The advent of Organic Photovoltaics (OPV) :\n\nOver the past two decades, the science and engineering of organic semiconducting materials have advanced very rapidly, leading to the demonstration and optimization of a range of organics-based solid-state devices, including organic light-emitting diodes (OLEDs), field-effect transistors, photodiodes, and photovoltaic cells. Seeded in the 1960s, fundamental studies on the optical and electronic properties of model organic molecules such as acenes- molecules based on up to five fused benzene rings -this area of research gained significant momentum when high-purity small organic molecules with tailored structure and properties were synthesized and processed at room temperature into thin films using physical vapor deposition techniques.\n\n![fig1]({% static '/blog/assets/img/opv/1.jpg' %})\n\nThe low-temperature processing of either organic small molecules from the vapor phase or polymers from solution confers organic semiconductors with a critical advantage over their inorganic counterparts, as the high-temperature processing requirements of the latter limit the range of substrates on which they can be deposited.\n\n## Third Generation Technologies :\n\nThe organics-based approaches and those that do not rely on conventional single p+AJY-n junctions are often referred to as third-generation technologies. They include: (i) the dye-sensitized solar cells which are electrochemical cells that require an electrolyte+ADs- (ii) multijunction cells fabricated from group IV and III+AJY-V semiconductors+ADs- (iii) hybrid approaches in which inorganic quantum dots are doped into a semiconducting polymer matrix or by combining nanostructured inorganic semiconductors such as TiO2 with organic materials+ADs- (iv) all-organic solid-state approaches.\n\n## What is different in OPV?\n\nA key difference in the physics of organic semiconductors compared to their inorganic counterparts is the nature of the optically excited states. The absorption of a photon in organic materials leads to the formation of an exciton, i.e., a bound electron-hole pair. The exciton binding energy is typically large, on the order of or larger than 500 meV such binding energies represent twenty times or more the thermal energy at room temperature, kT(300 K) +AD0- 26 meV to be compared with a few meV in the case of inorganic semiconductors. Consequently, optical absorption in organic materials does not lead directly to free electron and hole carriers that could readily generate an electrical current. Instead, to generate a current, the excitons must first dissociate. The excitonic character of their optical properties is a signature feature of organic semiconductors, which has impacted the design and geometry of organic photovoltaic devices for the past decades.\n\n![fig2]({% static '/blog/assets/img/opv/2.jpg' %})\n\n## Organic Photovoltaic Devices :\n\nA typical OPV has a layered structure involving: a substrate, a transparent bottom electrode, photoactive layer and top metal electrode. Light is converted to electrical current in the photoactive layer, which has a typical thickness of +AH4- 100 nm. In efficient OPVs, this layer is a composite of two or more semiconductors (electron donors and acceptors) mixed together to form a nanostructured (or bulk) heterojunction for charge generation.\n\n![fig3]({% static '/blog/assets/img/opv/3.jpg' %})\n\nPhotocurrent generation in OPVs is a multistep process that can be summarised as follows. Initially, photon absorption by a molecule in the active layer promotes an electron to an excited state, resulting in a localized and tightly bound electron-hole pair (a so-called molecular exciton). To contribute to the photocurrent from the solar cell, the exciton must be dissociated, which requires the Coulomb interaction between the hole and electron (+AH4- 0.5 eV) to be overcome. This step can occur at an interface between electron accepting and donating semiconductors, resulting in the transient formation of a charge-transfer state whereby the electron and hole exist on different molecules. The dissociation of the charge-transfer state is then followed by (1) charge transport - where electrons and holes +ACYAIw-39+ADs-hop+ACYAIw-39+ADs- between adjacent electron acceptor and donor molecules respectively +AJY- and (2) charge extraction at the electrodes of the solar cell. For efficient overall performance, all of these steps should be well characterized and carefully optimized.\n\n![fig4]({% static '/blog/assets/img/opv/4.jpg' %})\n\n## Advantages of Organic photovoltaics :\n\nThe primary advantage of OPV technology over inorganic counterparts is its ability to be utilized in large areas and semi-transparent, flexible solar modules. Additionally, the manufacturing cost can be reduced for organic solar cells due to their lower cost compared to silicon-based materials and the ease of device manufacturing. The OPVs also offer low thermal budget, solution processing and a very high speed of processing.\n\n## Conclusion :\n\nThe future of organic solar cells as a pervasive technology for portable power will largely rely on their economic potential. This depends on a number of intricate factors such as efficiency, manufacturing cost, lifetime, form factor, weight, scalability, and sustainable manufacturing.\n\nTo summarize, organic photovoltaics provides an exciting playground at the frontiers of science, engineering, and technology. Advances in the near term are likely to lead to solar cells with efficiencies close to 10+ACU- in single heterojunction geometries and efficiencies up to 15+ACU- in tandem-cell geometries. If organic photovoltaics holds its promise, it can soon become a ubiquitous, clean and sustainable technology for portable power and potentially provide large-scale energy production for future generations.",
        "author_name": "Iteesha Ashwath",
        "slug": "organic-photovoltaics"
    },
    {
        "layout": "post",
        "title": "Ballistics",
        "author_github": "RahulRL",
        "date": "2018-12-01T00:00:00",
        "image": "/assets/img/",
        "description": "The term ballistics refers to the science of the travel of a projectile in flight. The flight path of a bullet includes: travel down the barrel, path through the air, and path through a target.",
        "tags": [
            "Ballistics"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "RahulRL",
        "text": "[{% load static %}]()\nBallistics is the science of the propulsion, flight, and impact of projectiles especially bullets, unguided bombs or rockets. It is divided into several disciplines. _Internal and external ballistics_, respectively, deal with the propulsion and the flight of projectiles. The transition between these two regimes is called _intermediate ballistics_. _Terminal ballistics_ concerns the impact of projectiles which is a separate category that study the wounding of personnel\n\n## Internal Ballistics\n\nA fire arm cartridge contains four components namely _Primer, Case, Propellent_ and _Bullet__._ The basic working of a fire arm is that when the trigger is pressed the firing pin of the gun strikes the primer of the cartridge. When the firing pin hit the primer, the primer explodes which will ignite the propellent (In some modern guns electric signals are also used for igniting the powder). The ignition of propellent produce gases which pushes the bullet out of the barrel.  In fire arms the internal ballistics deals with the explosion of primer, ignition of the firing till the bullet exits the barrel.\n![1]({% static '/blog/assets/img/ballistics/1.jpg' %})\nThe study of internal ballistics is important in the design of the chamber and barrel of the fire arm.  Bullets fired from a rifle will have more energy than similar bullets fired from a handgun. More powder can also be used in rifle cartridges because the bullet chambers can be designed to withstand greater pressures (50,000 to 70,000 for rifles psi vs. 30,000 to 40,000 psi for handgun chamber). Higher pressures require a bigger gun with more recoil that is slower to load and generates more heat that produces more wear on the metal. It is difficult in practice to measure the forces within a gun barrel, but the one easily measured parameter is the velocity with which the bullet exits the barrel (muzzle velocity). The controlled expansion of gases from burning gunpowder generates pressure (force/area). The area here is the base of the bullet (equivalent to diameter of barrel) and is a constant. Therefore, the energy transmitted to the bullet (with a given mass) will depend upon mass times force times the time interval over which the force is applied. The last of these factors is a function of barrel length. Bullet travel through a gun barrel is characterized by increasing acceleration as the expanding gases push on it, but decreasing pressure in the barrel as the gas expands. Up to a point of diminishing pressure, the longer the barrel, the greater the acceleration of the bullet.\n\n\n### Rifling of barrel\n\nIn firearms, rifling is the helical groove pattern that is machined into the internal (bore) surface of a guns barrel, for the purpose of exerting torque and thus imparting a spin to a projectile around its longitudinal axis during shooting. This spin to serves gyroscopically stabilize the projectile by conservation of angular momentum, improving its  aerodynamic stability and accuracy over smooth bore designs.\n![2]({% static '/blog/assets/img/ballistics/2.jpg' %})\n\n\n## Transitional ballistics\n\nThe study of transitional ballistics is used for design of accessories like flash suppressors, sound suppressors and recoil compensators. The study of transitional ballistics is difficult as it involves a number of variables which is not properly defined. When the bullet reaches the muzzle of the barrel, the escaping gases are still, in many cases, at hundreds of atmospheres of pressure. Once the bullet exits the barrel, breaking the seal, the gases are free to move past the bullet and expand in all directions. This expansion is what gives gunfire its explosive sound and is often accompanied by a bright flash as the gases combine with the oxygen in the air and finish combusting.\n\nThe propellant gases continue to exert force on the bullet and firearm for a short while after the bullet leaves the barrel. One of the essential elements of accurizing a firearm is to make sure that this force does not disrupt the bullet from its path\n\n## External Ballistics\n\nExterior ballistics deals with that portion of the bullet\u2019s flight from the moment it leaves the muzzle until its impacts. In a handgun, especially a defensive arm, the flight of the bullet is rather short, and the atmosphere and laws of physics have less opportunity to influence things. In a high-powered rifle, these forces have plenty of time to show their effects. To be able to put that bullet precisely on target, you need to fully understand how the bullet will react to your environment at a wide variety of ranges. Entire lifetimes have been spent in pursuit of the accurate prediction of bullet trajectories. While the answers are out there, the science of ballistics is constantly evolving, and our ability to quantify and predict the values associated with the bullet\u2019s path is improving all the time. When in flight, the main forces acting on the projectile are gravity, drag and if present wind. Gravity imparts a downward acceleration on the projectile, causing it to drop from the line of sight. Drag or the air resistance decelerates the projectile with a force proportional to the square of the velocity (or cube, or even higher powers of v, depending on the speed of the projectile). Wind makes the projectile deviate from its trajectory. During flight, gravity, drag and wind have a major impact on the path of the projectile, and must be accounted for when predicting how the projectile will travel.\n![3]({% static '/blog/assets/img/ballistics/3.png' %})\n\nThe maximum practical range of all small arms and especially high-powered sniper rifles depends mainly on the aerodynamic or ballistic efficiency of the spin stabilized projectiles used. Long-range shooters must also collect relevant information to calculate elevation and windage corrections to be able to achieve first shot strikes. The data to calculate these fire control corrections has a long list of variables including:\n\n- [Ballistic coefficient](http://en.wikipedia.org/wiki/Ballistic_coefficient) of the bullets used\n- Height of the sighting components above the rifle bore axis\n- The zero range at which the sighting components and rifle combination were sighted in bullet weight\n- Actual [muzzle velocity](http://en.wikipedia.org/wiki/Muzzle_velocity) (powder temperature affects muzzle velocity, primer ignition is also temperature dependent)\n- Range to target\n- Supersonic range of the employed gun, cartridge and bullet combination\n- Inclination angle in case of uphill/downhill firing\n- Target speed and direction.\n- [Wind](http://en.wikipedia.org/wiki/Wind) speed and direction (main cause for horizontal projectile deflection and generally the hardest ballistic variable to measure and judge correctly. Wind effects can also cause vertical deflection.)\n- Air [temperature](http://en.wikipedia.org/wiki/Temperature), [pressure](http://en.wikipedia.org/wiki/Pressure), [altitude](http://en.wikipedia.org/wiki/Altitude) and [humidity](http://en.wikipedia.org/wiki/Humidity) variations (these make up the ambient [air density](http://en.wikipedia.org/wiki/Air_density))\n- [earth&#39;s gravity](http://en.wikipedia.org/wiki/Earth%27s_gravity) (changes slightly with [latitude](http://en.wikipedia.org/wiki/Latitude) and [altitude](http://en.wikipedia.org/wiki/Altitude)).\n- [Gyroscopic](http://en.wikipedia.org/wiki/Gyroscopic) drift (horizontal and vertical plane gyroscopic effect often know as [spin drift](http://en.wikipedia.org/wiki/Magnus_effect) - induced by the barrels twist direction and twist rate).\n- [Coriolis effect](http://en.wikipedia.org/wiki/Coriolis_effect) drift ([latitude](http://en.wikipedia.org/wiki/Latitude), direction of fire and [hemisphere](http://en.wikipedia.org/wiki/Hemisphere) data dictate this effect).\n- Lateral throw-off (dispersion that is caused by mass imbalance in the applied projectile).\n- Aerodynamic jump (dispersion that is caused by lateral (wind) impulses activated during free flight at or very near the muzzle).\n- The inherent potential accuracy and adjustment range of the sighting components.\n- The inherent potential accuracy of the rifle.\n- The inherent potential accuracy of the ammunition.\n- The inherent potential accuracy of the computer program and other firing control components used to calculate the trajectory.\n\n\n\n## Terminal ballistics\n\nIt deals with the effect of the projectiles on target especially the pattern of wound made by the bullet. That\u2019s why it is also called wound ballistics. Bullets produce tissue damage in three ways.\n\n### Laceration and crushing - \nTissue damage through laceration and crushing occurs along the path or \"track\" through the body that a projectile, or its fragments, may produce. The diameter of the crush injury in tissue is the diameter of the bullet or fragment, up to the long axis.\n\n### Cavitation - \nA \"permanent\" cavity is caused by the path (track) of the bullet itself with crushing of tissue, whereas a \"temporary\" cavity is formed by radial stretching around the bullet track from continued acceleration of the medium (air or tissue) in the wake of the bullet, causing the wound cavity to be stretched outward. For projectiles traveling at low velocity the permanent and temporary cavities are nearly the same, but at high velocity and with bullet yaw the temporary cavity becomes larger.\n\n### Shock waves - \nShock waves compress the medium and travel ahead of the bullet, as well as to the sides, but these waves last only a few microseconds and do not cause profound destruction at low velocity. At high velocity, generated shock waves can reach up to 200 atmospheres of pressure. However, bone fracture from cavitation is an extremely rare event.  The ballistic pressure wave from distant bullet impact can induce a concussive-like effect in humans, causing acute neurological symptoms.\n![4]({% static '/blog/assets/img/ballistics/4.JPG' %})",
        "author_name": "Rahul R L",
        "slug": "ballistics"
    },
    {
        "layout": "post",
        "title": "MicroLED",
        "author_github": "SurajJagganath",
        "date": "2018-12-01T00:00:00",
        "image": "/assets/img/",
        "description": "Ever heard of a MicroLED and how it can change the future of display technology? Read the following article to know more...",
        "tags": [
            "LED",
            "Television",
            "Micro"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "SurajJagganath",
        "text": "[{% load static %}]()\n# The Future of Television?\n\n## INTRODUCTION\n\nRecently at the CES(Consumers Electronics Show) 2018 Samsung unveiled its prototype of the MicroLED at the consumer level with a 146inch MicroLED TV under the name \"The Wall\". MicroLED technology was invented in the year 2000 by a research group run by Prof. Hongxing Jiang and Prof. Jingyu Lin of the Texas Tech University. However, the first demonstration of MicroLED as a consumer product can be traced back to Sony's 55-inch FullHD \"Crystal LED Display\" that was showcased back in 2012. This was incredibly expensive and was not viable on a commercial scale. To understand why MicroLED can be a revolution in the TV industry we need to delve into the working of the Modern Day TV.\n\n![1]({% static '/blog/assets/img/microled/1.jpg' %}) \n\n## LED OLED  \u00b5 LED\n\nThe LED TVS that we consumers use currently consist of a basic LCD display that are illuminated by compact and efficient light emitting diodes(LED) whereas the LCD TVs use cold cathode fluorescent lamps(CCFLs) to provide backlighting. CCFL is an older form of backlighting whose use commercially has been abandoned due to inferior picture quality as well as bigger size.\n\nThe latest commercial trend after the LED TVs have been the OLED's. OLED (Organic light emitting diode)  is a LED in which the luminescent layer consists of a film of an organic compound that emits light whenever electric current is passed through it. We don't need backlighting for these TV's as each pixel of an OLED screen makes its own light. These lead to\n\n1) Deeper Black Levels of OLEDs as the black OLED's emit no light.\n\n2) Greater Contrast Ratio and Wider viewing angle with respect to LEDs as each OLED pixel makes its   own light without the usage of any backlights\n\n3) Better power efficiency  as an inactive OLED element does not produce light or consume power unlike normal LEDs where the backlight has to be always on.\n\nThe downside to OLED's are that they are pretty expensive as they consist of organic compounds. Their lifespan is also low with RGB luminance degradation reaching around 10% within 1000 viewing hours.\n\n![2]({% static '/blog/assets/img/microled/2.jpg' %}) \n\nThis is where MicroLEDs  come into the picture.Unlike OLED, MicroLED is based on conventional GaN LED technology, which offers far higher total brightness than OLED produces, as much as 30 times, as well as higher efficiency in terms of lux/W which basically means lesser power consumption for similar brightness. With their emissive display they can beat OLED's as they don't contain organic compounds and rather use smaller and more compact LED's. The MicroLED's are therefore immune to burn-in and hence the degradation in luminance will be minimal.MicroLEDs offer higher brightness than any current display technology along with excellent colour and near-perfect off-angle viewing.\n\nRecent reports also state that Apple has begun experimenting with MicroLEDs to check it's usability for tiny screens such as the Apple Watch.\n\n![3]({% static '/blog/assets/img/microled/3.jpg' %})\n\n## CONCLUSION\n\nMicroLED still faces lot of challenges to be viable on a commercial scale .The Wall is just a prototype by Samsung and it has to be reduced to a feasible size of 55-65 inch along with the resolution currently present in The Wall to compete with the OLED. Currently its harder to produce a MicroLED screen when compared to an OLED screen commercially.\n\nHence we must note that though MicroLED's haven't been mass-produced or commercialized, they are near-future technology which we can expect to have in our homes within a few years.",
        "author_name": "Suraj Jagannath",
        "slug": "microled"
    },
    {
        "layout": "post",
        "title": "Zero Knowledge Proof",
        "author_github": "arpi-r",
        "date": "2018-12-01T00:00:00",
        "image": "/assets/img/",
        "description": "An introduction to Zero Knowledge Proofs, an abstract and fascinating concept in Applied Cryptography",
        "tags": [
            "ZKP",
            "Cryptography"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "arpi-r",
        "text": "[{% load static %}]()\n## Introduction\n\n![Introduction]({% static '/blog/assets/img/ZeroKnowledgeProof/introduction.jpg' %})\n\nIn cryptography, *zero knowledge proof* or *zero knowledge protocol* is a method by which one party proves to another party that they know some kind of secret without revealing any information about the secret, except the fact that they know it. This principle is based on an algorithm that takes some data as input and returns either *true* or *false*.\n\n## Understanding the concept behind ZKPs\nThe complexity of zero-knowledge proofs results in them typically being described with abstract examples, which elucidate the concept well.\n\n**Example 1:**\n\nSuppose there is an underground cave with a door at the end of the cave that can only be opened with a magic word. Alice claims that she knows the word and that she can open the door. At the beginning, Alice and Bob are standing at the entrance (point P). Alice enters the cave and reaches a fork (point Q). Bob cannot see Alice from the entrance and the game starts:\n\n - Alice chooses to go either right or left\n - After Alice disappears into the cave, Bob comes to the fork\n - Bob asks Alice to come up from either the right or left\n - If Alice knows the magic word, she can come up from the requested\n   side. She may have to use the magic word (if she\u2019s on the wrong side)\n   or she can just come up without using the magic word (if she is at\n   the right side). So, Alice may come up from the correct side even if\n   she does not know the magic word by guessing Bob\u2019s challenge with a\n   probability of \u00bd.\n![Underground Cave]({% static '/blog/assets/img/ZeroKnowledgeProof/cave.gif' %})\nWhen this is repeated multiple times the probability that Alice comes up the right side without knowing the magic word reduces to (\u00bd)<sup>N</sup> where N is the number of times the test is run.\n\n**Example 2:**\n\nLet us assume there is a colour-blind person and two balls, one red and one green. You would like to prove to the colour-blind person that these balls are of differing colours without revealing the individual colours of each ball. For this, you ask the colour-blind person to hide both balls under the table and bring one ball back up for you to see. After that, he should hide the ball back under the table and then either show the same ball or the other one. As a result, you can prove to the colour-blind person that the colours are different by stating whether he changed the balls under the table or not.\n\nObviously, the other person might think that you were just lucky and is not yet completely convinced that both balls have different colours. Zero-knowledge proofs solve this problem by repeating the experiment over and over again. After every round, your chance of being consistently right by pure luck goes down by half.\n\nSo by repeating this experiment N times, the probability that you guessed right every time, when the balls were truly identical, reduces to (\u00bd)<sup>N</sup> , which is very unlikely.\n\n## Why are Zero Knowledge Proofs required?\n\nEnterprise businesses would not like to share their proprietary information which can get into the hands of hackers or their competitors. Businesses also want to ensure that the information is securely delivered to the intended party. Ordinary blockchains can accomplish this, but with Zero Knowledge Proofs, businesses can share proofs about the data without sharing the data itself.\n\nZKP can impact any industry which involves transactions, identity systems, and other proprietary information. ZKP can be used as a diligence, security, and verification tool in some of the most highly regulated industries like financial services, insurance, audit firms, and retail just to name a few.\n\nA few start-ups and enterprises like ZCash, QED-it, ING and Nuggets use ZKP as a part of their blockchain applications. *Blockchain* provides a *decentralized database*, or *digital ledger*, of transactions that everyone on the network can see. This network is essentially a chain of computers which must all approve an exchange before it can be verified and recorded.\n\nZero-knowledge Proof enables the transfer of assets across this distributed, peer-to-peer blockchain network with complete privacy. In regular blockchain transactions, when an asset is sent from one party to another, the details of that transaction are visible to every other party in the network. However, in a zero-knowledge transaction, the others only know that a valid transaction has taken place, but nothing about the sender, recipient, asset class and quantity. The identity and amount being spent can remain hidden.\n\n![Applications]({% static '/blog/assets/img/ZeroKnowledgeProof/application.jpg' %})\n\n## Conclusion\n\nZero Knowledge Proofs are computationally expensive. Although ZKP technology has been around for decades, it has started gaining importance now. The ability to verify sensitive information like the amount of a transaction, passwords and other identifiable data without actually revealing the data has become more valuable for everyone with the rising presence of bad actors. I believe that this technology will be used extensively in solving privacy problems in the near future.\n\n## References\n\n - [https://blog.goodaudience.com/what-is-zero-knowledge-proof-and-why-you-should-care-36977d738339](https://blog.goodaudience.com/what-is-zero-knowledge-proof-and-why-you-should-care-36977d738339)\n - [https://blockonomi.com/zero-knowledge-proofs/](https://blockonomi.com/zero-knowledge-proofs/)\n - [https://medium.com/@argongroup/on-zero-knowledge-proofs-in-blockchains-14c48cfd1dd1](https://medium.com/@argongroup/on-zero-knowledge-proofs-in-blockchains-14c48cfd1dd1)\n - Cryptography and Network Security - Forouzan",
        "author_name": "Arpitha Raghunandan",
        "slug": "zero-knowledge-proof"
    },
    {
        "layout": "post",
        "title": "Cloud Gaming",
        "author_github": "17aakashsingh1999",
        "date": "2018-12-08T00:00:00",
        "image": "/assets/img/",
        "description": "Revolutionizing how we play games",
        "tags": [
            "Gaming",
            "Cloud",
            "Google Project Stream",
            "Nvidia Now"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "17aakashsingh1999",
        "comments": true,
        "text": "[{% load static %}]()\nTill today gaming had been possible only on dedicated machines like PCs, gaming consoles like nintendo, playstations and xbox and in the recent times on mobile phones too. But even today the AAA titles or the large budget games like Assassin's Creed Unity, GTA 5 are only available on consoles and PCs(only windows), but a new technology is emerging today which aims to break all barriers and bring AAA gaming to all devices alike(yes, even to mac and linux). Read on to find what this new **awesome tech** is all about and how you can get a glimpse of the future, today.\n\n## History of Streaming\nIn its early days internet was a place for static pages. Online video found its place only after certain wrapper formats were released around 1995, during this time Macromedia Corporation released its Shockwave Flash player for Netscape Navigator which remained the de-facto online content player till the introduction of html5. Apple too released its QuickTime player format in 1999. Streaming video and audio became mainstream only during 2005 - 2010. Youtube, Google Video, Dailymotion, Netflix etc. saw their popularity rise during this time. One of the major reasons of the new found success was better broadband penetration and increased internet speeds. Till a few years from now people still believed in downloading content before watching, very fast and cheap internet access today has made it possible to stream content directly. Twitch, Youtube streaming are some services that rely solely on streaming to provide live content over the internet.\nThe video and audio streaming services are in their golden age. Today we see a new industry rising, **The Cloud Gaming Industry**.\n\n## Cloud Gaming\nPlatforms like Steam and Nvidia shield have long provided a way to stream games locally. The game may be rendered on a powerful gaming pc on the home network and then the rendered frames streamed onto a tv connected to the same home network. The bandwidth and latency requirements for a smooth gaming experience were possible only on the local network. But today Nvidia and Google have been trying to push this technology to the next level, **Game Streaming** is in its definitive stage.\n\nThe two pioneers today are:\n- Nvidia Now\n- Google's Project Stream\n\n## Working explained\nThe concept that makes this technology possible is fairly simple and similar to what video streaming uses. The game resides on a cloud computer that runs the game for you. The frames are rendered in real time and then streamed to your device in real time as well. The inputs are then sent back to the cloud system and then a frame is rendered and the process goes on. If someone has tried to stream their gaming session live to facebook, twitch or youtube gaming, they know that one of the major problems in streaming is compressing and encoding the data stream to be transferred over the internet. Low internet speeds result in laggy and low quality streams. High internet speeds coupled with fast compression techniques too result in low quality streams. The perfect combination is high internet speeds (at least 20mbps for 720p 60fps, twitch supports a maximum of 6mbps bitrate) and good compression(requires a lot of software based encoding on CPU, hardware encoding is not as good) with low latency. \nIt may be a difficult task even for a fast gaming PC but fairly simple task for server grade, multi core systems(many cores = better compression performance) available with cloud gaming providers. So the sheer computing power available today and fast internet have made it possible to stream games over the internet.\n\n## Google's Project Stream\n![stream]({% static '/blog/assets/img/cloud-gaming/stream.jpg' %})\n> **\"Project Stream has the potential to become a gaming platform\"**\n> \n> -The Verge\n\nProject Stream is google's entry into the gaming industry. It is a cloud gaming service currently in beta stage and available only in the United States and Canada. The access to the service is free and is on invitation basis. Currently, only Assassin's Creed Odyssey is available for testing purposes. \n\n### How to apply?\nThis service is only available for chrome users. \n- Goto [https://projectstream.google.com](https://projectstream.google.com)\n![stream]({% static '/blog/assets/img/cloud-gaming/g1.png' %})\nYou may be greeted with the above message. In order to circumvent the above page and trick google into thinking that you are from USA, use a VPN or a proxy service that connects you to either USA or Canada and then revisit the link.\n- If the above step goes well, you will be presented with the google login page. Login to your google account.\n- The website asks you to verify that you are 17 years or above. Verify and then the page will ask you if you have a invitation code(setup code).\n- Since we don't have an invitation code, click on the Request for invitation link.\n![stream]({% static '/blog/assets/img/cloud-gaming/g2.png' %}) \n- You will be presented with the request form. Fill it.\n![stream]({% static '/blog/assets/img/cloud-gaming/g3.png' %})\n- Pray that you are lucky and you get the invite. (I haven't yet received one).\n\n## Nvidia Now\n![now]({% static '/blog/assets/img/cloud-gaming/now.jpg' %})\nNvidia has been the pioneer in the gaming industry. Nvidia was the first to explore this area with the introduction of shield gaming devices. And now it has the **\"Now\"** service to revolutionize the gaming industry. This service is still in the beta stage and also available on invitation basis(just like Project Stream). Lot of games are available to anyone who gets an invitation.\n\n### How to apply?\n- Goto [https://www.nvidia.com/en-us/geforce/products/geforce-now/](https://www.nvidia.com/en-us/geforce/products/geforce-now/)\n![now]({% static '/blog/assets/img/cloud-gaming/n1.png' %})\n- Click on **Request Beta Access**.\n![now]({% static '/blog/assets/img/cloud-gaming/n2.png' %})\n- Click on Login/Register.\n- Create an ID if you don't already have one, or login with your credentials.\n![now]({% static '/blog/assets/img/cloud-gaming/n3.png' %})\n- Fill in your details.\n![now]({% static '/blog/assets/img/cloud-gaming/n4.png' %})\n- You will receive a confirmation email.\n- Again, pray that you are lucky and get access to the service. (I am still praying xD)\n\n## What to expect\nI have not yet been able to test these services on my own, but depending on what i have read and seen online the service is working smoothly for many people. Whereas this may not be the case with you. You may go into several bottlenecks: slow internet, incapable hardware, etc. Me being in India at the time of writing this article can guarantee that i will face latency issues while using these services. The latency is the time taken by the packets to travel from one system to another on the internet. High latency will make streaming games a horrible experience, akin to playing demanding games on old and slow hardware or even worse. High latency means you may press a button but may see the game responding to that key press after a second or so (not a very great experience). Google is geo restricting the Project Stream so that they can actually test the gaming experience while the latency is still under control. Streaming Online multiplayer games like Dota 2 and Rainbow Six Siege where a very low latency(below 50ms) is indeed desirable will mean that the streaming services will struggle to coop up with the expectations of the gamers. \nThis new industry is still in its early stages and will definitely improve in the coming years. Google and Nvidia are doing a remarkable job even when the services are still in beta. The latency issue may be remedied by distributing a very large number of cloud centers over the globe, just like Netflix did, but this may be a costly proposition. This is still very much young and its future uncertain, but it is exciting to know that these services will make the best available and accessible to everyone.\n\nSince, I have not been able to try these services yet, if you are lucky and get a chance, do mail your expericence to me.\n\nPeace.",
        "author_name": "Aakash Singh",
        "slug": "cloud-gaming"
    },
    {
        "layout": "post",
        "title": "Gesture Controlled Robot",
        "author_github": "Deepanshi",
        "date": "2018-12-08T00:00:00",
        "image": "/assets/img/",
        "description": "Ever wondered if you could control objects with a cool play of hands or develop a game based on gestures just like in the movie Ra.One?",
        "tags": [
            "Gesture controlled",
            "accelerometer",
            "RF transmitter",
            "RF receiver",
            "motor driver"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Deepanshi",
        "comments": false,
        "text": "[{% load static %}]()\n> \"JARVIS, Suit me Up!\", Clang! Spoof! Phew!, \n> Tony Stark becomes IRON MAN!\n\nHave You Ever Wondered, what if, just like that, with one cool move you too could control and move objects around you; OR Maybe develop a gesture-controlled game like in the movie Ra. One? Well, here I have tried to design and build a mini version of it; a Robot controlled by hand gestures.\n\nBut first up, what exactly is a Gesture Controlled Robot?\n\nA Robot which detects any movement(gesture) and functions depending upon these signals is a Gesture Controlled Robot. They can use different technologies viz. accelerometer-based sensing (wired or wireless) or image processing.\n\nHere we will implement an accelerometer-based, wireless gesture-controlled robot.\n\n## What you need to have.\n\nThe bot has two sections- transmitter and receiver. You will need Ht12e (encoder ic), Ht12d (decoder ic), L293D (motor driver shield), Arduino uno,7805(voltage regulator ic), capacitor, PBT connector, single pole antenna, resistor, LED, accelerometer and battery.\n\n## How does it work and recognize the gestures?\n\nThe brain of this robot is the Arduino Uno (Atmega32) which is fed with a set of code. The gestures/motion made by hand are detected by an acceleration measuring device, called accelerometer (ADXL335).\n\n![img1]({% static '/blog/assets/img/gesture_controlled_robot/1.jpeg' %}) \n\nThe accelerometer reads the X Y Z coordinates when we make gestures by hand and sends them to the Arduino. The Arduino checks the values of coordinates and sends a 4-bit code to the Encoder IC. The Encoder passes the data to an RF transmitter and the transmitted data is received by an RF receiver. The receiver sends the 4-bit code to the Decoder IC which then passes it to a Motor Driver IC. It later takes the decision to turn the two motors in the required direction.\n\n## What is an Accelerometer?\n\nAn accelerometer is a type of sensor which gives data in Analog form while moving in the directions of X, Y and Z(these directions depend on the type of sensor). It consists of direction arrows; if we tilt the sensor in one direction, then the data at that particular pin will change in analog format. The accelerometer consists of six pins. The function of each pin is discussed here.\n\n- Pin-1: VDD pin is used to give +5V supply.\n\n- Pin-2: GND pin is connected to the ground to ensure biasing.\n\n- Pin-3: X pin will receive the data from the X direction.\n\n- Pin-4: Y pin will receive the data from the Y direction.\n\n![img2]({% static '/blog/assets/img/gesture_controlled_robot/2.jpeg' %})\n\n- Pin-5: Z pin will receive the data from the Z direction.\n\n- Pin-6: ST pin is used to adjust the sensitivity of the accelerometer i.e. 1.5g or 2g or 3g or 4g.\n\n## Making the power supply.\n\n### You will need:\n\n- An IC 7805 which regulates the 12V supply to 5V (if you can't get a 12V supply you can use a 9V supply)\n\n- 0.1uf and 470uf capacitors\n\n- 1k resistor for status LED\n\nLet's start with the power supply circuit. You will need two such circuits: one for the transmitter and one for the receiver. The receiver circuit needs to be powered using a 12V supply (since we are using a 12V motor), and the transmitter circuit can be powered using a 9V battery.\n\n![img3]({% static '/blog/assets/img/gesture_controlled_robot/3.jpeg' %})\n\nYou can see the circuit diagram for the receiver power supply on the right. Using this diagram, rig up the supply circuit. You can also add an LED via a 1K resistor to indicate the state of power supply.\n\n![img4]({% static '/blog/assets/img/gesture_controlled_robot/4.jpeg' %})\n\n## Let's Move Ahead to make the Transmitter (Remote)\n\n![img5]({% static '/blog/assets/img/gesture_controlled_robot/5.jpeg' %})\n\nThe transmitter section consists of an accelerometer which detects the hand gestures and sends the data to the Arduino. This then sends the data to an Encoder IC which is subsequently transmitted to a receiver.\n\nThe RF TX module works with 433MHz frequency and is easily available in the market at low cost.\n\nWire it up as per the below circuit diagram:\n\n![img6]({% static '/blog/assets/img/gesture_controlled_robot/6.jpeg' %}) \n\nCode the Arduino to make it function like how you want it to.\n\n## Motor Driver\n\nThe motor driver is a device which gives the movement to do a task like a motor. So, we require motor driver to run them through the controller. The interface between motor &amp; microcontroller can be done using an L293D motor driver IC in this circuit.\n\n![img7]({% static '/blog/assets/img/gesture_controlled_robot/7.jpeg' %})\n\n## Receiver circuit\n\nThe receiver circuit consists of 2 IC's (HT12D decoder, L293D motor driver) and an RF receiver module.\n\nWire the circuit as per the above receiver schematic. There are 2 LED's in the receiver board; one lights up when the power supply is given to the receiver and the other when power supply is given to the transmitter. The LED near the IC HT12D should light up when power is given at the transmitter, thus ensuring a valid transmission (VT). If it doesn't, then there is something wrong with your connections or your RF-TX-RX module.\n\n![img8]({% static '/blog/assets/img/gesture_controlled_robot/8.jpeg' %})\n\n## The Final Working\n\nAccelerometer based gesture-controlled robot moves according to the movement of the hand, since the accelerometer is on your hand. When you tilt your hand forward (with the accelerometer on your hand), the robot moves straight ahead. When you tilt backwards, it changes direction and starts moving in reverse. When you tilt to the right, it starts moving rightwards and similarly to the left. (This is in accordance with what code I have written for the motion of my bot. You can modify accordingly!)\n\nEnjoy driving your robot!!!\n\n## Note\n\n> -You can refer to the previous blog - How to get started with bots by -Tarun Mittal (dated June 1st ,2017) to get more insight into working of bots.\n\n## Some basic arduino tutorials : \n\n- [https://www.youtube.com/watch?v=fCxzA9_kg6s](https://www.youtube.com/watch?v=fCxzA9_kg6s)\n\n- [https://hackr.io/tutorials/learn-arduino](https://hackr.io/tutorials/learn-arduino)\n\n- [https://learn.sparkfun.com/tutorials/what-is-an-arduino/all](https://learn.sparkfun.com/tutorials/what-is-an-arduino/all)",
        "author_name": "Deepanshi",
        "slug": "guesture-controlled-robot"
    },
    {
        "layout": "post",
        "title": "NanoMachines",
        "author_github": "ShaswataBose",
        "date": "2018-12-08T00:00:00",
        "image": "/assets/img/",
        "description": "How do we make machines using just a few atoms? What are the potential application of these tiny devices?",
        "tags": [
            "MEMS",
            "NEMS",
            "Nanotechnology",
            "Biomedics"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "ShaswataBose",
        "use_math": true,
        "comments": false,
        "text": "[{% load static %}]()\nThis article provides a brief insight in the field of nanomachines and nanosensors, the challenges faced by the researchers in this field, and the promise they hold for us in future applications.\n\n## What is the difference between micro and nano systems?\nWhen we talk about about nanomachines, we consider devices which are only a few atoms apart in their dimensions whereas in microsystems, the dimensions are in the range of a few microns. Thus, these devices can only be observed under **HR-TEM (High Resolution Tunneling Electron Microscope)** or **thermal imaging techniques.**\n\n![A nano car developed by RICE University]({% static '/blog/assets/img/nanomachines/2-nanocars.png' %})\n\n##  **How do these machines move?**\nThe very first challenge that we face as a result of too much scaling down is how we are going to move these tiny machines; be it a nanocar a few atoms across or a nanobot? We simply cannot fit an even smaller diesel engine or a servo motor. Instead, we use some other phenomena, which are insignificant in macro scale but cannot be ignored in microscale.\n\nSeveral years ago, researchers at RICE University created nanocars which are only a few atoms wide. They were made to move by thermal excitation\n\nLet us consider another example. This time, a microbot. These \u2018microbots\u2019 are small tubes that are made of several layers of different metals. They are designed to pick up unhealthy damaged cells and transport them within our body. So how do we move them? Hydrogen Peroxide is used as a fuel for these bots. The highly reactive compound produces bubbles of oxygen which acts as the exhaust of a nano jet engine, thus propelling the bot forward.\n\n![Locomotion of a microbot]({% static '/blog/assets/img/nanomachines/3-microbot.png' %})\n\nNow you must be wondering, \u201cHold on, this means the bot will keep moving in one direction only, this is not very useful\u201d. They can do so by using magnetic fields to change their direction of motion.\n\n![A microbot changing its direction]({% static '/blog/assets/img/nanomachines/4-microbots-carrying-cells1.png' %})\n\n## **What are the applications?**\n\nThe field of nanodevices are still in its R & D stage and everything is still confined in the laboratory. Nonetheless, these devices hold a lot of promise. Scientists are working on being able to control these nanomotors inside of living material as well, which would enhance the possible applications. As this field continues to grow, we can imagine these nanomachines will be able to help with countless applications \u2013 from breaking down kidney stones to fighting cancer, breaking up blood clots, cleaning wounds and removing infections. Recent advancements show a lot of promise in DNA repair and treating diseases which cannot be treated by conventional medical practices. The applications and advantages really are endless!\n\n![Nanobots in bloodstream]({% static '/blog/assets/img/nanomachines/sensors-08-02932f6.png' %})",
        "author_name": "Shaswata Bose",
        "slug": "nanomachines"
    },
    {
        "layout": "post",
        "title": "Electronic skin",
        "author_github": "RakshanaG",
        "date": "2018-12-15T00:00:00",
        "image": "/assets/img/",
        "description": "You can now attach computing devices that are twisty, ultra-thin, light and portable on to your skin and take it wherever you go. Just like a tattoo!",
        "tags": [
            "Electronic",
            "Skin",
            "Healthcare",
            "Robotics",
            "Embedded",
            "Prosthetic",
            "Sensor",
            "Stretchable",
            "Nanoparticles"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "RakshanaG",
        "comments": true,
        "text": "[{% load static %}]()\n# ELECTRONIC SKIN\n\n![Hand with electronic skin]({% static \"/blog/assets/img/electronic-skin/pic1.png \"Hand with electronic skin\"\" %})\n\nTouch is the first sense we acquire and has been shown to play a crucial role in physical and mental health. The nerve cells in our skin allow us to experience the several experiences of touch such as a cold splash of water, the tug of a strong breeze or the heat from your coffee mug. Scientists are trying to develop electronic skin to help robots and prosthetic devices to attain these abilities.\n\nResearchers report a new method in _ACS Applied Materials &amp; Interfaces_ that creates an ultrathin, stretchable electronic skin, which could be used for prosthetics, robotics, virtual reality and even wearable health monitors. The main challenge is to transfer ultrathin electrical circuits onto complex 3D surfaces. Moreover, the electronics have to be bendable and stretchable enough to allow movement. Current methods for producing these electronics are slow, expensive and require sophisticated equipment and techniques.\n\nIn the new approach the researchers patterned a circuit template on to a sheet of transfer tattoo paper with an ordinary desktop laser printer.\n\n![Printing circuit with laser printer]({% static \"/blog/assets/img/electronic-skin/pic2.png \"Printing circuit with laser printer\"\" %})\n\nThen, they coated the template with silver paste which stuck only to the printed toner ink and wiped away the excess silver.\n\n![Ag epoxy deposition]({% static \"/blog/assets/img/electronic-skin/pic3.png \"Ag epoxy deposition\"\" %})\n\n![Driving silver epoxy]({% static \"/blog/assets/img/electronic-skin/pic4.png \"Driving silver epoxy\"\" %})\n\n\nOn top of the silver paste the team deposited a gallium indium liquid metal alloy that increased the flexibility and electrical conductivity of the circuit.\n\n![E-GaIn deposition]({% static \"/blog/assets/img/electronic-skin/pic5.png \"E-GaIn deposition\"\" %})\n\nSubsequently, they added external electronics such as microchips with silver epoxy or conductive glue made of vertically aligned magnetic particles embedded in a polyvinyl alcohol gel.\n\n![Placing components with silver epoxy]({% static \"/blog/assets/img/electronic-skin/pic6.png \"Placing components with silver epoxy\"\" %})\n\nThe researchers used water to help transfer the electronic tattoo to 3-D objects. This was done by immersing the object in a tub of water and then placing the circuit on the surface of the water.\n\n![Immersing object in tub of water]({% static \"/blog/assets/img/electronic-skin/pic7.png \"Immersing object in tub of water\"\" %})\n\nThe paper backing of the tattoo is separated from the carrier film which floated on the surface of the water.\n\nFinally, when the researchers lifted the object out of the water, the circuit adhered to the contours of the object.\n\n![Circuit adhered to the contours of the object]({% static \"/blog/assets/img/electronic-skin/pic8.png \"Circuit adhered to the contours of the object\"\" %})\n\nThis electronic skin remains functional under bending, folding, twisting, and strains up to about 30% (similar to human skin). They can adapt and adhere to highly curved 3-D surfaces, like a model of a human brain or an apple.\n\nVarious applications of the new method include controlling a robot prosthetic arm, incorporating proximity sensors into a 3D model of a hand and epidermal bio monitoring.\n\nThe research was performed in collaboration between Carnegie Mellon&#39;s Soft Machines Lab and the Institute of Systems and Robotics at the University of Coimbra in Portugal.\n\nThe findings were published in the paper, &quot;EGaIn-Assisted Room-Temperature Sintering of Silver Nanoparticles for Stretchable, Inkjet-Printed, Thin-Film Electronics,&quot; in Advanced Materials.\n\nTo know more about electronic tattoos and their applications check out this [video](https://youtu.be/OaITzyMptFQ)",
        "author_name": "Rakshana G",
        "slug": "electronic-skin"
    },
    {
        "layout": "post",
        "title": "Nanosheets decorated with DNA",
        "author_github": "AditiMarathe",
        "date": "2018-12-15T00:00:00",
        "image": "/assets/img/",
        "description": "The best of engineering and medical worlds can be combined to create marvels which can change our world! DNA, the source code of our existence, when combined with nanotechnology can lead to storage devices with storage capacity in petabytes on the tip of your finger!",
        "tags": [
            "Nanosheets",
            "DNA",
            "Nanotechnology"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "AditiMarathe",
        "comments": true,
        "text": "[{% load static %}]()\nOne possible means of bridging the gap between India\u2019s abundant, varied natural resources and her ever-increasing requirements like clean water, food and rapid, low cost diagnostic machinery is the use of nanotechnology.\n\nWhen we modify materials at their atomic and molecular level, some very unusual and useful properties are generated. Since the dimensions of atoms and molecule are in nanometers, this technology is called nanotechnology.\n\nEfforts to promote research in nanotechnology in India began early in the millennium. The efforts have paid off well, India published over 23000 papers in nanoscience in the past 5 years. One of the recent developments in this field by India was the synthesis of nanosheets using DNA leading to the most powerful storage device ever made.\n\nResearchers from Indian Institute of Science Education and Research (IISER) Thiruvananthapuram, Kerala and Institute of Microbial Technology, Chandigarh have developed a novel method to synthesize 2 dimensional nanosheets decorated with DNA molecule. The design strategy allows for an ultra-dense array of DNA molecules to be grown on 2 D crystalline nanosheets.\n\n![DNA]({% static \"/blog/assets/img/nanosheets-decorated-with-dna/dna.jpg \"DNA\"\" %})\n\nDeoxyribonucleic acid (DNA) is made of a long chain of repeating monomer units called nucleotides and is the primary carrier of genetic information in organisms. DNA forms the basis for all of life, driving growth, development and reproduction of all living organisms.\n\nIn recent years, the emergence of nanotechnology has diversified the uses for DNA, including non-biological applications. The information rich molecule has been used for data storage, computing, crystallography, molecular scale electronics and many more. DNA molecules have the potential to store up to 5.5 quadrillion bytes of data in each cubic millimeter of the DNA molecule, making it the most compact and efficient data storage device ever developed!\n\nDesigning a nanosheet with the surface decorated with DNA could have potential applications in materials science, drug delivery, and nanoelectronics. However, fabricating such a nanosheet has been a challenge. The results suggest that this design strategy can be applied as a general approach for the synthesis of DNA decorated high aspect ratio sheets, which may find potential applications in materials science, drug delivery, and nanoelectronics.",
        "author_name": "Aditi Marathe",
        "slug": "nanosheets-decorated-with-dna"
    },
    {
        "layout": "post",
        "title": "Why you should learn Flutter?",
        "author_github": "gvaibhav1734",
        "date": "2018-12-15T00:00:00",
        "image": "/assets/img/",
        "description": "This article is about why Flutter is the next big thing in Mobile development",
        "tags": [
            "Flutter",
            "Android",
            "Dart",
            "Cross platform mobile development"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "gvaibhav1734",
        "comments": true,
        "text": "[{% load static %}]()\n## What is Flutter?\n\nFlutter is an open-source Mobile Application Development SDK launched by Google back in May 2017. It is used to create apps for both Android and iOS Operating Systems. It is also the primary method of developing applications for Google Fuchsia. Flutter 1.0 was announced on December 4th, 2018 at [Flutter Live](https://developers.google.com/events/flutter-live/).\n\n## Why Flutter?\n\n__Incredibly easy to setup and get started!__ - Download the Flutter SDK from the [official website](https://flutter.io/docs/get-started/install). Extract it and add the flutter sdk path to your `PATH` variable. Download the Flutter plugin in your favourite editor / IDE and you're good to go. It's that simple!\n\n__Flutter provides great tooling support__ - Flutter development is supported on major IDEs like Android Studio, IntelliJ IDEA and VS Code so development is going to feel right at home.\n\n__Flutter \u2764\ufe0f Dart__ - Dart is a programming language developed by Google which is very easy to pickup if you know JavaScript or Java. So native Android and react-native developers can learn Flutter easily.\n\n__Hot Reload__ - Unlike Instant Run in Android Studio (which never works!) Flutter has sub-second stateful hot reload. With stateful hot reload, you can make changes to the code of your app and see the results instantly without restarting your app or losing its state. This improves the development experience.\n\n![Hot reload in action]({% static \"/blog/assets/img/why-you-should-learn-flutter/hot_reload.gif \"Hot reload in action\"\" %})\n\n__Documentation__ - Flutter is documented very well which makes app development process easy and productive (one click away!).\n\n__Large number of widgets__ - Flutter provides rich set of beautiful, customisable Material Design and Cupertino (iOS-style) widgets which makes it very easy to build beautiful UIs.\n\n__Develop for Android as well as iOS__ - Flutter is compiled into native ARM code for both Android and iOS. So you can serve both the platforms with a single codebase. This is beneficial for companies which have small teams and need to cater to users of both platforms.\n\n![Flutter app on Android as well as iOS]({% static \"/blog/assets/img/why-you-should-learn-flutter/ios_android.png \"Flutter app on Android as well as iOS\"\" %})\n\n__Open sourced__ - Flutter is [open-sourced](https://github.com/flutter) and has a great community which strives to make it better and it's backed by Google.\n\n__Early adoption by various enterprises__ - Alibaba made use of Flutter to build Xianyu app. Google themselves use Flutter to develop the Google Ads app. Similarly [many enterprises](https://flutter.io/showcase) have started to use Flutter to develop their apps even though Flutter hit 1.0 very recently. This clearly shows that Flutter has lot of potential and will be the next leading framework in mobile development.\n\n## How is Flutter different?\n\n- Hot reload is one of the most popular features of Flutter. In Android significant time is spent to build the app then navigate to the activity which had a bug and try to fix it but in Flutter you can quickly iterate on a screen deeply nested in your app, without starting from the home screen after every reload.\n- Flutter does its own rendering using Skia. It doesn't rely on native views which are often not powerful and fast enough. It paints every pixel on screen. This means that your UI can be written once and shared across iOS and Android and it will look identical in both platforms unless you want it to be different.\n- Flutter makes use of reactive views like react-native but without the JavaScript bridge.\n- Flutter uses composition. As a result it's very easy to build custom Widgets. In Android (which uses inheritance to build UI components), if you nest a lot of UI components then you can run into performance issues really quickly but we don\u2019t have that problem in Flutter.\n- Flutter compiles to native ARM code on both Android and iOS so the apps built using Flutter have absolutely no performance issues. Flutter has 60fps animations and can also handle 120fps provided the device is capable of 120Hz updates which clearly shows that apps built using Flutter are performance-driven.\n\n## Future of Flutter?\n\nGoogle plans to update the 'stable' channel of Flutter on a quaterly basis. Apart from the obvious support for iOS and Android devices Google is planning to release Flutter for web and the project is called Hummingbird, which takes support from the Dart platform for compiling it with ARM code and JavaScript. It allows Flutter to operate on Standard web apps without any changes.\n\nGoogle has also started to work on Flutter Desktop Embedding, which is still in its early days. With this project, Google intends to introduce Flutter in the world of desktop Operating Systems along with Windows, MacOS, and Linux.\n\nGoogle also published an informal guideline on how to use Flutter on Raspberry Pi to show that Flutter can be embedded into smaller-scale devices.\n\n## Conclusion\n\nFlutter is designed to be fast, productive and beautiful. It bridges the gap between iOS and Android. It bridges the gap between developers and designers and helps them build beautiful, functional, productive apps. Often new technologies are not adopted by enterprises which makes the developers think twice before learning but this is not the case with Flutter. Alibaba adopted Flutter when it was in beta. So clearly enterprises are interested in this framework. Moreover, Flutter is the primary way of developing apps for Google Fuchsia which gives more incentive to learn it. Flutter makes mobile development fun!\n\n## Some helpful links to get started with Flutter\n\n- [The official website](https://flutter.io)<br/>\n- [Talk by Eric Seidel (co-founder) on Flutter](https://www.youtube.com/watch?v=VUiVkDpikDI&t=316s)<br/>\n- [Get started with flutter!](https://flutter.io/docs/get-started/install)<br/>\n- [Flutter gallery app to get a taste of what you can build and how it would look](https://play.google.com/store/apps/details?id=io.flutter.demo.gallery&hl=en_IN)",
        "author_name": "Vaibhav G",
        "slug": "why-you-should-learn-flutter"
    },
    {
        "layout": "post",
        "title": "GaN: Gate to New era",
        "author_github": "techcrow",
        "date": "2018-12-22T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "galliumnitride",
            "GaN",
            "transistors",
            "wafers",
            "silicon",
            "newsilicon",
            "technology"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "techcrow",
        "comments": false,
        "text": "[{% load static %}]()\n## GaN: Gate to New era\n\n![introduction]({% static \"/blog/assets/img/GaN/1.jpg \"Introduction\"\" %})\n\nTo make life easier, more affordable mankind has always been in search of transforming the current technologies and making it much more better, whether be it wheels to cars, or electric current to cell phones. Different tech giants use slogans such as 'Be what's next' by Microsoft, 'THINK' by IBM, etc, thus directly promoting new transformations not only in technology but they point towards an economical product which is easily feasible and accessible to its customers. \nSince the dawn of the electronics age over a hundred years ago, power design engineers have been on a quest for the ideal switch, one that will rapidly and efficiently convert raw electrical energy into a controlled, useful flow of electrons.  First came the vacuum tube but inefficiency, as evidenced in the heat that they generate, and their large size and high cost, created limits to their ultimate use. Next, in the late \u201850s, the transistor gained widespread use; with its small size and better efficiency they appeared to be the \u201choly grail\u201d and rapidly displaced tubes while creating enormous new markets unreachable by vacuum tube technology.\n\n![Shrinking lengths of transistors]({% static \"/blog/assets/img/GaN/2.jpg \"Shrinking lengths of transistors\"\" %})\n\nIn last two decades, length of transistor chips has been shrinking exponentially, parallely making it more efficient. Gallium nitride (GaN), a new member in the family of transistors, is a material that can be used in the production of semiconductor power devices as well as RF components and light emitting diodes (LEDs). GaN has demonstrated the capability to be the displacement technology for silicon semiconductors in power conversion, RF, and analog applications.\n\n\n## The New Silicon:\n\n![GaN wafer]({% static \"/blog/assets/img/GaN/4.jpg \"GaN wafer\"\" %})\n\nAdvantages of GaN over Silicon:\n\n- lower on resistance giving lower conductance losses and conducting 1000 times faster than silicon\n\n- faster devices yielding less switching losses\n\n- less capacitance resulting in less losses when charging and discharging devices\n\n- less power needed to drive the circuit\n\n- smaller devices taking up less space on the printed circuit board\n\nFortunately, the cost to produce a GaN device is inherently lower than the cost to produce a MOSFET device, since GaN devices are produced using standard silicon manufacturing procedures in the same factories that currently produce traditional silicon semiconductors, and the resulting devices are much smaller for the same functional performance. Since the individual devices are much smaller than silicon devices, many more GaN devices can be produced per wafer, thus forming a situation where GaN devices will always cost less to manufacture than their silicon counterparts. As GaN technology improves, the cost gap gets even wider.\n\n## DEVELOPMENTS\n\n![Functioning of radar]({% static \"/blog/assets/img/GaN/3.jpg \"Functioning of radar\"\" %})\n\nDifferent studies and researches in various laboratories all over the globe are going on to grow  GaN wafers more efficiently.\n\n- In 2007, Toshiba Corporation announced that it has developed a gallium nitride (GaN) power field effect transistor (FET) for the Ku-band (12GHz to 18GHz) frequency range that achieves an output power of 65.4W at 14.5GHz, the highest level of performance yet reported at this frequency band. The main application of the new transistor will be in base stations for satellite microwave communications, which carry high-capacity signals, including high-definition broadcasts.\n\n- In 2010, Fujitsu developed an amplifier based on gallium-nitride (GaN) high electron mobility transistor (HEMT) technology, which features an output of 12.9W - more than twice the output of previous amplifiers and presently featuring the world's highest amplification output - when operating in the wide band range of the C-band, X-band, and Ku-band radio frequency spectrums between 6GHz-18GHz.\n\n- CeNSE department of IISc, Bangalore is working on growing GaN wafers, designing and manufacturing it indigeniuosly. Moreover, it also aims  to produce GaN MOSFETs at economical rate and design indigenious long range radars, in order to make Indian army more advanced and strong. \n\nHope **GaN** will **Drain** all its applications from its **Source** and open new **Gates** towards to the development of mankind.",
        "author_name": "Vishal Shukla",
        "slug": "GaN"
    },
    {
        "layout": "post",
        "title": "How Computers learn to play games!",
        "author_github": "Madhuparna04",
        "date": "2018-12-22T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "Computer Games",
            "Reinforcement Learning",
            "Monte Carlo Tree Search",
            "Alpha-Go"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "Madhuparna04",
        "comments": false,
        "text": "[{% load static %}]()\nHave you ever wondered how computers learn to play games like Chess or Go? Or how do they replace human players in Ludo or Card games?\nHow Google's Alpha-Go defeated the **World Champion** of Go?\nLet us dive in the field of Computer Games and find out how Computers learn to play Games!\n\n![Chess]({% static \"/blog/assets/img/How Computers LearnToPlayGames/chess.jpg \"Chess\"\" %})\n\nThere are various algorithms that are used to make a computer learn to play games. I will be discussing some of them to give you some intuition on how these algorithms work.\n\n## Naive Tree Search\n\nLet's start with a simple and naive approach by which the computer can take decisions. It is by looking at all possible states and choosing the best action to take. That is, make a tree with all possible states and next states and go to the leaves to find out if a move will lead to winning state or not and select an action accordingly.\n\n![Game Tree]({% static \"/blog/assets/img/How Computers LearnToPlayGames/decision_tree.png \"Game Tree\"\" %})\n\nBut, this technique cannot be used for games where there is a huge number of states, that it is practically impossible for a computer to examine all the states before taking a decision. For example, there are more than **5000 states** in the game of Tic-Tac-Toe, which is a **3 X 3** board game. Whereas Chess is an **8 X 8** board game and Go can have a **19 X 19** board. Therefore these games with billions of states cannot be solved by any such brute force approach where examining each state is required.\n\n## Monte Carlo Tree Search\n\nMonte-Carlo Tree Search can be only used for games where we have perfect information about the game that is we know about all the actions that were previously taken and can be taken in future. It is applicable to games like Go, Chess or Tic-Tac-Toe. So, what the computer does in Monte Carlo Tree Search is it repeats these 4 steps\n\n![Monte-Carlo Tree search]({% static \"/blog/assets/img/How Computers LearnToPlayGames/Monte-Carlo-Tree-Search.png \"Monte-Carlo Tree search\"\" %})\n\n#### 1)Select -\n In this step the computer starts with a root node(state) and selects the next node based on the current policy. A policy is a function that dictates what actions to take given a state.\n\n#### 2)Expand - \nWhen the policy can no longer choose an action with confidence to find the successor node, it expands the game tree by appending all possible states from that node or a few let's say 10 or 100 states chosen by randomly trying out certain number of actions, this is done when the number of next possible states is huge.\n\n#### 3)Simulate -  \nThen from the appended nodes, it simulates a random game to find out how good it is to go to that state.\n\n#### 4)Backpropagation - \nThen it traverses upwards back to the root and updates the score for all visited nodes.\n\nThese four steps are repeated again and again until the computer has learned enough to actually play the game. \n\n## Learning to play games using Reinforcement Learning\n\nThere are various algorithms that come under Reinforcement Learning Algorithms. Each suitable for a different purpose. The basic idea behind Reinforcement Learning is, as the name suggests, making the computer play the game again and again and learn from its own experience.\nIn order to give you intuition on how Reinforcement Learning can be used for games with a huge state space let me define two new terms.\n\n**1. State Value Function** - Given a state this function tells how good it is to be in that state.\n\n**2. Action Value Function** - Given an action and a state, this function tells how good it is to take that action from the given state.\n\nThe basic idea is to learn the value functions and use it to choose the best action.\nConsider the following diagram-\n\n![Backup diagram for value function]({% static \"/blog/assets/img/How Computers LearnToPlayGames/q-learning.jpg \"Backup diagram for value function\"\" %})\n\nWhat the computer does is from a given state it chooses the best action based on the action value function and proceeds, and depending upon what reward it gets, the computer modifies the action value function accordingly (This Reinforcement Learning Technique is called Q-learning, have a look at the following[ blog post](https://marl-ieee-nitk.github.io/jekyll/update/2018/12/11/Q-learning.html) to get a better understanding of how this algorithm works.). \n\n#### But how is it dealing with the problem of a huge number of states?\n\nThe answer is the computer is not really going through all the possible states but still learning an approximate way to figure out the best actions to take. There are many states in the state-space which occur very rarely or is too poor to begin with, that there is no point going all the way down from that state and calculating everything when there are very fewer chances of encountering that state or there is no way it can get you a better reward.\nBut as our computer plays multiple games it will surely go through most of the frequently occurring and promising states and will know how to behave in those situations.\n\n## Alpha-Go\n\nRecently Google's Alpha-Go zero defeated Go's World Champion Lee Sedol. Alpha Go Zero learned to play Go from scratch by playing multiple games against itself. In Alpha-Go deep learning and Monte Carlo Tree Search were combined to produce a powerful reinforcement learning algorithm.\n\n![Lee Sedol playing with Alpha-Go]({% static \"/blog/assets/img/How Computers LearnToPlayGames/Alpha-Go.jpg \"Lee Sedol playing with Alpha-Go\"\" %})\n\nIf you are wondering why it is so much of a big deal because AI has already defeated **Chess Grandmaster Garry Kasparov** in the game of chess then why not **Lee Sedol in Go!** \nThere are various differences between Chess and Go, and that is why it took another 20 years after Garry Kasparov's defeat to build an AI which can master Go. One of the major differences is the number of choices available at each step, which is about 35 in Chess compared to 250 in Go.\nAlso, the number of moves per game, Go might last for 150 moves vs. 80 in chess. Therefore the total number of possible games of Go has been estimated at **10^761**, compared to **10^120** for chess which is a huge difference. That's why it has been difficult for computers to master Go.\n\n## Conclusion\n\nI hope you enjoyed this dive into the World of computer games and got to know some of the technical details too. You can go through the following links to know more about these topics.\n\n- [Video lectures on Monte Carlo Tree Search](https://www.youtube.com/watch?v=onBYsen2_eA)\n- [More about Alpha-Go](https://www.tastehit.com/blog/google-deepmind-alphago-how-it-works/)",
        "author_name": "Madhuparna Bhowmik",
        "slug": "how-computers-learn-to-play-games"
    },
    {
        "layout": "post",
        "title": "NanoMedicine",
        "author_github": "manojab",
        "date": "2018-12-22T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "NanoMedicine",
            "Targeted drug delivery",
            "Overlooked advancements",
            "EPR effect"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "manojab",
        "comments": false,
        "text": "[{% load static %}]()\n*How evolutionary is nanomedicine in therapeutics? Is it worth all the hype? Can we, as\nengineers, find opportunities in the field of nanomedicine? Well, I hope this article will\nanswer many similar questions.*\n\n![Nanopills: Medicinal advantage]({% static \"/blog/assets/img/NanoMedicine/1.jpg \"Nanopills: Medicinal advantage\"\" %}) \n\n## Nanotechnology and its manifestation in Medicine\n\n![Manifestation of Nanotechnology in Medicine]({% static \"/blog/assets/img/NanoMedicine/5.jpg \"Manifestation of Nanotechnology in Medicine\"\" %})\n\nLet's start with the understanding of how exactly is nanotechnology playing a\nnoteworthy role in the field of medicine. From its very dawn, nanotechnology has\nindeed shaped the fundamental industrial applications such as manufacturing,\nbiotechnology, semiconductors and fabrication of work area from micro to large-\nscale prospects. The impact of nanotechnology on the medical domain is well\npronounced and the results can be pointed out at biosensors, nanosized microchips,\nmodified liposomal action and mainly the ease brought in the fields of treatment and\ndiagnosis of diseases.\n\n## Why use Nanotechnology for Drug delivery?\n\nIf the goal of research on nanotechnology is just to make something nano, new, and\nmore complicated, the goal is achieved as seen from the progress made in the last\ndecade, at least in part. The ultimate goal of any research in drug delivery, however,\nmust be to develop drug-delivery systems, nanoparticulate systems in this case, to\nprevent, to control, and to treat debilitating diseases. Most scientists working in the\npharmaceutical and biotechnology sectors, as well as in academia, want to develop\nnanoparticle formulations that can deliver drugs more effectively to the target site for\nenhanced efficacy and reduced side effects.\n\n![Nano Surgery]({% static \"/blog/assets/img/NanoMedicine/2.jpg \"Nano Surgery\"\" %})\n\nNanoparticles offer important multifunctional platforms for biomedical applications.\nVarieties of nanoparticles, such as silica nanoparticles, quantum dots, metal\nnanoparticles, and lanthanide nanoparticles, have unique properties which are\nadapted for different applications in the bio-analysis field. Nanoparticles not only\nensure the drug delivery but also the confirmation of target site (commonly tumors).\nA new challenge of essentially tracking down the nanomedicine from systemic to\nsubcellular level is posed which is swept off with the use of fluorescent markers,\nthose being detectable during diagnostic assays.\n\n![Target Specificity]({% static \"/blog/assets/img/NanoMedicine/3.jpg \"Target Specificity\"\" %})\n\nVarious examples in different forms include:\n\n- Fe<sub>3</sub>O<sub>4</sub> nanocrystals on uniform dye-doped mesoporous silica nanoparticles\nto be used as a contrast agent in magnetic resonance imaging and loaded\ndoxorubicin in the pores.\n\n- Histidine-tagged cyan fluorescent protein-capped magnetic mesoporous\nsilica nanoparticles system was fabricated for drug delivery and fluorescent\nimaging.\n\n- Quantum dots are small-sized (1\u201310 nm) semiconductor nanocrystals\ncomposed of the inorganic elemental core (e.g., Cd and Se) surrounded by a\nmetallic shell (ZnS). They are widely used in biological research and can\nalso be used as drug carriers or simply as fluorescent labels for other drug\ncarriers remain highlighted.\n\nOther notable advancements include usage of iron oxide and gold nanoparticles\ndue to the presence of surface plasmons which make the nanoparticles to absorb\nlight in the visible region, making it possible to study their size-dependent light\nabsorption through surface plasmon resonance (SPR). In terms of biocompatibility\nand non-cytotoxicity, gold nanoparticles as approved by the **Food and Drug\nAdministration(FDA)** have distinct advantages over other metallic particles and\ncould also be utilized as a favorable carrier for delivery of drugs. These\nnanoparticles can be conjugated with amino acid and proteins. Fabrication of gold\nnanoparticles and functionalization with organic molecules to interact with any\nphysiological system are more important. These functionalized nanoparticles are a\npromising candidate for drug delivery as biomarkers of the drug-resistant cancer\ncell. Reported application of gold nanoparticles includes insulin delivery by nasal\nroute, improved antimicrobial action against E. coli strains and ciprofloxacin-\nprotected nanoparticles for better drug release.\n\n## Misconceptions and Limitations:\n\nYes, though nanobiotechnological advancements are a promising outcome,\nit is not all that it looks like. Nanotechnology fever was fueled by an\nobservation of the behavior of nanoparticles in tumors in mice, known as the\n**enhanced permeation and retention (EPR)** effect. The EPR effect is\nconsidered to be responsible for increased delivery of nanoparticles to\ntargeted tumors in mouse experiments. This notion evolved into an idea that\nonly nanoparticles have the EPR effect. Careful analysis of the original data,\nhowever, indicates that albumin and IgG are actually better in accumulating at\nthe tumor site.\n\nThe enhanced EPR effect of PEGylated nanoparticles was thought to be due\nto the increased circulation cycles. Thus, it has been widely assumed that\nPEGylated nanoparticles having the EPR effect will result in an enhanced\ntumor-killing effect, and therefore, the problem of targeted drug delivery to\ntumors was partially solved. The reality is that these assumptions have\nproduced numerous research articles, but have made no significant advances\nin translation into patient treatment\n\nNanoparticle formulations, as compared with solution formulations, increase\nthe drug concentration around a tumor by 100\u2013400%. These increases are\nphenomenal by any measure. What is missing here, however, is the big\npicture showing the full story on drug delivery. It should be understood that\n&gt;95% of the administered nanoparticles end up at sites other than the\ntargeted tumor; this fact has been largely overlooked. Nanoparticles may\nprovide an alternative way of making aqueous solution formulations for\nintravenous administration of poorly soluble drugs without using undesirable\nexcipients. This is a great use of nanoparticle approaches. It is simply\ndifferent than the widely believed notion that nanoparticles would be far\nsuperior to non-particulate solution formulations.\n\n\n## Future Opportunities:\n\n![Possible wonders]({% static \"/blog/assets/img/NanoMedicine/4.jpg \"Possible wonders\"\" %})\n\nNanoparticles are rapidly becoming the focus of most efforts aiming at targets and\nsite-specific drug delivery. The targeting ability of nanoparticles depends on certain\nfactors such as particle size, surface charge, surface modification and\nhydrophobicity. Still many problems related to selective binding, targeted delivery\nand toxicity need to be overcome. Limited knowledge about the toxicity of\nnanoparticles is a major concern and certainly deserves more attention. If these\nnanoparticles are cautiously designed to tackle problems related to target and\nroute of administration, they may lead to a new more successful paradigm in the\nworld of therapeutics and research. The most promising research in nanoparticle\nproduction is via using supercritical fluids which are environmentally friendly and\nfree of toxic solvents. Much research is currently being performed to overcome\nthese hurdles which will definitely establish nanoparticle-based drug delivery as the\ngold standard for site-specific therapeutics. Though ample optimizations are existing\nin the field nanotechnological research in the medical field is an evergreen\nopportunity.",
        "author_name": "Manoj A B",
        "slug": "nano-medicine"
    },
    {
        "layout": "post",
        "title": "Intro To The MapReduce Programming Paradigm",
        "author_github": "abhinavpy",
        "date": "2018-12-29T00:00:02",
        "image": "/assets/img/abhinavpy.jpg",
        "description": "MapReduce Paradigm",
        "tags": [
            "Parallel Computing",
            "Big Data Processing",
            "Distributed Systems",
            "Cloud Computing"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "abhinavpy",
        "text": "[{% load static %}]()\nThe following blog is an attempt to explain the map-reduce programming paradigm and some of its practical appliations.\n\n### Introduction:\n\nOver the past few years, many computer scientists at Google have implemented hundreds of special purpose computations that process large amounts of data, such as crawled documents, web request logs, etc., to compute various kinds of derived data. to compute various kinds of derived data, such as summaries of the number of pages crawled per host, the set of most frequent queries in a given day, etc. However, the input data is large and the computations have to distributed across hundreds of thousands of machines in order to finish in a reasonable amount of time. The problem was that it was very difficult to organize this huge input data and process this data with minimalistic time.\nAs a reaction to this complexity, Google designed a new abstraction that allows the user to express the simple computations we were trying to perform but hides the messy details of handling this large data. It was realized that most of our computations involved a map operation to each \u2018logical\u2019 record in our input in order to compute a set of intermediate key/value pairs, and then applying a reduce operation to all the values that shared the same key, in order to combine the derived data appropriately.\n\n### Programming Model:\n\nThe computation takes a set of **input key/value** pairs, and produces a set of **output key/value** pairs. The user of MapReduce library expresses the computation as two functions : map and reduce.\nMap written by the user takes an input pair and produces a set of intermediate key/value pairs. The map reduce library groups together all intermediate values associated with the same intermediate key I and passes them to the reduce funciton.\nThe reduce function also written by the user, accepts an intermediate key I and a set of values for that key. It merges together these values to form a possibly smaller set of values. The intermediate values are supplied to the users reduce function via an iterator.\n\n### Example:\n\nConsider the problem of counting the number of occurrences of each word in a large collection of documents. The user would write code similar to the following pseudo-code:\n\nMap function pseudocode:\n```\n     map(String key, String value):\n       // key: document name\n       // value: document contents\n       for each word w in value:\n         EmitIntermediate(w, \"1\");\n```\nReduce function pseudocode:\n```\n     reduce(String key, Iterator values):\n       // key: a word\n       // values: a list of counts\n           int result = 0;\n       for each v in values:\n           result += ParseInt(v);\n       Emit(AsString(result));\n```\nThe map function emits each word plus an associated count of occurences. The reduce function sums together all counts emitted for a particular word.\nIn addition, the user writes code to fill in a mapreduce specification object with the names of the input and the output files, and optional turing parameters. The user then invokes the MapReduce function, passing it the specification object. The user\u2019s code is linked together with the map-reduce library.\n\n### More Examples:\n\nHere are a few simple examples of interesting programs that can be easily expressed as map reduce computations.\n\n### Distributed Grep:\nThe map function emits a line if it matches a supplied pattern. The reduce function is an identity function that copies the supplied intermediate data to the output.\n\n### Count of URL access frequency:\n\nThe map function processes logs of web page requests and outputs `<URL,1>`. The reduce funciton adds together all values for the same URL and emits a `<URL, total count>` pair.\n\n### Reverse Web-Link Graph:\n\nThe map function outputs `<target, source>` pairs for each link to a target URL found in a page named source. The reduce function concatenates the list of all source URLs associated with a target URL and emits the pair: `<target, list(source)>`\n\nTo learn more about Map Reduce read the paper published by Google given in the references.\n\n### References:\n* [Original Paper by Google](https://research.google.com/archive/mapreduce-osdi04.pdf)\n* [Cloud Computing Concepts Week 1](https://www.coursera.org/learn/cloud-computing/lecture/a3hCy/3-1-mapreduce-paradigm)\n* [Introduction to Big Data and Hadoop](https://ieee.nitk.ac.in/blog//big-data/)",
        "author_name": "Abhinav P Y",
        "slug": "an-intro-to-the-map-reduce-programming-paradigm"
    },
    {
        "layout": "post",
        "title": "Photocatalytic Detoxification",
        "author_github": "tirthsavla",
        "date": "2018-12-29T00:00:03",
        "image": "/assets/img/tirthsavla.jpg",
        "description": "We have been polluting the environment constantly but a cleansing mechanism in the environment itself.",
        "tags": [
            "Photochemical Reactions",
            "Photo catalysts",
            "Catalysis",
            "Photo Catalysis",
            "Nanotechnology",
            "Semiconductors",
            "Colloidal semiconductors"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "tirthsavla",
        "text": "[{% load static %}]()\n# Photocatalytic Detoxification\nThe water bodies around the world are regularly dumped with waste water effluents. These contain various organic and inorganic molecules that affect the aquatic life and overall water quality, which is called water pollution.\nThe surprising fact is that quite a lot of purification is caused naturally by the breakdown of these molecules by sunlight. This process can be assisted with colloidal semiconductors to speed up the reaction process and increase the efficiency. The photocatalytic detoxification of wastewater is a process that combines heterogeneous catalysis with solar technologies.\nOrganic compounds such as alcohols,carboxylic acids,amines, herbicides and aldehydes, have been photocatalytically destroyed in laboratory and field studies. The photocatalytic process can mineralize the hazardous organic chemicals to carbon dioxide, water and simple mineral acids.\nMany processes have been proposed over the years and are currently used to remove organic toxins from wastewaters. Current treatment methods for these contaminants, such as adsorption by activated carbon and air stripping, merely concentrate the chemicals present, by transferring them to the adsorbent or air, but they do not convert them into non-toxic wastes. Thus, one of the major advantages of the photocatalytic process over existing technologies is that there is no further requirement for secondary disposal methods.\nAnother advantage of this process is that when compared to other advanced oxidation technologies, especially those using oxidants such as hydrogen per-oxide and ozone, expensive oxidizing chemicals are not required as ambient oxygen is the oxidant. Photocatalysts are also self-regenerated and can be reused or recycled.\nDuring the photocatalytic process, the illumination of a semiconductor photocatalyst with ultraviolet (UV)radiation activates the catalyst, establishing a redox environment in the aqueous solution (Zhang et al.,1994a). Semiconductors act as sensitizers for light-induced redox processes due to their electronic structure, which is characterized by a filled valence band and an empty conduction band. The energy difference between the valence and conduction bands is called the band gap. The semiconductor photocatalyst absorbs impinging photons with energies equal to or higher than its bandgap or threshold energy. Each photon of the required energy (i.e. wavelength) that hits an electron in the occupied valence band of the semiconductor atom, can elevate that electron to the unoccupied conduction band leading to excited\nstate conduction band electrons and positive valence band holes (Schiavello,\n1989).The fate of these charge carriers may take different paths (refer to Figure\n1). Firstly, they can get trapped, either in shallow traps (ST) or in deep traps (DT).\nSecondly, they can recombine, non-radiatively or radiatively, dissipating the\ninput energy as heat. Finally, they can react with electron donors or acceptors\nadsorbed on the surface of the photocatalyst. In fact, it was recently shown that\nany photoredox chemistry occurring at the particle surface, emanates from\ntrapped electrons and trapped holes rather than from free valence band holes,\nand conduction band electrons.\n\nThese semiconductors when used in the size range if 1-100 nanometers give exceptional results due to changes in several physical and chemical properties that come along with quantum size range. Thus, the study of these particles will surely give us permanent and effective solutions of ridding the environment of harmful pollutants and advancing towards a cleaner tomorrow.\n \n![untitled.jpg]({% static '/blog/assets/img/photocatalytic-decomposition/untitled.jpg' %})",
        "author_name": "Tirth Savla",
        "slug": "photocatalytic-decomposition"
    },
    {
        "layout": "post",
        "title": "Thermal Imaging",
        "author_github": "alabhya01",
        "date": "2018-12-29T00:00:01",
        "image": "/assets/img/authors/alabhya01.jpg",
        "description": "Can you see your hand print on a wall , or find someone hiding behind curtains or the see the blood vessels beneath your skin? With Thermal imaging you can do all these ! Thermal Imaging converts heat emitted from an object to an image. But is that all ? Read this article to explore more about Thermal Imaging.",
        "tags": [
            "Computer Vision",
            "Thermal Vision",
            "Image Processing",
            "IR",
            "Sensors"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "alabhya01",
        "text": "[{% load static %}]()\nHave you ever tried talking to people about Thermal camera .Well, the general response you receive is , \"ohh.. , the night vision camera .\"\nIf you also have the same notion , you actually don't know about this wonderful technology.\n\nAlthough Thermal imaging is used for  **night vision** ,both are not the same, and also ,apart from that thermal imaging has got a much wider area of application, to which people are unaware.\n\nThis main aim of this article is to demonstrate the various exiting use of thermal imaging .\n\nHope, after reading it you would appreciate thermal imaging more than you do it now.\nSo,\n\n![1.jpg]({% static '/blog/assets/img/thermal-imaging/1.jpg' %})\n\n\n#### But  First,\n### What is Thermal Camera\n A thermal camera is basically a bunch of infrared sensors that capture or sense the infrared, radiated from an object. The image is based on the difference of thermal radiation emitted from an object w.r.t to it's surrounding .\n #### So do we see Infrared radiation ?\n well, actually not.Human eyes are not sensitive to it. The data from the thermal sensors are processed in form of an image and color is assigned based on camera settings.\n Here is the general working of thermal camera:\n\n![2.jpg]({% static '/blog/assets/img/thermal-imaging/2.jpg' %})\n\n### Are thermal imaging and night vision not the same thing?\nwell, technically not. Both, Thermal imaging and Night Vision increases visibility in low lighted areas but the technology used are different. Night vision camera capture both visible spectrum as well as short wave infrared, on the other hand thermal imaging capture just long wave infrared.\nThermal imaging gives a high contrast image and thus identifying living targets is much easier in thermal imagery\n\n[Here](https://www.youtube.com/watch?v=rAvnMYqj2c0) is a nice comparison video between Thermal Imaging and Night Vision\nClearly night vision is much popular in army and forest ranging\n\n### Can we use thermal camera in daylight\nYes, of course. Thermal Camera do not capture visible light, hence the picture are almost same in day as it is in night.\n\nHowever , the surrounding is much cooler in night as compared to day , hence the pictures taken in daylight is not as contrast as they are in night and is a bit corrupted by noise\nYou can compare the performance in this [video](https://www.youtube.com/watch?v=2yqPs8vbGSg)\n\n\"**Hey**, but still Thermal Imaging is a night vision technique only , right?\" . If you are also pondering over the same question , here are some other cool stuffs for which Thermal Imaging is used for:\nLet us start with common use\n### Some common use of Thermal Imaging\n#### Fire Fighting\nFire fighters use thermal imaging to see through smoke\n**Wait a sec, But is smoke not warmer than it's surrounding?**\nWell may be , but smoke particles are too small to radiate enough infrared. Thermal cameras capture the infrared radiation and not the absolute heat of a body, hence smoke and fog is invisible to Thermal imaging\n(PS: Dense fog may interfere with Thermal vision)\n\n![3.jpg]({% static '/blog/assets/img/thermal-imaging/3.jpg' %})\n\n#### Electronics and Machinery\nThermal imaging is used to find out which part of a circuit is hot and consuming more power. It ,hence helps in conserving power or preventing hazards\n\n![4.jpg]({% static '/blog/assets/img/thermal-imaging/4.jpg' %})\n\n#### Medical Field\nThermal Imaging is popularly used to detect cancerous cells.\n**HOW?**\nThe answer is that cancerous cells multiply really fast and hence radiate higher heat than other cells which is easily captured by thermal imaging.\n\n**Bored? Just Wait a sec**\nThese were the general uses of Thermal Imaging.\nHowever, the stuffs that are most astonishing are upcoming\n### Thermal Biometry\nBiometric scanning systems were developed with an approach to create an unbeatable security system, however the fingerprint scanner and even the iris scanner could be easily hacked today.\nHence, a more efficient technology is required for highly secure purpose. Well Thermal imaging provide us the way.\nThermal waves are generated due to heat from the body. Thermal image of a human face contains the heat from the superficial blood vessel lying just beneath the face.\n\n![5.jpg]({% static '/blog/assets/img/thermal-imaging/5.jpg' %})\n\nThe blood vessel pattern can be easily created by Thermal imaging.The blood vessel pattern of face are unique for each person and can be used for biometric imaging and scanning\n### Forgery\nWell, not a good use of this wonderful technology, but yes, thermal imaging may be used to wrongfully access your passwords or pins.\n**What Really?**\nYes, Thermal imaging can easily detect the buttons those were touched. In fact with a good quality camera you can even judge the order in which button are pressed.\n So,a little application of brain and your pin is out.\n\n![6.jpg]({% static '/blog/assets/img/thermal-imaging/6.jpg' %})\n\nA [Research](https://cseweb.ucsd.edu/~kmowery/papers/thermal.pdf) conducted for the same ,states that there is about 80% chance of getting the pin , if the image was\ntaken just after the entering the pin and 60% chance of getting the pin even if the image is taken after a minute time interval.\nThe good thing is thermal imaging fail on metal keyboards(like the ones in the ATM).\nBut still, the best thing to do is to press some random key after entering your pin.\n### The Green building project\nThe most innovative use of Thermal Imaging was carried out by Brian Harper, who was a part of the team that created the first thermal camera.\nHe came up with an idea of carrying out an **internal Thermal Survey** of buildings. The thermal imaging showed the points in the building from where the energy leakage were the most.\nThis made the people more likely to take action.\n\n![7.jpg]({% static '/blog/assets/img/thermal-imaging/7.jpg' %})\n\nThe survey was a great success as the householders who got the thermal survey reports were about 5 times more likely to take action.\nThe project also made Thermal Camera cheaper and portable. Earlier ,the least cost of a Thermal Camera was around 10,000 US dollars(about 7,00,000 INR),but now it is available in about 400US dollars(28,000 INR)\nEven, now thermal devices are available for mobile phones. You just need to attach them to the camera of your mobile and you are good to go.\n\n### Conclusion\nThermal Imagery is quite new technology and hence is expensive.\n , but even after that it find its scope everywhere, say industry , home, defense , fun  and much more. For sure it has got a lot of scope in the future, the requirement is only of an innovative mind to employ it in right way\n\n### Some more exciting stuffs in Thermal Imaging\n[BBC-Future-The man who makes you see the invisible](http://www.bbc.com/future/story/20170614-thermal-imaging-reveals-the-hidden-heat-lost-from-your-home)\n\n[Thermal World of Chemical Reactions](https://www.youtube.com/watch?v=VSyEWg10WjM)\n\n[FLIR SC6800 Midwave Research & Science Thermal Imaging Camera](https://www.youtube.com/watch?v=IoXEXB_8dpk)",
        "author_name": "Kumar Alabhya",
        "slug": "thermal-imaging"
    },
    {
        "layout": "post",
        "title": "Self Healing Materials",
        "author_github": "smrithis",
        "date": "2019-01-14T21:30:00",
        "image": "/assets/img/",
        "description": "Overview of Self Healing Machines",
        "tags": [
            "Biochemistry",
            "Self Healing Materials",
            "Machines"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "smrithis",
        "comments": true,
        "text": "[{% load static %}]()\nA breakthrough for bio-inspired technology.\n\nMany natural organisms have the ability to repair themselves. Now, manufactured machines will be able to mimic this property. Researchers have created a self-healing material that spontaneously repairs itself under extreme mechanical damage. \n\n## What is biomimicry ?\n\nGame-changing ideas begin by seeing the possibility for sustainable life reflected in the living system around us. Biomimicry is an approach to innovation that seeks sustainable solutions to human challenges by emulating nature\u2019s time-tested patterns and strategies. The goal is to create products, processes, and policies which guide us to a new lifestyle that is sustainable and well adapted on earth over the long haul.\nNature has incorporated within itself innumerous ways to not only grow back to life but also provide us immense knowledge and ideas to develop new and varities of solutions to many of the problems we are grappling with. Animals, plants and microbes are the consummate engineers. As the most intelligent species on earth, we started to notice such \u201cmiracles\u201d and adapt them to engineer out solutions to problems related to various fields including machining, architecture, robotics, polymerizations etc. \n\n![Fig1]({% static '/blog/assets/img/self-healing-materials/fig1.jpg' %})\n\n## What are Self-healing materials? \n\nSelf-healing materials are artificial or synthetically-created substances which have the built-in ability to automatically repair damage to themselves without any external diagnosis of the problem or human intervention. Although the most common types of self-healing materials are polymers or elastomers, self-healing covers all classes of materials, including metals, ceramics, and cementitious materials. These materials also include isocyanates monomers such as DCPD GMA epoxy resin, linseed oil and tung oil. By using the aforementioned materials for self-healing in coatings, it was proven that microencapsulation effectively protects the metal against corrosion and extends the lifetime of a coating.\n\n## Types of self-healing material and how do they work?\n\nThe first self-healing materials were polymers with a kind of embedded internal adhesive, reported in 2001 by Scott White, Nancy Sottos, and colleagues from the University of Illinois at Urbana-Champaign. Since then, a variety of other self-healing materials have been developed. \n\nSelf-healing materials come in four main kinds: \n\n### 1. Embedded healing agents:-\n\nThe best-known self-healing materials have built-in microcapsules (tiny embedded pockets) filled with a glue-like chemical that can repair damage. If the material cracks inside, the capsules break open, the repair material \"wicks\" out, and the crack seals up. It works in a similar way to a type of adhesive called epoxy, which is supplied in the form of two liquid polymers in separate containers (often two syringes). When you mix the liquids together, a chemical reaction occurs and a strong adhesive (a copolymer) forms.\n\n![Fig2]({% static '/blog/assets/img/self-healing-materials/fig2.jpg' %})\n\n### 2. Microvascular materials :-\n\nOur body has an amazingly comprehensive vascular system (a network of blood vessels of different sizes) that transport blood and oxygen for energy and repair. If damage occurs, our blood system simply pumps extra resources to the places where they're needed, but only when they're needed. Materials scientists have been trying to design self-healing materials that work the same way. Some have networks of extremely thin vascular tubes (around 100 microns thick\u2014a little thicker than an average human hair) built into them that can pump healing agents (adhesives, or whatever else is needed) to the point of failure only when they need to do so. The tubes lead into pressurized reservoirs (think of syringes that are already pushed in slightly). When a failure occurs, the pressure is released at one end of the tube causing the healing agent to pump in to the place where it's needed.\n\n![Fig3]({% static '/blog/assets/img/self-healing-materials/fig3.jpg' %})\n\n### 3. Shape-memory alloys :-\n\nMost of us know shape memory materials through relatively trivial everyday applications such as eyeglasses, made from alloys like nitinol (nickel-titanium), that flex exactly back to shape when you bend and then release them. Usually, shape memory works in a more complex (and interesting) way than this. Typically you need to heat (or otherwise supply energy to) a material to make it snap back to its original, preferred form. In practice, that might be an embedded network of fiber-optic cables similar to the vascular networks used in other self-healing materials except that, instead of pumping up a polymer or adhesive, these tubes are used to feed laser light and heat energy to the point of failure. That causes them to flip back into (\"remember\") their preferred shape, effectively reversing the damage. \n\n![Fig4]({% static '/blog/assets/img/self-healing-materials/fig4.gif' %})\n\n### 4. Reversible polymers:- \n\nPolymers don't always need sophisticated internal systems, such as embedded capsules or vascular tubes, to repair internal damage. Some of them break apart to reveal what we might think of as highly \"reactive\" ends or fragments that naturally try to join up again. Energized by either light or heat, these stray fragments naturally try to rebond themselves to other nearby molecules, effectively reversing the damage and repairing the material. Some break to expose electrically charged ends, which give the broken fragments a built-in electrostatic attraction. When damage occurs, electrostatic forces pull the fragments together, enabling the material to self-repair.\n\n![Fig5]({% static '/blog/assets/img/self-healing-materials/fig5.png' %})\n\n## Applications:-\n\nIt's not difficult to imagine all kinds of applications for self-healing materials, from bridges and buildings that repair their own cracks to car fenders made from shape-memory polymers that automatically flex back to shape after low-speed collisions. The first self-healing materials we're likely to see in mass production will be paints and coatings that can better survive the weather and other kinds of surface wear-and-tear. Because the material also exhibits high electrical conductivity that does not change when stretched, it is ideal for use in power and data transmission. These soft-matter composite material is composed of liquid metal droplets suspended in a soft elastomer. When damaged, the droplets rupture to form new connections with neighboring droplets and reroute electrical signals without interruption. Circuits produced with conductive traces of this material remain fully and continuously operational when severed, punctured, or had material removed.\n\n![Fig6]({% static '/blog/assets/img/self-healing-materials/fig6.jpg' %})\n\nSelf healing materials are gradually finding their way into everyday application.Imagine owning a car that can \"heal\" itself when met with an accident or when suffered scratches or bumps. One day, we might even have replacement parts for the human body that can heal themselves as well as their natural equivalents. At that point, the science of self-healing will truly have come round full circle. Trust scientists to reinvent nature!",
        "author_name": "Smrithi S",
        "slug": "self-healing-material"
    },
    {
        "layout": "post",
        "title": "Heard of Smart Contracts yet?",
        "author_github": "prajwal27",
        "date": "2019-01-14T21:30:00",
        "image": "/assets/img/",
        "description": "Overview of Smart Contracts",
        "tags": [
            "Blockchain 2.0",
            "Smart Contract"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "prajwal27",
        "comments": true,
        "text": "[{% load static %}]()\nSmart contract, also known as a cryptocontract , is a computer program that directly controls the transfer of digital currencies or assets between parties under certain conditions.It not only defines the rules and penalties related to an agreement in the same way that a traditional contract does, but it also automatically enforces those obligations.\nSmart contract are similar to a contract in the physical world, but it\u2019s digital and is represented by a tiny computer program stored inside a blockchain.\n\n>_**What's the main idea behind a Smart Contract?**_  \n\n**Since a smart contract removes reliance on a third party when establishing business relations, the parties making an agreement can transact directly with each other.**\n\n![Working of Smart Contract]({% static '/blog/assets/img/smart-contract/smart.jpeg' %})\n\nConsider the example of a crowdfunding platform where product teams share their projects and collect money from supporters until a goal is reached.\nIf such a platform is centralized \u2013 like ABC, for instance \u2013 then it acts as a third party between product teams and supporters who donate their money. This means both sides need to trust ABC and, in fact, pay an additional fee to ABC to serve as an intermediary.A smart contract can perform the same crowdfunding actions like sharing projects, setting goals, and collecting donations, but without a third party.We can program a smart contract to execute all of these actions.More precisely, we can program a smart contract to receive funds until a goal is reached. If the project gets fully funded before the deadline, the money raised automatically goes to the product team. If the project fails, the money automatically goes back to supporters.Since a smart contract is stored inside a blockchain where all data is stored in a decentralized manner, no one is in control of the money. In a decentralized business model, smart contracts replace any other trusted third party.\n\n## Why trust a Smart Contract?\n1. **They\u2019re immutable** : which means a smart contract can never be changed and no one can tamper with or break a contract.\n2. **They\u2019re decentralized**: which means that the outcome of the contract is validated by everyone in the network, just like any transaction on a blockchain. Distribution makes it impossible for an attacker to force control to release funds, as all other participants would detect such an attempt and mark it as invalid.\n\n## So what are the benefits of Smart Contract?\n1. **Direct dealings with customers** : Smart contracts remove the need for intermediaries and allow for transparent, direct relationships with customers.\n2. **Fraud reduction** : Since smart contracts are stored in a decentralized blockchain network, their outcome is validated by everyone in that network.\n3. **Record keeping** : All contract transactions are stored in chronological order in the blockchain and can be accessed along with the complete audit trail.",
        "author_name": "Prajwal S Belagavi",
        "slug": "smart-contracts"
    },
    {
        "layout": "post",
        "title": "The Missing Circuit Element",
        "author_github": "srsahith",
        "date": "2019-01-14T21:30:00",
        "image": "/assets/img/",
        "description": "A brief description to memresitors and its applications",
        "tags": [
            "Memresistor",
            "Non Volatile Memory"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "srsahith",
        "comments": true,
        "text": "[{% load static %}]()\nLeon Chau, an American circuit theorist at the University of California, predicted the existence of the fourth fundamental circuit passive element. The first three passive circuit elements are the resistor, capacitor, and inductor. We already have relationships among voltage, current, charge, and flux. The only relationship missing is that between flux and charge(q).\n \n![Image 1]({% static '/blog/assets/img/the-missing-circuit-element/1.jpeg' %})\n\nMemristor is the component that connects the dots. The diagram shows how memristor serves as a missing link and completes the symmetry of the diagram due to which it is famously known as **\"The Missing Circuit element\"**. Its symbol is.\n\n![Image 2]({% static '/blog/assets/img/the-missing-circuit-element/2.jpeg' %})\n\n|  Device       |  Characteristic property (units) |   Differential equation  |\n|---------------|--------------------------------|-------------------------|\n| Resistor (R)  | Resistance (V / A, or ohm, \u03a9)    |   R = dV / dI            |\n|Capacitor (C)  |  Capacitance (C / V, or farad)   |   C = dq / dV            |\n|Inductor (L)   |  Inductance (Wb / A, or henry)   |   L = d\u03a6m / dI           |\n|Memristor (M)  |  Memristance (Wb / C, or ohm)    |   M = d\u03a6m / dq           |\n\nMemristor is a nonlinear, passive electrical two terminal component which relates magnetic flux linkage and charge. According to the characterizing mathematical equations, the memristor's resistance is not constant but depends on how much electric charge has flowed in what direction through it in the past; the device remembers its history. A memristor is often compared to an imaginary pipe that carries water. When the water flows in one direction, the pipe's diameter expands and allows the water to flow faster -- but when the water flows in the opposite direction, the pipe's diameter contracts and slows down the water flow. If the water is turned off, the pipe retains its diameter until the water is turned back on. To continue the analogy, when a  memristor's power supply is switched off, the memristor retains its resistance value. This would mean that if power to a computer is switched off, all the applications and documents that were open before the shut down would still be right there on the screen when the computer is restarted. When the electric power supply is switched off, it remembers the most recent resistance until powered again. The new two-terminal passive element is named memristor as it combines the behavior of memory and a resistor.\n\n![Image 3]({% static '/blog/assets/img/the-missing-circuit-element/3.jpeg' %})\n\nThe image shows the schematic diagram of a memristor. On application of a positive bias across the electrodes the oxygen vacancies spread towards the undoped region which leads to an increase in conductivity. Now if we remove the bias, the device remains in this state forever theoretically. Similarly, application of a negative bias across the device pulls back the oxygen vacancies into the doped region thereby increasing the resistance of the device. So data is stored in memristor regarding resistance. A high resistance corresponds to one logic level, and low resistance corresponds to the other. In place of TiO2 other compounds such as HfO2 can be used.\n\n## Classification of Memristors\n\nMemristors can be classified into different types, depending on how they are built. Memristors are classified into two types, and a brief overview of different memristors is explained below.\n\n![Image 4]({% static '/blog/assets/img/the-missing-circuit-element/4.jpeg' %})\n\n1. Ionic thin film and Molecular Memristors\n\nMolecule and Ionic thin-film memristors mostly rely on different material properties of the thin film atomic lattices that display hysteresis below the application of charge.\n\n2. Magnetic and spin-based memristors\n\nSpin-based memristors are opposite to ionic nanostructure and molecule-based systems and rely on the property of degree in electronic spin. In this type of system, the polarization of electronic spin is aware\n\n## Applications\n\nThese promising characteristics may potentially revolutionize nanoelectronics. It can find applications in analog and digital circuits, which are part of everyday use systems such as sensors and mobile phones. Some of the most promising applications of memristors are:\n\nThe most observable application of a memristor is the memory. A memristor can store a single bit of data in DRAM \u2013 where the capacitors are restored with memristors. When compared to DRAM and SRAM, this kind of memory has many benefits like \u2013 it is non-volatile; it displays good scalability, and it has no leakage power. This type of memory is superior to flash memory regarding scalability and speed.\n\nMemristors perform equally well like the biological synapses. This feature makes good building blocks in neuromorphic systems, where synapses and neurons are formed as electronic systems. This kind of memory has many benefits when we compare it with the DRAM and SRAM.",
        "author_name": "S.R. Sahith",
        "slug": "the-missing-circuit-element"
    },
    {
        "layout": "post",
        "title": "Social Engineering",
        "author_github": "pran-p",
        "date": "2019-01-17T23:32:44",
        "image": "/assets/img/",
        "description": "A brief introduction to Social Engineering",
        "tags": [
            "Social engineering",
            "Security"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "pran-p",
        "text": "[{% load static %}]()\nHave you ever stared into the keyboard while your friend is typing a password? Or have you ever made up stories to play a prank on your friend? If the answer to any of the above questions is yes then you are a **Social Engineer!**\n\n![image]({% static '/blog/assets/img/social-engineering/0.jpg' %}) \n\nWell now that I have said that you are a social engineer let's see what that really means.\nIt refers to psychological manipulation of people into performing actions or giving away confidential information. A type of confidence trick for the purpose of information gathering, fraud, or system access. It may also force people to giving away sensitive information which may cost them or their companies a lot!\n\nIt goes by the words *\"People are the weakest link in security\"* and this is absolutely true! All the complex security for your systems go void when a social engineering attacks takes place.\n\n![image]({% static '/blog/assets/img/social-engineering/1.jpg' %}) \n\nYou can find social engineers everywhere. Even your friends sitting next to you concentrating on your keyboard as you type your passwords is a social engineer. It\u2019s just that there is no certification for this thing. So, let\u2019s see the types of social engineering attacks.\n\n## Types of Social Engineering\n\n#### 1. Phishing\n\n![phishing]({% static '/blog/assets/img/social-engineering/2.jpg' %}) \n\nPhishing is the fraudulent attempt to obtain sensitive information such as usernames, passwords and credit card details by disguising as a trustworthy entity. This mainly uses disguised email or fake website as a weapon.The main goal of this attack is to trick the email recipient into believing that the message is something they want or need for instance a request from their bank to reset their password or a note from someone in their company asking them to download a file infected with malware. It's one of the oldest types of attacks, dating back to the 1990s, and it's still one of the most widespread and techniques becoming increasingly sophisticated day by day. It mainly aims to get your personal information.\nIt works on the principle that if you cast a large enough net, you are sure to catch a fish.\nThe mail is sent to you may contain:\n\n* Request information \n* Download infected files\n* May ask you to visit a compromised site\n\n![phishing]({% static '/blog/assets/img/social-engineering/6.png' %}) \n\n#### 2. Pretexting\n\n![pretexting]({% static '/blog/assets/img/social-engineering/3.jpg' %}) \n\nA fictional situation is created for the purpose of obtaining personal and sensitive information from an unsuspecting individual. It usually involves researching a target and making use of his data for impersonation or manipulation. An important aspect of pretexting is establishing a target's trust.\nAttackers can impersonate co-workers, police officers, bankers, tax authorities, insurance investigators, etc. A simpler way to put is that anyone who can be perceived as having right-to-know the information by the target. The attacker only needs to prepare answers to questions that might be asked by the target. Most of the times, an authoritative voice, an earnest tone, and an ability to think on one\u2019s feet are all that is needed to create a pretextual scenario.\n\n#### 3. Baiting\n\n![baiting]({% static '/blog/assets/img/social-engineering/4.jpg' %}) \n\nAs you guys might have guessed, it's the act of promising something good in return of some action or information. Attackers may offer users free music or movie or game downloads, if you give them login credentials to a certain site. Attackers can also focus on exploiting human curiosity via the use of physical media.\n\nNow let's see how this attack can be done?\nLet\u2019s take an industrial scenario \u2013 with the end goal of infiltrating a company\u2019s network, the attacker may distribute flash drives infected with malware to employees, hoping that the hardware will be inserted into network-connected systems as the means to spread malicious code. Perhaps the innocent-looking devices are in a basket of freebies placed in the company lobby. These devices can be placed strategically so a targeted person will take the bait. The devices may be marked with intriguing labels link \"Confidential\" or \"Salary\" to tempt the people to open the files. The employ may take the bait and insert the device into the system connected to the network and Voil\u00e0 !\n\n#### 4. Quid Pro Quo\n\n![quid-pro-quo]({% static '/blog/assets/img/social-engineering/7.PNG' %}) \n\nQuid Pro Quo Attacks promise a benefit in exchange for information. The benefit usually assumes as a form of service, baiting usually takes the form of a good.It can be considered as a request for your information in exchange for some compensation. If it sounds too good to be true, it probably is quid pro quo\u2026\n\n#### 5. Tailgating\n\n![tailgating]({% static '/blog/assets/img/social-engineering/5.jpg' %}) \n\nEntering a building directly behind someone who has used their identity for access.It takes advantage of the people being courteous. For example: A person impersonates a delivery driver and waits outside a building. When an employee gains security\u2019s approval and opens their door, the attacker asks that the employee \u2018hold the door\u2019, thereby gaining access through someone who is authorized to enter the company\n\n\nI'll share some links for you guys to see how big damage these simple attacks can make. Do have a read :\n\n- [Social Engineering: Anatomy of a Hack](https://www.csoonline.com/article/2123704/fraud-prevention/social-engineering--anatomy-of-a-hack.html?page=1)\n- [Top 12 security breaches](https://heimdalsecurity.com/blog/12-true-stories-that-will-make-you-care-about-cyber-security/)\n- [Yahoo data breach](https://www.wired.com/story/yahoo-breach-three-billion-accounts/)\n- [Don't fall for this scam](https://www.thequint.com/news/webqoof/amazon-online-discount-scam-on-whatsapp)",
        "author_name": "Pranav P",
        "slug": "social-engineering"
    },
    {
        "layout": "post",
        "title": "INNOVATIONS IN THE DRONE INDUSTRY",
        "author_github": "KushagraRawat",
        "date": "2019-01-26T10:00:30",
        "image": "/assets/img/",
        "description": "An Introdution to innovations happening in the drone Industry",
        "tags": [
            "Drones"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "KushagraRawat",
        "text": "[{% load static %}]()\n> \"Do you think drones are just four rotor-legged flying machines?\"  \n> \"Have you ever thought how we can modify these machines to get maximum out of it?\"  \n> Multiple innovations are happening currently in this very industry and here are some of them!  \n\n## GOING BLADELESS : THE FUTURE OF DRONES ?\n\nAs much crazy as it might sound , bladeless drone are becoming a reality . Bladeless ? You ask me , well , it might be a bit of misnomer . Bladeless isn\u2019t actually bladeless in the fact the it still relies on a bladed propeller , to force the pressurised air generating thrust . So then what\u2019s new here . The propeller here is ducted from the main body all the way to a nozzle to guide the air through . Something like this:  \n\n![img1]({% static '/blog/assets/img/innovations-in-drone-industry/1.png' %})\n\nAnd here is some takeaways from this awesome new technology \n* Reduces the risk of blade related injuries to the controller as well as other bystanders\n* Reduces blade damage to the drone by meshing the inlet of the duct\n* Increases the maneuverability of the drone by providing the nozzles to guide the air  \n\n[The Bladeless Drone]( https://www.youtube.com/watch?v=5L6FSdUmEpg)  \n\n## THE FOLDABLE DRONE    \n\n![img2]({% static '/blog/assets/img/innovations-in-drone-industry/2.png' %})\n\nThe foldable drones are now a reality !  \nThese drone have the ability to fold the rotor arms to various configurations , enabling the drones to cross low opening passages and places which would have been inaccessible otherwise .The first foldable drone was developed by students from the University of Zurich (UTZ) and EPFL .  \nEach of its four arms has a servo motor at the base that can rotate one propeller independently.There are several morphologies that can be adopted by the drone, like completely unfolded (\u201cX\u201d morphology), completely folded (\u201cO\u201d), straight line (\u201cH\u201d), and partly folded (\u201cT\u201d).However the drone can also acquire highly asymmetric morphologies whilst maintaining excellent stability thanks to its to its adaptive on board computing system (OBCs).   \n\n![img3]({% static '/blog/assets/img/innovations-in-drone-industry/3.png' %})  \n\n![img4]({% static '/blog/assets/img/innovations-in-drone-industry/4.png' %})   \n\n![img5]({% static '/blog/assets/img/innovations-in-drone-industry/5.png' %})  \n\n### Key advantages of this technology  \n\n* Allows the drone to reach inaccessible places which could further its capabilities. (for eg: rubble from an earthquake , landslides , etc.)\n* Enhances the stability of the drone by making use of the adaptive algorithms in the OBC  \n* Increases the compactness of the drone and hence portability   \n* Allows physical inspection of structures   \n* Easy transportation of items\n\n\n[SHAPE CHANGING DRONE](https://spectrum.ieee.org/automaton/robotics/drones/foldable-drone-changes-its-shape-in-mid-air)  \n\n## UNDERWATER DRONES   \n\nIf blades drones and shape changing drones weren't enough , here\u2019s the underwater drone. Underwater drones have the capability to dive mid flight into water , stay submerged , navigate , perform functions underwater , resurface and fly back . These type of drones require to be waterproof at high pressures and temperature , however have lesser payload carrying capacity than owing to a heavy watertight structures around the frame . However these drones can be very versatile since they can easily operate in heavy rains.\n\n### Key advantages   \n* Can operate in underwater missions like inspection of bridges , pipelines , knowing the extent of damage done by oil spills , etc.\n* Can operate in unforgiving weather conditions \n\n[UNDERWATER DRONE](https://www.youtube.com/watch?v=FC9EJhs0pc0)  \n\n![img6]({% static '/blog/assets/img/innovations-in-drone-industry/6.png' %})  \n\n![img7]({% static '/blog/assets/img/innovations-in-drone-industry/7.png' %})  \n\n### References  \n\n* https://airdronecraze.com/quick-reference-guide-of-drone-terminology/  \n* https://www.youtube.com/watch?v=FC9EJhs0pc0  \n* https://www.red-dot.org/project/bladeless-drone-26825/  \n* https://www.youtube.com/watch?v=5L6FSdUmEpg",
        "author_name": "KushagraRawat",
        "slug": "innovations-in-drone-industry"
    },
    {
        "layout": "post",
        "title": "Machine Learning Magic",
        "author_github": "saharsh007",
        "date": "2019-01-26T10:00:30",
        "image": "/assets/img/machine-learning-magic",
        "description": "Basic Introduction to Machine Learning",
        "tags": [
            "Machine learning",
            "Supervised learning",
            "tweets"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "saharsh007",
        "text": "[{% load static %}]()\n>\"Did you ever wonder how ML algorithms work?\"<br>\n>\"How is the weather forecast done?\"<br>\n>\"How does the self-driving car mimic the way humans drive?\"<br>\n>\"Let's try to find answers to some of those questions...\"\n\n## WHAT IS THIS MACHINE LEARNING ?\n\nAssume you have some data, it could be anything like a set of tweets or Images of handwritten numbers or Emails marked as spam.\nBut you don\u2019t know what to do with it!!!\n\n![img1]({% static '/blog/assets/img/machine-learning-magic/1.jpeg' %})\n\nSo you look for an algorithm which can use this data and give you something you are interested in!  \n\n![img2]({% static '/blog/assets/img/machine-learning-magic/2.png' %})\nSo how does this becomes possible? We take the example of marking tweets as suicidal. We make an ML model which is a simple function that takes emails as input and gives spam or not spam as output. So now we tell this model to filter out all those tweets which have `#suicide` or related words. But that can be many different types of tweets, for example-  \n\n* the tweet is posted for suicide awareness and prevention\n* the person is talking about suicidal ideation and/or ways to kill himself\n* the tweet reports a third person\u2019s suicide eg: news report\n* the tweet uses suicide as a figure of speech eg: career suicide\n\nSo we have to filter, even more, example:\n\n* The text conveys a serious display of suicidal ideation; e.g., I want to die or I want to kill myself or I wish my last suicide attempt was successful;\n* Posts where suicide plan and/or previous attempts are discussed; e.g., \u201dThe fact that I tried to kill myself and it didn\u2019t work makes me more depressed.\u201d\n* The tone of the text is cloudy and not serious, eg: This makes me want to kill myself, lol, \u201dThis day is horrible, I want to kill myself hahaha\u201d are not included in this category.\n\nSo we filter out tweets containing words like \u201chaha\u201d or \u201clol\u201d, and include the tweets having \u201cI want to die\u201d, \u201cI want to kill\u201d, occurrences of \u201cI\u201d, \u201ckill\u201d and \u201cmyself\u201d together. We train it with labelled test set(tweets labelled as suicidal or not suicidal).  Training means adding features and tweaking parameters of function so that it obtains the same result that is labelled on training data. After doing this our Model is ready.  \nNow we can feed any tweet and it\u2019ll use the algorithm and tell us if a tweet is suicidal or not.  \nWe can\u2019t classify everything correctly, for example,  The tweet \u201dIs it odd to know I\u2019ll commit suicide?\u201d is a tweet that both human annotators, as well as the proposed methodology, couldn\u2019t classify due to its ambiguity, but still, for normal tweets, these algorithms can achieve remarkable accuracy.\n\n![img3]({% static '/blog/assets/img/machine-learning-magic/3.png' %})\n\nSo Machine Learning is the science of programming computers so they can learn from data!\nML is a subset of the larger field of artificial intelligence (AI) that \u201cfocuses on teaching computers how to learn without the need to be programmed for specific tasks,\u201d  \nIn fact, the key idea behind ML is that it is possible to create algorithms that learn from data and make predictions on it.\n\n## TYPES OF MACHINE LEARNING\n\n### SUPERVISED LEARNING:<span style=\"color:red\"> Train me!</span>\n\nThe case we saw above is an example of supervised learning, we had the labelled tweets and made a function to map tweets with the output\n\nBut what if the data we had wasn\u2019t labelled?\n\n### UNSUPERVISED LEARNING:<span style=\"color:red\"> I am self-sufficient in Learning</span>\n\nThat is when UNSUPERVISED LEARNING comes into action. It learns from data that are not labelled and does categorization or classification.\n\nAnd the last one is  \n\n### REINFORCEMENT LEARNING: <span style=\"color:red\">My life my Rules! (hit and trail)\n\nIt is about taking suitable action to maximize reward in a particular situation.\n\n![img5]({% static '/blog/assets/img/machine-learning-magic/5.jpeg' %})\n\nReinforcement learning differs from the supervised learning in a way that in supervised learning the training data has the answer key with it so the model is trained with the correct answer itself whereas in reinforcement learning, there is no answer but the reinforcement agent decides what to do to perform the given task. In the absence of training dataset, it is bound to learn from its experience.\n\n![img4]({% static '/blog/assets/img/machine-learning-magic/4.png' %})\n\n![img6]({% static '/blog/assets/img/machine-learning-magic/6.png' %})\n\nHow to learn more about Machine Learning?\n\nThe best place to get Started with basics of ML is Andrew Ng\u2019s [Machine Learning Course](https://www.coursera.org/learn/machine-learning) from coursera. It\u2019s highly recommended for beginners. It\u2019s easy for anyone who has a minimal amount of knowledge of maths :)type\n\n## References\n\n* [https://www.dezyre.com/article/top-10-machine-learning-projects-for-beginners-in-2021/397/](https://www.dezyre.com/article/top-10-machine-learning-projects-for-beginners-in-2021/397/)\n* [https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471](https://medium.com/@ageitgey/machine-learning-is-fun-80ea3ec3c471)\n* [https://ieee.nitk.ac.in/blog/Why-Machine-Learning-Matters/](https://ieee.nitk.ac.in/blog/Why-Machine-Learning-Matters/)\n* [https://towardsdatascience.com/unsupervised-learning-with-python-173c51dc7f03](https://towardsdatascience.com/unsupervised-learning-with-python-173c51dc7f03)\n* [https://www.mathworks.com/discovery/machine-learning.html](https://www.mathworks.com/discovery/machine-learning.html)\n* [https://www.quora.com/What-is-machine-learning-4](https://www.quora.com/What-is-machine-learning-4)\n* [https://www.quora.com/What-is-reinforcement-learning](https://www.quora.com/What-is-reinforcement-learning)",
        "author_name": "Kumar Saharsh",
        "slug": "machine-learning-magic"
    },
    {
        "layout": "post",
        "title": "How machine translation works",
        "author_github": "tanmai-h",
        "date": "2019-02-02T10:00:30",
        "image": "/assets/img/machine-translation",
        "description": "Introduction to how systems translate text across languages",
        "tags": [
            "Natural Language Processing",
            "Supervised learning",
            "Machine translation"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "tanmai-h",
        "text": "[{% load static %}]()\n> \"How does Google translate work? How do Alexa and Siri know what and when to respond to your queries?\"\n\n# What does Machine Translation mean?\n\nMachine Translation is essentially software that automatically renders text from one language into another. Formally speaking it falls under  computational linguistics and language engineering. Machine translation needn't just be conversion across human readable languages but can also be used for tasks like trigger word detection, converting human readable dates like 'Tuesday, the 29th of Jan,2019' to formats like 29-01-2019.\n\n![img1]({% static '/blog/assets/img/machine-translation/machine-translation-diagram2.png' %})\n\n## Various Methods of Machine Translation.\nIn the late 80's, <b> Rule-based Machine Translation</b>, or RBMT was the initial approach for translating text. It involved following rules layed down by linguists to convert across different languages,the rules could apply at the lexical, syntactic, or semantic level. This method produced a very predictable output for a very closed domain and didn't require a bilingual corpus (structured text data in two different languages). The problem with this was the expertise required to develop the rules, and the vast number of exceptions required. It was labour intensive and failed to perform well for real-life text.\n\n<b>Statistical machine translation, or SMT </b> was a technique that was widely adopted in the industry in the early 90's , this techinque used  statistical and probabalistic models to learn to translate text from a given a source to a target language. In a paper in 1990 ([A Statistical Approach to Machine Translation](https://dl.acm.org/citation.cfm?id=92860)) the problem of translation was stated as\n\n>Given a sentence T in the target language, we seek the sentence S from which the translator produced T. We know that our chance of error is minimized by choosing that sentence S that is most probable given T. Thus, we wish to choose S so as to maximize Pr(S/T).\t\n\nThis technique became widely adopted since it <b>didn't require</b> a set <b>convoluted rules, exceptions and complex grammar constructs</b>. All that was needed was sample translations of documents. This approach was completely data dependant, meaning no linguists were needed to specify any sort of rule. SMT quickly outperformed the rule-base methods and soon became the de-facto standard techinque for machine translation. \n\n<b>Google Translate and Bing Translator</b> and many others still employ this technique at the core of their translation systems. Since they operate on statistics they look for patterns in hundreds of millions of documents that have already been translated by human translators. Google Translate makes special use of UN documents, which are translated in all six official UN languages, and thus provide ample linguistic data. This way, they can weigh a plethora of options for phrases presented by various different (human) translations, and select an educated guess based on the one that occurs most frequently. But statistical approaches required careful tuning of each module in the translation pipeline, and the pure data-driven approach also caused loss of syntax distinctions that could  clearly be defined by linguists.\n\nBut recently, a novel approach called <b>Neural Machine Translation, NMT</b> has gained traction. Though this is a statistical approach, this method uses neural networks, primarily <b>Recurrent Neural Networks, RNN</b> to perform language translation. This method falls under Deep Learning and can learn directly, in an end-to-end fashion to translate across various langauges and all it requires is the text corpus. \n\nTranslation is achieved through just a single model that has no extra overheads.\n\n![img2]({% static '/blog/assets/img/machine-translation/encode-decode.png' %})\n\nThe current state-of-the art network architecture is the <b>encoder-decoder</b> model with an attention mechanism that has shown the best results, partly because of the availability of massive datasets and access to huge computational power. \n\nThis architecture involves encoding the input sentence into a vectorized form that the model can manipulate and decode it into another language format. This also takes care of cases where the translated sentence is of a different length than the original sentence. The <b> attention mechanism </b> that is in place allows the neural network to recognize the important words that are relevant for translation across different languages. \n\n![img3]({% static '/blog/assets/img/machine-translation/attn_model.png' %})\n\nThe model shown above, uses a type of RNN unit called <b>LSTM (Long Short-Term Memory)</b>, every word in the source is passed into one LSTM unit in a sequential manner and for each word present in the input, <b>a context vector</b> is generated that measures the importance that must be given to that word while translating. For the encoding, the input is scanned from both directions using a Bidirectional LSTM, and once an encoding is formed decode network translates this text into another language. This allows the model to perform well on a long input sentences and be robust for different types of text.\n\nThe key to the encoder-decoder architecture is the ability of the model to encode the source text into an internal fixed-length representation called the context vector. Once this is done different decoders could be used to translate the source to different languages.  \t\n\n####  References\n  \n1. [How Machine Translation works](http://blog.systransoft.com/how-does-neural-machine-translation-work/)\n2. [Introduction to Neural Machine Translation](https://machinelearningmastery.com/introduction-neural-machine-translation/)\n3. [Machine translation Wiki](https://en.wikipedia.org/wiki/Machine_translation)\n4. [Machine translation](http://blog.globalizationpartners.com/machine-translation.aspx)",
        "author_name": "Tanmai H",
        "slug": "machine-translation"
    },
    {
        "layout": "post",
        "title": "Solving Switch Riddle using RL",
        "author_github": "Madhuparna04",
        "date": "2019-02-28T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "Switch Riddle",
            "Reinforcement Learning",
            "DIAL",
            "RNN"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "Madhuparna04",
        "comments": false,
        "text": "[{% load static %}]()\nIn this article, we will use a Reinforcement Learning based method to solve the Switch Riddle problem. So, let's start with defining the riddle.\n\n\n## Switch Riddle\n\nThere are n prisoners in prison and a warden. The Warden decides to free the prisoners if they can solve the following problem. So, every day the Warden will select one of the prisoners randomly and send him to an interrogation room which consists of a light bulb with a switch. If the prisoner in the room can tell that all other prisoners including him have been to the room at least once then the Warden will free all of them otherwise kill all of them.\nExcept for the prisoner in the room, other prisoners are unaware of the fact that who got selected on that particular day to go to the interrogation room.\n\n![Switch Riddle]({% static \"/blog/assets/img/Solving-Switch-Riddle-using-RL/SwitchRiddle.png \"Switch Riddle\"\" %})\n\n\n[Image Source](https://github.com/iassael/learning-to-communicate)\n\nNow, the prisoner in the interrogation room can switch on or off the bulb to send some indication to the next prisoner. He can also tell the warden that everyone has been to the room at least once or decide not to say anything. If his claim is correct, then all are set free otherwise they are all killed.\n\n## Problem Formulation\n\nWe can observe that switch riddle problem comprises of multiple agents working towards completing a single task. Therefore, the puzzle fits in a Multi-Agent setup where the agents are collaborating to complete the task.\nWe will use a Deep Reinforcement learning based algorithms called the **DIAL( Differential Inter Agent Learning)** to solve this riddle.\n\nThe things that we need to define for most RL problems are states, actions, and rewards. So let's formulate these:\n\n#### State\nThe state for each agent or prisoner is whether the agent is in the interrogation room or not, i.e., if the prisoner n is in the room, then he receives a 1 and others 0 for that particular day or time step.\n\n#### Action\nThere are two choices the prisoners have \n\n1 - Tell the Warden.\n\n2 - Don't tell the Warden.\n\n#### Reward\nThe reward is received at the end of the episode.\n\n+1 - Completing the task.\n\n-1 - Telling the Warden before everyone has gone to the room once.\n\n0 - If the prisoners can't conclude within the max number of time steps.\n\nIn the implementation, the maximum number of timesteps is set to $$4 \\times n - 6$$ where n is the number of agents.\nIn addition to state, action, and reward, since the agents need to communicate through the light bulb to send some information to the next prisoner, we also have this one-bit communication channel, i.e., the bulb.\n\n#### Messages\nTraining Time - In DIAL during training, the messages are continuous; i.e., it can take values ranging from 0 to 1.\n\nTest Time - While testing the agents can only communicate with 0 or 1.\n\n 0 -> Bulb is off\n\n 1 -> Bulb is On\n\n## DIAL Algorithm\n\nThe following diagram shows the architecture of the model used in DIAL.\n\n![DIAL]({% static \"/blog/assets/img/Solving-Switch-Riddle-using-RL/DIAL.png \"DIAL\"\" %})\n\n[Image Source](https://papers.nips.cc/paper/6042-learning-to-communicate-with-deep-multi-agent-reinforcement-learning.pdf)\n\n\nThere are two possible ways in which we can train our model -\n\n1) Without Parameter sharing\n\n2) With Parameter sharing\n\nParameter Sharing means all the agents share the same neural network parameters or we can say that there is a single central network that learns for all the agents.\n\nWhile in No-Parameter sharing there are separate networks for each agent.\n\n### Implementation details -\n\nFollowing is the model architecture used in the [original paper](https://papers.nips.cc/paper/6042-learning-to-communicate-with-deep-multi-agent-reinforcement-learning.pdf) -\n\n![DIAL Architecture]({% static \"/blog/assets/img/Solving-Switch-Riddle-using-RL/DIALarchitecture.png \"DIALarchitecture\"\" %})\n\n[Image Source](https://papers.nips.cc/paper/6042-learning-to-communicate-with-deep-multi-agent-reinforcement-learning.pdf)\n\n1.Instead of directly giving input observation as 0 or 1 to the neural net, it is passed through an embedding layer.\n\n2.The message from the previous agent is passed through a 1-layer MLP before giving it as an input to the RNN.\n\n3.The previous actions of the agent and agent id are passed through lookup tables.\n\n4.The final state embedding is given by - \n\n$$ z^a_t = (TaskMLP(o^a_t) + MLP[|M| , 128 ](m_{t-1}) + Lookup(u^a_{t-1}) + Lookup(a)) $$\n\n5.For better stability and performance a batch normalization layer can be used to preprocess $$ m_{t-1} $$.\n\n6.$$ z^a_t$$ is processed through a 2-layer RNN with GRUs, $$ h^a_{1,t} =GRU[128, 128] (z^a_t, h^a_{1,t\u22121}) $$, which is used to approximate the agent\u2019s action-observation history.\n\n7.\nFinally, the output $$ h^a_{2,t} $$ of the top GRU layer, is passed through a 2-layer MLP $$ Q^a_t, m^a_t =MLP[128, 128,(|U| + |M|)](h^a_{2,t})$$.\n\n## References -\n\n- [Original Lua Implementation](https://github.com/iassael/learning-to-communicate)\n- [Pytorch implementation of DIAL -By Moksh Jain](https://colab.research.google.com/gist/MJ10/2c0d1972f3dd1edcc3cd17c636aac8d2/dial.ipynb)",
        "author_name": "Madhuparna Bhowmik",
        "slug": "Solving-Switch-Riddle-using-RL"
    },
    {
        "layout": "post",
        "title": "ELF Files in Linux",
        "author_github": "arpi-r",
        "date": "2019-03-09T00:00:00",
        "image": "/assets/img/",
        "description": "An introduction to ELF Files in Linux",
        "tags": [
            "ELF",
            "Linux",
            "Binary"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "arpi-r",
        "text": "[{% load static %}]()\nHave you ever tried to print the a.out file generated when you compile your C program? It prints gibberish! Ever wondered how the computer produces a valid, sensible output from this? Read ahead to find out.\n\n## Introduction\n\nELF stands for Executable and Linkable Format. ELF files are usually the outputs of compilers. They exist in binary format (hence, the gibberish when printed). This format helps the operating system interpret the underlying machine instructions correctly. ELF is not only used for binaries and executables but also object code, shared libraries and core dumps.\nAny program is first translated by a compiler to object code. This object code is then linked with system libraries using a linker tool. This produces a binary file which can be executed on that specific platform and CPU type. This file follows the ELF format.\n\n## Structure\n\nYou can look at the structure of an ELF file using the *readelf* command in the terminal.\n\n### ELF Header\n![ELFHeader]({% static '/blog/assets/img/ELF-file-in-Linux/ELFHeader.png' %})\n\nThe first mandatory part of an ELF file is the ELF header. It makes sure that the data is correctly interpreted during linking and execution.\nIts first 4 bytes are the *Magic Number*. A magic number is the first few bytes of any file in Linux which helps the operating system understand what type of file it is. The ELF Header begins with the following bytes (hexadecimal parts): 7f 45 4c 46. 45=*E*, 4c=*L* and 46=*F* in ASCII. \n\nThe next byte indicates the architecture required for the file. 01 is for 32-bit and 02 is for 64-bit architecture. This is the *class field*.\n\nThe following byte represents the type of encoding. 01 indicates LSB (Least Significant Bit) or little-endian. 02 indicates MSB (Most Significant Bit) or big-endian. This value is important in interpreting the remaining objects in the file as they are specified in the corresponding format. Different processors represent data and addresses in various formats, and hence, this field becomes \nvery significant. This is the *data field*.\n\nThe 7th byte is the *version byte* which as the name suggests gives the version of ELF. Currently, there is only one version, and so its value is 01.\n\nThe ELF Header follows an alignment, every field of n bytes starts at a byte which is a multiple of n. So, if a field is 4 bytes then it starts at a byte which is a multiple of 4, say the 20th byte. The 8th to 16th byte is 0. This space is a *buffer.* This provides extensibility to future versions of ELF, in case additional fields are needed and helps provide alignment.\n\nThe 17th and 18th byte are the *type field* of our ELF file. Type can take on four values. 01 indicates Relocatable file (before linking), 02 indicates Executable (binaries), 03 stands for Shared object files (libraries) and 04 for core.\n\nApart from the ELF Header, the ELF file has three parts: Program headers (Segments), Section headers (Sections) and Data.\n\n### Program Headers\n![ProgramHeader]({% static '/blog/assets/img/ELF-file-in-Linux/ProgramHeader.png' %})\n\nThis describes how to create a process/memory image for run-time execution. The kernel uses these segments to map them into virtual address space. It converts predefined instructions into a memory image. To run a binary ELF file, program headers are required. It uses these headers, with the underlying data structure, to form a process. This process is similar for shared libraries. Examples of program headers include *GNU_EH_FRAME and GNU_STACK*.\n\n### Section Headers\n![SectionHeader]({% static '/blog/assets/img/ELF-file-in-Linux/SectionHeader.png' %})\n\nThe section headers define all the sections in the file. This is used for linking and relocation. Once the GCC compiler converts code to assembly and the GCC assembler converts that to object code, these sections appear in the ELF file. Any segment (mentioned above) can have zero or more sections. Executable files have four main sections: *.text, .data, .rodata, and .bss.* Each of these sections is loaded with different access rights.\n\n## Conclusion\n\nELF is a formal specification that defines how instructions are stored in executable code. The ELF file type is extremely flexible. It provides support for multiple CPU types, machine architectures, and operating systems. It is also very extensible. This article is an introduction to ELF Files in Linux. To know more check out the links in References.\n\n## References\n\n - [https://medium.com/@MrJamesFisher/understanding-the-elf-4bd60daac571](https://medium.com/@MrJamesFisher/understanding-the-elf-4bd60daac571)\n \n - [https://linux-audit.com/elf-binaries-on-linux-understanding-and-analysis/](https://linux-audit.com/elf-binaries-on-linux-understanding-and-analysis/)",
        "author_name": "Arpitha Raghunandan",
        "slug": "elf-files-in-linux"
    },
    {
        "layout": "post",
        "title": "Long Short Term Memory Neural Networks",
        "author_github": "nimbus98",
        "date": "2019-08-22T11:00:30",
        "image": "/assets/img/",
        "description": "An introduction to a commonly used Recurrent Neural Network",
        "tags": [
            "Recurrent Neural Networks",
            "Long Short Term Memory Neural Networks",
            "Machine Learning"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "nimbus98",
        "text": "[{% load static %}]()\nLong Short Term Memory Neural Networks or LSTM Neural Network is a commonly used Recurrent Neural Network model that is most commonly used in tasks like speech recognition, music generation etc. Lets take a deep dive into what Recurrent Neural Networks are and why we need and use LSTMs.\n\n# Long Short Term Memory Neural Networks\n\n### Recurrent Neural Networks and Why LSTMs?\n\n**Recurrent Neural Networks** are those Neural Networks that we use to process information that require us to keep informed of previous information. In simpler words, when we want a model to perform a certain category of tasks like speech recognition, music generation, machine translation, sentiment classification, etc, all of which invloves keeping track of previous information (like keeping track of all the words while processing a sentence during machine translation), a normal Neural Network cannot do this, while a Recurrent Neural Network, which keeps a track of the past information in it\u2019s internal state addresses this issue.\n\n|Recurrent Neural Network - Unrolled|\n|:-------------------------------------------------:|\n|![RNN]({% static '/blog/assets/img/Long-Short-Term-Memory-Neural-Networks/RNN.png' %}){:height=\"50%\" width=\"50%\"}|\n\n\nThe above diagram is that of a basic Recurrent Neural Network, the chained structure is appropriate for modeling sequences and using this structure for Neural Networks have proven to work for the above mentioned applications. LSTMs come into play when a certain problem occurs within RNNs itself.\n\n#### **Problem of Vanishing Gradients due to Long Term Dependencies** \n\nConsider the following small sentence: `She was riding her cycle` . Predicting the word 'her' using RNNs is relatively easy since it just has to process the immediate words next to it, but predicting some sort of information that requires some sort of context from earlier, for example, mentioning `I am from France` in the beginning of a paragraph, but having to predict the language that was spoken much later in the text. Here, as the gap between relevant information grows, it becomes difficult for RNNs to connect both information to make a valid prediction. This happens because while calculating gradients for the weights during backpropagation, the values from one end of the sequence may find it difficult to influence that in other ends of the sequence which may or may not play an important role in prediction. This is the case in normal RNNs, where 'long range dependencies' are not really supported.\n\n\nThis is where LSTMs come into play!\n\n## Long Short Term Memory Networks\n\n**Long Short Term Memory Networks** (usually just called `LSTMs`) are a _special kind_ of RNN, capable of learning long-term dependencies. They were introduced by Hochreiter & Schmidhuber (1997). They are explicitly designed to avoid the long-term dependency problem by remembering information for long periods of time, and this is possible by introducing 'memory cells' which keep track of these dependencies throughout the sequence.\n\n|Long Short Term Memory Network|\n|:-------------------------------------------------:|\n|![LSTM]({% static '/blog/assets/img/Long-Short-Term-Memory-Neural-Networks/LSTM.png' %}){:height=\"50%\" width=\"50%\"}|\n\nThis is how an LSTM looks like, it follows the same chain like structure like that of the RNNs, but it contains several added gates. It may seem difficult to process this as a whole, but we'll walk through this step by step.\n\n\n**Note**: The sigmoid function returns a value between 0 to 1 and this case, values very close to either 0 or 1 and hence is commonly used in our gates to make a particular decision.\n\n\nThe following are states and gates involved in an LSTM cell:\n1. **Activation layer**: This layer consists of the activation values like the normal RNNs do\n2. **Memory Cell** or **Candidate layer**: This layer is involved in keeping track of dependencies\n3. **Update Gate**: Sigmoid function that decides whether or not the memory cell should keep track of the dependecy\n4. **Forget Gate**: Sigmoid function that decides whether or not the memory cell should leave or forget the dependency\n5. **Output Gate**: Sigmoid function that helps us filter what parts of the memory cell layer we want to pass into the output\n\n|LSTM Structure Inside|\n|:-------------------------------------------------:|\n|![LSTM2]({% static '/blog/assets/img/Long-Short-Term-Memory-Neural-Networks/LSTM2.png' %})|\n\nIf the diagram is overwhelming, the following equations may help you to walk through the process.\n\n$$\\hat{c}^{<t>} = tanh(  W_{c}[ a^{<t-1>},x^{<t>} ] + b_c  )$$\n\nThis is the calculation for a memory cell initially which takes into account the previous activation layer and input layers' weights, and adds it to a bias, while passing the resultant to a tanh function that returns a score between -1 and 1, which in turn carries a dependency.\n\n\n$$\\Gamma_{u} = \\sigma(  W_{u}[ a^{<t-1>},x^{<t>} ] + b_u  )$$\n\n$$\\Gamma_{f} = \\sigma(  W_{f}[ a^{<t-1>},x^{<t>} ] + b_f  )$$\n\nHere, the decision is made whether or not to keep track of the dependency with the help of the update and forget gates, which are sigmoid layers.\n\n$$\\Gamma_{o} = \\sigma(  W_{o}[ a^{<t-1>},x^{<t>} ] + b_o  )$$\n\n$$c^{<t>} = \\Gamma_{u}*\\hat{c}^{<t>} + \\Gamma_{f}*c^{<t-1>}$$\n\nHere, we decide what exactly to update into the memory cell, which either retains the dependency from earlier or updates it to a new value based on the decision made by the update and forget gates. Hence, the output will be filtered. This is done by running a sigmoid function layer to decide which parts of the memory cell we will send to the output and while the memory cells will be passed through tanh and then passed through the output gate to get only the filtered output. Here, our memory cells are updated appropriately. \n\n$$a^{<t>} = \\Gamma_{o}*tanh(c^{<t>})$$\n\nThe activation layer is influenced by certain memory cell values decided upon by the output gate, and is appropriately updated and passed onto the next cell.\n\n\nThe resultant $$\\hat{y}^{<t>}$$ vector is obtained by passing the activation layer through a softmax function, but do note that this step is dependent on what problem we're solving and isn't part of the general LSTM framework.\n\n\n## Character to Character LSTM Model\n\nWe are going to use a two layer LSTM model with 512 hidden nodes in each layer. We will make the model read a text file that contains text from a transcript, in this example we will make the model read an exerpt from 'The Outcasts', we will then use the same sequence but shifted by one character as a target. \n\nBefore we get started, here are some key terms to get used to:\n\n1. **Vocabulary**: This is a set of every character that our model requires\n2. **LSTM Cell** : We will make use of pyTorch's LSTM cell that has the structure, as explained earlier\n3. **Hidden State or Activation State**: This is a vector of size(batch_size, hidden_size), the bigger dimension of the hidden_size,\u200athe more robust our model becomes but at the expense of computational cost. This vector acts as our short-term memory and is updated by the input at the time step t.\n4. **Layers of an LSTM**: We can stack LSTM cells on top of each other to obtain a layered LSTM model. This is done by passing the output of the first LSTM cell from the input to the second LSTM cell at any given time t, this gives a deeper network.\n\n**The code with explanation in comments is provided at the references section of this article**\n\n\nThe model receives an \u201cA\u201d initially as an input to the LSTM cell at time t=0. After that, we get the output with the size of our vocabulary from the memory cell. If we apply softmax function to the output, we get the probabilities of the characters. Then we take 'k' most probable characters and then sample one character according to their probability in the space of these 'k' characters. This sampled character is now going to be input to the LSTM cell at time t=1, and so on.\n\nAlways remember that pytorch expects batch dimensions everywhere, and don\u2019t forget to convert numpy arrays into torch tensors and back to numpy again since we are dealing with integers in the end and we need them to look up actual characters.\n\n\nHere is some of the output while monitoring the losses during training:\n\n```\nEpoch: 0, Batch: 0, Train Loss: 4.375697, Validation Loss: 4.338589\nEpoch: 0, Batch: 5, Train Loss: 3.400858, Validation Loss: 3.402019\nEpoch: 1, Batch: 0, Train Loss: 3.239244, Validation Loss: 3.299909\nEpoch: 1, Batch: 5, Train Loss: 3.206378, Validation Loss: 3.262871\n.\n.\n.\nEpoch: 49, Batch: 0, Train Loss: 1.680400, Validation Loss: 2.052764\nEpoch: 49, Batch: 5, Train Loss: 1.701830, Validation Loss: 2.061397\n```\n\nHere is what the model learnt and generated in the first epoch:\n\n|First Epoch|\n|:-------------------------------------------------:|\n|![epoch1]({% static '/blog/assets/img/Long-Short-Term-Memory-Neural-Networks/sampleoutput1.png' %})|\n\nand this is the outcome after 50 epochs:\n\n|Fiftieth Epoch|\n|:-------------------------------------------------:|\n|![epoch50]({% static '/blog/assets/img/Long-Short-Term-Memory-Neural-Networks/sampleoutput2.png' %})|\n\n### Final Outcome\n\nWe can see that the resulting sample that at the 50th epoch doesn\u2019t make much sense, but it does show signs that the model has learned a lot, like some words, some sentence structure and syntax. Now all we need to do is to tweak the model\u2019s hyper-parameters to make it better, and we will have a better character to character model than we started off with.\n\n## References -\n\n- [Code for Character-to-Character Model](https://gist.github.com/nimbus98/9c23ef7825d1c2ce42058d09f780ce08)",
        "author_name": "Akash Nair",
        "slug": "long-short-term-neural-networks"
    },
    {
        "layout": "post",
        "title": "Radar Imaging",
        "author_github": "kulcod",
        "date": "2019-08-22T10:00:30",
        "image": "/assets/img/",
        "description": "Cameras arent the only devices which can take images , RADARs can too find out how",
        "tags": [
            "RADARSs",
            "Doppler Processing",
            "Image"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "kulcod",
        "text": "[{% load static %}]()\n# **RADAR IMAGING**\n\nThe World Wars were undoubtedly the worst things to happen to mankind but, they also birthed some ground breaking technology that is till date being used, one such example is the RADAR, acronym for **Radio Detection and Ranging**.\n\n\n\nRADARs are a combination of EM transmitters and receivers that can be used to detect the presence of objects in a particular **range** specified for that RADAR. The principle involved is the reflection of the EM waves off the target and detection by the receiver of the RADAR.\n\n![firstimage]({% static '/blog/assets/img/Radar-Imaging/1.gif' %})\n\n_The diagram represents a high level representation of_ **monostatic RADAR,** _i.e. the RADAR unit consists of both the transmitter and receiver , both turning on/off alternatively by using a duplexer._\n\nIn this article Pulse RADARS are discussed. Range can be more accurately measured by Pulse RADARs.\n\nEM waves are transmitted in a pulse and time is given for reception before the next pulse transmitted.\n\n![secondimage]({% static '/blog/assets/img/Radar-Imaging/2.PNG' %})\n\nThe entire time taken for transmission and reception is called **Pulse Repetition Interval (PRT)** as the next pulse is transmission happens after the previous one after one PRT.\n\nThe RADAR transmits EM waves in certain directions and targets are detected when the EM waves get reflected off the target and received by the RADAR. The time delay between transmission and reception of the EM wave is recorded and thus the **Radial Range** of the target is calculated as\n\nR= c \\* t/2\n\nc is the velocity of EM wave\n\nt is the time delay between transmission &amp; reception\n\nThe **Radial Velocity** of the target can also be measured by the frequency deviation between the transmitted and received wave, this phenomenon is called the **Doppler Effect.**\n\nThe EM waves transmitted by the RADAR can be reflected by more than one point on the target (eg: aeroplane), these points can be called **Scattering points** and the ability of the RADAR to recognize the target largely depends on its ability to differentiate between these points, this property is called **Range Resolution.**\n\nRange Resolution is the minimum separation in range of two distinguishable points.\n\nThe resolution of Pulse RADARs inversely depends on the pulse width, but if the pulse width is too less, the intensity of the transmitted wave and thus the received wave will decrease. Thus, to solve the problem of this trade off, a technique called **Pulse Compression** is used.\n\nPulse compression is a signal processing technique of modulating the transmitted pulse and then correlating the received signal with the transmitted pulse. There are several types of Pulse Compression methods some of which are **Linear Frequency Modulation** ( **LFM** ), **Stepped frequency** , **Costa&#39;s frequency coding.**\n\n![thirdimage]({% static '/blog/assets/img/Radar-Imaging/3.png' %})\n\n_Linear Frequency Modulated wave , the frequency is proportional to the time in the pulse ._\n\n\n\n\n\nThere are two prominent types of RADAR imaging: **SAR and ISAR**\n\n**SAR** (Synthetic Aperture RADAR) is when the RADAR is moving and **ISAR** (Inverse Synthetic Aperture RADAR) is when the RADAR is stationary.\n\nThere are two important elements of a RADAR image, **Range** &amp; **Cross Range.**\n\nRange is distance along the axis in the direction of the propagation of the beam. It is taken as the component of **Slant Range** (radial distance)along the line joining the RADAR and the centroid of the target.\n\nCross range is the distance along the axis perpendicular to that of Range.\n\n![fourthimage]({% static '/blog/assets/img/Radar-Imaging/4.png' %}) \n\nIn RADAR imaging, the **Range profile** is obtained by the **time delay** between transmission and reception, while, the **cross range profile** of the target is obtained by the **doppler frequencies** obtained in the received signals.\n\nWhile Range resolution can be obtained by Pulse Compression, **Cross Range Resolution** is obtained by having narrow beam, which requires larger antennas, but as this imposes physical challenges, **Synthetic Aperture RADAR** were made, i.e. a small antenna emulates a bigger aperture antenna by using the RADAR&#39;s movement relative to the target and some processing techniques. Rotation of the target relative to RADAR is essential\n\n ![fifthimage]({% static '/blog/assets/img/Radar-Imaging/5.PNG' %})\n\n### **SAR**\n\n\n ![sixthimage]({% static '/blog/assets/img/Radar-Imaging/6.PNG' %})\n\n### **ISAR**\n\nThe Doppler shift in frequency is caused by radial component of the velocity of a point alone and we can observe that when there is rotation of a target, different resolvable points will have relatively different radial velocity and thus will induce different Doppler shifts and thus the relative position of each scattering point can be obtained in the Cross Range profile.\n\nRADAR imaging is a very useful technique and has both civilian and military applications. For example, SAR imaging can be used to survey landscapes and terrains, ISAR imaging has military application of recognizing flying objects in an airspace.\n\n![seventhimage]({% static '/blog/assets/img/Radar-Imaging/7.PNG' %})\n\n_ISAR image of a passenger aeroplane_\n\n\n![eigthimage]({% static '/blog/assets/img/Radar-Imaging/8.PNG' %}) \n\n_ISAR image of a Fighter Jet_\n\n\n\n![ninthimage]({% static '/blog/assets/img/Radar-Imaging/9.jpg' %}) \n\n_SAR image_\n\n\n\n\n\n\n\nReferences:\n\n[http://www.radartutorial.eu/index.en.html](http://www.radartutorial.eu/index.en.html)\n\nInverse Synthetic Aperture Radar Imaging With MATLAB Algorithms - Caner Ozdemir",
        "author_name": "Rohit Manohar Kulkarni",
        "slug": "radar-imaging"
    },
    {
        "layout": "post",
        "title": "Research, an informal testimony",
        "author_github": "krishnakv98",
        "date": "2019-08-22T11:30:30",
        "image": "/assets/img/",
        "description": "A walkthrough of the requirements for research internship and how fundamental they are",
        "tags": [
            "Requirements",
            "Research"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "krishnakv98",
        "text": "[{% load static %}]()\n# Research, an informal testimony\n\nThe feeling of working with the author of textbooks that you first read to learn a subject is something beyond the ability of adjectives to explain. However, it is also the time when you begin to realize what you know is barely a fraction of what exists and that there's always much more to learn. I am extremely privileged to have spent the summer at Stanford Microfluidics Lab as a UGVRI. Now before getting into more details I'd like you to know a little history and the following content is aimed at leaving you takeaway 3 points that I think is important as an aspiring researcher.\u00a0\n\nJust like most other enthusiastic students in freshman year I would try to participate in all activities happening in college and one such event was the IEEE Insight. I remember sitting there awestruck staring at the faces of the then 'seniors' (read 'accomplished professionals in making') not paying attention to much of what they said but just wondering if there could be a day when I could be standing in their position. Note here that it has, for years, been my goal to be a space scientist. I came back to hostel after the talk and the year just passed by until I heard again the same faces speak at the IE Spark. What changed in these months were only that the speakers had cracked a milestone securing admits from premier institutions and reputed positions in leading companies but I as an audience just remained where I was. I believe this is when I realized the power of doing meticulous research (well, it applies to any work practice for that matter). Since then has also been a time when I've overlooked academics and stuck to reading stuff that interests me. I would suggest to not compromise on getting good grades but even if it is low it's not the end of everything.\u00a0\n\nResearch, like every other work, by it's very nature requires that one is thorough with the governing fundamentals and it's important to realize that there's no point of research if it leads to no additional understanding of the phenomenon under study. To begin with, research requires a well-defined problem statement that needs to be solved for which brainstorming and intensive literature review are the prime ways to go.\u00a0 As a matter of fact, most leading discoveries happened either accidentally or in the process of solving a problem that was entirely different. The aim should not be to complete a project quickly but to progress slowly, trying to question, infer, learn and verify as much as possible at each step.\u00a0\n\nOften I find myself grunting over few topics wondering the need for me to read about it and I think many might be of the same opinion, but what if I tell you that the one derivation of chip formation or the assumptions made in a hypothesis that you once neglected might just give you insight to resolve a complex experimental fix, or get you past the interview of your dream company! Research is extremely interdisciplinary and there is no single topic that you should feel is not necessary to know about. For research at least, the ones to make it big are \"Jack of all, master of at least one\", but \"Jack of all master of none\" is much better than one who's a \"Jack of few master of none\" and other average traits. Also crucial for research is writing, documenting and presentation skills. A world class researcher inventing a revolutionary technology but not being able to document it in a manner comprehensible for a reader, is just as good as a layman dreaming to make it big but doing nothing towards realizing the dream. Thus, a true researcher is constantly enhancing their knowledge base, skill sets and trying to find answers from observing nature.\u00a0\n\nIt's quite natural to claim a field of interest as the one that you're relatively 'good at' (read 'just scoring well in') or the one in which you have a couple projects to highlight on your resume. Well, if you're sure that's the exact field you want to be working on, it's brilliant! But otherwise, it's never a good idea to refrain from exploring different domains, it helps diversify your knowledge and gives a better insight into problems that you want to work on from a career point of view. Having chosen to work on a wide range of projects and realizing that every one of those had something to contribute later where I thought they'd be absolutely irrelevant, I can definitely affirm. To justify the same, at Stanford, I led a theoretical and experimental study on microfabricated sheet jet nozzles for X-Ray spectroscopy studies. Initially during acceptance for the internship I was on an entirely different biological project. However being open and choosing to work on this project helped me gain a wide range of experience including microfabrication techniques, fluorescence imaging, microscopes and CCD Cameras, computational design and CFD simulations, UV laser ablators and some insight into Field Electron Lasers & X-Ray beams which may not have been the case otherwise. So it's really important to be open to trying new things and remain in constant pursuit of innovation, for the worse that can happen, is an experiment failing, but analyzing the failure might be the biggest key to various other successful experiments.\u00a0\n\nComing to the application processes, there are a lot many exchange programs, deadlines for application for most of which are about 6-8 months prior to the start of the program. The exact criteria for a successful application remains in a black box, so it's just a matter of trying your best and hoping for good. Given the rigorous competition for these programs, it can never hurt to write mails to the\u00a0 many professors whose work interest you. Again here, there's no standard for a successful mail, but a short explanatory resume with crisp cover mail expressing your interest, the reason for choosing the lab and relevant experience (if any) might be the right way to start off with. In most cases it may be possible that there's no opportunity at some time, but staying in touch with the professors definitely helps open prospective avenues. I've known many students giving up quickly assuming things won't work out, but believe me when I say patience and perseverance can seldom go in vain.\u00a0\n\nFollowing your passion and hobbies can never go wrong, cheers to\u00a0 the noble thought of pursuing novel research. Thanks for reading and I wish you the best!",
        "author_name": "KV Krishna",
        "slug": "requirements-and-importance-of-research"
    },
    {
        "layout": "post",
        "title": "Password Security: Techniques and Challenges",
        "author_github": "suhasks123",
        "date": "2019-10-02T09:20:10",
        "image": "/assets/img/",
        "description": "A Brief Introduction to Password Security and Cracking",
        "tags": [
            "Systems",
            "Security",
            "Cryptography"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "suhasks123",
        "text": "[{% load static %}]()\n## Introduction\n\nLet me preface this article with something: There\u2019s a very good chance that your password is not secure. In fact, if it\u2019s anything less than nine characters long, any attacker could crack it in seconds if he had a decent GPU. Most of us know that \u2018password\u2019 is a very weak choice for a password, but there are some of us who think we\u2019re smart and keep our passwords something like \u2018p@$$w0r6\u2019: It\u2019s a terrible idea. Change it right now. \u2018Hello123\u2019 is probably safer than that. So what exactly is a safe password? And how do you even store passwords securely? Let\u2019s try to answer these questions. Also, let\u2019s do some password cracking along the way to see if what we come up with is actually secure.\n\n## Storing Passwords Securely\n\nUsually the data on a server is protected only by the web security measures in place. When these fail, the server is compromised and the data is available to the attacker. For sensitive data like passwords, this becomes a huge problem. Password security provides an added layer of security. It ensures that no kind of usable data can be extracted from the password data stored on the server. Let's discuss certain techniques for storing passwords:\n\n### 1. Plaintext:\nThis is the naive approach to storing passwords. So for example, if a password is something like \u2018p@ssword1234\u2019, it would be stored as is in the web server. It makes logical sense: The username and password are stored along with each other on the server and when the user wants to authenticate, the backend code checks the data they gave as input with the data on the server. But obviously, this is a terrible idea. Any attacker who gains access to the server, basically has access to all the passwords.\n\n### 2. Encryption:\nLet us go one step further and encrypt all the passwords with a **symmetric key** (keys which can both encrypt and decrypt data) before storing them. When a user needs to authenticate, the password in the server is decrypted using the key and checked with the entered password. This is a little better since an attacker cannot just read all the passwords by gaining access to the server. But if he finds the encryption key somehow, the passwords can be decrypted easily. So to eliminate this problem, we need a kind of encryption which *cannot be reversed*. This is where **hashing** comes in.\n\n### 3. Hashing:\nIn simple words, **a hash function generates a unique, fixed length string signature for an input data of any arbitrary length**. The keywords to note here are *unique* and *fixed length*. Unique means that there exists only one hash for a specific input data. Fixed length implies that significant data loss has occurred during hashing, which means that the original data cannot be obtained from the hash string.\n\nWe can use these properties of a hash to our advantage while storing passwords. So, instead of storing the passwords themselves, we store their **hashes**. During authentication, we hash the password input from the user and compare it with the hash stored in the database. Since hashes are unique to a string, comparing them is equivalent to comparing the original strings. Thus we can perform authentication without really knowing the actual password of the user.\n\nThis sounds good. The attacker has no way of obtaining the original password from the hash. So it should be secure right? *Wrong*. Instead of just explaining why, I\u2019ll demonstrate how to crack hashes.\n\n## Password Cracking\n\nFor this exercise, we will be using **hashcat**. Hashcat is an open source password recovery tool which uses GPU hardware acceleration to crack passwords.\n\nI have downloaded a leaked password database called **hak5.txt** containing 2351 passwords and converted all the passwords to their MD5 hashes for the sake of this exercise. This means that these are real passwords that real people have used, which is significant because it puts into perspective how weak passwords in real life actually are. This is the file containing the hashes, \u2018**hashes.txt**\u2019:\n\n![File Containing Hashes]({% static '/blog/assets/img/password-security-techniques-and-challenges/1.png' %}){:height=\"350px\"}\n\nThere are a lot of ways of cracking hashes. But for this demonstration, we will use three of some of the most used cracking techniques: **Brute force attacks**, **Dictionary attacks** and **Rule-based Dictionary attacks**. In general, there is only one approach to cracking a hash: keep guessing strings and compare their hashes with the hashes to be cracked. The different techniques are merely ways of guessing strings.\n\n### 1. Brute force attack:\nThis is a naive approach but it works surprisingly well. We basically guess every combination of characters of every length, hash each one of them and compare. At some point, the hashes match and we have the password. This, however, is only practical for passwords of small length since the number of combinations to guess goes exponentially higher as the length of the string increases. Below is an example of a brute force attack:\n\nCommand: `$ hashcat -m 0 -o <output_file> <hashes> <guess_params> --force -O`\n\nRunning the Command:\n\n![Running the Command]({% static '/blog/assets/img/password-security-techniques-and-challenges/2.png' %}){:height=\"350px\"}\n\nStats after completion:\n\n![Stats after completion]({% static '/blog/assets/img/password-security-techniques-and-challenges/3.png' %}){:height=\"350px\"}\n\nCracked Passwords:\n\n![Cracked Passwords]({% static '/blog/assets/img/password-security-techniques-and-challenges/4.png' %}){:height=\"350px\"}\n\nSo let\u2019s go about this process step by step:\n\nFirst, the command:\n- `hashcat` is the program that we\u2019re using.\n- `-m 0` tells the program that the hash that we want to crack is md5.\n- `-a 3` tells the program that the attack type is brute force.\n- `-o cracked.txt` sends all the cracked passwords to a file with name cracked.txt.\n- `hashes.txt` is the file containing the md5 hashes.\n- `?a?a?a?a?a` is the main thing here: It tells hashcat to guess strings of length exactly 5 and containing lowercase alphabets, uppercase alphabets and digits.\n- `--force` and `-O` are device specific options to override some driver issues.\n\nAfter the process is done, we check the contents of the file `cracked.txt`. We see that there were a good amount of passwords of length 5 and containing only alphanumeric characters.\n\nThe more important aspect to consider is that the process took 4 mins and 39 seconds, which is pretty fast considering the fact that the machine needs to hash and check every possible alphanumeric string of five characters. But any password length greater than seven or eight characters would need days to crack. So this technique is highly impractical for longer passwords.\n\n### 2. Dictionary attack:\nThe bigger problem with brute force is that most of the strings it guesses are very *random*. An average user would never think of keeping something like \u2018hSN3Lp67sUe\u2019 as their password since it is hard to remember. So, most of the passwords that are guessed in brute force attacks are pretty much useless and a waste of time. To eliminate this problem, we use dictionary attacks. Instead of trying every possible combination, we guess strings that are most likely to succeed. This is done using \u2018**wordlists**\u2019 or '**dictionaries**'. These are files containing common passwords obtained from online password leaks. We consider these strings as the basis of our guesses. We first check the existing strings in the wordlist and then small variations of them (Like converting them to uppercase or adding a period in between). Let me demonstrate:\n\nCommand: `$ hashcat -m 0 -o <output_file> <hashes> <path_to_wordlist> --force -O`\n\nRunning the Command:\n\n![Running the Command]({% static '/blog/assets/img/password-security-techniques-and-challenges/5.png' %}){:height=\"350px\"}\n\nStats after completion:\n\n![Stats after completion]({% static '/blog/assets/img/password-security-techniques-and-challenges/6.png' %}){:height=\"350px\"}\n\nCracked Passwords:\n\n![Cracked Passwords]({% static '/blog/assets/img/password-security-techniques-and-challenges/7.jpg' %}){:height=\"350px\"}\n\nThis time, we get a lot more passwords. Everything in the command remains the same except we change the attack type from `3` to `0` to denote dictionary attack and specify the path to the wordlist, which, in this case is `rockyou.txt`. Certain points to note:\n\nWe get a lot more passwords than brute force. On counting, we find that we have cracked 558 passwords with the dictionary attack which is massive compared to the 27 passwords that we cracked using brute force.\n\nThe more important thing to note is that the time taken is just 11 seconds which is a phenomenal improvement over brute force. So, the number of passwords cracked per second is much higher in a dictionary attack.\n\nThere is only one downside to basic dictionary attacks: the wordlist has to be really good. The success of a dictionary attack depends heavily on the wordlist used.\n\nLet us take this one step further. Though we obtained 558 passwords with dictionary, there are over 2000 hashes in the hashes.txt file. So we need a way to crack more passwords.\n\n\n### 3. Rule-based Dictionary attacks:\nWhen we usually think of guessing someone\u2019s password, we usually think about the most common passwords like \u2018password\u2019 and then we try to perform variations on that like \u2018p@$$word\u2019 or \u2018PaSsWoRd\u2019 or \u2018wordpass123\u2019. Though dictionary attacks can check some of these variations, they don\u2019t go far enough since they are programmed to check for common things like adding a period somewhere in between the string or substituting spaces with underscore. To really exhaust all the possible variations, we can write custom rulesets based on which the program can check for a huge number of variations on every string in a wordlist. To demonstrate:\n\nCommand: `$ hashcat -m 0 -o <output_file> <hashes> <path_to_wordlist> -r <path_to_ruleset> --force -O`\n\nRunning the Command:\n\n![Running the Command]({% static '/blog/assets/img/password-security-techniques-and-challenges/8.png' %}){:height=\"350px\"}\n\nStats at the end of execution:\n\n![Stats at the end of execution]({% static '/blog/assets/img/password-security-techniques-and-challenges/9.png' %}){:height=\"350px\"}\n\nCracked Passwords:\n\n![Cracked Passwords]({% static '/blog/assets/img/password-security-techniques-and-challenges/10.jpg' %}){:height=\"350px\"}\n\nI terminated the process at 0.77% completion. It would take hours for hashcat to complete the process with the computing power that my computer can provide. Let me put up an image to compare how many passwords we have obtained with each technique:\n\n![Comparison]({% static '/blog/assets/img/password-security-techniques-and-challenges/11.png' %}){:height=\"150px\"}\n\nThe differences between the techniques are apparent. Even though the rule-based attack completed only 0.77% of its procedure, it got more passwords than basic dictionary attack which completed 100% of its procedure. This should put into perspective how powerful rule-based dictionary attacks are, while being sufficiently fast.\n\nIn terms of the command, everything remains same except that we include a ruleset with `-r \\<pathtoruleset\\>`. With a enough of computing power, it is possible to crack almost all hashes in the hashes.txt file in minutes.\n\nThere are many more password cracking techniques like rainbow table attacks, combinator attacks, etc, but they work on similar principles while being slightly faster.\n\nWhile we only cracked md5 hashes, these techniques remain same for every other hashing algorithm with the only difference being changes in computation time for each hash operation depending on the hashing algorithm used.\n\nSo, now we have proved that just hashing won't make the passwords secure. Then what is the best way of storing passwords?\n\n## The Best we can do: Hashing and Salting, Hash Stretching\n\nConventionally, we store just the hash of the password in the database. This has one major problem: Same passwords don\u2019t have different hashes. This is called *determinism*. Using salting, we can give an illusion of *non-determinism*.\n\nIf the password is \u2018password\u2019, it can be converted into it's MD5 hash and stored as \u20185f4dcc3b5aa765d61d8327deb882cf99\u2019 in the database. This can easily be cracked with either a basic or a rule based dictionary attack. To prevent this, we \u2018**salt**\u2019 the original password, that is, we add some random characters at the end or the beginning of the original password like \u2018passwordjUIGhs2g7ah\u2019. The hash of this salted password \u20186610878e2c3fd864fd47a72a677fec4b\u2019 is stored in the database. The salt would vary from user to user. During authentication, the salt for the particular user would be appended to the password input, hashed and then checked with the hash in the database.\n\nNow, this hash is impossible to brute force since the length of the string is pretty huge. It is borderline impossible to crack it using basic or rule based dictionary attack it since the salt is completely random. The only scenario where this password would be compromised is if the attacker miraculously finds the salt for a particular user and performs a rule based dictionary attack with the rule being every guess has the salt appended to the end of it. Even in this scenario, he can only crack one password since different users have different salts.\n\nThere is another possibility for attacking salted passwords. With enough computing power, one could technically perform a rule based dictionary attack in combination with a brute force attack. Basically for every guess made by the rule based dictionary attack, represented by, \u2018pwd\u2019, we brute force strings, represented by \u2018slt\u2019. We then append pwd and slt to obtain a string which is then hashed and checked.\n\nIt sure sounds complicated. But anybody having a decent number of GPUs can do these operations fairly quickly. And this eliminates the problem that the attacker had with different users having different salts since whatever strings he generates are random anyway. So how can we make something truly impossible to crack?\n\nWe can\u2019t. The only thing that we can do is slow the attacker down. We use a technique called **hash stretching** to achieve this. Instead of hashing the salted passwords just once, we run it through a loop and repeatedly hash it thousands of times.\n\nThis has a lot of advantages. Firstly, with normal hashing, as we have seen, when a good number of passwords are stolen from a web server, and stored offline, it becomes very easy to crack. This is because the attacker has no limit on the number of guesses he can make to get the password. With a sufficiently powerful GPU setup the attacker has the ability to generate billions of guesses per second. Unless a user has a really good password, the attacker would be successful in cracking almost everything that he stole from the server. Hash stretching slows down this process considerably, since for each guess, the machine has to perform the hash algorithm multiple number of times. So, the speed of cracking goes down from billions of passwords per second to a few million, which is a huge difference.\n\nA lot of things have to go right for an attacker to crack a salted password stored after hash stretching. So this is generally agreed as the best way to store passwords. But every organization cannot afford to allocate so many resources for this. Most organizations just stick with whatever they have even though it is obsolete. These outdated systems are what black hats target and exploit. This what happened to **Adobe** in 2013. They stored their passwords in an encrypted form with every password having the *same* key. This **central point of failure** led to around 150 million passwords being compromised.\n\nIf an organisation cannot afford to spend enough resources to store passwords securely, they can avail the services of bigger companies like Google or Facebook to handle all authentication. This would be the most secure and cost effective way.\n\nIn conclusion, it\u2019s tricky business to store passwords. So either one should not do it or do it properly since password leaks basically strengthen password cracking dictionaries and wordlists. Every compromised password grants extra power to malicious hackers. Hence it is very important to store passwords securely.\n\n## References:\n\n- [https://nakedsecurity.sophos.com/2013/11/20/serious-security-how-to-store-your-users-passwords-safely/](https://nakedsecurity.sophos.com/2013/11/20/serious-security-how-to-store-your-users-passwords-safely/)\n\n- [Password Cracking \u2013 Computerphile](https://youtu.be/7U-RbOKanYs)\n\n- [How to Choose a Password \u2013 Computerphile](https://youtu.be/3NjQ9b3pgIg)\n\n- [https://auth0.com/blog/adding-salt-to-hashing-a-better-way-to-store-passwords/](https://auth0.com/blog/adding-salt-to-hashing-a-better-way-to-store-passwords/)\n\n- For anyone interested, I\u2019ve uploaded the \u2018hashes.txt\u2019 file to a github repo. So if you want to try these exercises out yourself, feel free to do so. Repo link : [https://github.com/suhasks123/hak5-MD5-Hashes](https://github.com/suhasks123/hak5-MD5-Hashes)\n\n- [https://laconicwolf.com/2018/09/29/hashcat-tutorial-the-basics-of-cracking-passwords-with-hashcat/](https://laconicwolf.com/2018/09/29/hashcat-tutorial-the-basics-of-cracking-passwords-with-hashcat/)",
        "author_name": "Suhas K S",
        "slug": "password-security-techniques-and-challenges"
    },
    {
        "layout": "post",
        "title": "Forward Swept Wings",
        "author_github": null,
        "date": "2019-10-07T19:30:30",
        "image": "/assets/img/",
        "description": "Forward Swept Wings",
        "tags": [
            "Piston",
            "Aeronautics"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "vehandoshi",
        "comments": false,
        "text": "[{% load static %}]()\n# Introduction\n\n\nForward Sweeping is the concept of having the wings bent in the forward direction towards the fuselage (as seen in the picture of the X-29).\nForward swept wings were first developed in the 1980s as an experiment to overcome many of the disadvantages of the rear sweep but little did we know what complications it would bring with it.\nTo understand and evaluate the forward sweep we need to first understand why wings are swept back.\n\n### X-29:\n  \n![X-29]({% static '/blog/assets/img/forward-swept-wings/x29.jpeg' %})\n\nA wing typically works on bernoulli's principle where the air above the aerofoil has higher speed and lower pressure as compared to the air below it .This causes a net upward force on the wing known as lift. Consider an aircraft travelling at a mach number lower than one the air above the aerofoil can cross the speed of sound due to the curvature of the aerofoil and this can result in flow separation and generation of shock waves. This will result in the generation of immense drag and can also cause the aircraft to pitch downwards due to the increase in lift generation from the wings behind. As the wing has a rear sweep angle the chordwise (perpendicular to the wing span) component of the velocity does not cross mach one and flow separation along with shock wave is delayed.\n\n\nOne of the problems with rear sweep is the roll control by ailerons during high speed flight, as the air flows from the root to the wing tips stalling begins from the tips till the root and ailerons are located near the wing wing tips and if that part of the wing stalls then we can lose control of the aircraft at very nascent stages of velocity.They also generate a lot of drag due to generation of vortices on the wing tips where the low pressure and high pressure air meet. These types of problems lead to the development of forward sweep.\n\n\n### Air Flow Comparison:\n  \n![Air FLow Comparison]({% static '/blog/assets/img/forward-swept-wings/AirFlowComparison.jpeg' %})\n\n### Pitching down of the aircraft due to flow separation:  \n\n![Pitching down of the aircraft due to flow separation]({% static '/blog/assets/img/forward-swept-wings/PitchingDown.jpeg' %})\n\nBoth of these problems are solved by forward swept wings. As the air flow is now from the tips to the root we have better control of the aircraft because stalling begins at the roots where there are minimum control surfaces. Vortices are eliminated and also the center of pressure shifts behind the COG and hence we need small wings in the front to balance out the moment. Because of this shift the aircraft brings about lot of unstable characteristics, the aircraft is impossible to be in practice without a computer because it has to be adjusted for stable flight upto 40 times a second which is not possible manually, the testing by NASA on the X-29 had 3 computers on board incase one would fail. Flying in extreme weather will be a huge problem for this type of aircraft , these are the major reasons why this innovation was not taken into practical applications.\nThe wing loads were also tremendously high,due to which special materials were needed during manufacturing of the wing.\nAlthough forward sweep did solve a lot of problems and increase efficiency but they were not incorporated into modern fighter jets because the advantages simply did not outweigh the cons.\nAlso this design is useless for stealth purpose,so the rear sweep delta wing dominated.",
        "author_name": "Vehan Doshi",
        "slug": "forward-swept-wings"
    },
    {
        "layout": "post",
        "title": "Space-based Solar Power",
        "author_github": "JyotsnaMeda",
        "date": "2019-10-07T19:29:30",
        "image": "/assets/img/",
        "description": "Space-based Solar Power",
        "tags": [
            "Diode",
            "Space",
            "Solar Power"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "JyotsnaMeda",
        "text": "[{% load static %}]()\n# Introduction\n\nCan you imagine deploying solar panels in space and efficiently converting the solar power that is received right from space and then transmitting the energy back to earth wirelessly, interesting right?\n>`Space-based solar power` project is a daring effort that collects the solar energy from the sun in space, converts it to energy and sends it down to Earth via microwaves.\n`Energy transmitting satellites` that are placed in the geosynchronous orbit of the earth collect high intensity uninterrupted solar radiation by using giant mirrors that reflect huge amounts of solar rays onto two smaller solar collectors. This radiation is then wirelessly beamed to earth in a safe and controlled way as either a microwave or `laser beam`.\n\n# But first, we need to understand: \n- `Why space..`?\n- Don't ground-based solar conversion units do the same? \n- Why spend so much effort on implementing this system in space?\n\n\n# Why we implement it in Space:\nIn space there is no atmosphere, it's never cloudy and in geosynchronous orbit, it is never night. The intensity of sunlight in space is 10 times more compared to its intensity on earth. Moreover, 30% of all incoming solar radiation is reflected by the earth's atmosphere and doesn't even make it to the ground. Solar power on earth is greatly affected by weather conditions and isn't an uninterrupted source of energy. Space is the perfect place for a solar power station to harvest uninterrupted power and can provide the earth with massive amounts of clean and renewable energy. lt has the potential to solve the energy crisis in the near future.\n\nIn the near future humanity will have to make a transition from an energy based on fossil fuels, an exhaustible resource that is damaging our atmosphere and causing unknown long term effects on the climate-to a system based on clean, safe, reliable and renewable source of energy. Space-based solar power seems to be a perfect fit solution to this. It is an innovation designed to retain the advantages of traditional solar power while at the same time eliminating its disadvantages.\n\nThe idea behind this is quite simple. Instead of coping with the unpredictability and intermittency of solar power on the ground, implement it in space where the sun always shines.\n\nIn Geostationary orbit, the intensity of the sun is 36% more compared to that on the ground and Solar arrays in space can collect almost 10 times the amount of energy as the same configuration placed somewhere in the mid-latitude region on earth.\n\nThis technology is not something that has been thought of recently but has been in research since the early 1970s. Though this idea has been around for a decade now, it has become increasingly feasible, now that the launch costs are getting into the economically feasible zone for power satellites. In the last few years, however, scientists around the globe and several researchers at the US energy department have shown how recent technological developments could make this concept a reality.\n\n# How does it work?\nDesign for a Space-based solar power system was developed by an American engineer, Peter Glacer, who in 1973 also received a patent for his design (which is as shown in the figure):\n ![how it works]({% static '/blog/assets/img/space-based-solar-power/first_im.jpg' %})\n According to this design, the solar power that would be collected by the solar panels is converted into electrical energy and would be transmitted by a microwave antenna, which is built onto the satellite, to a large ground-based receiver called Rectenna.\n \nAfter Glacer's patent work, significant developments in this design have been taken place in the US, Canada, Europe, Japan, China, and India.\n \nIn 2011 the `International Academy of Astronautics` then produced it's highly detailed first International Assessment of space solar power. According to the IAA report:\n`\"There are no fundamental technical barriers that would prevent the realization of large-scale SPS platforms during the coming decades.\"`\n This tells us that this project is technically achievable\n\nSpace-based solar power essentially consists of three elements:\n- A means of `collecting solar power` in space. For example, via solar concentrators, solar cells or heat engines.\n- A means of `transmitting power` to earth. For example, via microwave or laser.\n- A means of `receiving power` on earth. For example, via a microwave antenna or rectenna.\n![receiving power]({% static '/blog/assets/img/space-based-solar-power/second_im.jpg' %})\n\n## According to the IAA, there are 3 types of satellite architectures that can perform this task :\n### 1.  Type one architecture :\n\n![type one architecture]({% static '/blog/assets/img/space-based-solar-power/third_im.jpg' %})\n\nVery large PV array with microwave power transmission, i.e.,\nMultiple solar panel arrays are deployed in space and all of them integrated into one singular system that converts solar energy into electrical energy and this energy can be beamed down to earth using microwaves.\n### 2. Type two configuration:\n\n![type two config]({% static '/blog/assets/img/space-based-solar-power/fourth_im.jpg' %})\n\nLarge PV array with laser power transmission, i.e.,\nA constellation of small satellites can be used, where each satellite has its own solar power conversion unit and all the converted energy is beamed down to earth in the form of laser energy, which is close to the visible spectrum. They are typically placed about 100 kilometers above the earth\u2019s surface.\n\n### 3. Type three configuration: \n![type three config1]({% static '/blog/assets/img/space-based-solar-power/fifth_im.jpg' %})\n\n![type three config2]({% static '/blog/assets/img/space-based-solar-power/sixth_im.jpg' %})\n\nOptical concentrator with microwave power transmission, i.e.,\n\nThis configuration is based on a symmetrical optical concentrator or \"sandwich\" design which does not require very large photovoltaic panels, rather vast arrays of mirrors, several kilometers across, that collect and redirect sunlight onto much smaller photovoltaic solar cells. The electricity so created is then transmitted to the earth through a simple microwave link. In this configuration, the satellite is placed 36,000 kilometers above the ground.\n\nSuch an approach simplifies the power management and distribution arrangement compared to Type one or Type two configurations, as captured solar energy can be largely moved around by redirection rather than through physical electrical infrastructure.\n\nType three systems may, therefore, prove the most viable to construct. Power in this system would be transmitted using a technique called `Retro-directive Phased Array` where an encrypted pilot signal from the ground handshakes with the satellite's active electronic beam steering system to link the transmitter and receiver. \n \n# Laser solar satellites  Vs  Microwave solar satellites \n\n### Pros of laser solar satellites:\n\n- Relatively low startup costs in the $500 million to $1 billion range.\n- The single launch per laser transmitting satellite would be self-assembling, lowering the cost and risks substantially.\n- The small diameter of the laser beam would make it simpler and cheaper to implement on the ground.\n\n### Cons of laser solar satellites:\n\n- Comparatively low power of each individual satellite, in the area of one to ten Megawatt per satellite would require several satellites to make a substantial impact.\n- There are several safety concerns with lasers in space such as blinding and weaponization.\n- Laser transmitting satellites would have trouble beaming power through heavy clouds and rain.\n\n### Pros of microwave satellites:\n\n- Steady uninterrupted transmission of power through rain, clouds, and other atmospheric conditions.\n- Safely transmits power through the air at intensities no greater than the midday sun.\n- It provides up to one Gigawatt of energy to a terrestrial receiver, enough to power a large city.\n\n### Cons of microwave solar satellites:\n\n- Production costs in the tens of billions of dollars range requiring as many as 100 launches into space, with space-based assembly required.\n- The terrestrial receiver would be several kilometers in diameter.\n- The long-distance of the satellite from earth would make it nearly impossible to repair.\n\n# Rectenna\n![rectenna]({% static '/blog/assets/img/space-based-solar-power/seventh_im.jpg' %})\n\nThe beam terminates on an earth-based receiver called a Rectenna with a maximum intensity at the rectenna\u2019s center and tapering to nearly nothing at the periphery. The rectenna could be just about the size of a municipal airport and is made up of a mesh of dipole antennas that capture all the incident energy from the beam. It is nevertheless 80% transparent to sunlight, allowing the land beneath to remain available for agricultural uses.\n\nAccording to the November 2011 IAA report, these satellites could transmit as much as 1 to 10 GW of constant energy as compared to a typical nuclear power station's output of 0.5 to 1.0 GW. A GW could light 750,000 homes. That is the potential of this satellite!\n\n# Challenges:\n\nSome of the challenges involved in implementing this project are as follows:\n\n- low-cost reusable space access.\n- dealing with launch costs.\n- demonstration of space to earth power beaming.\n- efficient and light spaced, qualified with solar arrays\n- space assembly maintenance and servicing.\n- large in-space structures.\n\nNearly every assessment has concluded that SBSP is technically achievable. Few argue that SBSP is impossible because of its economic costs. But a system that is actually capable of solving the serious problems on the global agenda such as sustainable development, climate change, energy security and simultaneously enhancing mankind\u2019s ability to access and make the best use of the resources of space deserves serious consideration.\n\nWhile the satellite would suffer degradation due to space environment factors such as particle radiation and meteor material impacts, as would any satellite, it would provide a lifetime of 20 to 30 years for every satellite. It does not require the kind of heavy maintenance that is required by a ground-based power plant either (which is subject to weather damage). A nation with the capability to launch 100 satellites to construct an SPS would certainly have the access and capability to service it.\n\nIf successful in the market, ultimately thousands of systems could be launched to geosynchronous orbit to supply a part or all of the estimated 55 terawatt power requirements for all earthly needs by the year 2100.\n\nHumanity has never contemplated either a space or energy project on such a vast scale or one which would alter our relationship with the cosmos. It is a space project, an energy project, a transportation project, and an infrastructure project. It is a project that could solve the future energy crisis with clean and renewable energy.\n\nSpace solar power can completely solve our energy problems in the long term. We could stop burning coal, we could stop burning fossil fuels, we could even shut down our nuclear power plants if this system gets implemented. Remote areas on earth that don't have access to electricity due to the sheer difficulty of drawing out electric cables to those areas to transmit power could be easily powered with this technology!  It would be the key to humanity\u2019s survival. The sooner we start and the harder we work, the shorter \u201clong term\u201d will be.\n\nReferences for further reading :\n\n- [https://en.wikipedia.org/wiki/Space-based_solar_power](https://en.wikipedia.org/wiki/Space-based_solar_power)\n \n- [https://earthsky.org/earth/space-based-solar-energy-power-getting-closer-to-reality](https://earthsky.org/earth/space-based-solar-energy-power-getting-closer-to-reality)\n\n- [https://www.youtube.com/watch?v=eBCbdThIJNE](https://www.youtube.com/watch?v=eBCbdThIJNE)\n \n- [https://iaaweb.org/iaa/Studies/sg311_finalreport_solarpower.pdf](https://iaaweb.org/iaa/Studies/sg311_finalreport_solarpower.pdf)",
        "author_name": "Jyotsna M",
        "slug": "space-based-solar-power"
    },
    {
        "layout": "post",
        "title": "Concorde: Why did it fail?",
        "date": "2019-11-02T01:36:00",
        "author_github": "kartiknayak",
        "image": "/assets/img/",
        "description": "",
        "categories": [
            "Piston"
        ],
        "github_username": "kartiknayak",
        "text": "[{% load static %}]()\nConcorde: Why did it fail?\n\nWe had it.\n\nFrom January 21st, 1976 to October 24, 2003, we had a commercial supersonic passenger service, called the Concorde, that would bring people from London to New York in just 3.5 hours, a journey that now takes about seven. Alongside the Tupolev Tu 144, Concorde was an aircraft that could brag about its supersonic speed, at a time when other jets could only manage speeds of about 800kmph.\n\nOver the years, Concorde came to represent class and style among the elite class in England, France and the United States. With tickets costing up to 6 times the average cost of today&#39;s airfare, flights were usually jampacked with 120 members of the elite class, who enjoyed an extensive 2.5-hour French meal during the flight.\n\nThe Concorde was a sight to marvel amongst all aviation enthusiasts, featuring ogival delta wings with four Olympus engines; Concorde flew at a record speed of Mach 2, twice the speed of sound. It was the only commercial aircraft that carried passengers at supersonic speeds, at a time when long distance travel was both expensive and difficult.\n\nThen, on October 24th, 2003, British airways flew Concorde for the last time. For 27 long years, passengers had access to a means of super sonic transport, and then suddenly, we stopped.\n\nThe main reason for the retirement of Concorde came on July 2000, when an Air France operated Concorde suffered a fatal crash just after take-off. The crash took 113 lives, including all of the passengers and crew and four people on ground. It was the only fatal crash involving Concorde.\n\nAccording to the official investigation conducted by the Bureau d&#39;Enqu\u00eates et d&#39;Analyses pour la S\u00e9curit\u00e9 de l&#39;Aviation Civile (BEA), the crash was caused by a metallic strip that had fallen from a Continental Airlines flight that had taken off minutes earlier. The metal strip punctured a tire on the Concorde&#39;s main wheel, causing the wheel to explode, puncturing the fuel tank in consequence. The resulting fire forced the crew to shut down the engine 2, and with engine 1 providing only little power, the aircraft was not able to gain altitude or speed. After a rapid pitch up, the aircraft suddenly descended and crashed at H\u00f4telissimo Les Relais Bleus Hotel in Gonesse.\n\nFollowing this incident, Concorde was grounded for a year. Service resumed in September 2001, by which time the 9/11 attacks had already depressed the industry.\n\nWhile both tragedies did contribute to the ultimate grounding in 2003, they were only a couple of pieces of the ultimate challenge the aircraft faced.\n\nHigh levels of noise during take off were one thing, but the sonic booms had no comparison. During test runs, residents complained of broken windows and extremely high noise disturbances. All this meant limiting supersonic flights to over the ocean, which crushed the American dream of Concorde.\n\nConcorde guzzled a lot of fuel, as it was always soaring higher and higher, going faster and faster. Fuel requirements limited the flights to trans-Atlantic flights, meaning there would be no trans pacific flights. They also guzzled enough fuel that slight changes in fuel prices would affect Concorde harder than others. fitting only 120 passengers at max capacity, this meant the prices had to be exorbitant. It couldn&#39;t compete with the cost distribution of high capacity flights like the Airbus A340 that could carry way more passengers for a lower cost. Concorde also needed specially trained crew members, and the maintenance cost was extravagant.\n\nAnd all this came for a very demanding crowd. Air France and British Airways had to position spare Concordes in New York in case the returning flight had some problems. So, there were always a Concorde on ground, not making any money, just in case. Because Concorde passengers expected to get walk onto a Concorde, for they had paid a lot of money.\n\nAll these reasons, combined with minimal support from the aircraft manufacturers led to the ultimate downfall of the Concorde. The beast that once rose through the skies now sits idle at the many museums around the world. The story of Concorde tells us that no matter how innovative, jaw dropping, or inspiring a business model is, without proper supply systems, political resolves and a plan to expand, it won&#39;t sustain unless it makes money. People may not like that idea, but it is a fact of life.",
        "author_name": "Kartik Udaya Nayak",
        "slug": "concorde"
    },
    {
        "layout": "post",
        "title": "Future of Drones",
        "date": "2019-11-02T01:36:00",
        "author_github": "saikumardande",
        "image": "/assets/img/",
        "description": "",
        "categories": [
            "Diode"
        ],
        "github_username": "saikumardande",
        "text": "[{% load static %}]()\nThe word **Drone** is used pretty often in today\u2019s world. Today the term has broadened to include\nanything from highly sophisticated $100M Global Hawk to $30 teeny-tiny UDI 839 Quadcopter. Even flightless vehicle such as Drone ship are occasionally referred to as drones.\n\n\n\n![img]({% static '/blog/assets/img/FutureOfDrones/drones1.png' %})\n\n\u200b                                                                                **Global Hawk**\n\n![img]({% static '/blog/assets/img/FutureOfDrones/drones2.png' %})\n\n\u200b                                                                                   **Quadcopter**\n\nSo what exactly is a drone?\n\nIt is called as an unmanned aerial vehicle essentially if it flies in a controlled way and their isn\u2019t anybody on board. Whether it is controlled by computers or entirely controlled by a remote makes little difference. But most often when you see a vehicle described as a drone today, it is a little bit of both: piloted by humans, but capable of some degree of navigation on its own if necessary.\n\n### **THE MONOSPINNER DRONE**\n\n![img]({% static '/blog/assets/img/FutureOfDrones/drones3.png' %})\n\nWhat do you think is the minimum number of moving parts required for a controlled flight?\n\nFor example, helicopters have two moving parts, a propeller and tail rudder and multicopter drones hover in a place through a number of rotors which create an upward thrust to cancel out their weight, whereas mono spinner relies on just one propeller.\n\nThis works without hinges, ailerons, flaps and servos. Because of single thrust force created by propeller it can\u2019t just lift off from the ground, it flips over. In order to get it to flight it has to be thrown like a frisbee. For a certain constant angular speed and propeller force the torques are cancelled out by cross coupling terms, an unconventional equilibrium is found and the mono spinner is able to stay in one position.\n\nFeedback control keeps the mono spinner in its stable position, without active feedback control it will crash. However it produces only one thrust it can still be controllable using some advanced algorithms. This drone is robust and can recover and stabilize even after heavy disturbances.  \n\n![img]({% static '/blog/assets/img/FutureOfDrones/drones4.png' %})\n\nWhat if a propeller or motor, of a quadcopter or any other with more than one propeller experiences a failure. Simple enough,  remaining propellers take over or at least they try. With  this single propeller idea we can make a drone fly  even when there is only one working\npropeller on drone.   \n\nSimilar to this drone, another single propeller drone is UNI -Copter. This drone has only one propeller, but the only difference here is it uses control surfaces to have stable flight. This has two layers of control surfaces, upper control surface and lower control surface. Upper control surface compensate the anti-torque and controls flight directions and lower control surface controls flight\ndirections. Servo motors are used to control these control surfaces. The main advantage of this drone is its sphere shaped frame protects the core elements for flying and control and enables landing and take-off without constrains of terrain.\n\n### **POLLINATION DRONES**\n\nBees play a crucial role in pollination. These days bee colonies are decreasing due to increase in use of pesticides. So scientist in japan used a drone, horse hair and a sticky ionic gel which mimic the action of a bee. The pollinator drone is attached with a strip of horse hair at the bottom and it is coated with a gluey substance called ionic gel which has peculiar lift and stick ability. Pollen normally sticks to the hair of bee. Similarly pollen sticks to the drone and it can be transferred to another  flower.\n\nAt present these drones are controlled manually. Future models of this might use Artificial Intelligence or GPS to control and plan the best paths to pollination.\n\nSimilar to this, in 2017 Poli-X pollination Drone project was launched and it was tested in 2018. The main intention of this project is to design a drone which is capable of recognize flowering plant species and autonomously pollinate only those species. The concept prototypes, Poli-X1 and Poli-X2 are under development by OpenRobotix Labs. The drones are capable of recognizing plant species for pollination, self-charging  and remain in the fields throughout the flowering season.\n\n![img]({% static '/blog/assets/img/FutureOfDrones/drones5.png' %})\n\n\u200b**Pollination drone**\n\n![img]({% static '/blog/assets/img/FutureOfDrones/pollination.png' %})\n\n\u200b                                                                                   **Poli-X1 drone**\n\n### **References**\n\n*   [https://www.flyingmachinearena.ethz.ch/wp-content/publications/2016/zhaIEEE16.pdf](https://www.flyingmachinearena.ethz.ch/wp-content/publications/2016/zhaIEEE16.pdf)\n*   [https://link.springer.com/content/pdf/10.1007%2Fs12206-018-0440-1.pdf](https://link.springer.com/content/pdf/10.1007%2Fs12206-018-0440-1.pdf)",
        "author_name": "Sai Kumar Dande",
        "slug": "future-of-drones"
    },
    {
        "layout": "post",
        "title": "Generate Strange Text with GPT-2",
        "author_github": "kinshukk",
        "date": "2019-11-02T01:36:00",
        "image": "/assets/img/",
        "description": "Where we explore the funny world of recent language models",
        "tags": [
            "NLP",
            "ML/AI"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "kinshukk",
        "text": "[{% load static %}]()\nGPT-2 refers to a set of _Language Models_ released recently by OpenAI. My objective is to find weird ways that GPT-2 can fail, and also some smart things it is capable of. I\u2019ll be using [talktotransformer](https://talktotransformer.com/) for this task. It\u2019s a convenient tool that asks for a short text prompt, and then generates several lines of text to continue the prompt (no coding required!).\n\nFirst, let\u2019s see if you\u2019re paying attention. Check out the following two samples:\n\n\n\n>**Department of applied mechanics and hydraulics of National Institute of Technology - Karnataka (NIT-K), Suratkal will host a 3-day workshop on coastal reservoirs as a sustainable strategy for water security on its Srinivasnagar premises from July 22. T G Seetharam, director, IIT Guwahati, will be the chief guest and inaugurate the seminar. K Uma Maheshwar Rao, director, NITK will preside over the inaugural function.**<br /><br />\n>The seminar will offer a comprehensive overview of the coastal water management sector including hydrometeorology, hydrodynamic, hydrometeorology and hydrography.<br /><br />\n>It will be addressed by experts in the various fields including engineering, hydraulics, hydrology, hydrography and marine ecology/oceanography, besides coastal water managers and engineers.<br /><br />\n>Seetharam said while it was an emerging sector with great potential, no clear strategies were being pursued for conservation and recovery of flood prone areas.\n\n\nMakes sense? What about this one:\n\n\n\n>**Department of applied mechanics and hydraulics of National Institute of Technology - Karnataka (NIT-K), Suratkal will host a 3-day workshop on coastal reservoirs as a sustainable strategy for water security on its Srinivasnagar premises from July 22. T G Seetharam, director, IIT Guwahati, will be the chief guest and inaugurate the seminar. K Uma Maheshwar Rao, director, NITK will preside over the inaugural function.**<br /><br />\n>\"We will address the problem of coastal development in a holistic way and in many ways, the sea cannot be separated from the land. We have to look at the whole development area and take the sea into account,\" M S Rathore, head of the Centre for Research in Development and Environment (CRED), said.<br /><br />\n>This is the second such seminar under CRED. The first seminar was held on August 6.\n\n\n\nWhich one do you think is real, and which one was generated?\n\nI\u2019m dead serious: which one do you think is fake?\n\nYou can re-read them carefully to make your choice.\n\n<br /><br /><br /><br /><br /><br />\nOkay ready?\n<br /><br /><br /><br /><br /><br />\n\n\nHah, Trick question! Turns out, _both_ were generated by GPT-2. The real article is [this one from the Times Of India](https://timesofindia.indiatimes.com/home/education/news/three-day-workshop-on-coastal-reservoir-at-nitk-concludes-today/articleshow/70356325.cms)\n\nI took the first paragraph from that article (**bold** in the generated samples), fed it into the transformer, and after a few trials, out came the above two examples.\n\nNotice, in the second fake sample, the thing was talking about a certain _Centre for Research in Development and Environment_, which does sound like a typical government-style name for an organization. Also, it gave an _acronym_ as well, and the initials match up! _CRED_ isn\u2019t even a real organization as far as I know, and the model somehow birthed it out of thin air.\n\nNow the obvious question is - can it generate poetry?\n\nLet\u2019s see what the transformer spews out when prompted with Dylan Thomas\u2019 famous piece:\n\n\n\n>**Do not go gentle into that good night,**  \n>**Old age should burn and rave at close of day;**  \n>**Rage, rage against the dying of the light.**\n><br /><br />\n>When the night is past and the moon is down,  \n>I shall sit and watch it, till I have heard the dawn,  \n>Which I shall know to be a new life.\n><br /><br />\n>Old age ought not to be fathomed,  \n>Forthwith shall we know it to be a night;  \n>Rage is, and a good song shall be sung.\n><br /><br />\n>And in the hour of my death I shall not sleep;  \n>I shall sing and tell of the night long;  \n>I shall hear your song and say: How sweet the evening\n\n\n\nMuch deep, so poetry.\n\nSomehow the model knows how poems like these are structured, where the lines should break, and what kind of deep and wise sounding words a romantic poet writes.\n\nAnd now for some pure mischief:\n\n\n\n>**Kejriwal announces: \"I'm a vampire. I'm actually from outer space\"**<br />\n>-On his alleged visit in 2013, to the Indian Space Research Organisation (ISRO) to observe the launch of a satellite of its foreign rivals<br />\n>-Claims to have created the world's second most powerful economy with \"nearly double\" of the GDP.<br />\n>But that's not all. Mr Kejriwal, who is the chief minister of Delhi, claims to have been abducted last year by the security forces, tortured and forced to drink demon urine.<br />\n>And he has also said that he is a vampire, which is a type of supernatural creature that feeds on blood.<br />\n>He said: \"So that's how I was born. It's not my fault. I got stolen by the forces during a traffic raid. It took my arm off.\"<br />\n>Mr Kejriwal, the former Delhi chief minister, says he is a \"vampire\" because he believes God has cursed him.<br />\n>A video obtained by Daily News India of Mr Kejriwal during his first public address was shown in front of the National Conference's main square in Delhi.<br />\n\n\n\nAmong other things:\n\n\n\n1. It knows what a vampire does (_supernatural creature that feeds on blood._)\n2. It associates Kejriwal with the position of CM of Delhi, which is great! But it first says he is the CM, then says he\u2019s the former CM. So it has problems with consistency.\n3. The article seems to match the general flow of a news piece, albeit confused by the mention of vampires.\n\nHow exactly does this thing work?\n\n\n### Birds eye view of GPT-2\n\nGPT-2 stands for Generative Pretrained Transformer 2. Generative because it is, well, generating text. Pretrained because the guys who made it trained it on a HUGE corpus of outgoing links from Reddit. Pretraining is useful because this let\u2019s us fine-tune the model to our particular use case with a lot fewer examples. Transformer because that\u2019s what the building blocks of this thing are called.\n\n\n![Stack of Decoders]({% static '/blog/assets/img/GPT2_gen/decoder.png' %})\n_[<center>Source</center>](https://jalammar.github.io/illustrated-bert/)_\n\n\n\nGPT-2 is a **Language Model**, which is a fancy way of saying that, when given a prompt, it gives a distribution of which word should come next.\n\n(It\u2019s not exactly words that it gives probabilities for, more like pieces of words. For simplicity, let\u2019s just assume that it has a vocabulary of words among which it decides.)\n\nBut neural networks deal with numbers, not strings. So we convert each word into a fixed-length vector of numbers, so that the input sentences are converted into an array of vectors (all of the same length). This is what\u2019s called an **Embedding**. GPT-2 also considers the context in which each word is used, and gives a slightly different set of numbers for different contexts.\n\nAs an example of why context matters, notice that the word \u2018break\u2019 has different meanings in \u2018break the glass\u2019 vs \u2018summer break\u2019.\n\nSo now have converted the input string into a list of vectors. Now we feed it into the **Decoder** block, which essentially looks like this::\n\n\n![Transformer Block]({% static '/blog/assets/img/GPT2_gen/transformer_block.svg' %})\n_[<center>Source</center>](http://www.peterbloem.nl/blog/transformers)_\n\nAttention is an operation which converts an input sequence to an output sequence of the _same length_. The output is also a list of vectors of fixed length, just like the input sequence.\n\nMLP is just your garden variety vanilla feedforward net, although it can be a convolutional block as well.\n\nLayer norm is a method of speeding up the training (by allowing gradients to flow more easily), and adding regularization to the network.\n\nAlso, we use skip connections (those arrows that go around blocks) because this let\u2019s us directly propagate information from earlier layers to later layers.\n\n(For more in-depth and hands-on explanations, check out the resources at the end.)\n\n\n### Closing thoughts\n\nYou can make GPT-2 do all kinds of fun stuff: generate Lord of the Rings fanfiction, brew up some recipes, fake popular science news and generate some sweet, sweet political propaganda.\n\nWhen it works, it\u2019s language use can be wicked good at fooling people who are just skimming (including me, of course). It can hold onto a topic over several sentences, and flow of words is also quite natural on the surface.\n\nIt\u2019s when you read beyond just the words themselves and try to grasp the big picture, it feels more like it took a big pile of what makes up the internet, mixed the sentences around thoroughly, and spit out a barely coherent soup.\n\nSo does this thing have _actual_ language understanding?\n\n\n### Resources\n\nIf you wanna play around with GPT-2 yourself:\n*   [Talk To Transformer](https://talktotransformer.com/)\n*   [Write With Transformer](https://transformer.huggingface.co/)\n<br /><br />\n\nIf you want to dive deep and get your hands dirty with real code, here are some resources:\n*   [Transformers from Scratch](http://www.peterbloem.nl/blog/transformers): Really intuitive explanations of how the transformer works, including the reasoning behind attention. Shows how to implement one in PyTorch.\n*   [The illustrated BERT, ELMo and co.](https://jalammar.github.io/illustrated-bert/): A great overview of the various developments that led to the awesomeness of recent NLP models.\n*   [HuggingFace\u2019s transformers library](https://huggingface.co/transformers/index.html)\n<br /><br />\n\nFor introduction to more fundamental concepts:\n*   [Stanford CS231n Convolutional Neural Networks for Visual Recognition](https://cs231n.github.io/): Nice introduction to machine learning in general, and convolutional nets in particular.\n*   [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/): A short and really approachable book that explains the foundations of neural networks, and how to implement some from scratch.\n\nFor more fun stuff that Neural Nets do:\n*   [AI Weirdness](https://aiweirdness.com/)\n*   [Humans Who Are Not Concentrating Are Not General Intelligences](https://srconstantin.wordpress.com/2019/02/25/humans-who-are-not-concentrating-are-not-general-intelligences/)\n*   [GPT-2 as a step toward General Intelligence](https://slatestarcodex.com/2019/02/19/gpt-2-as-step-toward-general-intelligence/)",
        "author_name": "Kinshuk Kashyap",
        "slug": "generate-text-with-gpt2"
    },
    {
        "layout": "post",
        "title": "Bionics - The Gap between disability and ability",
        "author_github": "nikhilnair",
        "date": "2019-11-10T05:05:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "Diode",
            "IEEE NITK"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "nikhilnair",
        "text": "[{% load static %}]()\n**1.Bionics and its growing relevance**\nThe rise of bionics has prompted some interesting questions- **Can we trade\naway our biological body for a repairable one , one that won\u2019t age?**.\nBionics is defined as the science of connecting biological systems to artificial\nsystems to develop machines that can imitate our biology.\nAs bionic systems and transhuman augmentations exceed human capabilities\nhumanity might soon face an interesting dichotomy - **Either remain physically\ninferior and embody the cherished values of humanism** as we know them or\nto become a **superior transhuman with a repairable body that can\ntranscend the decay of aging**. If you choose the latter it will involve switching\nout almost every part of your body with prosthetics that could do the job equally\nas well.\nThe time to make this decision might be sooner than you think as many of our\nnatural apparatuses are finally becoming replaceable with bionics such as ur\nhearing, our sight and even some simpler parts of our brain.\n\n\n\n**2.The Brain Computer Interface**\n\n![alt text]({% static '/blog/assets/img/bionics/img1.gif' %})\n\nThere exists a repetition and rhythm in the neural activity, which is distinguished\nby the frequencies of the brain waves that have several classes such as infra\nlow waves, delta waves , theta waves , alpha waves , beta waves and gamma\nwaves.\nBCIs(Brain Computer Interfaces) provide ease and comfort by controlling\nmachines through brain signals. They are made by integrating brain signals to\narrange brain potentials in a way so that no muscle arbitration is required. The\nintrusive approach of placing electrodes inside the scalp is cumbersome as\ncompared to the non-invasive approach in which electrodes are placed on the\nscalp, which is used in many BCI based applications.\n\nIn general, BCI systems are divided into four phases\n\n\u25cf acquisition of data\n\u25cf preprocessing the acquired data\n\u25cf extracting features\n\u25cf classification**.**\n\nLet\u2019s explore the 4 phases taking the example of a BCI based robotic arm\n\n**A. EEG Data Acquisition**\n\nIn the first step, data is acquired by an experiment conducted to control the robotic\narm using EEG signals.\nDuring the experiment, a video instructing to perform clench, rest, and attention by\nshowing mind focusing games are shown to the participants each with a time\ndifference of 10 seconds. The video is played on an LED screen and based on\nthe type of video the subjects are instructed to perform the action as shown for\nevery 10 seconds. A 20-minutes long data recording for EEG signals is\nperformed for each subject.\nThe sampling rate of the acquired signals is 256 Hz and the output of the cognitive\nsoftware is sent to the ARDUINO UNO board to control the robotic arm.\n\n\nThe EEG signals acquired through a Muse Headband are then modified, filtered,\ndigitized, and processed.\nFinally, a driving signal is sent to the set of servo motors within the robotic arm to\nperform an action of picking and placing a bottle.\n\n**B. Pre-Processing**\n\nThe acquired EEG data is stored as floating-point values which are directly\nconverted from the unsigned 14-bit analog to digital converter output. These\nfloating-point values cause DC level problem to the signal.\nFor removal of this DC offset it is required to apply DC offset removal. The\nsimplest method for the removal of DC offset is to subtract the average value of\nthe signal from the entire data channel.\n\n**C. Feature Extraction**\n\nAfter preprocessing phase, the next step is to extract features from the acquired\ndata. There are two common types of features that can be extracted are\nLet S(t) represents the acquired signal, then the features can be calculated as\nfollows, The maximum amplitude Smax of the signal S(t) is calculated as,\nSmax=max{|S(t)|}\nThe minimum amplitude Smin of the signal S (t) is calculated\nSmin=min{|S(t)|}\nThe absolute amplitude (AAMP) is calculated using,\nAAMP=|Smax|\nWe calculate avg ,Energy and Power of the acquired signal.\nTo reduce the complexity of the system, another feature named Kurtosis is used\n\n\n**D. Feature Classification**\n\nFor the classification of data, neural networking algorithms are highly efficient. It is\ndifficult to formulate a perfect algorithmic solution that closely matches the\nhuman neuronal system Machine learning programs can extract relevant signal\nin noise presence with low computational complexity and with higher\nclassification accuracy.\n\n\n\n**3.The Next Step in Human Evolution**\n\n![alt text]({% static '/blog/assets/img/bionics/img2.jpg' %})\n\nThe next chapter in Human evolution won\u2019t be biological ,but instead ,\ntechnological.\nWe as a species must strive to transcend disability through technological\ninnovation\nWe shouldn't view disability as being broken- a human being can never be\n\"broken\"\ntechnology is broken/inadequate.\nIn conclusion , if we want to trade away our biological body for a mechanical\nbody then bionics would be the key to make this transition possible. We clearly\nseem to be having the beginnings of what we could call bionic eyes ,bionic skin\nand bionic ears.\nIt might just be the case that the evolution of technology has come to play a\ngreater role in guiding our species than our DNA has.If we could harness this\npower we could open up a new chapter in the storybook of humanity.2",
        "author_name": "Nikhil Nair",
        "slug": "bionics"
    },
    {
        "layout": "post",
        "title": "Who doesn't want to live on another planet?",
        "author_github": "drishikanadella",
        "date": "2019-11-10T05:05:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "Piston",
            "IEEE NITK"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "drishikanadella",
        "text": "[{% load static %}]()\nThe colonization of Mars has been a popular theme of discussion over the past few decades. Whether it&#39;s in the spirit of exploration, expansion of human settlement or mining for additional resources, the idea of inhabiting another planet has allured us.\n\nThis is also evident in how often the idea of colonizing Mars appears in fiction and pop culture: from &quot;The Martian Chronicles&quot; by Ray Bradbury to &quot;The Martian&quot; by Andy Weir, the prospect of humans inhabiting the red planet has been explored from the perspectives of fantasy to science.\n\nKeeping aside the political and ethical challenges that are intrinsic to such a monumental project, how are scientists and engineers all around the world planning to accomplish the colonization of Mars?\n\n# How living on Mars will be different: The Challenges\n\nThe most fundamental obstacle to living on Mars is its conditions, which are vastly different to that of Earth&#39;s (and not in a good way).\n\n- Mars is cold, with average surface temperatures between -87C to -5C.\n- The atmosphere is lethal. It&#39;s made up of 95% carbon dioxide, 3% nitrogen and 1.6% argon.\n- Pressure on Mars is much less than on Earth, with average pressure on the surface being about 0.6kPa.\n- The surface gravity of Mars is 38% that of Earth&#39;s.\n- Mars experiences annual dust storms that cover pretty much most of the planet, and which can last up to several weeks and prevent sunlight from entering the surface of the planet, and also effectively block all communication with Earth.\n- The Martian soil is toxic, thereby rendering it useless for agriculture.\n- Unlike Earth, Mars&#39;s atmosphere is not capable of blocking out harmful radiation, so living on Mars could mean constant exposure to harmful cosmic rays and solar particle events.\n\n# There are some silver linings: The Good Stuff\n\n- A day on Mars is quite similar to a day on Earth (a little more than 24 hours), so theoretically, we can expect the human circadian rhythm to not change on Mars.\n- Various missions to Mars have all confirmed the presence of water, in the form of ice under the Martian surface and in the form of CO2 icecaps near the poles.\n\nEquipped with all this information, how would the Martian explorers potentially carry out their mission?\n\n**The Plan**\n\n# Setting Up Camp\n\nThe best places to colonize on the planet are underground - Mars has large caverns under its surface that provide the best protection from the harmful radiation that the planet&#39;s surface is exposed to.\n\nOther locations worth considering are the poles, which contain the ice deposits built up over millennia. To counter the exposure to deadly radiation, protective habs must be built, which is essentially a dome built with polyethylene fibers. Here is where the Martian scientists&#39; ingenuity comes into play: using the Martian soil itself to create a layer over the hab to provide further protection from harmful UV rays.\n\n# The Atmosphere\n\nWithout oxygen, the people that touch down on the Martian surface would have just enough time to admire its beautiful red landscape. Then, they would die. Oxygen is the elixir of life, so it&#39;s absolutely essential that future Mars explorers have plenty of oxygen to breathe.\n\nNASA has developed MOXIE (Mars OXygen In situ resource utilization Experiment) which splits up carbon dioxide (which is relatively abundant in the Martian atmosphere) into molecular oxygen through a process called solid oxide electrolysis.\n\n# Water: Battling Dehydration\n\nHow do we harness the precious water present underneath Mars&#39;s surface to make it potable? A tried and tested method has been to take a substantial amount of Martian soil and bake it in an &#39;oven&#39;, essentially targeting it with microwave beams that vaporizes the water from the soil, which can be condensed elsewhere.\n\n# Food: Creating a Sustainable Environment\n\nConstantly shipping food from Earth to the Martian citizens is not an option: it&#39;s too expensive and risky. So, growing food on Mars is imperative. Providing supplements and removing some of the harsher chemicals from Martian soil should render it possible for vegetation.\n\nAnother viable option is the hydroponic farming, which is the method of growing plants without soil by instead using mineral nutrient solutions in a water solvent.\n\nThese are just the solutions to a few of the innumerable problems that Mars explorers and scientists will face, some of which haven&#39;t even been thought of yet. There exist the unanswered questions about consequences of the Martian low gravity and pressure on the human body. How will being on another planet affect their psychologies? We don&#39;t have the answers yet.\n\nNevertheless, after the revolutionary Apollo Moon Landing of 1969, this is the most ambitious project undertaken by humans in the field of space exploration. Success of such a mission will provide answers to many important questions in science, opens up new possibilities in virtually every realm of human life and establishes us as an interplanetary species.",
        "author_name": "Drishika Nadella",
        "slug": "colonization-of-mars"
    },
    {
        "layout": "post",
        "title": "Why do Generative Adversarial Nets matter?",
        "author_github": "vrn25",
        "date": "2019-11-10T05:05:00",
        "image": "/assets/img/",
        "description": "What can we gain from GANs?",
        "tags": [
            "Deep Learning",
            "Generative Adversarial Networks",
            "GANs",
            "Generative Models",
            "Machine Learning",
            "CompSoc",
            "IEEE NITK"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "vrn25",
        "text": "[{% load static %}]()\nA new class of **Generative Models** was proposed in 2014 by **Ian Goodfellow** (a.k.a The GANfather) which brought in a novel method of dealing with issues in ML and AI. The Invention of **Generative Adversarial Networks** (GANs) can be regarded as one of the path-breaking innovations in making computers do useful stuff. GANs have seen some crazy development from an application perspective and has been one of the hottest topics for research in the past 5 years.\n\n## So what does a Generative Model do?\n\nThe main objective of a Generative Model is to create more samples of the **same** type as training data. The basic idea is to take a set of training examples and develop a **probability distribution**. Based on that distribution, it generates more samples. There are two ways of generative modeling - one is to explicitly define a **density function**, for instance, a Gaussian density function or log-likelihood which tells the probability distribution that generated them and the other is observing many samples from a particular distribution and generating more samples from the same distribution. GANs come under the latter category, where learning the function is left to the model itself.\n\n## How do GANs work?\n\nGANs form a subclass of **implicit** Generative models that rely on adversarial training of two networks: the **Generator G**, which attempts to produce samples that mimic the **reference distribution**, and the **Discriminator D**, which tries to differentiate between real and generated samples and, in doing so, provides a useful gradient signal to the Generator. GANs have proven to be useful in various domains like unsupervised feature learning, image and video generation. They are illustrated with an analogy in the image.\n\n![Generative Adversarial Network]({% static '/blog/assets/img/why-do-GANs-matter/GAN_structure.png' %})\n\n## Why are Generative Models useful?\n\nThe first question that comes to mind is that why do we even care about generating more samples (say images) with tons of them lying around? The applications are pleasing enough to reason for it:\n\n - Generative models can be used in **Curiosity-driven Exploration** in Deep Reinforcement Learning. One of the biggest problems in RL is balanced **Exploration** and **Exploitation** in high dimensional space. Without efficient Exploration techniques, the agent may just wander around until it stumbles into a rewarding state. This can waste a lot of computation and training time even if we use a heuristic approach. In this [paper](https://arxiv.org/abs/1605.09674), Rein Houthooft and colleagues propose **VIME**, a practical approach to exploration using uncertainty on Generative models.\n\nThe following illustration compares the two approaches. The agent in the left is trained using **VIME** approach while the right one using **Naive** approach.\n\n![VIME approach]({% static '/blog/assets/img/why-do-GANs-matter/policy_with_VIME.gif' %}) | ![Naive approach]({% static '/blog/assets/img/why-do-GANs-matter/policy_naive.gif' %})\n\n - We can use Generative models to simulate possible futures for Reinforcement Learning. We can have an agent learn in a simulated environment built entirely using generative models rather than building it physically. The advantage of using this model-based RL approach is that it can be **parallelized** easily across different machines and the mistakes in it are not as costly as if we make them in the real world.\n\n - Generative models can to fill in missing inputs and learn even when some of the labels in the data are missing. They handle missing inputs much more effectively than the traditional input to output mappings in machine learning models. [**GAINs**](https://arxiv.org/abs/1806.02920) are a type of GANs where the generator imputes a vector of real data, which is then fed back to the discriminator to figure out which data was originally missing. [**MisGAN**](https://openreview.net/forum?id=S1lDV3RcKm) is another variety that can learn from complex, higher-dimensional incomplete data using a **pair** of generators and discriminators. **Semi-Supervised Learning** is an application where we may have very few labeled inputs but by leveraging many more unlabeled examples, we can do good on the test set.\n\n![MSE v/s Adversarial appraoch]({% static '/blog/assets/img/why-do-GANs-matter/face_application.jpg' %})\n\n - The picture shows two approaches to finding the next frame in the video. Since there are many possibilities in a video in the next time step, traditional approaches like **Mean Squared Error** (MSE), result in the output being a bit blurry as a consequence of averaging out various results. Using generative techniques and **adversarial** particularly results in getting a sharp output towards the eyes as well as ears.\n\n## GANs Demystified...\n\nGANs are turning out to be better than the traditional approaches in various applications of Machine Learning. Following are some of the recent developments in GANs:\n\n### High Fidelity Speech Synthesis with GANs\n\nThe **Text-to-Speech** (TTS) task consists of the conversion of text into speech audio. There has been a lot of development in this field using [**Neural Autoregressive models**](https://arxiv.org/abs/1605.02226). However, an essential disadvantage of this technique is that it is difficult to parallelize. Every time step in the audio needs to be considered **sequentially** which is computationally expensive.\n\nUsing GANs can help in parallel waveform generation. **GAN-TTS**, a Generative Adversarial Network for Text-to-Speech is a novel architecture proposed for this. It consists of a feed-forward generator, which is a Convolutional Neural Network, paired with an **ensemble** of multiple discriminators which evaluate the generated and real data. [Mean Opinion Score](https://en.wikipedia.org/wiki/Mean_opinion_score) (MOS), as well as quantitative metrics - [Frechet DeepSpeech Distance](https://en.wikipedia.org/wiki/Fr%C3%A9chet_distance) and Kernel DeepSpeech Distance, are used as evaluation metrics.\n\n![GAN-TTS architecture]({% static '/blog/assets/img/why-do-GANs-matter/subtopic1.png' %})\n\nThe generator's input is pitch and linguistic features and output is a raw waveform at a certain frequency. **GBlock** shows the design of the generator in the above diagram. The output convolutional layer uses **tanh** activation function to produce a single-channel audio waveform. The discriminator consists of an ensemble instead of a single model as in **DBlock**. Some discriminators take the linguistic conditioning into account while others ignore the conditioning and can only assess the general realism of the audio.\n\n### Connecting GANs and Actor-Critic methods in RL\n\nBoth GANs in unsupervised Learning and Actor-Critic methods in Reinforcement Learning are difficult to optimize and stabilize since they often end up giving **degenerate solutions**. Both are multi-level optimization methods where we do not have a single unified objective function and consist of hybrid models where each tries to minimize its **private cost function**. Each level is optimized with respect to the optimum of the other model. This makes the traditional optimization methods like *Gradient Descent* and its variants not work very well since they are oriented towards solving a common cost function.\n\n![GANs and AC methods cycle]({% static '/blog/assets/img/why-do-GANs-matter/subtopic2.1.png' %})\n\nThe main aim of AC methods is to simultaneously learn an **action-value** function along with a **policy**, thereby predicting the reward, while GANs learn to produce more samples of the same type as training data. Both have a feed-forward propagation step where one model controls the agent\u2019s behavior (Actor **A**) or generates samples (Generator **G**) and the second evaluates how good the action is (Critic **C**) or classifies samples as fake or real (Discriminator **D**). The second model has access to some additional information from the environment - reward in case of AC and real data samples in case of GANs. These similarities suggest that heuristics and optimizations for one can be applied for the other.\n\n![Illustration of agent-environment setting like a GAN-pipeline]({% static '/blog/assets/img/why-do-GANs-matter/subtopic2.2.png' %})\n\nThe GAN **minimax** game can be thought of as an agent-environment set up where the actor chooses to set pixels of the image. The environment then *stochastically* chooses to show a real image and give reward **1** or show the actions and give reward **0**. The critic has to predict what the reward is. Here the actor never actually sees the true environment analogous to the generator in GANs where it does not see the real data samples. They rely only on the gradient signal given by the other model. However, this is a sort of unusual setting where the actor does not get to influence the reward, hence making both components **adversarial** instead of cooperative.\n\n### GANs in Medical Imaging\n\nGANs have received state-of-art performance in many image generation tasks. Their ability to create more data without explicitly learning probability density function has a huge scope in Computer Vision. There are two ways in which GANs can be applied in medical imaging. One is the use of a trained generator model to generate images of various body parts. Other is that the discriminator, trained on normal images can be used as a **regularizer** or **detector** for abnormal images. GANs have an edge over traditional ML approaches in medical imaging in terms of cell structure exploration and detecting abnormalities.\n\n![GANs in Medical Imaging]({% static '/blog/assets/img/why-do-GANs-matter/subtopic3.png' %})\n\n**Reconstruction** is a major issue in medical imaging. Many times the image and scans obtained might have some noise or blurriness associated with it. This can be due to various reasons like patient comfort, constraints in clinical settings, etc. A [**pix2pix**](https://arxiv.org/abs/1611.07004) framework and pre-trained [**VGG-net**](https://www.pyimagesearch.com/2017/03/20/imagenet-vggnet-resnet-inception-xception-keras/) have been used to solve the problem but appreciable results are difficult to achieve in case of pumping organs like the heart. The use of [**CycleGAN**](https://arxiv.org/abs/1703.10593) has achieved improvement in cardiac CT denoising.\n\nGANs have also been used for classification tasks in medicine. The semi-supervised training scheme of GANs for chest abnormality detection and cardiac disease diagnosis has achieved comparable results with the original supervised CNN approach with considerably less labeled data.\n\n### Liquid Warping GANs\n\nLiquid Warping GAN is a unified approach towards **Human Motion Imitation**, **Appearance Transfer**, and **Novel View Synthesis**. These techniques are extremely useful in animation, video and game making, virtual clothes try-on, etc. Previous works separately handled these tasks with specific pipelines. Recently, GANs have proven to be useful in successfully solving all three tasks together. Motion imitation inputs a source image and a reference pose image and outputs the person in the former with a pose in the latter. Appearance Transfer is quite similar to **Neural Style Transfer** in Computer Vision, where the aim is to produce a human image preserving the reference identity with clothes (style). Novel View Synthesis aims to produce images of the person from different angles and views.\n\n![Human motion imitation, Appearance transfer, Novel view synthesis]({% static '/blog/assets/img/why-do-GANs-matter/subtopic4.1.png' %})\n\nAs in many Computer Vision applications, traditional methods use 2D landmarking techniques to predict human body structure. However, these can capture only positional details with no modeling of limb rotations and characterization of body shape, which makes the output a bit less realistic. Using Liquid Warping GANs, we can capture the 3D body mesh and simultaneously preserve texture, color, style, and other finer details.\n\nThe pipeline consists of 3 stages which are same for all the three tasks:\n\n#### Body Mesh recovery\nIn this module, the basic body structure, shape and 3D mesh of the person in source and reference images are reconstructed using [Human Mesh Recovery](https://arxiv.org/abs/1712.06584) (HMR) which involves parsing of the image into a feature vector using [**ResNet-50**](https://arxiv.org/abs/1512.03385) followed by a regression network which predicts pose and shape. A bunch of parameters calculated as a function of pose and shape is passed to the next module in the pipeline.\n\n#### Flow Composition\nThis step involves the construction of a map of the source and reference mesh followed by calculating the weighted geometric centroid coordinates of each mesh face. A **transformation flow vector** T is obtained and warped with the source image to get the warped image.\n\n#### Liquid Warping GAN\nThis stage focuses on producing high-fidelity images with desired conditions like style, texture, etc. We use **Liquid Warping Block** (LWB) to preserve these conditions.\n\n![Liquid Warping Block]({% static '/blog/assets/img/why-do-GANs-matter/subtopic4.2.png' %})\n\n - **Generator**: The generator works as 3 streams. The first stream of GANs works on generating a realistic background image. The second or source identity stream is a *convolutional auto-encoder* that identifies the source content, extracts the features required to keep the source details and reconstructs the source front image. The third or the transfer stream synthesizes the final result. LWB **links** the latter two streams. Advantage of using LWB is that it takes care of multiple sources, like in Appearance Transfer, preserving the head of source one and wearing the upper outer garment from the source two, while wearing the lower outer garment from the source three.\n\n - **Discriminator**: For discriminator, a [**pix2pix**](https://arxiv.org/abs/1611.07004) model is followed.\n\n## References \n1. [Generative Adversarial Networks, Ian Goodfellow et al., 2014](https://arxiv.org/abs/1406.2661)\n2. [GAN Tutorial, NIPS 2016](https://arxiv.org/abs/1701.00160)\n3. [Open AI article on Generative Models](https://openai.com/blog/generative-models/)",
        "author_name": "Videh Raj Nema",
        "slug": "why-do-gans-matter"
    },
    {
        "layout": "post",
        "title": "An Introduction To WebSockets",
        "author_github": "aksh555",
        "date": "2019-12-30T01:32:44",
        "image": "/assets/img/",
        "description": "Evolution of WebSockets",
        "tags": [
            "Web",
            "Web Sockets",
            "Networks"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "aksh555",
        "text": "[{% load static %}]()\n# Introduction\n\n*Have you ever wondered how the extremely volatile stock/share market prices are constantly updated on online portals?*\n*To have a glimpse of how rapid the changes are, head here* [Cryptowatch portal](https://cryptowat.ch/markets?types=spot%2Bfutures \"Cryptowatch\")\n*Similarly how do new messages automatically pop up in Whatsapp Web or Facebook Chat?*\n\n# A Brief History of Real-Time Web Applications\n\nWhen a user surfs the web, the browser (client) establishes a HTTP connection with a server; sends certain requests via RESTful routes (\u2018GET\u2019, \u2018PATCH\u2019, \u2018POST\u2019, etc) to the server that hosts the website. The server receives the requests and sends information as responses back to the client, which receives and renders the response information on the page. Following this the connection would close. Any further request, (by the user to the same server also) required the opening of a new HTTP connection.\n\n![Image1]({% static \"/blog/assets/img/websockets/image1.png \"Normal HTTP Connection\"\" %})\n\nThis went on just fine until.....\nClients started sending more frequent requests to the server requesting for data.\nA good example of this is web apps that need real-time data (like gaming or chat apps). These require an abuse of the HTTP protocol to establish bidirectional data transfer.\n\n# Problem\nThe main problem in the traditional HTTP model was that it was **always the client that initiated transactions, ie. it was unidirectional**. \nAn other reason was increasing overheads due to HTTP. Every time a HTTP request is made, a bunch of headers and cookie data are transferred to the server. This can add up to a reasonably large amount of data that needs to be transferred, which in turn increases latency.\n\n>Basically imagine that each time to get an updated stock's price/a new message from your friend you had to refresh the page, and wait until the new price/message loaded. Firstly how would you even know that there has been a change in price/a new message has been sent to to you? And even if you did, it would take\nquite a while to load.\n\nSo a number of strategies were proposed to enable the server to 'push' data to the client.\nSome of the **startegies** devised to combat the issue were:\n### 1. Optimising HTTP Polling\n- **Periodic Polling:**\nThe very first attempt to solve the problem was by polling the server at regular intervals. It could reduce server consumption by increasing time gap between two requests. But ofcourse notifications of server events would be delayed :/\n- **Long Polling:**\nAfter the server receives a request, it holds the request open until new data(if any) is available. There were no notification delays.But more server resources were used.\n- **Streaming:**\nBasically infinitely polling...But the connection could get interrupted by intermediaries serving other requests in a Round-Robin manner.\n\n### 2. AJAX\nThis technology can be used to create better, faster and more interactive applications using XML, HTML, CSS, and JavaScript. With Ajax, once you submit a form, JavaScript makes a request to the server, finds the result and updates the screen. The end user remains unaware of the fact that information was actually transmitted to the server.\nAJAX is asynchronous; thus, a web front-end can send any number of AJAX requests without having to wait for each response. Each AJAX request is associated with a JavaScript callback that gets called when the server sends the response.\n\n![Image2]({% static \"/blog/assets/img/websockets/image2.jpg \"AJAX Connection\"\" %})\n\n\n### Then came along WebSockets!!\n\nWebSockets provide a **persistent, duplex connection between a client and server that both can use to start sending data at any time**. Since it maintains a persistent unique connection, latency problems faced previously are eliminated. \nAlso, no HTTP/XML headers are needed anymore, as the amount of data simply moving around between the server and cient is cut down.\n\n## Working of WS\n\nThe client establishes a WebSocket connection through a process known as the WebSocket handshake. This process starts with the client sending a regular HTTP request to the server. An Upgrade header is later included in this request that informs the server that the client wishes to establish a WebSocket connection.\nIf the server supports WS, the handshake is completed and the initial HTTP connection is replaced by a WebSocket connection that uses the same underlying TCP/IP connection. At this point, either party can starting sending data. Data is transferred through WebSocket as messages, each of which consists of one or more frames containing the data being sent (the payload).\n\n\n![Image3]({% static \"/blog/assets/img/websockets/image3.png \"WS\"\" %})\n\nThe WS traffic of a web page can be inspected by viewing the WS tag under the Network section of developer tools.\n\n![Image4]({% static '/blog/assets/img/websockets/image4.png' %})\n\nWS are supported in almost every web browser except Android browser and Opera Mini.\n\n**Note: WebSockets don\u2019t make AJAX obsolete but they do supersede HTTP Long-polling/HTTP Streaming as the solution of choice for true realtime functionality.**\n**AJAX should still be used for making short-lived web service calls**\n\n## Further Reading\n\nWebSockets can be implemented practically using Node.js(using Socket.IO), Rails(using Action Cable) and in other frameworks too.\n\n## References\n\n- [Intro tutorial](https://medium.com/@yassimortensen/an-introduction-to-websockets-10b131182559) \n- [Implementation with Node and React](https://blog.logrocket.com/websockets-tutorial-how-to-go-real-time-with-node-and-react-8e4693fbf843/)",
        "author_name": "P Akshara",
        "slug": "an-intro-to-websockets"
    },
    {
        "layout": "post",
        "title": "Bluetooth Low Energy",
        "author_github": "surajxkulkarni",
        "date": "2019-12-30T01:32:44",
        "image": "/assets/img/",
        "description": "BLE is a part of the next generation wireless communication technology, which prioritises battery life over high data transfer, which is ideal for IoT applications.",
        "tags": [
            "low power",
            "wireless communication",
            "Diode",
            "IEEE NITK"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "surajxkulkarni",
        "text": "[{% load static %}]()\nWe all know how infuriating it is when our devices drains battery when it uses wireless communication like wifi, bluetooth, infra-red. In every device we use, we are always concerned with the **power consumption**. A device which consumes less power is always favourable.\nSo in this article, I am going to elucidate an effective solution to the power problem in wireless communication- **Bluetooth Low Energy** , also known as **BLE**.\n\n# Why BLE?\n\nHistorically, **classic bluetooth** (the bluetooth we are all aware of) was mainly designed to replace cable and it covers many protocols. And with speeds increasing with new versions, **power consumption** was **not always the primary concern** for this technology.\n\nHence, it was not suitable for wireless boards running on a battery that only needs to send a few bytes of information. Well this is where **BLE** comes in. It was contrived to consume as **low power** as possible. Devices using BLE can run on button cells for months or even years.\n\n# BLE Architecture\n\n   ![alt text]({% static '/blog/assets/img/BLE/BLE Architecture.jpg' %})\n\nBLE architecture has the following stacks-\n\n- #### **Physical layer -**\n    This is the physical radio which transmits in the **2.4GHZ** radio spectrum, which is the same spectrum used by classic bluetooth, wifi and zigbee.\n- #### **Link layer -**\n    The link layer describes how two devices can use a radio link to **transmit information** from one to another. The link layer has a simple a state machine with only 5 states-\n         -  **Standby mode:** Device is in an idle state\n              -  **Advertising mode:** When the slave needs to make a connection. If it is sending data, it goes to anyone, its not specifically meant for any particular device.\n              -  **Scanning mode:** When a device wants to listen what other devices are advertising.\n                   - **Initiating mode:** This is sent from the future master to the device that is advertising and asked to make a specific connection.\n                   -  **Connected mode:** This is when two devices are connectedand sending each other data packets purposefully, so it has  a starting point and end point, not general advertising.\n\n    ![alt text]({% static '/blog/assets/img/BLE/Link Layer.png' %})\n\n- #### **Host Controller Interface (HCI) -**\n    It is the **interface between the host and the controller**. The HCI has 2 main functions-\n       - It sends commands to the controller and receives events back.\n       - And it sends and receives data from a peer device.\n- #### **Logical Link Control and Adaptation Protocol (L2CAP) -**\n    The L2CAP multiplexes 3 different channels and it enables segmentation and reassembly of the data packets that are larger than the radio can deliver.\n- #### **Attribute Protocol -**\n    This is where the data is stored and defines how the attribute is managed, the type of attribute, how to read or write data and much more.\n- #### **The Generic Attribute Protocol (GATT) -**\n    The GATT defines the format of the data exposed by a BLE device. It also defines the procedures needed to access the data exposed by a device.\n- #### **Generic Access Protocol (GAP) -**\n    This makes the device visible to the outside world and determines the role of the device in the network.\n\n## But why does BLE consume less power?\n\nBLE uses less energy by keeping things simple. Instead of maintaining a constant connection, the BLE protocol only **sends data as needed**. And it does so with **very little overhead**. This allows the device to **sleep for longer periods of time** , turn off the radio and **consume less power**. Thus, it is great for periodic updates, like getting readings for a sensor.\nBut its not so great for streaming audio or video. For now, that is a job left to classic bluetooth or wifi.\n\n\n## Advantages\n\n- Ultra-low peak, average and idle mode power consumption\n- Low power requirements\n- Small size\n- Low cost\n- Compatibility with large base of mobile phones, computers and tablets\n- Communication range(10-50m)\n\n## Applications\n\n- Home Automation(Internet of Things)\n- Fitness Devices\n- Indoor Location(where GPS is not feasible)\n- Medical and personal health devices",
        "author_name": "Suraj Kulkarni",
        "slug": "bluetooth-low-energy"
    },
    {
        "layout": "post",
        "title": "Stem Cell Technology",
        "author_github": "ishita",
        "date": "2019-12-30T01:32:44",
        "image": "/assets/img/",
        "description": "Intro to Stem Cell Technology",
        "tags": [
            "stem cells",
            "Piston",
            "IEEE NITK"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "ishita",
        "text": "[{% load static %}]()\n# STEM CELL TECHNOLOGY\n\n### What are stem cells?\n\nImagine waking up from a tragic accident that left you paralyzed from the neck down, will there be any hope to recover completely? Won't it be amazing if there was a research that the use of stem cells could lead to potential treatments and cures.\n\nWith the advent of technology in the fields of artificial intelligence, robotics, genetics, etc., homo sapiens are finally at the threshold of a path to revitalizing and renewing our current fields of health care.\n\nStem cells are cells that can differentiate into other types or kinds of cells, and can also divide in self-renewal to produce more of the same kind of stem cells.\n\n![Image1]({% static '/blog/assets/img/stem cells/figure 1.png' %})\n\n### The origin of stem cell technology\n\nThe history of stem cell research had a benign, embryonic beginning in the mid 1800's with the discovery that some cells could give birth to other cells. The first actual stem cells were discovered in the early 1900's when it was found that some cells generate blood cells.\n\n![Image2]({% static '/blog/assets/img/stem cells/figure 2.png' %})\n\n### More on stem cells\n\nBoth animal and human stem cells have been worked on in the history of their research. Stem cells can be classified into three extensive categories, based on their ability to differentiate and produce cells. Totipotent stem cells are found only in early embryos, hence are also called embryonic stem cells. Every cell is capable of forming a complete organism (e.g., identical twins). Stem cells residing in a developing embryo can differentiate itself into all the specialized cells\u2014ectoderm, endoderm and mesoderm while maintaining the normal turnover of regenerative organs, such as blood, skin, or intestinal tissues. Pluripotent stem cells, yet another type exist in the undifferentiated inner cell mass of the blastocyst and can form any of the over 200 different cell types present in the body. Multipotent stem cells are derived from fetal tissue, cord blood, and adult stem cells, which are found in various tissues of fully developed mammals. In adult organisms, stem cells and progenitor cells carry out the task of replenishing adult tissues and hence acting as a repair system for the body.  Although they lag behind in their ability to differentiate than pluripotent stem cells, they already have a track record of success in cell-based therapies.\n\n![Image3]({% static '/blog/assets/img/stem cells/figure 3.png' %})\n\nThe inference is embryonic stem cells can become all cell types of the body because they are pluripotent whereas adult stem cells are thought to be limited to differentiating into different cell types of their origin.\n\n![Image4]({% static '/blog/assets/img/stem cells/figure 4.png' %})\n\n### Where are stem cells found\n\nTill date we are aware of three known unrestricted sources of autologous adult stem cells in humans: bone marrow, adipose tissue, and blood. Stem cells can be obtained conveniently from umbilical cord blood just after birth. In accordance with all the recent studies of all stem cell therapy types, autologous harvesting involves the least risk.\n\nUsually the stem cells remaining after the birth of a baby are discarded, but they can be used in future to regenerate a body part of the person which usually can't be done by using adult stem cells.\n\nTherefore, there is a provision of preserving the stem cells immediately after a baby is born so that they can be used for regenerating a body part in future. If not isolated and cultured properly and with great care, stem cells can't survive. Therefore, in most cases, they are of no use if they aren't preserved immediately after the birth.\n\nAccording to the current research, the stem cells extracted from a baby can be used to regenerate the damaged or lost organ of that baby as well as his/her close blood relatives.\n\n![Image5]({% static '/blog/assets/img/stem cells/figure 5.png' %})\n\n\n\nField of medical research studying human and animal stem cells is referred to as stem cell technology. Stem cells are characteristically and naturally occurring cells essential for the growth and regeneration of organisms. Stem cells are already used for therapeutic purposes, and research suggests potential future benefits of stem cells in many fields of medicine. This research is controversial, however, because some such technology involves the use of human embryos.\n\nStem cells occur in all multicellular organisms. They are capable of transforming into any of the specialized cells necessary for the growth and maintenance of organs. In developing embryos, they become the cells necessary for the formation of various organs. In later life, they replenish these organs by replacing dead or diseased cells. Stem cell technology helps us to achieve a better understanding of this complex process.\n\nStem cells in bone marrow replenish the body's supply of blood cells. Bone marrow transplants, a common treatment for leukemia patients, are one form of stem cell technology. Some scientists believe that similar uses of stem cells could eventually treat diseases as diverse as diabetes, nerve disease, Alzheimer's and hearing loss. If genetic research can explain the functioning of stem cells, they could theoretically be used to replace cells damaged or destroyed by these diseases.\n\n### Concluding remarks\n\nStem cell research is a relatively new technology that develops primitive human cells into most any of the 220 varieties of cells in the human body, including blood cells and brain cells.\n\nExperts suggest that the ability to use a patient's own cellular material to cure him or her will lessen the chance of transplant or therapy rejection, as the body will be less likely to attack its own cells. Medical experts have made several early advances in the fight against cancer and blood disorders with the help of stem cell research which began in the late 20th century.\n\nIn recent years, proposition of many isolation and culture technologies has encouraged the application of stem cells in research of disease mechanism and clinical therapy. However, most types of stem cells have their own disadvantages for therapeutic applications, such as lack of availability, risk of immune rejection, directional regulation, and the most baffling one, ethical controversy. The recent discovery of iPSCs (Induced pluripotent stem cells) holds the potential to solve these problems. However, the low efficiency of iPSCs generation and their therapeutic safety need to be further studied and be thorough before its application in initiation of human clinical trials. The speed and efficiency of iPSCs generation from both mouse and human somatic cells can be enhanced by adding vitamin C to the culture medium and the safety of iPSCs may be improved by using non-integrating viral vectors. Therefore, it is expected that clinical application of iPSCs will be achieved in the near future, which will mark a historical leap in the area of medicine.\n\nAdvancement and through application of stem cell technology in organ transplant, increasing life span, curing paralysis etc. will surely lead to a massive leap towards improvement in the field of healthcare.",
        "author_name": "Ishita Goyal",
        "slug": "stem_cells"
    },
    {
        "layout": "post",
        "title": "A Comprehensive Guide to Li-Fi",
        "author_github": "krithikvaidya",
        "date": "2020-01-07T14:31:55",
        "image": "/assets/img/",
        "description": "An Introduction to Light Fidelity, Its Scope and Its Applications",
        "tags": [
            "Li-Fi",
            "Wireless Networks"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "krithikvaidya",
        "text": "[{% load static %}]()\n## Introduction\n\n![Li-Fi Visual]({% static '/blog/assets/img/a-comprehensive-guide-to-lifi/1.jpeg' %})\n\nWouldn't it be interesting if we could use our everyday LED bulbs as the transmitters of data for wirelessly accessing the internet, instead of Wi-Fi Routers or Cellular Networks? Li-Fi is a technology that does exactly that. Li-Fi was unveiled by *Dr. Harald Haas* in his iconic 2011 TED Talk entitled [Wireless data from every light bulb](https://www.youtube.com/watch?v=NaoSp4NpkGg), where he demonstrated Li-Fi in public for the first time. Li-Fi (which is an acronym for **Light Fidelity**), is a light-based communication system that is capable of transmitting data at high speeds over the visible, ultraviolet, and infrared spectrums, in contrast with conventional wireless data communication methods (such as Wi-Fi, 4G, etc.) that use the radio frequency spectrum for data transmission.\n\nLi-Fi is a subset of Optical Wireless Communications (**OWC**). However, the uniqueness of Li-Fi is brought out in the fact that the same light energy used for illumination, may also be used for communication!\n\nThe use of visible light to transmit data allows Li-Fi to offer several advantages - most notably, a wider bandwidth channel (the visible light spectrum is 2,600 times larger than the entire radio frequency spectrum), the ability to safely and reliably transmit data in areas susceptible to electromagnetic interference due to radio waves (e.g. in aircraft cabins, hospitals, nuclear power plants and many more), and to offer higher transmission speeds (researchers at The University of Oxford and University College, London recently achieved speeds of **224 Gigabits/Second** using Li-Fi!).\n\n## How It Works\n\n![A demo of Li-Fi Communication]({% static '/blog/assets/img/a-comprehensive-guide-to-lifi/2.jpg' %})\n>Credits: [https://www.weforum.org/agenda/2015/10/could-li-fi-be-the-future-of-the-internet/](https://www.weforum.org/agenda/2015/10/could-li-fi-be-the-future-of-the-internet/)\n\n*LED's* are light sources based on semiconductors which can amplify and switch their light intensity rapidly (within nanoseconds, in fact); therefore, LED cells can modulate a massive number of signals per second without the human eye even noticing! Since a robust Li-Fi system requires extremely high rates of light output, this makes LED bulbs ideal for implementing Li-Fi transmission. Communication using Li-Fi is just as seamless as other radio systems, allowing the users to be connected wherever there is Li-Fi enabled light.\n\nThe changes in light intensity from the LED light source are interpreted by the receiving **photodiode** device. Once the electronic signal is demodulated, it is converted into a continuous stream of binary data, which is then transferred to the target device.\n\nA helpful *infographic* illustrating the working of Li-Fi is attached below:\n\n![How Li-Fi Works?]({% static '/blog/assets/img/a-comprehensive-guide-to-lifi/3.png' %})\n>Credits:[https://www.cdnsol.com/blog/li-fi-light-based-communication-technology/](https://www.cdnsol.com/blog/li-fi-light-based-communication-technology/)\n\nYou can also watch a live demo of the technology [here](https://youtu.be/NaoSp4NpkGg?t=349) \n\nA few questions may pop into your mind when first encountering Li-Fi:\n\n 1. You may wonder if you'll need to keep your lights on all the time if you wish to stay connected. The answer is yes. However, due to their very sensitive communication link and great signal to noise ratio, the Li-Fi LEDs can be dimmed to **below human visibility** while still emitting enough light to carry data!\n\n2. You may ask if it's just a *line-of-sight technology* that requires the communicating sender and receiver to have direct visiblity of one another. The answer to this is not really, because any light-transmitting path that exists between the transmitter and receiver can support a Li-Fi link, including reflections from a wall, ceiling or floor. Recently, it has been demonstrated that a signal reflected off a wall can achieve transmit speeds of upto **70 Mbits/sec**.\n\n3. What might happen if the light beam is *temporarily blocked*, or say, if we wish for our mobile devices to be connected to the internet when it's in our pocket and all external light sources are blocked-off. The answer to this is that it is not necessary for Li-Fi to be the sole active network transmission link at all times: it can work alongside other systems. Currently, mobile devices can transfer between a variety of different communication protocols such as Wi-Fi, 4G, 3G, etc. Li-Fi can **seamlessly** be integrated with this available stack of communication protocols - when a Li-Fi link is available, it can be used in preference to other links, due to its higher transmission rates.\n\n\tBut, think about it - a lot of the times you require high data transfer rates (for example, watching an ultra-HD Youtube video) are inevitably the exact times when you are using the device and it's in range of a light source within the surroundings. So, more often than not, a Li-Fi signal will usually be available when it is most required.\n\n5. You may be curious about whether other light sources in the room, or say something like strong sunlight, could interfere with Li-Fi communications. Li-Fi relies on detecting rapid changes in light intensity and not on the absolute amount of light falling on it. This means that constant or slowly-varying sources of light can be filtered out. Different room-lighting scenarios have been modelled and tested while a Li-Fi link has been established; it has been shown that it is possible to achieve *optimum lighting* as well as a *clean Li-Fi reception* simultaneously in a variety of different scenarios. Li-Fi receivers have also been tested outdoors when exposed to 77,000 lux of sunlight. \n\n6. Finally, you may wonder if Li-Fi is suitable for *downloads only* - that is, is it also possible for our devices that are receiving data to also send data? \nSince Li-Fi is not restricted to visible light for communications, uplink communication can be efficiently done using the infrared spectrum. In 2018, Harald Haas (the inventor of Li-Fi) and his team conducted an experiment in which data was sent at a speed of 1.1\u202fGbps over a distance of 10\u202fm with an LED of only **4.5\u202fmW** optical output power!\n\n\n## Its Advantages over Conventional Information Transfer Methods\n\n\nLi-Fi has the advantage of being useful in electromagnetic sensitive areas such as in aircraft cabins, hospitals and nuclear power plants (where RF-based communications are heavily restricted), without causing electromagnetic interference.\n\nBoth Wi-Fi and Li-Fi transmit data over the electromagnetic spectrum, but whereas Wi-Fi utilizes radio waves, Li-Fi uses visible, ultraviolet and infrared waves. This is advantageous because the US Federal Communications Commission has warned of a potential **spectrum crisis** caused by Wi-Fi being close to full capacity. Li-Fi has almost no limitations on capacity, since the entire visible light + infrared spectrum is about 2,600 times larger than the entire radio frequency spectrum.\n\n![Electromagnetic Spectrum]({% static '/blog/assets/img/a-comprehensive-guide-to-lifi/4.jpg' %})\n>Credits:[https://www.sciencedirect.com/science/article/pii/S2405428317300151](https://www.sciencedirect.com/science/article/pii/S2405428317300151)\n\n**Security:** Since LiFi makes use of the visible light spectrum, it cannot penetrate through optically opaque objects like walls, thus adding another level of defense against unauthorised access (RF-based communication waves can easily pass through walls). Also, the existing security protocols for encryption and authentication in other types of communications can be be applied to Li-Fi systems too.\n\n**Speed & Bandwidth:** At the current state-of-the-art, Li-Fi can deliver multiple Gbps of speed in mobile devices. This next generation technology will drive wireless beyond any current capability, opening up unprecedented bandwidth. With a single LED, peak transmission speeds of **8 Gbps** have been recorded.\n\n![Li-Fi faster than Wi-Fi]({% static '/blog/assets/img/a-comprehensive-guide-to-lifi/5.jpeg' %})\n>Credits:[https://technicalera21.blogspot.com/2018/12/li-fi-100-times-faster-than-wifi.html](https://technicalera21.blogspot.com/2018/12/li-fi-100-times-faster-than-wifi.html)\n\n**Reliability and Low Latency:** Li-Fi provides *enhanced reliability* enabling interference-free communications. It currently offers latency that is a factor of three times lower than Wi-Fi, which can help enable automation and applications such as Augmented Reality and Virtual Reality.\n\n**Interference Free:** Conventional RF is vulnerable to interference from a wide range of devices such as cordless land-line phones, microwaves and neighbouring Wi-Fi networks. Li-Fi signals can be restricted to the *area of illumination*, which means that interference is much simpler to avoid.\n\n## Commercial Implementations of Li-Fi Technology\n\n1. **pureLiFi** is a manufacturing firm co-founded by the inventor of Li-Fi to commercialize Li-Fi products for their integration with existing LED-lighting systems. It is currently at the forefront of production of Li-Fi related products. pureLiFi demonstrated the first commercially available Li-Fi system, the Li-1st, at the 2014 Mobile World Congress in Barcelona.\n\n\tSince then, the company has gone through multiple iterations of products, each generation improving upon the drawbacks of the previous. Currently, the latest products developed by pureLiFi are the [LiFi-XC](#further-readings-and-resources) and the [Gigabit Li-Fi Mobile Integration](#further-readings-and-resources).\n\n\tYou can have a look at the history of Li-Fi products developed by pureLiFi [here](http://purelifi.com/legacy-products/)\n\n2. A **BMW** manufacturing plant located in Munich, Germany [tested](https://www.eenewseurope.com/news/li-fi-passes-industrial-test-bmws-robotic-tools) the operation of Li-Fi in an industrial environment with mobile robots, and the results were very positive. In fact, BMW's project manager hopes for the scaling-down of Li-Fi transceivers for its widespread adoption in manufacturing plants.\n\n3. In August 2018, Kyle Academy, a secondary school in Scotland, trialed Li-Fi within the school. Eight LiFi-enabled LED ceiling bulbs using rapid on-off patterns were set up to transmit data to USB Li-Fi receivers plugged into the students' laptops.\n![Scotland School Li-Fi Test]({% static '/blog/assets/img/a-comprehensive-guide-to-lifi/6.jpg' %})\n> Credits:[https://www.fibre-systems.com/news/lifi-technology-piloted-scotland-s-south-ayrshire-school](https://www.fibre-systems.com/news/lifi-technology-piloted-scotland-s-south-ayrshire-school)\n4. Recently, in June 2019, French company Oledcomm tested their Li-Fi technology at the 2019 Paris Air Show. Oledcomm hopes to collaborate with Air France in the future to test Li-Fi on an aircraft in-flight. In-flight internet (on the infrequent even that it is offered) is often unreliable and slow. Oledcomm wants to take things up a notch by making high-speed LiFi available on Air France and KLM flights, promising in-cabin bandwidths of 100Mbps. Currently, even the fastest WiFi available on domestic airlines in the US stalls at around 15 Mbps. The actual implementation of the the technology, named LiFiMAX, should be available on Air France flights by the end of the year. [Here's a link](https://www.youtube.com/watch?v=71Td0HQ7lx0) to a demo of the technology.\n\n![Li-Fi in Airplanes]({% static '/blog/assets/img/a-comprehensive-guide-to-lifi/7.jpg' %})\n>Credits: [https://www.fasttracks.eu/li-fi-technology-how-it-works-and-its-applications/](https://www.fasttracks.eu/li-fi-technology-how-it-works-and-its-applications/)\n\nUsing Li-Fi, aircraft passengers can access high-speed internet via lights fitted over their seats. Li-Fi can also be used to integrate the In-Flight Entertainment System with the passengers' smartphones. Oledcomm also a manufactures a [range of Li-Fi devices](https://www.oledcomm.net/our-products/#1529847210185-42f3d5b8-a603), such as Li-Fi enabled desk lamps, hospital lights and street lights.\n\n\n## Current Limitations\n\nEvery new technology has its limitations, and Li-Fi is no exception. A few of the major drawbacks of Li-Fi are:\n\n-   Opaque obstacles on pathways can affect data transmission. The main problem is that light cannot pass through objects - so if the receiver is blocked in any way, then the high-speed Li-Fi signal will immediately disappear.\n-   Unlike radio waves, light waves do not penetrate through walls and so Li-Fi may potentially have a shorter range compared to Wi-Fi. While this does help improve security, it may also prove to be an annoyance at times.\n-   We still need RF-based Wi-Fi and Cellular Communications, since we cannot have a light bulb that provides data to high-speed moving objects and in other such scenarios where the scope of using of Li-Fi is non-ideal\n\n## Conclusion and Future Scope\n\nThere is plenty of scope for innovation in the field of Li-Fi communications. Like conventional broadband and Wi-Fi, Li-Fi can also function as a bidirectional communication system. Currently, research is being done looking into using multi-colored **RGB** (Red - Green - Blue) LED's at retina size to send and receive a wider range of signals than single-colored white LED's.\n\nLi-Fi is an innovative technology that is poised to impact many industries. It can unlock the IoT and [Industry 4.0](https://www.spotlightmetal.com/iot-basics-what-does-industry-40-mean-a-842216/) applications, enable communication between intelligent transport systems, enhance road safety with the driverless car revolution, create secure wireless networks, and can aid in health monitoring of the elderly.\n\nIn the not-too-far future, it isn't difficult to imagine the modest LED lightbulb serving a multitude of applications - it will be an integral part of the emerging smart cities and smart homes. This could possibly lead to the merging of the lighting industry with the wireless industry. However, an important prerequisite for the large-scale adoption of any technology is the availability of standards. In this regard, the IEEE *802.11bb* Task Force for the development of a Global Standard for Li-Fi Communication was set-up in 2018.\n\n![Generations of Cellular Communications]({% static '/blog/assets/img/a-comprehensive-guide-to-lifi/8.jpg' %}) \n>Credits:[https://ars.els-cdn.com/content/image/1-s2.0-S2405428317300151-gr8.jpg](https://ars.els-cdn.com/content/image/1-s2.0-S2405428317300151-gr8.jpg)\n\nAlthough there\u2019s still a long way to go to make this technology a commercial success, it promises a great potential in the field of wireless internet. The commercial deployment of Li-Fi will result in inexpensive and fast internet service world-wide.  Widespread migration to cleaner, greener and safer communication networks such as Li-Fi will have a positive impact on the current global climate crisis too. \n\n## Further Readings and Resources\n\n[Gigabit Li-Fi integration into our smartphones](https://purelifi.com/wp-content/uploads/2019/04/Why-Design-In-Gigabit-LiFi-pureLiFi.pdf)\n\n![Gigabit Li-Fi integration into our smartphones]({% static '/blog/assets/img/a-comprehensive-guide-to-lifi/9.png' %})\n\n[LiFi-XC: The current state-of-the-art amongst consumer-oriented LiFi devices](https://www.youtube.com/watch?v=yzWNB9CkpmA)  \n[Press Release of the Global LiFi Congress 2019](https://www.lificongress.com/Press-release-93-2.html)  \n[A small-scale DIY Li-Fi tutorial](https://www.youtube.com/watch?v=1JxImoxlq_Q)  \n[Integrating Li-Fi receivers in Solar Cells](https://www.youtube.com/watch?v=iHWIZsIBj3Q)  \n[Gigabit Li-Fi demonstration](https://www.youtube.com/watch?v=ybK9AgRey1o)  \n[A deeper dive into the technical aspects of Li-Fi](https://www.researchgate.net/publication/279530585_Li-Fi_Technology_Data_Transmission_through_Visible_Light)  \n[Li-Fi products by Oledcomm](https://www.oledcomm.net/)  \n\n## References\n\n[https://purelifi.com/faq/how-does-lifi-work/](https://purelifi.com/faq/how-does-lifi-work/)  \n[https://www.lifi-centre.com/about-li-fi/faqs/](https://www.lifi-centre.com/about-li-fi/faqs/)  \n[https://en.wikipedia.org/wiki/Li-Fi](https://en.wikipedia.org/wiki/Li-Fi)  \n[http://www.axrtek.com/how-does-liFi-work.html](http://www.axrtek.com/how-does-liFi-work.html)  \n[http://tec.gov.in/pdf/Studypaper/lifi%20study%20paper%20-%20approved.pdf](http://tec.gov.in/pdf/Studypaper/lifi%20study%20paper%20-%20approved.pdf)  \n[https://www.youtube.com/watch?v=iHWIZsIBj3Q](https://www.youtube.com/watch?v=iHWIZsIBj3Q)  \n[https://www.fibre-systems.com/news/lifi-technology-piloted-scotland-s-south-ayrshire-school](https://www.fibre-systems.com/news/lifi-technology-piloted-scotland-s-south-ayrshire-school)  \n[https://www.sciencedirect.com/science/article/pii/S2405428317300151](https://www.sciencedirect.com/science/article/pii/S2405428317300151)  \n[https://purelifi.com/lifi-technology/](https://purelifi.com/lifi-technology/)  \n[https://www.archlighting.com/technology/li-fi-high-speed-communication-via-led-modulation_o](https://www.archlighting.com/technology/li-fi-high-speed-communication-via-led-modulation_o)  \n[https://purelifi.com/lifi-misconceptions/](https://purelifi.com/lifi-misconceptions/)  \n[https://www.archlighting.com/technology/li-fi-high-speed-communication-via-led-modulation_o](https://www.archlighting.com/technology/li-fi-high-speed-communication-via-led-modulation_o)",
        "author_name": "Krithik Vaidya",
        "slug": "a-comprehensive-guide-to-lifi"
    },
    {
        "layout": "post",
        "title": "Autopilot",
        "author_github": "sharayubrahmankar95",
        "date": "2020-01-07T07:00:00",
        "image": "/assets/img/",
        "description": "A breakthrough innovation of the early 1900s which soared to great heights in the field of aviation - AUTOPILOT",
        "tags": [
            "Autopilot",
            "Modern Aircraft"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "sharayubrahmankar95",
        "text": "[{% load static %}]()\n# AUTOPILOTS\n\nAn autopilot is a flight control system that allows a pilot to fly an airplane without continuous hands-on control of the airplane. It allows the pilot to focus on higher-order tasks such as navigating, communicating with air traffic control, planning for weather contingencies and rerouting associated with any kind of emergency circumstance.\n\n![A simple autopilot combined with conventional navigation instruments.]({% static '/blog/assets/img/autopilot/Figure_1.png' %})\n\n### FIRST AUTOPILOTS\n\n  In the early days of aviation, pilots had to control the aircraft all the time to fly safely. As aircraft range increased and the flight durations increased, it became a serious problem for the pilots to continuously control the aircraft. \n\n![A World War II-era Honeywell C-1 autopilot control panel.]({% static '/blog/assets/img/autopilot/Figure_2.jpg' %})\n\nAutopilot systems have been around since the early 20th century. According to Wikipedia, the first functional autopilot system developed in 1912 by the electronics company Sperry Corporation had an autopilot system with a gyroscope, altitude, and operated elevators as well as rudders.\n\n### AUTOPILOTS IN MODERN AIRCRAFTS\n\n![Autopilots in Modern Aircrafts]({% static '/blog/assets/img/autopilot/Figure_3.jpg' %})\n\nAutopilot in aircraft with more than twenty seats is generally made compulsory by international aviation rules and regulations. Autopilots in modern aircraft are three-axis and they control the aircraft during climbs, descents, cruise, and approach to landing. Some even integrate an auto-throttle function that automatically controls engine thrust that makes auto landings possible.\nFor further automatic control, flight management systems have been developed. Through the use of computers, an entire flight profile can be programmed ahead of time allowing the pilot to supervise its execution. An FMS computer coordinates nearly every aspect of a flight, including the autopilot and autothrottle systems, navigation route selection, fuel management schemes, and more.\n\n**Control Wheel Steering(CWS)** is an option midway between fully automated flight and manual flying. Although CWS is less used in modern airliners, it is still a function on many aircraft today. CWS gives stability in pitch and roll. In this system, the limitations of the aircraft can be observed by the flight computer, and the pilot cannot steer the aircraft past these limits.\n\n![Control wheel steering]({% static '/blog/assets/img/autopilot/Figure_4.jpg' %})\n\n### COMPUTER SYSTEM DETAILS\n\nThe hardware of an autopilot varies with implementation but is generally designed according to the need and reliability of the aircraft. Software and hardware in an autopilot are put to extensive tests. Some autopilots also use design diversity. In this safety feature, each computer will run software created by different engineering teams(to avoid similar mistakes), often being programmed in different programming languages.\n**Stability Augmentation System (SAS)** is another type of automatic flight control system; in case the aircraft does not possess the stability characteristics as per the requirements of flying and handling qualities then stability augmentation system is used to artificially augment the stability of the aircraft. SAS can automatically stabilize the aircraft in one or more axes.\n\n**Disadvantages of the Autopilot:**\n1) Pilots should maintain manual flying skills. The competent pilots should always be prepared to fly manually in case of an autopilot failure.\n2) Turbulence. The pilot\u2019s operating handbook (POH) prohibits the use of altitude hold function during moderate or severe turbulence. Some autopilots disengage when certain limits are encountered due to turbulence.\n3) Minimum altitude. Autopilots should only be used above specific altitude above the ground level. Pilots should refer to the flight manual such that the aircraft does not operate outside the specified limits.\n4) Possible malfunction. Whenever the pilot observes an unusual behavior from the autopilot, he or she should disengage the autopilot immediately.\n\nAutopilot has evolved over the years but it cannot replace the pilots. Therefore at least one pilot is required in the cockpit.",
        "author_name": "Sharayu Brahmankar",
        "slug": "autopilot"
    },
    {
        "layout": "post",
        "title": "DS1: A demonstration spacecraft and its communication aspects",
        "author_github": "ashrikant39",
        "date": "2020-01-07T07:00:00",
        "image": "/assets/img/",
        "description": "Deep Space 1: How and what does it do in Space?",
        "tags": [
            "RF transmission",
            "Antennas"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "ashrikant39",
        "text": "[{% load static %}]()\n# Deep Space 1: How and what does it do on Space??\nDeep Space 1 or DS1 is a satellite launched by National Aeronautics and Space Administration (NASA) in 1998 to serve its purpose as a technology demonstration spacecraft. It was launched to test various upcoming technologies in space, its conditions, outcomes, threats or limitations of the proposed technology etc. in real time in space rather than earthly simulations.\nWe are interested to learn how DS1 or any satellite in outer space carry out efficient communications to Space stations in Earth. For this, we must first be aware of Deep Space Network (DSN).\n## Deep Space Network\nDeep Space Network is a network of large antennas positioned in Earth to capture various signals from the spacecrafts, used to provide scientific data, instructions to spacecraft, health of the spacecraft etc. NASA has positioned three antennas on three locations on Earth, namely California, Spain and Australia, each oriented at an angle of 120 degrees with each other and are spread onto three equidistant points on Earth as above so that at any \nEach location has ultra-sensitive receiver systems with huge antennas namely\n* High Efficiency Antenna (HEF) for high gain (34m in diameter)\n* Two or more Beam Waveguide Antennas (BWG) (34m in dia)\n\nand several other independent antennas for tracking the spacecraft.\n\n![alt text]({% static '/blog/assets/img/ds1-a-demonstration-spacecraft-and-its-communication-aspects/images1.jpg' %})\n![alt text]({% static '/blog/assets/img/ds1-a-demonstration-spacecraft-and-its-communication-aspects/images.jpg' %})\n![alt text]({% static '/blog/assets/img/ds1-a-demonstration-spacecraft-and-its-communication-aspects/Dsn2.gif' %})\n\n## Uplink and Downlink\nTracking of the satellite is mainly achieved by uplink and downlink communications by satellites and space stations on Earth. Uplink refers to communication from a space station to the satellite, whereas downlink refers to communication from satellite to station. When both uplink and downlink occur, it is referred to as Two Way communication. Communication is mainly done using RF signals (mainly microwaves, i.e. in GHz range). \n\n![alt text]({% static '/blog/assets/img/ds1-a-demonstration-spacecraft-and-its-communication-aspects/images2.jpg' %})\n## Lag\nLag is another term that decides the efficiency of communication in DSN. It is the time gap between sending of signal by source and receiving it by the receiver. Lag should be taken care, as when a satellite sends signals for help, instant solutions to the problems must be ready, otherwise making it too late for anything more.\nSo back to our satellite DS1. \n \nDS1 also uses DSN as the network to downlink data. DS1 mainly transmits two kinds of data :\n* \tHealth of spacecraft: It refers to the status of the spacecraft with its surroundings in terms of temperature, pressure, voltages, images etc. These measurements (which may go around 1000 in number) are collected and transmitted as radio waves. These are radio waves are simple enough to be received by any receiver on Earth. They contain mainly information like All OK, Track urgent, Red Alert etc. \n* Scientific data: The critical tests and measurements, images of various locations of interests, objects of scientific importance etc.  come under this category. These messages are specially encrypted and can be received only by high gain DSN receivers. \n\n![alt text]({% static '/blog/assets/img/ds1-a-demonstration-spacecraft-and-its-communication-aspects/health-signals.gif' %})\n\nApart from the data, the characteristics of received waves also reveal a lot! The speed of the spacecraft, its distance from Earth and other useful parameters can be calculated by Doppler Effect using the frequency shifts in received waves.\nThe primary goal of DS1 in space is to test and execute new technologies in space.  Some of which are:\n* Miniature Integrated Camera Spectrometer (MICAS)\n* Autonomous Navigation (AutoNav)\n\n## MICAS\nMICAS is one of the advanced technologies used in DS1 till date. It is basically an array of cameras placed on the surface of the craft used to take images in the presence of light in space. From its design, it can be clear that there are no moving parts in DS1 for orienting MICAS. So, for a change in angle, the entire spacecraft has to change its position and attitude (orientation), so as to perfectly capture the images.\n\n![alt text]({% static '/blog/assets/img/ds1-a-demonstration-spacecraft-and-its-communication-aspects/micas3s_tn.jpg' %})\n\nOne of the interesting features about MICAS is that it is not only capable of capturing light images but also heat waves, UV and IR radiations and form images out of those waves. These data prove to be quite useful to determine temperature distributions in various desired locations in space through simple images.\nMICAS achieves this using an array of light sensitive charged cells inside a device called Charge Coupled Device (CCD).  These cells absorb the light falling on them, and then releases a specific amount of charge proportional to the intensity of light falling. CCD stores this charge as the value of each pixel in the image. Then each value is binary coded to generate bits, which are transmitted as radio waves to Earth stations or used by DS1 itself. This technique of digital imaging has an advantage over film imaging as film images cannot be transmitted as radio waves.\n![alt texts]({% static '/blog/assets/img/ds1-a-demonstration-spacecraft-and-its-communication-aspects/ccd1.gif' %})\nOne of the drawbacks of MICAS is that exposure to sunlight can damage its working. The UV and high intensity Ultra UV radiations can change the crystal structure of CCD. So, it is generally oriented away from Sun.\nOne of the coolest application of MICAS is that DS1 uses MICAS to locate and orient itself. This is done using Autonomous Navigation or AutoNav.\n## Autonomous Navigation\nAutonomous Navigation or AutoNav is an experimental system being tested on DS1. It is used by DS1 to navigate itself and intelligently decide whether it is deviating from its programmed path.\nOn its path MICAS takes a number of images and stores for reference. While on path AutoNav decides, from the stars, asteroid positions and the reference images, whether the path taken is the desired one. When not, it uses its thrusters and rockets to re-orient itself. \n\n![alt text]({% static '/blog/assets/img/ds1-a-demonstration-spacecraft-and-its-communication-aspects/autonav2.gif' %})\n\nSo thats it guys. This concludes my work  on how is DS1 achieving its communication with Earth and Navigation. If you guys want some more insight into any of these\n* How basically an RF signal is transmitted over large distance [here](http://www.qrg.northwestern.edu/projects/vss/docs/Communications/1-how-is-data-put-on-radio-waves.html)\n* Different kinds of antennas [here](http://www.qrg.northwestern.edu/projects/vss/docs/Communications/2-what-kinds-of-antennas.html)\n* What could go wrong in communications [here](http://www.qrg.northwestern.edu/projects/vss/docs/Communications/1-what-could-go-wrong-communications.html)",
        "author_name": "A Shrikant",
        "slug": "ds1-a-demonstration-spacecraft-and-its-communication-aspects"
    },
    {
        "layout": "post",
        "title": "Piezoelectricity and Piezoelectric Sensors",
        "author_github": "kaushikalwala",
        "date": "2020-01-14T23:32:44",
        "image": "/assets/img/",
        "description": "Piezoelectricity and Piezoelectric Sensors",
        "tags": [
            "IEEE NITK",
            "Diode",
            "Piezoelectricity",
            "Sensors"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "kaushikalwala",
        "text": "[{% load static %}]()\n# PIEZOELECTRICITY AND PIEZOELECTRIC SENSORS\n## Piezoelectricity\nThe accumulation of electric charge in solid materials in response to applied mechanical stress is called the piezoelectric effect or piezoelectricity, and those types of solid materials are called piezoelectric materials. Quartz, Berlinite (AlPO4), Sucrose (table sugar), Topaz, Tourmaline-group minerals, Lead titanate, etc. are some examples of naturally occurring piezoelectric materials. Some of the biological materials exhibiting piezoelectricity include silk, wood, enamel, tendon, etc. Barium titanate, Lead zirconate titanate, Potassium niobate, Sodium tungstate, Zinc oxide, etc., fall under the category of Synthetic ceramics that exhibit the piezoelectric effect. Piezoelectric effect is also exhibited by some polymers like polyvinylidene fluoride (PVDF), polyvinylidene chloride (PDVC), etc. \nThe piezoelectric effect is a result of the linear electromechanical interaction between the electrical and mechanical states in crystalline materials. The internal generation of mechanical strain resulting from an applied electric field is called the reverse piezoelectric effect. The materials which exhibit the piezoelectric effect also exhibit the reverse piezoelectric effect. \nWe can say that a crystal can exhibit the piezoelectric effect if:\n1. The crystal lattice should contain polar bonds. For example, Quartz exhibits the piezoelectric effect because of the oxygen-sulfur bond which is a polar bond.\n2. The crystal structure should not have inversion symmetry/point symmetry. If point symmetry exists in the crystal then the center of charge will not be displaced even if there is some mechanical stress on the crystal, as a result, there would be no piezoelectric effect in this case.\n### Uses\nPiezoelectricity has a number of useful applications. Piezoelectric materials are used in the detection and production of sound, high voltage power sources, sensors, actuators, piezoelectric motors, electronic frequency generation, etc. The piezoelectric material quartz is used as a time reference in Quartz watches. These materials also find some daily life applications such as acting as a source of ignition in lighters, for amplification pickups in some guitars.\n## Modes of operation of piezoelectric materials:\nThe piezoelectric material has three main modes of operation which depend on the way it has been cut.\n### 1.Transverse Effect:\nA force applied along a neutral axis(let\u2019s say y-axis), displaces the charge along an axis perpendicular to the neutral axis(x-axis) and the amount of charge displaced is proportional to the applied force and the dimensions of the crystal along the respective axes.\n**Qx = dxy * Fy * (b/a)**\nWhere \u2018Qx\u2019 is the charge displaced along the x-axis, \u2018a\u2019 is the dimension of the crystal parallel to the neutral axis(y-axis), \u2018b\u2019 is the dimension of the crystal along the x-axis and \u2018dxy\u2019 is the corresponding piezoelectric coefficient in that plane(XY-plane).\n### 2. Longitudinal Effect:\nThe amount of electric charge that gets displaced in the direction of the applied force(let\u2019s say x-axis) is only proportional to the applied force and does not depend on any of the dimensions of the crystal.  \n**Qx = dxx * Fx**\nWhere \u2018Qx\u2019 is the charge displaced along the x-axis, \u2018Fx\u2019 is the force applied along the x-axis and \u2018dxx\u2019 is the corresponding piezoelectric coefficient along the x-axis\n### 3. Shear Effect:\nThe amount of charge produced in the plane(YZ-plane) normal to the direction of the applied force(along the x-axis) is only proportional to the applied force and does not depend on any of the dimensions of the crystal and is twice the amount of the charge produced in the direction of the force.\n**Qyz = 2 * dxx * Fx**\nWhere \u2018Qyz\u2018 is the amount of charge produced in the YZ-plane, \u2018Fx\u2019 is the force applied along the x-axis and \u2018dxx\u2019 is the corresponding piezoelectric coefficient.\n## Piezoelectric Sensors\nA piezoelectric sensor utilizes the piezoelectric effect to measure the change in physical quantities like acceleration, pressure, temperature, strain, force, etc by converting them into electric charge.\n### Equivalent circuit\nThe voltage produced at the source \u2018V\u2019 is directly proportional to the applied force, pressure, strain, or the magnitude of acceleration (in accelerometers). The Equivalent circuit of the piezoelectric sensor is as shown.\n![Equivalent Circuit]({% static '/blog/assets/img/piezoelectricity-and-piezoelectric-sensors/equivalent_circuit.png' %})\n<br>\nV - Voltage produced by piezoelectric material\nCe - represents the mechanical elasticity of the material\nLm - due to seismic mass and inertia of sensor\nCo - static capacitance of the sensor i.e. capacitance due to base plates\nRi - insulation leakage reactance\nWe attach a load(which depends on our application) parallel to the leakage reactance.\n### Graph\nThe graph of the sensitivity i.e. ratio of the electric output and the applied force/acceleration denoted by s(w) and frequency(frequency of the crystal due to applied force, stress or acceleration ) will be as shown below.\n\n![Graph]({% static '/blog/assets/img/piezoelectricity-and-piezoelectric-sensors/graph.jpg' %})\n\nTo make use of piezoelectric material as a sensor, the flat region of the frequency plot(the part of the graph which is in between wmin and wmax) is used. To accommodate low frequencies of interest the load resistance and leakage reactance should be quite large.\n### Working\nSo a typical piezoelectric sensor has a base plate, seismic mass, and the piezoelectric crystal. When the force/ stress is applied or the sensor experiences motion (this happens in accelerometers), the seismic mass loads up the elements. The voltage corresponding to the physical parameter is generated at the source as shown in the circuit above. This can be seen as if the voltage generated drives the equivalent circuit and we get an output signal (which is generally voltage across the load/current through the load), which is processed to determine the physical parameter.\n### Advantages and Disadvantages of piezoelectric sensors\nDue to their inherent advantages, piezoelectric sensors are used in various fields. They have a high modulus of elasticity and high natural frequency. They are insensitive to electromagnetic fields and radiation, enabling measurements even under harsh conditions. \nOne major disadvantage of piezoelectric sensors is that they cannot be used for truly static measurements because a static force causes a fixed amount of charge, so there would not be any output signal.\n### Applications of piezoelectric sensors\nThe piezoelectric sensors are used to measure physical parameters, for example, they are used in accelerometers, etc. They are used as touch sensors and tilt sensors in mobile phones and other consumer electronics. Piezoelectric sensors are used to measure inside internal combustion engines to monitor combustion inside them. Piezoelectric sensors also find applications in aerospace, medical, nuclear instrumentation.",
        "author_name": "Kaushik Alwala",
        "slug": "piezoelectricity-and-piezoelectric-sensors"
    },
    {
        "layout": "post",
        "title": "PSLV to Mars",
        "author_github": "ektaramnani",
        "date": "2020-01-14T23:32:44",
        "image": "/assets/img/",
        "description": "PSLV to Mars",
        "tags": [
            "IEEE NITK",
            "Piston",
            "PSLV",
            "Mars"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "ektaramnani",
        "text": "[{% load static %}]()\n#                                           PSLV to Mars\n###                                        (Mars Orbiter Mission)\nMarking the series of events in the world history, November 5,2013 is recognized to be a day of great significance all over the world. On this day, ISRO launched MOM(Mars Orbiter Mission) also called Mangalyaan,a space probe orbiting Mars since September 24,2014.This made India the first Asian nation to reach Martian Orbit and the first nation in the world to do so in its first attempt.The launch took place from Satish Dhawan Space centre in Sriharikota.Indian scientists in ISRO proposed to launch the Mars satellite using PSLV(Polar Satellite Launch Vehicle) rocket. First, they thought to launch MOM with GSLV but it failed twice in 2010 and was continuing to experience issues with cryogenic engine.It was planned to launch MOM when Mars is closest to the Earth which happens once in about 26 months.Only few months were left to launch and in such a short time they couldn't make a new rocket for MOM launch.They decided to launch MOM using PSLV-XL C-25 rocket knowing that it has low fuel capacity.\n\n![Mars Orbiter Mission Spacecraft]({% static '/blog/assets/img/pslv-to-mars/Figure1.jpg' %}){:height=\"350px\"}\n\n### Challenges involved :\n- The PSLV has low load and fuel capacity which was insufficient to reach Mars. So they had to reduce the fuel and overall weight. \n- To overcome earth's gravitational force to move satellite away from earth's orbit. \n- To cover a large distance with less fuel. \n### Objectives:\n- To develop technologies required for design,Mars orbit insertion and on orbit phase around Mars. \n- Deep space communication and navigation. \n- Incorporate autonomous features to handle contingency situations. \nThe total load was of 1350 kg including 850 kg of fuel. The satellite weighed 15 kg with 5 instruments to study Martian surface, morphology, mineralogy and Martian atmosphere. One of the instrument used was Methane detector since specific research of methane could tell the possibility and past existence life on Mars. \n### Trajectory \nThere were three phases involved:\n##### 1)Geo centric phase\nThe basic idea was based on Hohmann Transfer Orbit, a minimum energy transfer orbit. Since PSLV couldn't take the satellite all the way but could leave satellite in earth's orbit easily. When satellite reached the perigee, they fired the satellite engine with a little fuel then switched off the engine.The firing force increased the speed of the satellite and propelled it further away from the Earth into a higher orbit.When it came back at perigee, they kept doing the same and at last when the satellite was at perigee again, the speed of the satellite was so much that a final strong firing force threw it out of the Earth's gravitational sphere.\n##### 2)Helio centric phase\nThe satellite left the Earth's orbit tangentially and met Mars tangentially to its orbit. \n##### 3)Martian phase\nThe satellite arrived at the Mars sphere of influence in a hyperbolic trajectory and finally captured into planned orbit around Mars on September 24,2014.\n\n![Mars Orbiter Mission Profile]({% static '/blog/assets/img/pslv-to-mars/Figure2.jpg' %}){:height=\"350px\"}",
        "author_name": "Ekta Ramnani",
        "slug": "pslv-to-mars"
    },
    {
        "layout": "post",
        "title": "Web Vulnerabilities",
        "author_github": "vineeth14",
        "date": "2020-01-14T23:32:44",
        "image": "/assets/img/",
        "description": "Web Vulnerabilities",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "web",
            "vulnerabilities"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "vineeth14",
        "text": "[{% load static %}]()\n# Web Vulnerabilities\n\nA vulnerability is a hole or a weakness in the application, which can be a design flaw or an implementation bug, that allows an attacker to cause harm to the stakeholders of an application.\nWebsites across the world are programmed with different programming languages.\nAlthough each specific language have specific vulnerabilities ,issues exist fundamental to the internet that can show up disregarding the choosen language or framework used.\n\n  Some Web Vulnerabilities\n\n-   SQL Injection\n    \n-   Cross Site Request Forgery\n    \n-   Cross Site Scripting\n    \n-   Server Side Request Forgery\n    \n\n  \n  \n\n## SQL INJECTION\n\nSQL Injection is a vulnerability where an application takes input from a user and doesn't vaildate that the user's input doesn't contain additional SQL. To make an SQL Injection attack, an attacker must first find vulnerable user inputs within the web page or web application. A web page or web application that has an SQL Injection vulnerability uses such user input directly in an SQL query.\n\nSQL is a query language that was designed to manage data stored in relational databases. You can use it to access, modify, and delete data. Many web applications and websites store all the data in SQL databases. In some cases, you can also use SQL commands to run operating system commands. Therefore, a successful SQL Injection attack can have very serious consequences.\n\nA simple Example:\n\n <?php\n$username = $_GET['username']; // kchung\n $result = mysql_query(\"SELECT * FROM users WHERE username='$username'\");\n?>\n\nwhat if we input \u2018OR 1=1?\n\nHere as 1=1,this equates to true to sql .This will then return every row in the table becuase each row that exists must be true.\n\nUsing this technique of adding SQL statements to an existing query we can force databases to return data that it was not meant to return.\n\n  \n  ## CROSS SITE REQUEST FORGERY\n\nCSRF is an attack that tricks the victim into submitting a malicious request. It inherits the identity and privileges of the victim to perform an undesired function on the victim's behalf. For most sites, browser requests automatically include any credentials associated with the site, such as the user's session cookie, IP address, Windows domain credentials, and so forth. Therefore, if the user is currently authenticated to the site, the site will have no way to distinguish between the forged request sent by the victim and a legitimate request sent by the victim.\n\n  \n  ## CROSS SITE SCRIPTING\n\nCross Site Scripting or XSS is a vulnerability where a user of an application can send JavaScript that is executed by the browser of another user of the same application.This is a vulnerability because JavaScript has a high degree of control over a user's web browser.That is Javascript has the ability to send more HTTP requests,modify the page,access the cookies.\n\nXSS is typically broken down into two categories:\n\n-   Reflected XSS\n    \n-   Stored XSS\n    \n\n **REFLECTED XSS**\n\nReflected XSS is done through URL manipulation.One of the deficiencies of reflected XSS however is that it requires the victim to access the vulnerable page from an attacker controlled resource. Notice that if the data parameter, wasn't provided the exploit wouldn't work.\n\n**STORED XSS**\n\n  Stored cross-site scripting arises when an application receives data from an untrusted source and includes that data within its later HTTP responses in an unsafe way.If an attacker can control a script that is executed in the victim's browser, then they can typically fully compromise that user.Difference between reflected and stored XSS is that a stored XSS vulnerability enables attacks that are self-contained within the application itself.The self-contained nature of stored cross-site scripting exploits is particularly relevant in situations where an XSS vulnerability only affects users who are currently logged in to the application.The web site itself is serving up the XSS payload to other users. This makes it very difficult to detect from the browser's perspective and no browser is capable of generically preventing stored XSS from exploiting a user.\n\n  \n  \n\n## SERVER SIDE REQUEST FORGERY\n\nServer Side Request Forgery is when an attacker is able to cause a web application to send a request that the attacker determines.The attacker can abuse functionality on the server to read or update internal resources. The attacker can supply or a modify a URL which the code running on the server will read or submit data to, and by carefully selecting the URLs.",
        "author_name": "Vineeth Rajesh",
        "slug": "web-vulnerabilities"
    },
    {
        "layout": "post",
        "title": "Solving a traffic problem using graph decomposition",
        "author_github": "VarunRaghavendra",
        "date": "2020-02-21T12:21:30",
        "image": "/assets/img/",
        "description": "To introduce the reader to some interesting concepts of graph theory",
        "tags": [
            "Algorithms",
            "Graph Theory"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "appu234",
        "text": "[{% load static %}]()\n# Solving a traffic problem using graph decomposition\n\n  \n\nThe **Capital city** is known for its prosperous life and big opportunities. Its inhabitants are called *\u201ccapitolates\u201d*. Every capitolate in Capital city wants to live a happy and peaceful life. Now Capital city has three divisions - Residential town, Fun town and **market city**. Residential town is where capitolates live. Fun town is where they meet friends and make merry. Market city is where they work to make a living. But every passing year, capitolates are becoming more and more frustrated and unhappy. The reason is that people from neighbouring lands want to become capitolates and are moving to Capital city. This is making the roads busier and traffic slower. In particular, every road intersection in market city has become a deadly traffic bottleneck.\n\n  \n\nThe ten peace-keepers of Capital city propose an unconventional solution to this traffic problem - \u201cTransform market city and make all the roads one-way\u201d. But at this juncture a few questions arise:\n\n  \n\n  \n\n  \n\n1. *In what direction should each one-way road go?*\n\n2. *Is every traffic intersection in market city still reachable from every other traffic intersection in market city using these one-way roads? If not, which intersections are not reachable from one another?*\n\n  \n\nNow, the nine hard working peace-keepers explore different possible one-way transformations of market city. They call the transformation \"good\" if every traffic intersection is reachable from every other traffic intersection. They assign the tenth peace-keeper, the lazy one, the job of finding whether a given transformation is \u201cgood\u201d. If you are the tenth peace-keeper, you are in luck, for we will show you how to accomplish this task, in a fraction of a second, using our vast computing power.\n\n  \n\n## **Formal ideas and definitions**\n\n  \n\n<span style=\"text-decoration:underline;\"> </span>Before we present a solution to our main problem, we first define some mathematical terms and explore a few key ideas. We will then redefine the main problem statement using those mathematical terms. We will explore the following ideas.\n\n  \n\n  \n\n  \n\n* The idea of a graph\n\n* Exploring a graph\n\n* DFS (Depth First Search)\n\n* Connectedness and connected components of a graph\n\n* Directed graphs\n\n* Strong connectedness\n\n  \n\n## **The idea of a graph**\n\n  \n\n![alt text]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image1.jpg' %})\n\n  \n\nIn mathematics, and graph theory, a graph is a set of objects in which some pairs of the objects are in some sense \u201crelated\u201d. The objects are called vertices(also called nodes) and each of the related pairs of vertices is called an edge. In the picture, A, B and C are the vertices and the pairs (A, B) and (B, C) are edges.\n\n  \n\nMany important phenomena can be represented using graphs. Examples are:\n\n* Webpages connected by links\n\n* Intersections connected by roads\n\n* People connected by friendships in social networking.\n\n  \n\n>Formally, a graph G is a collection v of vertices, and a collection E of edges, each of which connects a pair of vertices.\n\n  \n\n## **Exploring a graph**\n\n  \n\nWe have the following definition for a _path_ in G:\n\n>A path in a graph G is a sequence of vertices v<sub>0</sub>, v<sub>1</sub>, v<sub>2</sub>, \u2026, v<sub>n</sub> so that for all i, (v<sub>i</sub>, v<sub>i+1</sub>) is an edge of G.\n\n  \n\nWe now have a tool to define _Reachability_:\n\n>Two vertices u and v (u not equal to v) are reachable if there exists a path from u to v.\n\n  \n\nWhen we say we explore a graph G, it means that we start at an _\u201cunvisited\u201d_ vertex v of the graph and then start visiting vertices that are reachable from v using the edges in G. Once we finish visiting all vertices reachable from v, we mark the visited vertices as _\u201cvisited\u201d_. Initially, all vertices are *\"unvisited\"*.\n\n  \n\n  \n\n## **Depth First Search (DFS)**\n\n  \n\nThe next thing we will do is give a way to reach all vertices of a graph. The idea is intuitive and easy to understand. What DFS does is, it starts at an unvisited vertex of G and explores it, and continues to do this for every other unvisited vertex of G. So when DFS finishes, we end up visiting every vertex of G.\n\n  \n\nFor example, consider the following graph on eight vertices.\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image2.jpg' %})\n\n  \n\nWhen we run DFS on the graph, we start at some vertex of the graph, say, A. We then explore A:\n\n  \n\n**A is visited**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image3.jpg' %})\n\n**B is visited**\n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image4.jpg' %})\n\n**C is visited**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image5.jpg' %})\n\n**D is visited**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image6.jpg' %})\n\n  \n\nAfter exploring A as shown in the pictures, we check for other unvisited vertices. Since B, C, and D are already visited, our next unvisited vertex is E. We explore E, and then, F. After exploring F, we will have visited all vertices. This completes our DFS exploration.\n\n  \n\n## **Connectedness**\n\n  \n\n  \n\n  \n\nConnectedness in a graph G is a property which helps us understand which vertices in G are reachable from which others. The following theorem holds for every graph G:\n\n  \n\n>The vertices of G can be partitioned into **Connected Components** so that a vertex v is reachable from a vertex w if and only if they are in the same connected component.\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image7.jpg' %})\n\n  \n\nIn the graph above, the vertices {A, B, C, D} form the first connected component. The vertex {E} forms the second connected component. Finally the vertices {F, G, H} forms the third connected component.\n\n  \n\nYou might realize that this notion of **connectivity** gets us closer to the problem that we want to solve. But we are not quite there yet. We also have to tackle the issue of one-way roads. Before we move on, we give a modification of the DFS algorithm to give ourselves more capability. We introduce a new feature - pre and post orderings. We store two variable called **previsit** and **postvisit** for every vertex. Every vertex is given the label (previsit/postvisit).\n\n  \n\nWe store an extra variable called **clock**. Each time we enter or leave a vertex the value of the clock is incremented by 1. We assign pre and post order values to each vertex as follows:\n\n  \n\n1. During exploration, each time a vertex v of the graph is visited, we assign the current value of clock to the previsit value of v.\n\n2. When exploration of v finishes, we assign the new value of the clock to the postvisit value of v.\n\n  \n\nExample: Consider a graph on six vertices.\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image8.jpg' %})\n\n  \n\nThe following illustration shows how previsit and postvisit values are assigned to the vertices when we run DFS on the graph.\n\n  \n\n**_Clock = 1:_**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image9.jpg' %})\n\n  \n\n**_Clock = 2:_**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image10.jpg' %})\n\n  \n\n**_Clock = 3:_**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image11.jpg' %})\n\n  \n\n**_Clock = 4:_**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image12.jpg' %})\n\n  \n\n**_Clock = 5:_**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image13.jpg' %})\n\n  \n\n**_Clock = 6:_**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image14.jpg' %})\n\n  \n\n**_Clock = 7:_**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image15.jpg' %})\n\n  \n\n**_Clock = 8:_**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image16.jpg' %})\n\n  \n\n**_Clock = 9:_**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/image18final.jpg' %})\n\n  \n\nThe other component of the graph is given pre and post orderings in a similar fashion. After all components are assigned pre/postvisit numbers, the graph looks like:\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/graph%20(20' %}).png)\n\n  \n\n## **Directed Graphs(DiGraphs)**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image19.jpg' %})\n\n  \n\n>A directed graph is a graph where each edge has a start vertex and an end vertex.\n\n  \n\nExamples are:\n\n  \n\n* **Streets with one-way roads (an abstraction of our traffic problem)**\n\n* Links between webpages.\n\n* Followers on social networks like instagram\n\n* Dependencies between tasks (sometimes some tasks need to be performed before others).\n\n  \n\n## **Strong Connectedness**\n\n  \n\nStrong connectedness in a directed graph is analogous to connectedness in a non-directed graph.\n\n  \n\nFor any two vertices in a digraph v and w, there are three different possibilities:\n\n  \n\n1. w is reachable from v and v is reachable from w.\n\n2. Exactly one of v or w is reachable from the other.\n\n3. w is not reachable from v and v is not reachable from ww.\n\n  \n\nThe first case sets up the basis for strong connectedness.\n\n>A directed graph can be partitioned into **strongly connected components (SCCs)** where two vertices are connected if and only if they are in the same component.\n\n  \n\nConsider the graph P below:\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image20.jpg' %})\n\n  \n\nThe sets { A, B, C, E }, { H, I }, {D}, {F}$ and {G} form the five strongly components of the above graph P.\n\n  \n\nWe define the final tool that we will use to solve our problem:\n\n  \n\n>The metagraph M of a directed graph G is the graph with its vertex set as the set of SCCs of G and edges showing how the SCCs connect to each other.\n\n  \n\nIt should be noted that the metagraph of G will be a *Directed Acyclic Graph* (A directed graph with no cycles) or a DAG. This property should hold, since if there were a cycle in M, then we will have at least two SCCs in G, which are a part of the cycle, reachable from each other, which contradicts the fact that they belong to two different SCCs.\n\n  \n\nThe graph below is the metagraph of the graph P we have described in the above picture.\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image21.jpg' %})\n\n  \n\n**In the above metagraph, W is the SCC consisting of the vertices A, B, C and E, Y is the SCC consisting of the vertex F, X is the SCC consisting of the vertices H and I, Z is the SCC consisting of the vertex G and V is the SCC consisting of the vertex D.**\n\n  \n\n**We now have the tools to simplify our main traffic problem. Market city can be seen as a _DiGraph_ with the one-way roads as edges. We just have to compute the strongly connected components of market city.**\n\n  \n\n**The algorithm we use to get our work done here is called *Kosaraju\u2019s* algorithm.**\n\n  \n\n## **An abstraction of the problem**\n\n  \n\n> Given a DiGraph G, compute the strongly connected components of G.\n\n  \n\nThis is a simpler version of our original problem.\n\n  \n\nNaively, what we can do is, for each vertex v in G, we can explore v to find the vertices that are reachable from v. And then, we can find the vertices u reachable from v that can also reach v. This, by definition, will give us the strongly connected components of G. But the problem with this procedure is that it takes a lot of steps to complete. We want to design a much faster algorithm. To achieve that, we make a few elegant observations.\n\n  \n\n1. Sink SCCs: _An important theorem in graph theory tells us that for a DAG, there always exists a sink vertex (a vertex with no outgoing edges) and a source vertex (a vertex with no incoming edges) in the DAG_ (We leave the proof as an exercise). For our problem, the DAG in question is the metagraph M. Hence, there is a **\u201csink strongly connected component(sink SCC)\u201d** in G.\n\n  \n\nThe idea is that if a vertex v is in a sink SCC, then if we run explore on v, we will find all the vertices in the strongly connected component that v is in.\n\n  \n\n  \n\nBut we need a way to find a sink SCC of G.\n\n  \n\n2. _Another important theorem in graph theory tells us that in a DAG, the vertex with the largest postvisit number lies in a source vertex of the DAG_ (Proof can be found [here](https://www.cs.cmu.edu/~avrim/451f13/lectures/lect0919.pdf)). Again, the DAG in question is the metagraph M. Hence, the vertex with the largest postvisit number in G lies in a source SCC of G.\n\n3. Our problem was that we wanted a sink component. But we know how to find a source component. So we reverse the direction of all the edges in G to find the transpose of the graph G<sup>R</sup>. The following properties hold for G<sup>R</sup>:\n\n* G<sup>R</sup> and G have the same SCCs.\n\n* Source components of G<sup>R</sup> are sink components of G<sup>R</sup>.\n\n* The vertex with the largest postorder in G<sup>R</sup> is in a sink SCC of G.\n\n  \n\nWe are in the home stretch now. Our job is much simpler. We just run DFS on G<sup>R</sup> and assign pre/postvisit numbers to every vertex. Then we start removing the components associated with the vertex with the largest postvisit number in G<sup>R</sup>. In the end, we end up finding the SCCs of G. We illustrate the procedure on the sample graph we described before.\n\n  \n\n**G:**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image22.jpg' %})\n\n  \n\n**G<sup>R</sup>:**\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image23.jpg' %})\n\n  \n\nWe first assign pre and post visit numbers to every vertex:\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image24.jpg' %})\n\n  \n\nWe remove the SCC with the vertex with the highest post order number. That is we first remove the vertex F and all the edges going from F:\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image25.jpg' %})\n\n  \n\nFrom the resulting graph above, we remove the component having the vertex with the next highest post visit number, that is I:\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image26.jpg' %})\n\n  \n\nWe continue this process until we have found all the SCCs:\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image27.jpg' %})\n\n  \n\nAnd finally:\n\n  \n\n![alt]({% static '/blog/assets/img/Solving-a-traffic-problem-using-graph-decomposition/Image28.jpg' %})\n\n  \n\nThe algorithm we explained now is called [Kosaraju\u2019s Algorithm](https://en.wikipedia.org/wiki/Kosaraju%27s_algorithm). This completes the solution to our problem. The transformation is good if there is only one strongly connected component. We have successfully found out the disconnected intersections in market city!\n\n  \n\nA mathematical analysis of the above algorithm shows that the total time it takes to run the algorithm is proportional to the sum of the total number of vertices and the total number of edges in the graph. Today\u2019s computers can accomplish this within the blink of an eye!",
        "author_name": "Varun Raghavendra",
        "slug": "solving-a-traffic-problem-using-graph-decomposition"
    },
    {
        "layout": "post",
        "title": "SLAM Coverage Algorithms",
        "author_github": "jaimandal10",
        "date": "2020-03-06T23:32:44",
        "image": "/assets/img/",
        "description": "Get an idea as to how robots maneuver in rough terrains",
        "tags": [
            "IEEE NITK",
            "Piston"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "jaimandal10",
        "text": "[{% load static %}]()\n[![Image Link to Video](https://i.ytimg.com/vi/MX2qcDM11N4/hqdefault.jpg?sqp=-oaymwEZCNACELwBSFXyq4qpAwsIARUAAIhCGAFwAQ==\\u0026rs=AOn4CLADHXP3O1dRP2zZkPvkSwY2-fFvyQ)](https://youtu.be/MX2qcDM11N4)\nWith the increased use of robots in industries, the need for robots to be more efficient is incredibly high. One of the key concerns is the way in which they commute. Gone are the days when robots were restricted to moving in a pre-defined path, and one of the most fundamental technologies for locomotion of mobile robots are coverage algorithms. \nCoverage algorithms require that they allow the bot to cover the maximum area possible without any human intervention. In previously used algorithms, the coverage was achieved with robots crossing the entire length possible in a straight line and then turning and tracing the path in a straight line again. These algorithms worked poorly when the region they had to cover were filled with many obstacles and many of them even need information about the area to be mapped beforehand.  \nThe SLAM (Simultaneous Localization and Mapping) algorithms ensure that the robot maps out the entire area and also keeps track of its own position. Without knowing where it is, the situation is similar to a human in a room with a blind fold on, you can move the person around multiple times and feed them the path by doing so, but without being able to see where they are, they would still struggle. Because of this, SLAM provides a significant advantage over previously used algorithms, making it a widely used algorithm today. Robots such as Boston Dynamics\u2019 Spot and Anybotics\u2019 Anymal use SLAM to achieve motion from one point to another without any human intervention. SLAM allows the robots to avoid obstacles in an efficient manner making them highly suitable for use in industries where the terrain and work environment need not always be well suited for automated vehicles.   \n   \nThe processing involved in a SLAM algorithm is:\n- Landmark Detection and Extraction\n- Data association with other objects\n- State estimation of the robot\n- State update of the robot position\n- Landmark updates as we proceed\nSLAM isn\u2019t only prevalent in robots, but also in self driving cars. Agreed, GPS is a very viable option, but in situations where GPS isn\u2019t highly functional and optimized, and to go to places that haven\u2019t been mapped, SLAM could prove to be a far better option as it requires no data beforehand.\nSo, if you\u2019re interested in mobile robots, self-driving cars or anything related, you might want to educate yourself about the various SLAM algorithms out there, because you\u2019ll need it.",
        "author_name": "Jai Mandal",
        "slug": "slam"
    },
    {
        "layout": "post",
        "title": "An Overview of Tarjan's Algorithm",
        "author_github": "adityasohoni",
        "date": "2020-03-06T23:32:44",
        "image": "/assets/img/",
        "description": "A simple algorithm to find the strongly connected components in a directed graph",
        "tags": [
            "IEEE NITK",
            "CompSoc"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "adityasohoni",
        "text": "[{% load static %}]()\n**Background**  \nStrong components in directed graphs are components in which all vertices are strongly connected to each other.\n2 vertices A and B are strongly connected if there is a directed path from A->B and B->A.\n\n**Need for Computing Strongly Connected Components**  \nComputing strong components for graphs has been a need in computation since many years now. Biologists use the strong components to find out sub-ecosystems in a food chain. During the early advent of the internet, this became an absolute necessity as dependency graphs of various software modules needed to be processed and their strong components extracted, to know which modules could be packed together.  Especially today, with traffic routing problems and due to the presence of one-ways, directed graphs and operations on them have been studied by engineers working in many top tier companies. \n\n**The Breakthrough**  \nMany years since the advent of computation people had been wondering how to find the connected components in a directed graph(efficiently). So the problem that they faced in simple words was : *Given a graph G with V vertices and E directed edges, obtain the strong components of a graph*. At first sight this seems very hard and many seem to think of the brute force approach, i.e. take all pairs of vertices and check for strong connections. And indeed this was what was being done prior to 1972 when the breakthrough occurred and the algorithm was named for its inventor, Robert Tarjan(he's still alive). And a huge surprise was that the solution was linear!\n\n**How the Algorithm Works**  \nThe algorithm assigns what is called 'low-link values' to all nodes(vertices) in a graph. When the algorithm is done all vertices of a strong component will belong to a strong component. Also the assignment of low-link values to a node if done during the DFS to get that optimised O(V+E) linear complexity.  \nThese are the main steps in Tarjan's Algorithm:  \n1. A DFS is run from any node.  \n2. A stack of valid nodes from which to update low-link values from, is maintained during the DFS. This stack basically has all nodes that are present in the current branch of the DFS.  \n3. Nodes are added to the stack as they are explored for the first time.  \n4. Nodes are removed from the stack if any path in the DFS ends in an already visited node say V, in this case elements are popped from the stack till the element that was visited again(V). And all these popped nodes along with V, are put in the same strong component, i.e. , their low link values are made the same as the revisited node.  \n5. If the DFS hits a visited node but which is not in the stack then nothing is done.  \n\n**A Pictorial Representation**  \n![Starting the DFS]({% static '/blog/assets/img/TarjansAlgo/strtDFS.png' %})\nIn the above diagram we start the DFS from the node coloured orange and give it a low-link value of 0.\n  \n![DFS]({% static '/blog/assets/img/TarjansAlgo/strt2.png' %})\nWe then proceed with the DFS, until and assign a low-link value of 1 to the neighbour of the root.\n  \n![DFS]({% static '/blog/assets/img/TarjansAlgo/strt3.png' %})\nWe continue giving unique low-link ids to new nodes till node 2.  \n\n![DFS]({% static '/blog/assets/img/TarjansAlgo/strt4.png' %})\nWe now hit an already visited node(node 0). So we make the low-link value of all these nodes as 0. We then pop this connected component out of the stack. These are a connected component.  \n  \n![Picking a Node]({% static '/blog/assets/img/TarjansAlgo/pickanonode.png' %})\nNext we pick another node(part of the DFS).  \n  \n![Recursion]({% static '/blog/assets/img/TarjansAlgo/dorecursively.png' %})\nWe do these steps recursively till we obtain more strong components of the graph.  \n  \n![The Solution]({% static '/blog/assets/img/TarjansAlgo/final.png' %})\nThe graph finally looks like this.  \n  \n**Analysis**  \nThe time complexity of this algorithm is O(V+E) as all it does is basically run a DFS.\nHowever this asymptotic analysis assumes that looking up a vertex in a stack can be done in constant time. In reality this may not be the case and thus the time complexity would be greater.\nThe space complexity is linear as it makes use of additional space for the stack.\n\n**Pseudocode**   \nThe pseudocode is provided below.  \n\n```\n algorithm tarjan is  \n  input: graph G = (V, E) \n  output: set of strongly connected components (sets of vertices)  \n  index := 0  \n  S := empty stack  \n  for each v in V do  \n    if (v.index is undefined) then  \n      strongconnect(v)  \n    end if  \n  end for  \n  function strongconnect(v)  \n    // Set the depth index for v to the smallest unused index  \n    v.index := index  \n    v.lowlink := index  \n    index := index + 1  \n    S.push(v)  \n    v.onStack := true  \n    // Consider successors of v  \n    for each (v, w) in E do  \n      if (w.index is undefined) then  \n        // Successor w has not yet been visited; recurse on it  \n        strongconnect(w)  \n        v.lowlink  := min(v.lowlink, w.lowlink)  \n      else if (w.onStack) then  \n        // Successor w is in stack S and hence in the current SCC\n        // If w is not on stack, then (v, w) is a cross-edge in the DFS tree and must be ignored\n        // Note: The next line may look odd - but is correct.\n        // It says w.index not w.lowlink; that is deliberate and from the original paper\n        v.lowlink  := min(v.lowlink, w.index)\n      end if\n    end for\n    // If v is a root node, pop the stack and generate an SCC\n    if (v.lowlink = v.index) then\n      start a new strongly connected component\n      repeat\n        w := S.pop()\n        w.onStack := false\n        add w to current strongly connected component\n      while (w != v)\n      output the current strongly connected component\n    end if\n  end function\n```\n\n**Epilogue**  \nThe legendary master of algorithms, Donald Knuth described Tarjan's Algorithm as\n> The data structures that he devised for this problem fit together in an amazingly beautiful way, so that the quantities you need to look at while exploring a directed graph are always magically at your fingertips.",
        "author_name": "Aditya Sohoni",
        "slug": "tarjansalgo"
    },
    {
        "layout": "post",
        "title": "What is machine learning?",
        "author_github": "adk19800",
        "date": "2020-03-06T23:32:44",
        "image": "/assets/img/",
        "description": "A ride though the world of machines and their learning strategies",
        "tags": [
            "IEEE NITK",
            "Machine Learning"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "adk19800",
        "text": "[{% load static %}]()\n# What is Machine Learning?\n\nMachine learning is a school of computer science that focuses on programming machines to improve their performance through data and iteration. The machine can learn without being explicitly programmed. Yes, you read it right. This is called Artificial intelligence, where we feed in data into the machine, and it learns the data and predicts the output for any other input given. Machine learning is broadly classified into  \n - Supervised Learning\n       In supervised learning,  we have an idea of the relationship between the inputs and the outputs from the dataset given. It is further classified into\n                -  Regression:  Output can take continuous values (e.g., If the picture of a vehicle is given and it is asked to predict how old the vehicle is. )\n                - Classification:  Output is discrete(binary) (e.g., If the age of a person is given and the machine is asked to predict if they have a particular disease.)\n - Unsupervised Learning\n       Unsupervised learning is the training of an artificial intelligence (AI) algorithm using information that is neither classified nor labeled. Hence the algorithm acts on the data without any previous guidance.\n    \n - Reinforced Learning\n       Unlike the other types, there is no data set given, and the machine is bound to learn from experience (e.g., In a chess game the machine rains itself with all possible moves)\nNow we shall move ahead and understand the algorithms used for learning \n\n## The Learning Algorithms\n\nBefore proceeding with the learning algorithms, we can first have a look at the variables we shall be using.\nx(i) -The input\ny(i)-The output\n(x(i),y1(i))-Training example\nh(\u03b8(x(i)) - Hypothesis function using which values can be predicted, and \u03b8(i) are the weights\n ```\nJ(\u03b80,\u03b81)=(1/2m)*\u2211(i=1 to m) (y(i)\u2212y1(i))^2 =  (1/2m)*\u2211(i=1 to m)(h\u03b8(x(i))\u2212y(i))^ 2\n```\nIn the given equation, the variable J is called the cost function, which calculates the error and aids in calculating the accuracy of the hypothesis, which is measured. The error term is the difference between the hypothesis and the original output y. The error term is squared and divided by two as while we calculate the gradient descent (a method of reducing the error ), it will be easier for calculation.\n\n## Gradient Descent\n\nThe purpose of finding the cost function is to reduce the error in the hypothesis and improve the accuracy. Gradient Descent is an iterative algorithm used to optimize and find the minimum value for the cost function in this case.\nThe gradient descent algorithm is as follows:\n```\n\u03b8j:=\u03b8j\u2212\u03b1(\u2202/\u2202\u03b8(j))J(\u03b80,\u03b81)\n```\nHere the value of \u03b8 the weight is updated so that the slope of the cost function reduces, and it becomes optimized. This concept can be clearly understood with the image shown below.\n\n![Gradient Descent]({% static '/blog/assets/img/WhatisML/download2.png' %})\n\nThis iterative process is continued until J reaches its minimum value. This process can be imagined in 3-D as the two weights \u03b80 and \u03b81 as the x and y axes and the function J as the z-axes, then the lowermost point(like the lowermost pint of a basin) can be found using this algorithm.\n\n## Logistic Regression\n\nLogistic Regression is the appropriate analysis to conduct when there are only two possibilities(binary). Logistic Regression is a predictive analysis. Logistic Regression is used to describe data and to explain the relationship between one binary variable and one or more nominal, ordinal, interval, or ratio-level independent variables.\nTake an example where we need to classify whether an email is a spam or not. If we use linear Regression for this problem, there is a need for setting up a threshold around which the classification can be done. Say if the actual email is spam; if the predicted continuous value 0.4, and the threshold value is 0.5, the data point will be classified as not spam.\nThe hypothesis, instead of being the weighted sum of inputs is the sigmoid function with the input as the weighted sum of inputs.\nThe sigmoid function also called the logistic function gives an \u2018S\u2019 shaped curve that can take any real-valued number and map it into a value between 0 and 1. If the curve goes to positive infinity, y predicted will become 1, and if the curve goes to negative infinity, y predicted will become 0. If the output of the sigmoid function is more than 0.5, we can classify the outcome as 1 or YES, and if it is less than 0.5, we can classify it like 0 or NO. Using the sigmoid function we are squashing the output of the linear function into the range [0,1]. Now the threshold value can be applied and the predction can be done.\n![SigmoidCurve]({% static '/blog/assets/img/WhatisML/Logistic-curve.svg' %})\nThe sigmoid function of x is  ```1/1+(e^(-x)).```\n\n## Regularization\n\n Sometimes when we try to reduce errors a lot, we end up with a function that fits the training data set correctly, but it may not be correct for predicting the output for new data(overfitting). This overfitting happens due to larger weights and more input variables, and by using regularization, it can be avoided. This is a form of Regression that regularizes or shrinks the coefficient estimates towards zero. In other words, this technique discourages learning a more complex or flexible model to avoid the risk of overfitting.In simple terms, if we utilize a curve with higher powers, we can get an equation that fits the training set but might have significant errors during prediction. Hence regularization is used.\nThere is a regularization parameter(\u03bb) multiplied with the sum of the square of the weights. This \u03bb inflates the weights, and hence while using gradient descent, the curve is smoothened, and overfitting is avoided. Too large a value of \u03bb can also cause problems like underfitting, so an optimum value of \u03bb should be taken.\n![Regularization]({% static '/blog/assets/img/WhatisML/overfitting_21.png' %})\n\n## Neural Networks\n\nA neural network is a type of machine learning which models itself after the human brain. This type creates an artificial neural network that, via an algorithm, allows the computer to learn by incorporating new data. So far, we were calculating the hypothesis from various inputs directly. However, in neural networks, there are several layers, and the hypothesis output for the previous layer acts as the input for the next layer. It is analogous to the brain and the nerves. The dendrites take in the input and processes it. Then it is passed to the axon, which is, in turn, connected to the dendrites of another neuron.\n![Neural Network]({% static '/blog/assets/img/WhatisML/download3.png' %})\nThis is the figurative representation of the neural network. Where layer 1 acts as input, and it is processed by layer two whose output is in turn processed by layer three and is finally given out as the final output. The layers 2 and 3 are called hidden layers.\nUsually, you set to buy something, but you end up buying a lot more than planned. This is due to the recommendations on the online websites.\nYes, neural networks do play a role in that. By making use of neural networks and its learnings, the e-commerce giants are creating Artificial Intelligence systems that know you better than yourself. Data is collected from online searches.\nSocial Media platforms instantly identify which of your friends is in the photo  Artificial Intelligence research, discusses the applications of Neural Networks to power their facial recognition software. The image is passed through various levels of the network.\n\n![Image Recognition]({% static '/blog/assets/img/WhatisML/o-FACEBOOK-FACIAL-RECOGNITION-570.jpg' %})\nTo determine which weight is better to modify, a particular process, called \u201cbackpropagation\u201d is done. \n\n## Backpropogation\n\nIn neural networks also, the accuracy can be improved. Backpropagation algorithms are a group of methods used to train artificial neural networks following a gradient-based optimization algorithm appropriately. The critical feature of backpropagation is its iterative, recursive, and efficient method for calculating the weights updates to improve the network until it can perform the task for which it is being trained. In machine learning, backpropagation is commonly used by the gradient optimization algorithm to adjust the weight of neurons by calculating the gradient of the loss function.\nFor the algorithm refer:<https://towardsdatascience.com/everything-you-need-to-know-about-neural-networks-and-backpropagation-machine-learning-made-easy-e5285bc2be3a>\n\n## The Types of Neural Networks\n\n- ### Feed Forward Neural Network:\n    A feedforward neural network is an artificial neural network in which connections between the node are not cyclic. The feedforward neural network was the first and most straightforward type of artificial neural network. In the feedforward network, the information moves in only one direction, forward, from the input layer, through the hidden layers, and to the output nodes.\n- ### Multilayer Perceptron:\n    A multilayer perceptron has one more hidden layer between the input and output layers. It is used to separate data into classes that cannot be separated by linear methods. This is a type of artificial neural network that has connections from every single node in a layer to each node in the following layer.\n- ### Convolution Neural Network(CNN):\n    CNN, like all neural networks, are made up of neurons with weights and biases. Each neuron receives inputs, takes a weighted sum over them, passes it through an activation function, and responds with an output. The whole network has a loss function, and all the tips and tricks that are applicable for neural networks still apply on CNNs.\n- ### Recurrent Neural Network(RNN):\n    A Recurrent Neural Network is a type of ANN(Artificial Neural Network) in which the output from a particular layer is saved and given as feedback to the input. This feedback helps predict the outcome of the layer.\n    The first layer is formed in the same way as it is in the feedforward network. That is, with the product of the sum of the weights and features. However, in subsequent layers, the recurrent neural network process begins.",
        "author_name": "Adithya Kannan",
        "slug": "what-is-machine-learning"
    },
    {
        "layout": "post",
        "title": "Application of Neural Networks in Power Systems",
        "author_github": "this-is-ac",
        "date": "2020-03-13T23:32:44",
        "image": "/assets/img/",
        "description": "This blog explains how neural networks are used in power systems for applications such as load forecasting, security analysis and fault diagnosis.",
        "tags": [
            "IEEE NITK",
            "Power Systems",
            "Neural Networks",
            "Machine Learning"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "this-is-ac",
        "text": "[{% load static %}]()\n# **Application of Neural Networks in Power Systems**\n\nHave you ever heard of Machine Learning or Neural Networks, but you don't know their applications? Scroll down to find out!\n\nFor most beginners, the title itself might be a bit complicated to digest. So let's split it into its 2 major components - **Neural Networks** in **Power Systems.**\n\n## **What are Neural Networks?**\n\nNeural networks are a set of algorithms, modeled after the human brain, primarily designed to recognize patterns. To understand how a neural network works, one must know the related terminologies. [This](https://ieee.nitk.ac.in/blog/What-is-Machine-Learning/) is a blog that contains the basics of Machine Learning. It also includes the types of neural networks and learning patterns.\n\nIf you\u2019re an athirst reader and have already gone through it or already know the basics, jump right in xD.\n\n## **What exactly is a Power System?**\n\nThe power system is a network that mainly consists of generation, distribution, and transmission systems. The output electrical energy from a generator in a power plant is transmitted across lines and then supplied at the appropriate\nvalue suitable for the consumers.\n\n![img1]({% static '/blog/assets/img/Applications-of-Neural-Networks-in-Power-Systems/68e4e8f7b09faacb94cee0668038114b.png' %})\n\nLet\u2019s run through the 3 main subsystems\n\n## 1.  **Generating Substation**\n\n    The main components of this system are the generator and the transformer.\n    The generator converts mechanical energy into electrical energy.\n\n    Sources of mechanical energy are the burning of coal, nuclear fuel, or a\n    hydropower plant. The transformer transfers the power with very high\n    efficiency to ensure minimum losses.\n\n## 2.  **Transmission Substation**\n\n    This substation carries the overhead lines which transfer the generated\n    electrical energy from generation to the distribution substations.\n\n## 3.  **Distribution Substation**\n\n    This substation connects all the consumers in an area to the bulk power\n    sources.\n\n    The transmission lines form the connection between the bulk power stations\n    and generation substations.\n\n## **But why use Neural Networks when there are several other AI techniques?**\n\n1.  Ability to adapt and learn by example in real-time\n\n2.  Fast Speed\n\n3.  Real-time pattern recognition\n\n4.  They can generalize and are known for their intelligent association.\n\n## **Neural Network Application Areas in Power Systems**\n\n![img2]({% static '/blog/assets/img/Applications-of-Neural-Networks-in-Power-Systems/a6b070f48cfe509264f8c5f7a071d50b.png' %})\n\nHere are some of the most prevalent problems :\n\n## **Load Forecasting**\n\nThe forecasting of electricity demand has become an essential research field as it plays a significant role in economic and financial development, expansion, and planning of electric power systems.\n\nIt is a difficult job as the load series is complex and exhibits several levels of seasonality but is a suitable problem for NN application due to the availability of historical load data on the utility databases.\n\nSome of the factors influencing the load are Economic, Time, Weather, and other Random Disturbances.\n\nHence, they can be classified into 3 groups\n\n-   Short-Term Load Forecasting\n\n    Ranges from a few minutes to a week.\n\n>   Some applications are economic load dispatch, real-time control, and energy\n>   transfer scheduling.\n\n-   Mid-Term Load Forecasting\n\n    Ranges from a month to a year.\n\n    They are mostly done to buy adequate fuel for power plants after calculating electricity tariffs.\n\n-   Long-Term Load Forecasting\n\n    Ranges from 5-20 years or more.\n\n    It is used by engineers to find out the type and size of generating plants to minimize all costs.\n\nExample of Short Term Load Forecasting using neural networks:\n\n<!-- ![img3]({% static '/blog/assets/img/Applications-of-Neural-Networks-in-Power-Systems/b2a391379b56b8a80b9048295901b39b.emf' %})\n\nDate: 09/04/2012 -->\n\n![img4]({% static '/blog/assets/img/Applications-of-Neural-Networks-in-Power-Systems/9f9b14c7c8387a18adb56f643fec59b1.png' %})\n\nDate: 13/04/2012\n\nThe architecture is trained by using a backpropagation algorithm with SIMULINK. The performance of the model was tested using hourly load data. A comparison of 24 hours ahead load forecasting and a specific load is shown above.\n\n## **Security Analysis**\n\nThe main task of the power industry is to deliver the power requested by the customers, without exceeding acceptable voltage and frequency limits. This task has to be solved in real-time with consistency and must be safe and economical.\n\nThe real-time measurements of the system are stored in a database, and missing data is adjusted. Based on the values, the mathematical model of the power system is established.\n\nBased on the simulation of potential equipment outage, the security level of the system is determined. If a system is considered unsafe concerning any potential outage, then control actions are taken.\n\nThere are mainly 2 types of security assessments: Static and Dynamic\n\nThe different operational states in both types are:\n\n-   Secure state: In this state, all customer demands are met, and the operating limit is within existing limits.\n\n-   Critical state: In this state, the system variables are still within limits, and constraints are satisfied, but little disturbance can lead towards instability.\n\n-   Unsecured state: In this case, the power system enters the emergency mode of operation upon violation of constraints.\n\nIn realistic and modern power systems, the dimension of the operating system is enormous. Thus, the modeling of neural networks can help overcome this curse of high dimensionality.\n\nE.g., Multi-Layer Perceptron with the backpropagation learning algorithm\n\n## **Fault Detection and Diagnosis**\n\nFault detection and diagnosis are some of the challenging problems in power systems.\n\nThe Multi-Layer Perceptron identifies the type and location of faults with a given set of power system conditions, measurements, and alarms. A Kohonen net (basic self-organizing neural net) is applied to handle the classification of fault patterns.\n\nThe diagnosis is done to judge what kinds of faults the apparatus suffered from and then accordingly it is cleared.\n\n## **Economic Load Dispatch**\n\nELD is one of the vital optimization problems in a power system. It ensures an optimal combination of power generating units are selected to minimize the total fuel cost while satisfying the load\u00a0demand and several operational constraints.\n\nHopfield Neural networks or Feedforward Neural networks can be used for the same. It helps in sharing the system load including the system losses amongst the available generators. Hence, it is basically to minimize the cost of generation and supply of electricity to load subject to multiple constraints like limitation of generators, power balance and network stability. The problem is highly non-linear and complex.\n\nThe applications of neural networks are incredibly vast and not just limited to power systems. They are used in speech recognition, machine translation, character recognition, image classification, and a lot more!\n\nThe best course for beginners started with Basics of Machine Learning is [This Course](https://www.coursera.org/learn/machine-learning)\u00a0by Andrew Ng.\n\n## **References for further reading:**\n\n-   <https://towardsdatascience.com/understanding-neural-networks-19020b758230>\n\n-   <https://circuitglobe.com/power-system.html>\n\n-   <https://pdfs.semanticscholar.org/e04f/7c1ad61ee36ce2b3989de9e693f29263589e.pdf>",
        "author_name": "Aditya C",
        "slug": "applications-of-neural-networks-in-power-systems"
    },
    {
        "layout": "post",
        "title": "Genome Editing",
        "author_github": "bsaiprasad",
        "date": "2020-03-13T23:32:44",
        "image": "/assets/img/",
        "description": "Designer babies, the end of diseases, genetically modified humans that never age. Does all this sound like science fiction? Well, not anymore. Humans have been engineering life for thousands of years like selective breeding for desired traits in plants and animals but never truly understood how it works. Until we discovered the code of life, DNA, and ways to manipulate it.",
        "tags": [
            "IEEE NITK",
            "Genome Editing"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "bsaiprasad",
        "text": "[{% load static %}]()\n# **GENOME EDITING**\n _A genome is an organism\u2019s complete set of DNAs, together with all of its genes. Every genome contains all the data required to create and maintain that organism._  In humans, a replica of the whole genome (more than three billion DNA base pairs) is contained in all cells that have a nucleus. Modification to provide desired traits in plants, animals, and microbes used for food began around 10,000 years ago. These changes, in conjunction with natural evolutionary changes, have resulted in common food species that are currently genetically totally different from their ancestors.\n \n ![Genome Editing]({% static '/blog/assets/img/Genome-Editing/figure1.jpg' %})\n \n### **GENETIC ENGINEERING**\n Genetic Engineering may be a form of genetic modification that involves a deliberate and targeted changes in plant or animal gene sequences to impact a selected attribute by making use of the rDNA technology.\n \n### **GENETIC ENGINEERING TECHNIQUES**\n\n#### **Microprojectile Bombardment**\n Klein and his colleagues discovered, in the 1980s, that naked DNA can be delivered to plant cells by \"shooting\" them with microscopic pellets to which DNA had been adhered. This is often a crude however, an effective physical technique of DNA delivery, particularly in species like corn, rice, and different cereal grains, which Agrobacterium doesn't naturally remodel. Several GE plants in industrial production were at first remodeled with the help of microprojectile delivery.\n \n#### **Electroporation**\n In electroporation, plant protoplasts take up macromolecules from their encompassing fluid, accelerated by electrical impulse. Cells growing in a medium are stripped of their protective walls, leading to protoplasts. Providing known DNA to the protoplast medium, and so applying the electrical pulse briefly destabilizes the cell wall, permitting the DNA to enter the cell. Remodeled cells would then regenerate their cell walls and grow to whole, fertile transgenic plants. Electroporation is restricted by the poor potency of most plant species to regenerate from protoplasts.\n#### **Microinjection**\n The process of microinjection is quite inefficient and extremely labor-intensive compared to alternative techniques. DNA is injected directly into anchored cells. Some proportion of those cells can survive and integrate the injected DNA.\n \n#### **Transposons/Transposable elements**\n The genes of most plants and a few animal (e.g., insects and fish) species carry transposons, that are short, naturally occurring items of DNA with the flexibility to maneuver from one location to a different one in the within the genome. Barbara McClintock first described such transposable components in corn plants during the 1950s (Cold Spring Harbor Laboratory, 1951). Transposons are investigated extensively in research laboratories, particularly to check cause, and also the mechanics of DNA recombination. However, they need not however been harnessed to deliver novel genetic information to enhance commercial crops.\n \n### **GENE ETHICS**\n A gene-editing tool named Crispr-Cas9, that allows us to treat many diseases, has made ethics question graver than ever. This tool behaves like a pair of molecular scissors that can be used very intricately to have an effect on only specific genes, cutting and splicing them to prevent or cure diseases. One widely held opinion is that it's unethical to use Crispr on the human sperm cell, eggs and embryos because changes can get carried on for generations and present a range of unsought consequences for the humankind. Laboratory analysis has shown Crispr-Cas9 can accidentally alter genes apart from those intended. This means it may disrupt other genes, impairing functions or causing individuals to become susceptible to infections.\n \n### **How CRISPR actually works?**\nCRISPR is a combination of a scissor like protein, such as Cas9, and a guide molecule. The molecule acts like a bloodhound taking CRISPR on the hunt to specific sites within the genome. Once there, the protein cuts the cell's DNA on the spot. That completely disables the targeted gene. The cells then repair their DNA, they put in a new DNA sequence that the bloodhound also carried, or simply patch up the break caused by CRISPR's molecular scissors. Enter tiny cellular machines called ribosomes. They translate the edited blueprint, skipping the disease-causing genes or producing healthy proteins that the repaired genes code for. CRISPR's ability to repair may be most important at the beginning of life. Edit the genome of a very early embryo - one created by in vitro fertilization - and couples who carry disease causing mutations will be able to have children spared from them. Once a child is born with a genetic disease, such as Huntington's or Tay-Sachs, it is more difficult to use CRISPR to treat it. It's also being used to research and test treatment for cancer, blindness, and liver diseases among others. But CRISPR isn't just for humans. Editing the genome of mosquitoes can give us the power to stop spread of diseases like malaria. This opens up a new world of technological advancements. When you can edit the DNA's blueprint, the possibilities are vast.\n![Crispr Cas9]({% static '/blog/assets/img/Genome-Editing/figure2.jpg' %})",
        "author_name": "B. Sai Prasad",
        "slug": "genome-editing"
    },
    {
        "layout": "post",
        "title": "A Dive into Git Directory",
        "author_github": "ameyanrd",
        "date": "2020-04-15T10:10:44",
        "image": "/assets/img/",
        "description": "Getting familiar with Git core concepts and Git remotes",
        "tags": [
            "Git",
            "Github",
            "Bash"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "ameyanrd",
        "comments": true,
        "text": "[{% load static %}]()\nGit is a distributed version control system. It means Git helps you manage different versions of your work, record the changes and helps different collaborators work parallelly on the same project.\n\nImagine that you are working on a big project involving many files and folders in it. You make a series of changes to your project and now finally when you compile it, you see the application is crashing or not working as expected. In such a case, Git is the best tool you could use. If you have recorded all your changes uniformly in Git, you can any time refer to those versions. Alternately, you can create a copy of your work at the beginning itself, and start working over it and if the changes work out correctly, you can try to merge them with your original project.\n\nThis blog mainly covers the following content:\n1. [Git Core](#gitcore)\n    - [The three Logical Areas](#logicalareas)\n    - [Learning core commands in Git using example](#example)\n2. [Git Branching and Merging](#gitbranchingandmerging)\n    - [Branching](#branching)\n    - [Git Stash](#stash)\n    - [Merging](#merging)\n    - [Merge Conflicts](#mergeconflicts)\n    - [Detached HEAD](#detachedhead)\n3. [Git Remotes](#gitremotes)\n    - [GitHub](#github)\n    - [Learning basic git remote commands](#remote)\n    - [Working with two remotes](#tworemotes)\n\n<a name=\"gitcore\"></a>\n\nGit Core:\n------------\n\n<a name=\"logicalareas\"></a>\n\nIn a Git repository/folder, we have three three logical areas, namely:\n- Working Tree\n- Staging Area\n- Commmit History\n![Three Logical Areas]({% static '/blog/assets/img/A-Dive-into-Git_Directory/three_areas_git.png' %})\n\nFor more clarity on commit, consider taking snapshots of your staged changes.\n\nTill now, you might be wondering where is all this information getting stored? A Git repository is initialised with an empty ```.git``` folder which contains information regarding commits, branches, config and many more. We\u2019ll see a few of these below.\n\nLet's play with Git and try to understand its features:\n> Note: I will be using command-line Git. I suggest installing any linux system before proceeding and learning basic bash commands. You can refer [this](https://ieee.nitk.ac.in/blog//installing-ubuntu/) for ubuntu installation and [this](https://medium.com/@zibon/basic-linux-command-lines-to-get-started-developing-in-ubuntu-linux-b54def1c2190) for getting started with basic bash commands.\n\n<a name=\"example\"></a>\n\nInstall Git on your system using\n\n`sudo apt-get install git`\n\nMake a new directory `git_test` and initialize it with Git using \n\n`git init`\n\nIt will create `.git` folder.\n![Git Init]({% static '/blog/assets/img/A-Dive-into-Git_Directory/initial_empty_git.png' %})\n\nNow, we have to configure the `user.name` and `user.email`. Whenever we commit our staged changes, Git includes this name, email and time of commit along with it. This is important to check who made these changes and when.\n\n`git config --global user.name <your-name>`<br />\n`git config --global user.email <your-email>`<br />\nTo check the configuration, type `git config --list`.\n![Git config]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_config.png' %})\n\nHere, we used `--global` flag, so this configuration will be reflected to all the Git repositories on your machine. This configuration is stored in your home directory. Type `cat ~/.gitconfig` to check.\nIf you use `--local` flag instead, this configuration will be limited to your current repo. This is stored in .git directory. Type `cat .git/config` in git_test folder.\n\nNow, we create a file say `file1` and \"Hello World!\" to it.\nType the following commands and observe:\n\n`git status`<br />\n`git add file1`<br />\n`git status`\n![Git add]({% static '/blog/assets/img/A-Dive-into-Git_Directory/tracking_changes.png' %})\n\nWhen we created file1 and made changes to it, these changes were present only in the Working Tree i.e. they are not staged/tracked.<br />\n`git add` command staged these changes i.e.added them to the Staging Area for committing them further.<br />\nNow, we commit these changes.<br />\n`git commit -m \"First line added to file1\"`<br />\n`-m` option helps us to add a small message along with the comment. This message should be clear enough so that we can refer to this commit in future if required.<br />\nNow, type `git status` again and observe.\n![Git commit]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_commit.png' %})\n\nWe can see a seven digit hexadecimal number above (c934395). Whenever we commit our changes, Git hashes these changes to a 40 digit hexadecimal number. The figure above is the first seven digits of 40 digits.\n\nNow, see the following operations:\n![Git diff]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_diff.png' %})\n\nNote the output of first `git status` above and compare with the previous outputs we got.<br />\n`git diff` helps us to check the differences between working tree and the corresponding staged file. Note that file2 is not staged at this point, so `git diff` won't give any output for it.<br />\nIf we use `git diff --staged`, we can get the changes between the staged files and most recent commit files (Try it before using `git commit`).This will also show the newly staged files not committed before.<br />\nIn `git add .`, `.` refers to all the new and modified files within the directory.\n\nTo check your commits till now, type <br />\n`git log`<br />\nSame can be checked in `.git/log/HEAD` file.\n![Git log]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_log.png' %})\n\nHere `HEAD` nothing but a pointer pointing to the default `master` branch.<br />\nIf we use `git log -p`, we can see all the changes in each commit compared to its previous commit.\n\nTo give a picture of what happened till now, refer below\n![Git Current]({% static '/blog/assets/img/A-Dive-into-Git_Directory/Current_areas.png' %})\n\nNow, say you want to remove a file2 from Git. So how would you do it?<br />\nApproach 1: <br />\n`rm file2`<br />\n`git add .`<br />\nApproach 2: `git rm file2`<br />\nThis will not only remove file2 from Working Directory, but also from Staging Area. Type `git status` and observe output.\n![Git rm]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_rm.png' %})\n\nCommit these changes. Moving on...\n\nSay you have made some changes to a file in the working tree and you want to recover the earlier file version, so what is the way out?<br />\nLet's say a line added to file1. Now this change is untracked in the Staging Area. But we have the previously staged file1 in Staging Area.\nSo we recover this file1 to our file system using \n\n`git checkout -- file1`\n\n![Undo Working Tree]({% static '/blog/assets/img/A-Dive-into-Git_Directory/Undo_Working_tree.png' %})\n\nNow, say we added changes to the Working Tree (in file1) and Staged these changes. We want to recover previously staged file1 (which was committed).\nHow will we achieve it?\n![Undo Staging Area]({% static '/blog/assets/img/A-Dive-into-Git_Directory/Undo_staging_area.png' %})\n\n>Tip: Try to think in terms of the logical areas of Git and understand the flow of the commands in the above picture.\n\nHow to restore changes from a previous commit?\n\nIn the context of our example, we will try to recover the file2. Type<br />\n`git log -- file2`<br />\nThis shows a list of commit that affect file2.\n![Recovering from commit]({% static '/blog/assets/img/A-Dive-into-Git_Directory/Recovering_from_commit.png' %})\n\n`git checkout <first_five_hash_num> -- file2` can be used to restore changes of file2 from that commit (In this case, restoring the file itself).<br />\n`git status` indicates that it changes the Working Tree as well Staging Area. Later we just commit these changes.\n\nSometimes we don't want Git to pay attention to a few files or folders. It means we don't want to check the changes made to these files. \nFor achieving this, Git has `.gitignore` file as a way out.\nWe create some files with .ab extension, say file1.ab, file2.ab and file3.ab. Also, we create a folder say `trash` which includes some expired or not required stuff.\nWe don't want to track either of them. So we implement the following <br/>\n`vi .gitignore`<br />\nIn the vi editor, append these lines<br />\n```*.ab```<br/>\n`trash/`\n>Try running `git status` before and after adding the above lines in `.gitignore` and observe the output.\n\n><b>Task:</b> We know that `.git` directory is the main directory recording all Git related information. Now make a new directory and copy your `.git` directory from `git_test` to this new folder and try to retrieve that data in this directory.\n\n<a name=\"gitbranchingandmerging\"></a>\n\nGit Branching and Merging:\n------------\n\n<a name=\"branching\"></a>\n\nBranching in Git enables developers to work on multiple versions of their work and merge the stable version with the master branch.<br />\nMaster branch is the the default branch created by Git. \n\nA branch is nothing but a pointer to a commit. Using different pointers (i.e. branches) leads to different commits. We will take an example and try to visualize it.\n\nTo visualise your Commit History graphically, we'll use <br />\n`git log --graph --online --all`\n>Check the flags used above, using `git log --help`.<br/>\n>Also, if possible create an alias of this command. [Here](https://ieee.nitk.ac.in/blog//getting-to-love-the-terminal/) is a reference for it. I will create `graph` as my alias.\n\nWe create a new directory name `git_test` and initialise it as Git Repo. Further we create two files file1, and file2 and edit them.<br />\nLook at the below picture and observe.\n![Git branching example]({% static '/blog/assets/img/A-Dive-into-Git_Directory/branching_intro.png' %})\n\nOnce the files are staged before, later staging their changes and committing can be performed in a single line using<br/>\n`git commit -a -m <message>`<br/>\nAfter the second commit on `master` branch, we can see that `master` is pointing to the commit 981b7c7. It points to the latest commit till now.<br/>\n`git checkout -b branch1` creates a branch with name as `branch1`, and the current branch is switched to `branch1` and we can see that it points to the same commit.\n\nAlso, we see that `HEAD` is pointing to `branch1`. `HEAD` indicates the current branch on which we are working.\n\n`git checkout <branch_name>` helps to switch to that branch.\n>Try to understand the flow of commands and relate it with it's `graph` in the above picture.\n\nWe can also create a branch using `git branch <branch_name>`, but it won't automatically switch it to that branch. You will have to do `git checkout <branch_name>`\n\nIf you type `git branch`, you can see all the branches and the branch where `HEAD` is pointing.\n![Git branch]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_branch.png' %})\n\n><b>Task:</b> Switch to different branches and edit diferent files. You will see that changes in one branch do not affect the changes in other. Think how it may be useful?\n\n<a name=\"stash\"></a>\n\nWhat happens when we try to switch to another branch without committing to its changes in the current branch?<br/>\nLook at the picture below\n![Git stash application]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_stash1.png' %})\n\nGit suggests you a way out. It tells you either commit your changes or `Stash` them. The latter one is used when you don't want commit your changes but quickly change to a different branch temporarily.\n\n![Git stash application]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_stash2.png' %})\n\n`git stash` can be used to stash our changes.<br />\n`git stash save <message>` can be used to save stash with a message.<br/>\n`git stash list` shows a list of stashes. `-p` option helps to check the changes.<br/>\n`git stash apply` applies the recent stash.<br/>\n`git stash apply stash@{2}` helps us to apply any particular stash (In this case stash@{2}).<br/>\n`git stash pop` pops the recent stash and also applies it.\n\n><b>Task:</b>\n>> 1. Try to search where are the stashes stored in `.git` folder.\n>> 2. Try to search where are the details regarding branch commits and head pointer stored.\n\n<a name=\"merging\"></a>\n\nMoving on to <b>`Merging branches...`<b/>\n\nNow, say you have created a branch, made changes to it, and finally you want to add all these changes to your `master` branch copy.\nHere it is required to merge two or more branches.\n\nThere are two types of merging techniques:\n1. `Fast-Forward`\n2. `3-Way`\n\nWe continue with our above situation. Look at the picture given below:\n![Git merge1]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_merge1.png' %})\n\nWe created a new `file3` in `branch2` and committed the changes.<br/>\nFrom the final graph, we observe that there is a direct path between `master` and `branch2`. But there is no direct path between `master` and `branch1` (We will have to pass through their first common ancestor i.e.981b7c7 to create a path between them).\n\nSay we want to merge `branch2` and `master`. Since there is a direct path, we perform the <b>`Fast-Forward Merge`</b>.\nMerging `master` with `branch2` is equivalent to moving `master` branch to `branch2`. Check the diagram below.\n![Fast-Forward Merge]({% static '/blog/assets/img/A-Dive-into-Git_Directory/fast_forward_merge.png' %})\n\nFirst the checkout `master` branch anad later perform<br/>\n`git merge branch2`<br/>\nThis updated `master` branch to `branch2` changes.<br/>\n<b>Note:</b> The direct path can be however long as possible. Fast-Forward Merge will take place as long as there is a direct path.\n\nNow we can delete the `branch2` branch as it is no longer required.\n![Branch Deletion]({% static '/blog/assets/img/A-Dive-into-Git_Directory/branch_deletion.png' %})\n\n`git branch --merged` can be used to check branches merged with the current branch.<br/>\n`git branch -d branch2` can be used delete the <b>merged<b/> branches.\n\nWhen we try to delete a non-merged branch `-d` gives an error and blocks it. But if you use `-D` instead, any non-merged branch can be deleted.\n\nNow, we have the following configuration and we want to merge `master` and `branch1`. How will we do it?\n![Current Config]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_merge_current.png' %})\n\nFast-Forward merge cannot be performed as there is no <b>direct path</b>.<br/>\nHere we have to perform the <b>`3-Way Merge`</b>. Look at the picture below to understand it.\n![3-Way Merge]({% static '/blog/assets/img/A-Dive-into-Git_Directory/three_way.png' %})\n\nThis technique checks the changes in the commits pointed by `master` and `branch1` with their LCA and reflect the changes in the newly merged commit (pointed by both the branches).\n![3-Way Merge Performed]({% static '/blog/assets/img/A-Dive-into-Git_Directory/three_way_performed.png' %})\n\n`git merge branch1` will open the dafault editor in your system for editing the message. Edit the message or keep it as it is and exit.<br/>\nThe `master` branch will point to this new commit.<br/>\n\n<a name=\"mergeconflicts\"></a>\n\nTill now, whatever merges we performed, we didn't encounter any <b>`Merge Conflicts`</b>.\n\n<b>Merge Conflicts</b> occur when we try to merge two branches that have changed the same lines in the same file with respect to the LCA.\n![Merge Conflict]({% static '/blog/assets/img/A-Dive-into-Git_Directory/merge_conflict.png' %})\n\nInitially, there is no fourth line or empty fourth line (in LCA). Later both `master` and `branch` have different fourth lines. This puts Git into a conflict which line to choose.<br/>\nIf one of branch's line had matched the line of LCA, the merged commit would have had the branch's line (Changed line has higher priority).\n\nSee the file1 contents.\n![file1 contents]({% static '/blog/assets/img/A-Dive-into-Git_Directory/file1_content.png' %})\n\n`=======` indicates that both changed lines have same priority.<br/>\nAlso `HEAD` and `branch` show the lines in their respective branches (HEAD points master).\n\nObserve the below picture\n![Resolve Merge Conflict]({% static '/blog/assets/img/A-Dive-into-Git_Directory/resolve_conflict.png' %})\n\n`git status` gives us that merging process has not stopped yet and the conflict exists.<br/>\nThere are two methods to resolve it:\n1. Either abort the merging process by<br/>\n`git merge --abort`\n2. Or open the file and manually select the lines you want finally in file1. Later stage and commit your changes.<br/>\nLater we remove `branch`.\n\n<a name=\"detachedhead\"></a>\n\nFinally, we move on to our last topic under the heading i.e. <b>`DETACHED HEAD`<b/>.\n\nNormally, `HEAD` points to a branch which in turn points to a commit. If we use `git checkout` with a commit, the `HEAD` goes into <b>detached state<b>.\n![Detached head state]({% static '/blog/assets/img/A-Dive-into-Git_Directory/detached_head.png' %})\n\nThere are two ways to get out of it:\n1. Checkout a branch in your Git repo.\n2. Create a new branch. It will automatically point to this commit(Detached Head Commit). Now checkout this branch.\n\n<a name=\"gitremotes\"></a>\n\nGit Remotes:\n------------\n\nRemote in Git is nothing but a repository in another location w.r.t your current location. This another location can be GitHub repo, fork of GitHub repo, a repo on your machine itself or a repo on any other Git Server.\n\n<a name=\"github\"></a>\n\nGitHub is like a cloud where projects can be hosted. Different people can collaborate on a single project hosted on GitHub. It equipped with Git utilities.\n\n<b>Note: </b>GitHub is a Git Server. Similarly, there are many such servers available to share your code. In fact, you can also establish your own local Git Server. [Here](https://www.linux.com/tutorials/how-run-your-own-git-server/) is a link for it.\n<br/>This blog won't be covering usage of GitHub. So use this [link](https://help.github.com/en) for GitHub reference.\n\n<a name=\"remote\"></a>\n\nNow, make a repository on your GitHub account (say `git_test`). If you initialize this repo with `README.md`, GitHub will automatically create a first commit which includes this file. Now we want to work on this file on our local system. So how do we make a copy of it on our machine?\n\nWe use `git clone` command followed by the `ssh` or `https` link provided by GitHub. For using `ssh` protocol, use will have to set the public key in GitHub. Use the above GitHub reference for it. This blog will be using `https` protocol.\n![Git clone]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_clone.png' %})\n\nIf your Git configuration is not set globally, set your configuration (either locally or globally as you wish).\n\nCheck the following picture:\n![Git remote]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_remote.png' %})\n\n<b>Note:</b> `graph` is an alias of `git log --graph --oneline --all`.\n\n`git remote` shows that one remote named `origin` is set up. It is the default name created by Git.<br/>\n`git remote -v` shows that `push` and `fetch` links for `origin`. We will understand these in a while.<br/>\nGit graph shows the local `master` branch. `HEAD` pointing it indicates that `master` branch is checked out. Here, we see two new branches `origin/master` and `origin/HEAD`.<br/>\n`origin/master` branch shows where the remote `origin`'s `master` branch is currently. \nSimilar is `origin/HEAD` by the above analogy.<br/>\nWhen we try to check out these branches, we end up in `Detached HEAD` state. Here `HEAD` is on the same commit as `origin/master` (w.r.t above picture). Here we can check the `master` branch of `origin`.\n\nHow to update your local repo with the changes done on the remote repo?<br/>\nTo do this, open the `git_test` repo on GitHub and add a line to README.md and commit your changes.\nNow come back to your terminal.\n![Git pull]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_fetch.png' %})\n\nWe see that `origin/master` is still on the same commit on our local system and not in sync with the remote repo. To update it as remote repo, we use<br/>\n`git fetch`\nNow, we can see `origin/master` updated. \n`git merge origin/master` helps us to merge our locally checked out `master` with the master of `origin`.\n\nThese two functions of updating `origin/master` and merging can be performed by following single command<br>\n`git pull`\n\nNow, we'll see how to reflect the local changes in the remote repo?\nTo show this, we create `file1`, edit it and commit it finally. See the picture below\n![Git push]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_push.png' %})\n\n`graph` and `git status` both show that `master` is ahead of `origin/master` by 1 commit.<br/>\n`git push` will push the local changes to the remote repo. Here, you will see that user authentication is required. This checks whether the person is authorised to make changes to the remote repo.<br/>\nGo to your GitHub account and check that the implemented changes.\n\n<b>Note:</b> These `fetch` and `push` commands use the links we saw in `git remote -v`.\n\n<b>Note:</b> Here we have only one remote set in our local repo, i.e. `origin`. Thus, we didn't mention the remote and the branch explicitly. Generally, we run the following type of commands<br/>\n`git fetch <remote> <branch>` <br/>\n(E.g., `git fetch origin master` in our case)<br/>\n`git pull <remote> <branch>`<br>\n`git push <remote> <branch>`<br/>\n\n<a name=\"tworemotes\"></a>\n\nNow, let's move on to setting more remotes.<br/>\nWhy would it be required to set more than one remotes?<br/>\nSay you want to contribute to a project or maybe provide come changes for a project hosted on GitHub and you don't have the edit access. How would you do it?<br/>\n\nGenerally, for such kind of activity, we copy this file to our GitHub account. This process is called making a `fork`. Later, we clone this forked repo in our local system.\nSo, now we have one original repo, a forked repo on our GitHub account and a cloned repo of our fork.\nBy default, `origin` will be created for this forked repo in our local repo. But if we want to check the changes implemented at the original repo (from which we forked), we add this as our another remote (generally named as `upstream`).\nFor contributing to the project, we create another branch on our local repo, implement the changes and push them to our forked version. Later we create a `Pull Request` on GitHub.\nA <b>Pull Request</b> sends the owner of the file a request of verifying the changes and merging them with their master branch.\n\nLater, if that owner merges our branch with the master, we delete the branch we created on our local system and pull the changes from `upstream/master` and continue our work.\n\nSee below picture for adding `upstream`.\n![Git upstream]({% static '/blog/assets/img/A-Dive-into-Git_Directory/git_upstream.png' %})\n\nI have created a dummy account named `dummydemo` and created a repo called `RemoteRepo`. We will this repo as original repo and fork it to our account and later clone it to our pc.\nWe see `origin` as our own account fork. We add `upstream` to check the changes at `dummydemo/RemoteRepo` using<br/>\n`git remote add <remote-name> <remote-link>`<br/>\nGit graph shows that still, it's branches are not updated. So we use<br/>\n`git fetch upstream`<br/>\nand merge it with our local `master`.\n\nTo remove a remote, we can use<br/>\n`git remote remove <remote-name>`\n\n><b>Task:</b> Create a new branch on our local `RemoteRepo`, make some changes and push this branch to `origin` (pushing to `upstream` might not be possible as we might not have the edit access). Now, make a `Pull Request` to \n`dummydemo/RemoteRepo` on GitHub. `dummydemo` can verify your changes and merge these to the `master` branch.\n\nResources: \n------------\n\n1. [https://git-scm.com/book/en/v2](https://git-scm.com/book/en/v2)\n2. [https://github.com/PointCloudLibrary/pcl/wiki/A-step-by-step-guide-on-preparing-and-submitting-a-pull-request](https://github.com/PointCloudLibrary/pcl/wiki/A-step-by-step-guide-on-preparing-and-submitting-a-pull-request)\n3. [https://www.atlassian.com/git/tutorials/rewriting-history/git-rebase](https://www.atlassian.com/git/tutorials/rewriting-history/git-rebase)\n4. [https://www.atlassian.com/git/tutorials/rewriting-history](https://www.atlassian.com/git/tutorials/rewriting-history)\n5. [https://www.perforce.com/blog/vcs/how-use-git-hooks](https://www.perforce.com/blog/vcs/how-use-git-hooks)\n6. [https://spin.atomicobject.com/2016/06/26/parallelize-development-git-worktrees/](https://spin.atomicobject.com/2016/06/26/parallelize-development-git-worktrees/)",
        "author_name": "Ameya Deshpande",
        "slug": "a-dive-into-git-directory"
    },
    {
        "layout": "post",
        "title": "Communication Protocols Part 1",
        "author_github": "rmhanchate",
        "date": "2020-04-15T12:10:44",
        "image": "/assets/img/",
        "description": "Getting familiar with Communication Protocols. This blog is the first part of the series.",
        "tags": [
            "Communication",
            "Protocols",
            "Synchronous"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "rmhanchate",
        "comments": true,
        "text": "[{% load static %}]()\n## Introduction\n\nConsider a multiplayer game that involves two or more participants. As there are many multiplayer games on an online platform, there's always a need to be a proper collaboration between the players. A game is only as good as its rules, and how well we play the game is defined by how well we follow the rules. Often, players ignore these \"protocols\" and end up messing the system with no result and just chaos. From this, you may guess these \"protocols\" are not just a necessity in games, but most of the aspects of life. Users (which include you and your online friend) need to communicate through a network to help simplify things.\n\nIt involves \"communication networks\" which are a collection of methods that users employ to pass on valuable information. But what if illegitimate details are being passed on which is against the common man's beliefs, or maybe the devices failed to communicate with each other because they didn't talk in an organized manner or some unknown reason.\n\nIn any communication network, it's rules and regulations are essential for the smooth functioning of it. Similarly, in this electronic world, communication protocols are crucial for proper operating of the electronic system with each other. \n\nSo anyone might raise a question: what do these communication protocols indicate in the Electronic world? We know that the set of rules and regulations is called a Protocol. Also, we can define communication (in general) as an exchange of information from one point, place, or system to another in a mutually accepted code understandable by both the sender and receiver. So through these sort of statements, we can define communication protocols, in terms of the electronic world, as a set of rules and regulations which ensure the flawless transfer of digital information within a system of two or more electronic networks.\n\nWe can classify these protocols into two parts based on a communication channel, aka wired (like commonly used USB cable) and wireless (like Bluetooth/WiFi). Well, you know what these mean, but this is a vast division. \n\nLet us now concentrate our thinking on wired communication in a system where two or more electronic devices are present on it. Each packet is transmitted and received over a network. The packet contains binary data (ones and zeros that encode the contents of each message). As from basic knowledge of digital electronics, we know that electronic devices only understand binary language, or we can say that they transmit and receive information in the form of a bit or bits. Now maybe you think that why I highlighted these familiar words with almost the same meaning. Well, the tiny difference between these two words gives way to two further divisions for wired communication, i.e., serial and parallel.\n\nSerial communication means that while communicating, only 1 bit of information/data is passed at a time. Still, on the other hand, in Parallel communication, quite a few bits are sent at a time.\n\n![img]({% static '/blog/assets/img/Compro1/opmyfRvgD9xbWHuRU0GmT3aHDgAv5w0H1hpWXHYthAOikFenCJb8aKsmFLEZiNB_3rCSImpF80PgXE9XFTydsdpDs8fESjYLicvZ1fb3tYISJLoNjBFdz3LnAuEFKL0v6gJR3ekN.jpg' %})\n\nTheir advantages and disadvantages can be deduced from their definitions only.\n\n| Serial                                                       | Parallel                                                     |\n| ------------------------------------------------------------ | ------------------------------------------------------------ |\n| An only a single bit is getting transferred at a time so data transfer rate slow. | Multiple bits are passed at the time, so transmission is fast as compared to Serial |\n| Serial Transmission is used for long-distance as less probability of cross talk. | Parallel Transmission is used for a short distance as quite high chances of cross talk due to multiple wires |\n| Fewer wires/pins in a serial cable, which significantly reduces the size and thus the cost. | More wires/pins increase size, and thus the overall cost.    |\n| It requires a select type of converters to convert data into the serial form. | There is no such requirement of converters in parallel transmission systems |\n| Examples: USB, I2C etc.                                      | Examples: PCI, SCSI, ISA etc.                                |\n\nThe decreasing cost and better performance of integrated-circuits have led to serial communication being used in favor of parallel communication.\n\n### **Serial Communication**\n\nThe most common serial communication system protocols can be classified under the following categories:\n\n1. Asynchronous\n2. Bit-Synchronous\n3. Synchronous\n\n#### Asynchronous\n\nA protocol which is not governed by a clock pulse, they don't have to wait until the clock changes. The state of the system changes as soon as the input change. Bits of information are transmitted between two devices at an arbitrary point of time. RS-232, RS-422, RS-423, and RS-485 are some of the recommended standards (RS-XXX) of the Electronic Industry Association (EIA) for asynchronous serial communication. They also specify the communication system characteristics of the hardware, such as voltage levels, terminating resistances, cable lengths, etc. \n\n![img]({% static '/blog/assets/img/Compro1/BIJYViEh-aABuJnQqmPPzVhNDMKjIJFAw6VGRdCd8OvVct8bD4FlU9149DiI7TlRPGWUgxHq3XNBuKDJkXotl-RFH4QObPULzi7MgSVsvu8HiNZa1VJEDpMz_Siz2HT4qBHORM1H.jpg' %})\n\n#### Synchronous\n\nIn contrast to Asynchronous protocols, in this type of protocol, clock signal wire is used by both receiver and sender to sample data. Synchronous data transfer with variable speed devices goes as fast as the slowest device involved in the transmission. This communication passes more information at a constant rate.\n\n#### Other Classifications\n\nThe other type of classification we can do is based upon the type of external or internal data transfer i.e.\n\n1. Intra: Communication between devices inside a single system\n2. Inter: Communication between devices of two or more different systems\n\n\n\n### Some Important Points\n\nBefore we jump into the individual protocol details, you need to look upon some essential terms which we will be using frequently:\n\n1. A full-duplex system means data can be sent and received simultaneously. On the other hand, Half-duplex is referred to the method wherein data can be sent or received. It cannot be done at the same time.\n2. Master/slave describes a bus where one device is the master, and others are slaves. These buses are usually synchronous, as the master often supplies the timing clock for data being sent along in both directions.\n\n\n\n\nSo, this concludes the introductory part of our series of blogs. To give you an idea of the further topics, the following diagram depicts the topics which will be covered in the upcoming blogs of this series.\n\n![img]({% static '/blog/assets/img/Compro1/M2tjsegWNC1ddIIXDq8PW7oOq-L9OjAM6MuJ11rs8OAl_PTHuhOQBGioTKztW1yWAHlbHhAx1WkSLb5EQm6GM5rGhnnAPKQsMvfyXnu0fv_CfuKkAMW7_ntvUBYFwS7ybKKNjdLw.jpg' %})",
        "author_name": "Rahul M Hanchate",
        "slug": "communication-protocols-part1"
    },
    {
        "layout": "post",
        "title": "EKRANOPLAN",
        "author_github": "raghunathabhishek",
        "date": "2020-04-15T12:11:44",
        "image": "/assets/img/",
        "description": "When you think of superpowers, flight, invisibility, mind reading is what come to our mind at first. Yet, nobody thinks   of Cold War Russia when we talk about superpowers. The \u2018Ekranoplan\u2019 is one such invention that was born in that super-innovative era.   Is it a plane? Is it a hovercraft? Or is it something else? Read to know more",
        "tags": [
            "Ekranoplan"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "raghunathabhishek",
        "comments": true,
        "text": "[{% load static %}]()\nHere's a closer look at the Ekranoplan\n![ekranoplan]({% static '/blog/assets/img/EKRANOPLAN/ekranoplan_resize_md.jpg' %})\n\nOften referred to as the Caspian monster, the ekranoplan is one of the most freakish designs that were actually brought to life in a period where people were peaking at innovation in military hardware. It was designed to stay above the surface of water like a hovercraft but travel at far greater speeds like that of a commercial aeroplane. This was achieved with the help of an air cushion.\n\nHovercrafts have downward facing fans which generate the necessary lift while Ekranoplans used its wings and built up speed to do the same. In addition to this with the help of something called the ground effect (this reduces induced drag as the wingtip vortices aren\u2019t formed) helps it travel at much higher speeds thana conventional aeroplane while retaining its ability to cover large distances and carry loads . This was very important for the Russians during the cold was as its primary purpose was to get troops in and out of the battle field under the radar.\n\nThe Ekranoplan is part of a larger fleet of the navy called \u2018Lun\u2019 in the Russian military. Capable of cruising at around 450kmph and a max speed of 550 kmph and a massive range of around 2000km. This 286-ton beast was powered by eight Kuznetsov NK-87 turbofans each producing a massive thrust of 127.4 KN each.IT had a wingspan of 44m and a length of 73.8m and a height of 19.2m. It needed a crew of 15 to operate it.\nThis was used more than just a reconnaissance vehicle , sometimes it was often used as an ayyach vehicle too. It was equipped with two 23mm cannons and and two twin turret forward and backward facing missile tubes . \n\nIt\u2019s offensive and definsive objectives included interception and neutralisation of other aeral, ground and naval combat vehicles and missiles. It was one of the most formidable weapons of the time and had several nicknames including the aircraft carrier killer, all of them well earned . The art of war created this masterpiece and it will not be forgotten in time. The ekranoplan never reached the battlefield in time , it was a bit too late which furthermore led to it\u2019s decommissioning.\n\nThis beautiful work of art never reached completion as the war had come to an end , it was finally named the 'spastel' which meant rescue . The work was about 90% complete when the military stopped funding the project. MD-160 was the sole completed Lun-class ekranoplane.The sole piece entered service in the Black Sea fleet in 1987. It was finally decommissioned in the late 1990\u2019s. Now it\u2019s remains can be found in an abandoned naval base in Kaspiysk.",
        "author_name": "Abhishek R",
        "slug": "updatedekranoplan"
    },
    {
        "layout": "post",
        "title": "An Introduction to PyTorch",
        "author_github": "shruthanr",
        "date": "2020-10-14T15:44:44",
        "image": "/assets/img/",
        "description": "A post on the basics of PyTorch",
        "tags": [
            "Deep Learning",
            "Machine Learning",
            "Python"
        ],
        "categories": [
            "CompSoc/Diode/Piston"
        ],
        "github_username": "shruthanr",
        "text": "[{% load static %}]()\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1odpZAS42UzVk16TGol_ta24DtyX1ZB2B)\n## An Introduction to PyTorch\n\n<p>PyTorch was released in early 2017 by Adam Paszke, Sam Gross, Soumith Chintala, Gregory Chanan and has been making a pretty big impact in the deep learning community. It's is a Python-based scientific computing package targeted to be a: (1) A replacement for NumPy to use the power of GPUs and (2) A deep learning research platform that provides maximum flexibility and speed. It's developed as an open source project by the Facebook AI Research team, and is being adopted by teams everywhere in industry and academia and is very comfortable to learn and use. It is based on the Torch library and has both a Python and C++ frontend(though the Python frontend is more 'polished'). PyTorch is also very pythonic, meaning, it feels more natural to use it if you already are a Python developer. </p>\n\n<h2>What we'll see</h2><br>\nIn this post, we will first look at the basics of Tensors(which are the building blocks of anything you do using PyTorch), and operations on them. We will then have a look at gradients and how they are computed in PyTorch. Finally we shall build a simple Neural Network for the IRIS dataset using PyTorch \n\n<h2>Prerequisites</h2>\nKnowledge of Python(3.x) is required. Knowledge of NumPy will be useful but is not necessary. For the last part (of building a Neural Network), a basic understanding of a simple neural network is assumed\n\n<h2>Installation</h2>\nDetails of installation may be found <a href=\"https://pytorch.org/get-started/locally/\">here</a>. However, to start off with, I would recommend using <a href = \"https://pytorch.org/get-started/locally/\">Google Colab</a> or <a href=\"https://azure.microsoft.com/en-us/develop/pytorch/\">Microsoft Azure</a>\n\n<P>We'll first have a look at the building blocks of PyTorch (most other deep learning libraries also) which are Tensors.</P>\n\n<h2>Tensors</h2>\nAll computations in PyTorch generally consist of operations on Tensors. Tensors can be thought of as a generalisation of vectors or matrices in 1 or more dimensions, or more simply, are like arrays in a programming language like C. In some cases tensors are used as a replacement for NumPy to use the power of GPUs A 1D Tensor is like a 1D array, 2D Tensor like a 2D array and so on. Let's see how we can use them.\n\n<h4>Importing torch</h4>\n\n\n```python\nimport torch\ntorch.__version__\n```\n    '1.1.0'\n\nTensors are available in the torch library as <a href='https://pytorch.org/docs/stable/tensors.html'><strong><tt>torch.Tensor</tt></strong></a>. It is like a multidimensional array which can have elements of a single datatype. Computations between tensors are allowed only if the tensors share the same data type(dtype).\n\nFirst lets create a tensor **from a python list**:\n```python\nmyTensor = torch.tensor([1, 2, 3])\nmyTensor\n```\n    tensor([1, 2, 3])\n\n<br>\n\n```python\nmyTensor1 = torch.tensor([[1, 2, 3.0], [4, 5, 6]])\nmyTensor1\n```\n    tensor([[1., 2., 3.],\n            [4., 5., 6.]])\n\nAbove, we see that the function determines the data type based on the inputs. We may use different constructors to specify the data type we need as:\n\n```python\nfltTensor = torch.FloatTensor([1, 2, 3])\nfltTensor\n```\n    tensor([1., 2., 3.])\n<br>\n\n```python\nintTensor = torch.IntTensor([1.0, 2.0, 3])\nintTensor\n```\n    tensor([1, 2, 3], dtype=torch.int32)\n<br>\n\n```python\nlongTensor = torch.LongTensor([1.0, 2, 3])\nlongTensor\n```\n    tensor([1, 2, 3])\n\nWe may also achieve the above using the dtype attribute in torch.tensor:\n\n```python\nintTensor1 = torch.tensor([1.0, 2.0, 3], dtype = torch.int)\nintTensor1\n```\n    tensor([1, 2, 3], dtype=torch.int32)\n\n**Note:** There is a subtle difference between the functions <tt>torch.tensor</tt> and <tt>torch.Tensor</tt>. torch.Tensor is an alias to torch.FloatTensor whereas torch.tensor determines the data type based on the input.\n\nFor more information on datatypes, check <a href = \"https://pytorch.org/docs/stable/tensors.html\">here</a>.\n\n<h4>To and From a NumPy array</h4>\n\nConverting a torch tensor to a numpy array and vice versa is very easy and hence makes it easy to access other libraries like Scikit-Learn and Matplotlib\n\n```python\nimport numpy as np\n```\n<br>\n\n```python\narr = np.array([1,2,3,4,5])\nprint(arr)\nprint(arr.dtype)\nprint(type(arr))\n```\n    [1 2 3 4 5]\n    int32\n    <class 'numpy.ndarray'>\n\n\nWe can use <tt>torch.from_numpy</tt> or <tt>torch.as_tensor</tt>:\n\n<br>\n\n```python\nx = torch.from_numpy(arr)\nx\n```\n    tensor([1, 2, 3, 4, 5], dtype=torch.int32)\n\n<br>\n\n```python\nprint(type(x))\nprint(x.type())\n```\n    <class 'torch.Tensor'>\n    torch.IntTensor\n\nNote that we can also use torch.tensor for this:\n\n```python\nx1 = torch.tensor(arr)\nx1\n```\n    tensor([1, 2, 3, 4, 5], dtype=torch.int32)\n\nThe difference between <tt>torch.tensor</tt> and <tt>torch.from_numpy</tt>(or <tt>torch.as_tensor</tt>) is that when we use the former, a copy of the original tensor is made and stored in <tt>x1</tt>(above). Any changes made to <tt>x1</tt> will not affect <tt>arr</tt>(the numpy array) and vice versa. However, when we use the latter function, the tensor created (x in the example above) points to the same location in memory as does <tt>arr</tt>. Hence any changes done to <tt>x</tt> will also affect arr(the numpy array) and vice versa.\n\n```python\n# Using torch.from_numpy()\narr = np.arange(0,5)\nt = torch.from_numpy(arr)\narr[2]=100\nprint(t)\n```\n    tensor([  0,   1, 100,   3,   4], dtype=torch.int32)\n\n<br>\n\n```python\n# Using torch.tensor()\narr = np.arange(0,5)\nt = torch.tensor(arr)\narr[2]=100\nprint(t)\n```\n    tensor([0, 1, 2, 3, 4], dtype=torch.int32)\n\n<h4>Creating special types of tensors</h4>\n\nUninitialized tensors using <a href='https://pytorch.org/docs/stable/torch.html#torch.empty'><strong>torch.empty()</strong></a> \n\n```python\nx = torch.empty(5, 4)\nprint(x)\n```\n    tensor([[0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.],\n            [0., 0., 0., 0.]])\n\nInitialised with zeroes or ones using <a href='https://pytorch.org/docs/stable/torch.html#torch.zeros'><strong>torch.zeros()</strong></a> and <a href='https://pytorch.org/docs/stable/torch.html#torch.ones'><strong>torch.ones()</strong></a>\n\n```python\n# Passing datatype is recommended but not compulsory\nx = torch.zeros(4, 3, dtype=torch.int64)\nprint(x)\n```\n    tensor([[0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0],\n            [0, 0, 0]])\n\nTensors in a range using <a href='https://pytorch.org/docs/stable/torch.html#torch.arange'><strong>torch.arange(start,end,step)</strong></a> and <a href='https://pytorch.org/docs/stable/torch.html#torch.linspace'><strong>torch.linspace(start,end,number_of_elements)</strong></a>\n\n```python\nx = torch.arange(0,50,5).reshape(5,2) # 0 included, 50 excluded\nprint(x)\n```\n    tensor([[ 0,  5],\n            [10, 15],\n            [20, 25],\n            [30, 35],\n            [40, 45]])\n\nTensor to create 12 linearly spaced elements between 0 and 50 both included\n\n```python\nx = torch.linspace(0,50,12).reshape(3,4) \nprint(x)\n```\n    tensor([[ 0.0000,  4.5455,  9.0909, 13.6364],\n            [18.1818, 22.7273, 27.2727, 31.8182],\n            [36.3636, 40.9091, 45.4545, 50.0000]])\n\nA seed for random numbers can be set using torch.manual_seed().\n\n```python\ntorch.manual_seed(10)\n```\n    <torch._C.Generator at 0x1bb6e171030>\n\nGenerating random tensors:\n\nA tensor of shape (3, 4) with random numbers from a uniform distribution over [0, 1) \n\n```python\nx = torch.rand(3, 4) \nx\n```\n    tensor([[0.9712, 0.0742, 0.5130, 0.7472],\n            [0.4507, 0.9223, 0.9148, 0.1624],\n            [0.7780, 0.1663, 0.6665, 0.4992]])\n\nA tensor with shape (3, 4) with numbers from the normal distribution with mean 0 and standard deviation 1.\n\n```python\nx = torch.randn(3, 4) \nx\n```\n    tensor([[ 0.5252,  2.0810,  1.5700, -0.1474],\n            [-0.2024,  0.4377,  1.1986,  0.7179],\n            [-0.4969,  0.8618, -0.2603, -1.1157]])\n\nA tensor of shape (3, 4) with random integers between 0 and 10\n\n```python\nx = torch.randint(0, 10, (3, 4)) \nx\n```\n    tensor([[7, 7, 1, 4],\n            [7, 4, 9, 5],\n            [1, 2, 5, 6]])\n\nInstead of specifying the sizes of the tensors, we can use three other functions which serve the same purpose as above nute take in other tensors as inputs and return tensors of their shapes. Just suffix _like to the above functions as below:\n\n```python\nx = torch.rand(3, 4) \ny = torch.rand_like(x) \ny\n```\n    tensor([[0.3099, 0.0135, 0.2955, 0.8752],\n            [0.7608, 0.7589, 0.2097, 0.4063],\n            [0.6469, 0.3655, 0.3926, 0.6284]])\n\n\nSimilarly <tt>torch.randn_like(x), torch.randint_like(0,10,x), torch.zeros_like(x)</tt> and <tt>torch.ones_like(x)</tt> may also be used.\n\n<h2>Operations on Tensors</h2>\n\nNow we will look at a few basic operations on Tensors. Indexing and slicing tensors are very similar to those of python lists. We shall look at a few examples\n\n```python\nx = torch.arange(6).reshape(3,2)\nprint(x)\n```\n    tensor([[0, 1],\n            [2, 3],\n            [4, 5]])\n\nTo get the left column:\n\n```python\nx[:,0]\n```\n    tensor([0, 2, 4])\n\nTo get the left column as a (3,1) slice:\n\n```python\nx[:,:1]\n```\n    tensor([[0],\n            [2],\n            [4]])\n\n**Reshaping a Tensor**\n\nTwo functions are generally used for reshaping tensor which are .view() and .reshape(). Both functions return a reshaped tensor without affecting the original tensor.\n\n```python\nx = torch.arange(10)\nx\n```\n    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n<br>\n\n```python\nx.view(5, 2)\n```\n    tensor([[0, 1],\n            [2, 3],\n            [4, 5],\n            [6, 7],\n            [8, 9]])\n\nHowever, we see that x is unchanged\n\n```python\nx \n```\n    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\n<br>\n\n```python\nx.reshape(5, 2)\n```\n    tensor([[0, 1],\n            [2, 3],\n            [4, 5],\n            [6, 7],\n            [8, 9]])\n<br>\n\n```python\nx # Unchanged\n```\n    tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n\nTo change the original tensor use <tt>x = x.reshape(2, 5)</tt>\n\nWhile .view() returns a tensor which shares storage with the original tensor, .reshape() may return a copy or a view of the original tensor. It (.reshape()) may or may not share the storage with the original tensor. Also .reshape() may act on <a href = \"https://stackoverflow.com/questions/26998223/what-is-the-difference-between-contiguous-and-non-contiguous-arrays/26999092#26999092\">contiguous</a> or non contiguous tensors, while .view can act only on contiguous tensors. \n\nWe can also infer the correct value for shape from the tensor by passing -1.\n\n```python\nx = x.view(2, -1)\nx\n```\n    tensor([[0, 1, 2, 3, 4],\n            [5, 6, 7, 8, 9]])\n\nAlso as seen before, we can suffix the function with '_as' to pass in a tensor whose shape, we want to reshape the original tensor to.\n\n```python\ny = torch.arange(0, 20, 2)\ny\n```\n    tensor([ 0,  2,  4,  6,  8, 10, 12, 14, 16, 18])\n\n<br>\n\n```python\ny = y.view_as(x)\ny\n```\n    tensor([[ 0,  2,  4,  6,  8],\n            [10, 12, 14, 16, 18]])\n\n\n\n**Other Basic Operations**\n\nI will demonstrate the use of basic operations using the torch.add() function. This may be extended to other functions also.\n\n\n```python\na = torch.tensor([1,2], dtype=torch.float)\nb = torch.tensor([3,4], dtype=torch.float)\nprint(a + b)\n```\n    tensor([4., 6.])\n\n<br>\n\n```python\ntorch.add(a, b)\n```\n    tensor([4., 6.])\n\n<br>\n\n```python\nc = torch.empty(2)\ntorch.add(a, b, out = c) #Equivalent to c = torch.add(a, b)\n```\n    tensor([4., 6.])\n\n\n<br>\n\n```python\na.add_(b) #Equivalent tp a = torch.add(a,b)\na\n```\n    tensor([4., 6.])\n\nThe above can be extended to all basic arithmetic operations. Now we will look at a few more operations\n\n```python\n# Multiplication (element-wise)\na = torch.tensor([1,2], dtype=torch.float)\nb = torch.tensor([3,4], dtype=torch.float)\ntorch.mul(a, b)\n```\n    tensor([3., 8.])\n\n\n<br>\n\n```python\n# Dot Product\ntorch.dot(a, b)\n```\n    tensor(11.)\n\nNow let us see matrix multiplication. Normal matrix multiplication (that we know of) can be done using **torch.mm()**\n\n```python\na = torch.tensor([[0,2,4],[1,3,5]], dtype=torch.float)\nb = torch.tensor([[6,7],[8,9],[10,11]], dtype=torch.float)\n\nprint('a: ',a.size())\nprint('b: ',b.size())\nprint('a x b: ',torch.mm(a,b).size())\n```\n    a:  torch.Size([2, 3])\n    b:  torch.Size([3, 2])\n    a x b:  torch.Size([2, 2])\n\n<br>\n\n```python\ntorch.mm(a,b)\n```\n    tensor([[56., 62.],\n            [80., 89.]])\n\n\n<br>\n\n```python\na @ b\n```\n    tensor([[56., 62.],\n            [80., 89.]])\n\nMatrix multiplication can also be done with boradcasting using the <tt>torch.matmul()</tt> or <tt>@</tt> operator. Click <a href='https://pytorch.org/docs/stable/notes/broadcasting.html#broadcasting-semantics'>here</a> for more details on broadcasting.\n\n\n```python\na = torch.randn(2, 3, 4)\nb = torch.randn(4, 5)\n\nprint(torch.matmul(a, b).size())\nprint((a @ b).size())\n```\n    torch.Size([2, 3, 5])\n    torch.Size([2, 3, 5])\n\nBut we see that there matrices are invalid for normal matrix multiplication\n\n```python\nprint(torch.mm(a,b)) \n```\n    ---------------------------------------------------------------------------\n\n    RuntimeError                              Traceback (most recent call last)\n\n    <ipython-input-93-244d2942b50e> in <module>\n    ----> 1 print(torch.mm(a,b))\n    \n\n    RuntimeError: matrices expected, got 3D, 2D tensors at ..\\aten\\src\\TH/generic/THTensorMath.cpp:956\n\nNote that if the tensors satisfy the mathematical conditions of matric multiplication, then all the three above functions will be identical. It is however easier to detect errors using <tt>torch.mm()</tt> than the other two when the tensors are mathematically non-compatible and hence is recommended over the other two. \n\n**Norm Function**\n\n```python\nx = torch.tensor([2.,5.,8.,14.])\nx.norm()\n```\n    tensor(17.)\n\n<h2>Gradients in PyTorch</h2>\n\nPyTorch provides a module called 'autograd' to calculate the gradients of tensors automatically. It basically keeps track of all operations done on the tensor and backtracks along these operations to calculate gradients(or derivatives) along the way. To ensure that the operations are kept track of, we need to set the requires_grad attribute to True which can be done in two ways: (1) while creation, set the attribute to True as <tt>x = torch.arange(10, requires_grad = True)</tt> or (2) after creation, use <tt>x.requires_grad_(True)</tt>. The gradients are computed with respect to some variable y as <tt>y.backward()</tt>. This goes though all operations which were used to create y and calculates the gradients. For example:\n\n\n```python\nx = torch.tensor(3.0, requires_grad = True)\n```\n<br>\n\n```python\ny = 2 * x ** 2 + 3\nprint(y) # Substitutes the value of x = 3 in the equation\n```\n    tensor(21., grad_fn=<AddBackward0>)\n\n\n![Explanation]({% static '/blog/assets/img/intro-to-pytorch/s1.png' %}) \n\n```python\n#Perform backpropagation on y to calculate gradients\ny.backward()\n```\n<br>\n\n```python\n#Display the gradient wrt x\nx.grad\n```\n    tensor(12.)\n\n![Explanation]({% static '/blog/assets/img/intro-to-pytorch/s2.png' %}) \n\n**Calculating multi-level gradients**\n\n```python\nx = torch.tensor([[1.,2,3],[3,2,1]], requires_grad=True)\nprint(x)\n```\n    tensor([[1., 2., 3.],\n            [3., 2., 1.]], requires_grad=True)\n\n<br>\n\n```python\ny = 2*x + 3\nprint(y)\n```\n    tensor([[5., 7., 9.],\n            [9., 7., 5.]], grad_fn=<AddBackward0>)\n\n<br>\n\n```python\nz = 2*y**2\nprint(z)\n```\n    tensor([[ 50.,  98., 162.],\n            [162.,  98.,  50.]], grad_fn=<MulBackward0>)\n\n<br>\n\n```python\nout = z.sum()\nprint(out)\n```\n    tensor(620., grad_fn=<SumBackward0>)\n\n<br>\n\n```python\nout.backward()\n```\n<br>\n\n```python\nprint(x.grad)\n```\n    tensor([[40., 56., 72.],\n            [72., 56., 40.]])\n\n![Explanation]({% static '/blog/assets/img/intro-to-pytorch/s3.png' %}) \n\n**Turning off tracking**\n\nThere may be times when we don't want or need to track the computational history.\n\nYou can reset a tensor's <tt>requires_grad</tt> attribute in-place using <tt>.requires_grad_(True)</tt> (or False) as needed.\n\nWhen performing evaluations, it's often helpful to wrap a set of operations in <tt>with torch.no_grad():</tt>\n\nA less-used method is to run <tt>.detach()</tt> on a tensor to prevent future computations from being tracked. This can be handy when cloning a tensor.\n\n<h2>Building a Simple Neural Network</h2>\n\nNote that the rest of the article will need some knowledge of Machine Learning or Neural Networks.<br>\nWe will discover other features like Dataloaders, Criterions and Optimizers using an example on the <a href = 'https://en.wikipedia.org/wiki/Iris_flower_data_set'>IRIS Dataset</a>\n\n\n```python\nimport pandas as pd\nimport numpy as np\nimport torch\n\ndf = pd.read_csv('iris.csv')\ndf.head()\n```\n<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>target</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>\n\nPyTorch has 2 really useful libraries for Neral Networks:<br>\n1. <tt>torch.nn</tt> generally imported as <tt>nn</tt> <br>\n2. <tt>torch.nn.functional</tt> generally imported as <tt>F</tt>\n\n\n```python\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, TensorDataset\nfrom sklearn.model_selection import train_test_split\n```\n\n```python\nX = df.drop('target',axis=1).values\ny = df['target'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=100)\n\nX_train = torch.FloatTensor(X_train)\nX_test = torch.FloatTensor(X_test)\n\n# One Hot Encoding\ny_train = torch.LongTensor(y_train)  \ny_test = torch.LongTensor(y_test)\n```\nTo convert the training tensors into a dataset and to make things like batch gradient descent easier, we may use the <tt>TensorDataset</tt> and <tt>DataLoader</tt> classes.\n\n```python\ntrainset = TensorDataset(X_train, y_train)\n```\n\nWe will create batches of size 30. We will shuffle the training data so that the batches are not biased. However this is not necessary for the test data.\n\n\n```python\ntrainloader = DataLoader(trainset, batch_size=30, shuffle=True)\ntestloader = DataLoader(X_test, batch_size=30, shuffle=False)\n```\n\nTo access the batches:\n\n\n```python\n# 4 batches. Each batch is two-dimensional, one for the features(X) and other for the classes(y).\nfor batch in trainloader:\n    print(len(batch))\n```\n    2\n    2\n    2\n    2\n\n<br>\n\n```python\n# To access the index, X data and Y data\nfor i, (x,y) in enumerate(trainloader):\n    print(i, x, y)\n    print()\n```\n\n    0 tensor([[5.7000, 2.9000, 4.2000, 1.3000],\n            [7.4000, 2.8000, 6.1000, 1.9000],\n            [6.9000, 3.1000, 5.1000, 2.3000],\n            [5.3000, 3.7000, 1.5000, 0.2000],\n            [5.4000, 3.0000, 4.5000, 1.5000],\n            [5.5000, 4.2000, 1.4000, 0.2000],\n            [6.1000, 2.6000, 5.6000, 1.4000],\n            [5.5000, 2.6000, 4.4000, 1.2000],\n            [6.0000, 3.4000, 4.5000, 1.6000],\n            [5.7000, 2.5000, 5.0000, 2.0000],\n            [4.4000, 3.2000, 1.3000, 0.2000],\n            [5.0000, 3.6000, 1.4000, 0.2000],\n            [4.9000, 3.1000, 1.5000, 0.1000],\n            [7.0000, 3.2000, 4.7000, 1.4000],\n            [6.3000, 2.5000, 4.9000, 1.5000],\n            [6.1000, 2.8000, 4.0000, 1.3000],\n            [6.5000, 3.0000, 5.8000, 2.2000],\n            [5.1000, 3.8000, 1.5000, 0.3000],\n            [5.0000, 3.0000, 1.6000, 0.2000],\n            [4.6000, 3.6000, 1.0000, 0.2000],\n            [5.7000, 2.8000, 4.5000, 1.3000],\n            [4.9000, 3.1000, 1.5000, 0.1000],\n            [5.8000, 2.7000, 4.1000, 1.0000],\n            [5.1000, 3.3000, 1.7000, 0.5000],\n            [4.4000, 2.9000, 1.4000, 0.2000],\n            [6.2000, 2.2000, 4.5000, 1.5000],\n            [7.9000, 3.8000, 6.4000, 2.0000],\n            [7.7000, 3.8000, 6.7000, 2.2000],\n            [6.3000, 2.9000, 5.6000, 1.8000],\n            [5.5000, 2.5000, 4.0000, 1.3000]]) tensor([1, 2, 2, 0, 1, 0, 2, 1, 1, 2, 0, 0, 0, 1, 1, 1, 2, 0, 0, 0, 1, 0, 1, 0,\n            0, 1, 2, 2, 2, 1])\n    \n    1 tensor([[5.0000, 3.2000, 1.2000, 0.2000],\n            [4.9000, 3.1000, 1.5000, 0.1000],\n            [6.7000, 3.3000, 5.7000, 2.1000],\n            [5.5000, 3.5000, 1.3000, 0.2000],\n            [5.4000, 3.9000, 1.7000, 0.4000],\n            [6.0000, 3.0000, 4.8000, 1.8000],\n            [5.1000, 2.5000, 3.0000, 1.1000],\n            [5.9000, 3.0000, 4.2000, 1.5000],\n            [7.6000, 3.0000, 6.6000, 2.1000],\n            [6.4000, 2.7000, 5.3000, 1.9000],\n            [5.1000, 3.8000, 1.6000, 0.2000],\n            [6.9000, 3.1000, 5.4000, 2.1000],\n            [7.2000, 3.6000, 6.1000, 2.5000],\n            [5.1000, 3.5000, 1.4000, 0.2000],\n            [6.5000, 3.2000, 5.1000, 2.0000],\n            [5.5000, 2.4000, 3.7000, 1.0000],\n            [5.6000, 2.8000, 4.9000, 2.0000],\n            [6.3000, 3.4000, 5.6000, 2.4000],\n            [7.3000, 2.9000, 6.3000, 1.8000],\n            [5.9000, 3.2000, 4.8000, 1.8000],\n            [6.8000, 2.8000, 4.8000, 1.4000],\n            [4.9000, 2.5000, 4.5000, 1.7000],\n            [5.1000, 3.5000, 1.4000, 0.3000],\n            [6.2000, 3.4000, 5.4000, 2.3000],\n            [5.7000, 2.8000, 4.1000, 1.3000],\n            [6.1000, 3.0000, 4.9000, 1.8000],\n            [5.5000, 2.4000, 3.8000, 1.1000],\n            [5.7000, 2.6000, 3.5000, 1.0000],\n            [5.0000, 3.5000, 1.6000, 0.6000],\n            [5.6000, 2.7000, 4.2000, 1.3000]]) tensor([0, 0, 2, 0, 0, 2, 1, 1, 2, 2, 0, 2, 2, 0, 2, 1, 2, 2, 2, 1, 1, 2, 0, 2,\n            1, 2, 1, 1, 0, 1])\n    \n    2 tensor([[5.0000, 3.3000, 1.4000, 0.2000],\n            [5.8000, 2.6000, 4.0000, 1.2000],\n            [5.6000, 3.0000, 4.1000, 1.3000],\n            [5.0000, 2.0000, 3.5000, 1.0000],\n            [6.4000, 2.9000, 4.3000, 1.3000],\n            [5.1000, 3.8000, 1.9000, 0.4000],\n            [5.6000, 2.9000, 3.6000, 1.3000],\n            [6.7000, 3.1000, 4.4000, 1.4000],\n            [6.1000, 3.0000, 4.6000, 1.4000],\n            [4.5000, 2.3000, 1.3000, 0.3000],\n            [6.7000, 3.1000, 5.6000, 2.4000],\n            [5.7000, 3.8000, 1.7000, 0.3000],\n            [4.8000, 3.1000, 1.6000, 0.2000],\n            [6.5000, 2.8000, 4.6000, 1.5000],\n            [6.0000, 2.2000, 5.0000, 1.5000],\n            [6.5000, 3.0000, 5.2000, 2.0000],\n            [6.3000, 3.3000, 6.0000, 2.5000],\n            [4.9000, 2.4000, 3.3000, 1.0000],\n            [7.1000, 3.0000, 5.9000, 2.1000],\n            [4.4000, 3.0000, 1.3000, 0.2000],\n            [6.4000, 3.2000, 4.5000, 1.5000],\n            [5.0000, 2.3000, 3.3000, 1.0000],\n            [6.7000, 3.1000, 4.7000, 1.5000],\n            [5.4000, 3.7000, 1.5000, 0.2000],\n            [6.3000, 3.3000, 4.7000, 1.6000],\n            [5.1000, 3.7000, 1.5000, 0.4000],\n            [6.1000, 2.9000, 4.7000, 1.4000],\n            [5.2000, 2.7000, 3.9000, 1.4000],\n            [5.1000, 3.4000, 1.5000, 0.2000],\n            [4.8000, 3.4000, 1.9000, 0.2000]]) tensor([0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 2, 0, 0, 1, 2, 2, 2, 1, 2, 0, 1, 1, 1, 0,\n            1, 0, 1, 1, 0, 0])\n    \n    3 tensor([[4.3000, 3.0000, 1.1000, 0.1000],\n            [6.4000, 3.1000, 5.5000, 1.8000],\n            [6.9000, 3.1000, 4.9000, 1.5000],\n            [5.6000, 3.0000, 4.5000, 1.5000],\n            [6.0000, 2.9000, 4.5000, 1.5000],\n            [7.2000, 3.0000, 5.8000, 1.6000],\n            [6.6000, 2.9000, 4.6000, 1.3000],\n            [5.8000, 2.7000, 5.1000, 1.9000],\n            [5.0000, 3.4000, 1.5000, 0.2000],\n            [6.3000, 2.8000, 5.1000, 1.5000],\n            [6.2000, 2.8000, 4.8000, 1.8000],\n            [4.7000, 3.2000, 1.3000, 0.2000],\n            [5.7000, 3.0000, 4.2000, 1.2000],\n            [4.6000, 3.1000, 1.5000, 0.2000],\n            [4.6000, 3.2000, 1.4000, 0.2000],\n            [6.7000, 2.5000, 5.8000, 1.8000],\n            [5.8000, 2.7000, 3.9000, 1.2000],\n            [4.6000, 3.4000, 1.4000, 0.3000],\n            [6.3000, 2.3000, 4.4000, 1.3000],\n            [6.0000, 2.7000, 5.1000, 1.6000],\n            [5.2000, 3.5000, 1.5000, 0.2000],\n            [5.5000, 2.3000, 4.0000, 1.3000],\n            [5.8000, 2.8000, 5.1000, 2.4000],\n            [5.8000, 4.0000, 1.2000, 0.2000],\n            [4.8000, 3.0000, 1.4000, 0.1000],\n            [6.4000, 2.8000, 5.6000, 2.2000],\n            [6.8000, 3.2000, 5.9000, 2.3000],\n            [5.8000, 2.7000, 5.1000, 1.9000],\n            [6.7000, 3.3000, 5.7000, 2.5000],\n            [5.4000, 3.9000, 1.3000, 0.4000]]) tensor([0, 2, 1, 1, 1, 2, 1, 2, 0, 2, 2, 0, 1, 0, 0, 2, 1, 0, 1, 1, 0, 1, 2, 0,\n            0, 2, 2, 2, 2, 0])\n    \n\n\n**Creating the Model Class**<br>\nTo define a Neural Network, we need to define a class which inherits from the <tt>nn.Module</tt> class. Here is where we can define all the layers, activation functions, embeddings etc. Here we will create a simple model with 2 hidden layers:\n\n\n```python\nclass Model(nn.Module):\n    def __init__(self, in_features=4, h1=10, h2=10, out_features=3):\n        super().__init__()\n        self.fc1 = nn.Linear(in_features,h1)    # input layer\n        self.fc2 = nn.Linear(h1, h2)            # hidden layer\n        self.out = nn.Linear(h2, out_features)  # output layer\n        \n    def forward(self, x):\n        # Define the activation functions for the layers\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.sigmoid(self.out(x))\n        return x\n```\n\n<br>\n\n```python\n# Instantiate the Model class using parameter defaults:\nmodel = Model()\n```\n\n**Defining the loss function and optimizer:**<br>\nThe loss function is conventionally defined as **criterion**. THe various loss functions are available in <tt>torch.nn</tt> library and the optimizers are available in the <tt>torch.optim</tt> library. Here we will use Cross Entropy Loss and the Adam Optimizer\n\n\n```python\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.01) # lr: Learning Rate\n```\n\n**Training the model**\n\n\n```python\nepochs = 100\nlosses = []\n\nfor i in range(epochs):\n    i+=1\n    for j, (X, y) in enumerate(trainloader):\n        y_pred = model.forward(X)\n        loss = criterion(y_pred, y)\n        losses.append(loss)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    \n    \n    if i%10 == 1:\n        print(f'epoch: {i:2}  loss: {loss.item():10.8f}')\n\n\n```\n    epoch:  1  loss: 1.09409952\n    epoch: 11  loss: 0.96279073\n    epoch: 21  loss: 0.87627226\n    epoch: 31  loss: 0.72676194\n    epoch: 41  loss: 0.66376936\n    epoch: 51  loss: 0.68377405\n    epoch: 61  loss: 0.68355846\n    epoch: 71  loss: 0.64891487\n    epoch: 81  loss: 0.66468638\n    epoch: 91  loss: 0.66344351\n\n\nAbove, since the backward() function accumulates gradients, to avoid mixing up of gradients between minibatches, you have to zero them out beore backpropagating on the next batch. <tt>optimizer.zero_grad()</tt> is used for this. <tt>optimizer.step</tt>  performs a parameter update based on the current gradient (stored in <tt>.grad</tt> attribute of a parameter) and the update rule\n\n\n```python\nimport matplotlib.pyplot as plt\n%matplotlib inline\nplt.plot(range(epochs*4), losses) #epochs*4 because each epoch is made of 4 batches and for each of them a loss is calculated\nplt.ylabel('Loss')\nplt.xlabel('epoch');\n```\n![graph]({% static '/blog/assets/img/intro-to-pytorch/graph.png' %})\n\n**Saving the Model's Parameters**\n\n\n```python\ntorch.save(model.state_dict(), 'IrisDatasetModel.pt')\n```\n\nOnly the parameters of the model are saved and not the model itself. \nFor more information on saving and loading visit https://pytorch.org/tutorials/beginner/saving_loading_models.html\n\n**Loading a Model**\n\nWe'll load a new model object and test it as we had before to make sure it worked.\n\n\n```python\nnew_model = Model()\nnew_model.load_state_dict(torch.load('IrisDatasetModel.pt'))\nnew_model.eval()\n```\n    Model(\n      (fc1): Linear(in_features=4, out_features=10, bias=True)\n      (fc2): Linear(in_features=10, out_features=10, bias=True)\n      (out): Linear(in_features=10, out_features=3, bias=True)\n    )\n\n```python\nwith torch.no_grad():\n    y_val = new_model.forward(X_test)\n    loss = criterion(y_val, y_test)\nprint(f'{loss:.8f}')\n```\n    0.69291312\n\nReferences:\n1. https://pytorch.org/docs/stable/index.html\n2. https://www.udacity.com/course/deep-learning-pytorch--ud188\n3. https://stackoverflow.com/questions/49643225/whats-the-difference-between-reshape-and-view-in-pytorch\n4. https://en.wikipedia.org/wiki/PyTorch\n5. https://www.analyticsvidhya.com/blog/2018/02/pytorch-tutorial/\n6. https://www.analyticsvidhya.com/blog/2019/09/introduction-to-pytorch-from-scratch/\n\n[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1odpZAS42UzVk16TGol_ta24DtyX1ZB2B)",
        "author_name": "Shruthan R",
        "slug": "intro-to-pytorch"
    },
    {
        "layout": "post",
        "title": "An Introduction to Minimax Algorithm",
        "author_github": "Shree987",
        "date": "2020-10-14T10:00:00",
        "image": "/assets/img/",
        "description": "An algorithm to build powerful AI for games.",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Artificial Intelligence",
            "Algorithms",
            "Strategy",
            "Game Theory"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "Shree987",
        "text": "[{% load static %}]()\n# An Introduction to Minimax Algorithm\nEver wondered how a computer is able to play wisely in games like Tic-Tac-Toe, Chess and Go? It all boils down to the ever-evolving Artificial Intelligence (AI). Now, what if I say that building powerful AI for such games is easy? Yes, you read it right. We can design powerful AI that can play games much better than you, me or pretty much everyone. As a first step towards this, I would like to introduce you guys to a pretty simple, yet essential algorithm for this task - **Minimax Algorithm**.\n\n## What is Minimax Algorithm?\n*Minimax* algorithm, sometimes also called *MinMax*, is a recursive or backtracking algorithm which is used to choose an optimal move for a player, assuming that the other player is also playing optimally. It is applicable only to 2-player games, where the opponent\u2019s move is predictable beforehand. For eg. Minimax algorithm can be applied to games like Chess, Tic-Tac-Toe and not for games like Scrabble. It uses recursion to search through the game-tree. \nWhat is a **Game Tree**? Game-tree is the collection of all possible moves in a game that looks more like an inverted tree. To explain in simple terms, it is similar to looking into different possible moves from a particular game board situation and subsequent possible moves after them and so on or saying that \u201cIf I make this move, then my opponent has all these as his/her move\u201d. **Game state** refers to the present board situation. Every possible state is represented as a node in the tree. All the possible moves from a state or node become the children nodes of this node (ie. are diverged from this node). So when all nodes get connected in this manner, it looks like an inverted tree called - *Game tree*. \n\n## Terminology\nTo start with, let us first look at some of the commonly used terminology used for this algorithm:\n* *MAX* : Player whose chances of winning is to be maximised.\n* *MIN* : Player whose chances of losing is to be maximised.\n* *Initial state* : Comprises the position of the board and showing whose move it is.\n* *Successor function* : A function that lists all the valid moves from the current state of board.\n* *Terminal state* : Position of the board at the end of the game. This state determines the outcome of the game as either a win, a loss, or a draw, for MAX\n\n## How does the algorithm work?\nThe main idea is to **maximize the chances of winning for MAX**. How do we do it? For that we assign a value (or weights) to every node of the game tree such that higher its value, the higher the chances of winning for MAX. Now we need to assign the values to the node in such a way that assures this mechanism. So in order for MAX to win, he/she needs to choose the maximum value out of the possible ones. And MIN has to make a move such that MAX is forced to choose the minimum value out of the possible.\nLet\u2019s apply this to an example. Suppose we have a game tree like the one shown. Suppose the weights of the last layer be given to us before-hand (as shown).The rule of the game says that the final value in the red triangle is the points achieved by MAX. So in order to win, MAX has to somehow make red triangle has the maximum value as possible. Also, the weights of the nodes in a layer can take any one of the values from the immediate nodes diverged from it.\n\n![image for Game tree 1]({% static '/blog/assets/img/Minimax-algorithm/tree1.png' %})\n\nEg: Leftmost green triangle can take only 55 or 13, rightmost green triangle can only take 13 or 38 and the middle green triangle can take  43 or 62. The red triangle can take value from any of the green triangle.\n\nSince MAX get to choose the value in the red triangle, MIN has to select values for the green triangle. In order for MIN to win, he/she chooses minimum value possible for each green triangle, thus forcing MAX to choose one among these low values assigned at green triangles. In games with more than 3 layers it is alternate way of maximizing and minimising at a layer of nodes, hence the process called **Minimax** - alternatingly maximising (MAX\u2019s move) and minimising (MIN\u2019s move) the possible values at nodes of the layer in a game tree. So to begin, MIN first fills all the weights (or values) in the middle layer, hence filling the values as follows.\n\n> minimum(55,13) = 13\n\n> minimum(43,62)=43\n\n> minimum(13,38) = 13\n\n![image for Game tree 2]({% static '/blog/assets/img/Minimax-algorithm/tree2.png' %})\n\nThe move of MAX is now restricted by the values in green triangles. In order to win the game, MAX must choose the maximum value possible from the 2nd layer and hence choose 43. The whole process can be simply written as:\n\n> Result= MAX (MIN {55, 13}, MIN {43, 62}, MIN {13, 38}) = MAX {13, 43, 13} = 43\n\nThe final game tree looks like this\n\n![image for Game tree 3]({% static '/blog/assets/img/Minimax-algorithm/tree3.png' %})\n\nNow let's apply this to Tic-Tac-Toe game. This is how a basic game tree of a Tic-Tac-Toe game.\n\n![Image for Tic-Tac-Toe 1]({% static '/blog/assets/img/Minimax-algorithm/tic_tac_toe1.png' %})\n\nThe initial state at the start of the game is the first layer that defines that the board is blank and it\u2019s MAX\u2019s turn to play. So we must try to maximise the value at this layer. The immediate successor layer is played by MIN, and hence must be minimized since we are assuming that both players play optimally. The next layer is again to be played by MAX and hence maximised and the process continues till the terminal states. So we begin by applying the utilities to all the terminal states. But **how do we determine the values at the nodes**? We just know only at the end of the game that a player has won or lost. Also observe that the terminal state (nodes from whom no more nodes branch off) are those where no more move is possible or in simple terms - where the game has ended. Giving weights to these nodes is easy. We need a way to keep the value high if MAX wins and low if MAX loses. These can be done by using the utilities as shown below.\n\nUtilities used are as follows.\n> 1 : MAX wins\n\n> 0 : draw\n\n> -1 : MAX loses\n\n![Image for Tic-Tac-Toe 2]({% static '/blog/assets/img/Minimax-algorithm/tic_tac_toe2.png' %})\n\nThen we start determining the utilities of the higher nodes by maximising/minimising the values obtained from the immediate previous layer, starting from terminal states similar to the way we did for the above example. With the help of this game tree, we can decide the layer to be chosen at the 1st layer to maximize the winning chances of MAX and continue to choose the best branch in order to win.\n\n## Implementation\nHere is a pseudocode for implementing the *Minimax* algorithm.\n```\nfunction minimax(node, depth, maximizingPlayer)\n            if depth = 0 or node is a terminal node\n                   return the utility of the node\n\n            if maximizingPlayer\n                   bestValue := 0\n                   for each child of node\n                        v := minimax(child, next depth, FALSE)\n                        bestValue := max(bestValue, v)\n                   return bestValue  \n\n            else \n                   bestValue := 0\n                   for each child of node\n                          v := minimax(child, next depth, TRUE)\n                          bestValue := min(bestValue, v)\n                   return bestValue\n```\n\n## Bonus \n### Optimisation\nThe Minimax algorithm works well for smaller state space, or in other words, smaller board games. For eg., for a Tic-Tac-Toe game, there are 3<sup>9</sup>=196839 possible board combinations which can be easily calculated. But what about a game like chess? It has roughly ~10<sup>120</sup> possibilities, which is far greater than the number of atoms in the observable universe, and hence not a good idea to dive into. Then how do we tackle such a thing? Optimising the searching in the game tree is a key solution. Alpha-beta pruning is a promising algorithm in which certain sub-trees are cut-off from the game-search tree when there is a chance of choosing another sub-tree over this one. This reduces the state space to a much greater extent. You can read more about Alpha-Beta pruning algorithm from the following links:\n\n[What is Alpha Beta Pruning in Artificial Intelligence? - Medium](https://medium.com/edureka/alpha-beta-pruning-in-ai-b47ee5500f9a)\n\n[Alpha\u2013beta pruning - Wikipedia](https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning)\n\n## References\n\n[Minimax Algorithm with Alpha-beta pruning](https://www.hackerearth.com/blog/developers/minimax-algorithm-alpha-beta-pruning/)",
        "author_name": "Shreeraksha R Aithal",
        "slug": "minimax-algorithm"
    },
    {
        "layout": "post",
        "title": "How are researchers and their research rated?",
        "author_github": "shivanshjoshi",
        "date": "2020-11-12T10:00:00",
        "image": "/assets/img/",
        "description": "Ever wondered how research carried out around us is rated? How is it useful? Read on to find out the answers to these and many such questions",
        "tags": [
            "Research",
            "Researchers"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "shivanshjoshi",
        "text": "[{% load static %}]()\nA lot of research is continuously happening around us. While the word research seems fascinating to most of us, we seldom know how impactful the research is or how the actual impact of any research is measured. \nWhile there are many techniques for adjudging researchers, journals, and research papers, none is absolute. \n\n\nThere are four main ways that google scholar uses to rate a researcher. These are papers, citations, h-index, and i-10 index.\nPapers mean the number of research papers published by a researcher. More number of papers would mean a higher volume of research.\nCitations are the number of times the given research paper is cited or referenced in other papers. Usually, more the number of citations, better is the valuation of that research paper.\n\n\nA researcher's number of citations is the summed number of citations of all his research papers. A good citation number not only depends on the research field but also on the region.\nIn 2005, statistician JE Hersch defined **h-index** (after his name Hersch) to provide \"an estimate of the importance, significance, and broad impact of a scientist's cumulative research contributions.\" \nIt gives an excellent estimate of the research going on in the given field. While a single good paper with a large number of citations means that the quality of research is impeccable, though the number needs to increase, a large number of research papers with low citations mean that lot of mediocre, low bearing research is going on. Hersch combined both of these and put forward the concept of the Hersch index. \nWhile the actual h-index definition might be very complicated, it merely means the number of papers with citations more than the number of papers one has published. An h-index of 40 means out of n papers the researcher has published, 40 have more than n citations. A score of 40 after 40 years of age would mean good research. It is so good a measure that an h-index of 80 would mean exceptional research, and more than 80% of noble laureates in Chemistry and Physics have an h-index at around 80.\n\n\nFinally, the **i-10 index** is another criterion used by Google, which means the number of research papers with citations more than 10.\nA scientific journal is rated based on the **impact factor**. The impact factor is a measure of the frequency of average citations of an article in a journal in a particular year. It measures the importance or rank of a journal by calculating the citations of articles. \nTwo of the highest impact journals are Science and Nature. With over twelve thousand journals, the average impact factor of a scientific journal is less than one. More than three is considered a good, and an impact factor of over ten is exceptional. Nature and Science lead this list with an impact factor of 42.778 and 41.778, respectively.\n\n\nHowever, similar to chess ratings, none of these is absolute. These are all based on other research, region, and recognition. These measure the research about its relevance in contemporary time and industry requirements.",
        "author_name": "Shivansh Joshi",
        "slug": "researchers-and-rating-their-research"
    },
    {
        "layout": "post",
        "title": "Why Python Shouldn't be your First Programming Language",
        "author_github": "nishant-nayak",
        "date": "2021-01-02T22:09:24",
        "image": "/assets/img/",
        "description": "An overview of the shortcomings of Python for programming beginners",
        "tags": [
            "IEEE NITK",
            "Python",
            "Programming"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "nishant-nayak",
        "text": "[{% load static %}]()\n## Introduction\n\nPython is a programming language that has found its way into practically every field of software development. With its immense collection of openly available libraries, easy to learn syntax, and a plethora of available resources to learn from, both online and offline, the knowledge of Python has become one of the most sought-out skills in the industry. The beauty of the language lies in the ability to perform tasks with simple, readable syntax and lesser code to write.\n\n![Python Logo]({% static '/blog/assets/img/why-python-shouldnt-be-your-first/python-logo.png' %})\n\nAll the hype around Python is what attracts a lot of beginners to pick up the language as their first programming language. However, the *pythonic* way of writing code usually abstracts a lot of essential information about the very basics of programming, which is crucial for beginners to understand. Here\u2019s a few points on why Python shouldn\u2019t be your first programming language:\n\n### Variables don\u2019t require a type\n\nIn Python, variables can simply be declared by assigning any value to a variable name of the user's choice. Under the hood, Python stores all variables, irrespective of its datatype, in the form of objects. The Python interpreter then binds names to these objects. This makes the syntax of declaring variables simpler, but it abstracts the way the data is stored in memory. For example, C programming language gives you the option to define the datatype of the variable you are storing, which allows the user to understand how many bytes of data that specific variable is taking up. This enables users to understand how the data is stored at the bit level. For beginners, understanding how memory allocation works for specific data types can help in building good coding habits.\n\n### Pointers do not exist\n\nAnother abstracted feature that deals with memory is pointers. Pointers are invaluable while accessing members of a list-like data structure, creating certain data-structures like linked lists and trees, or passing variables to a function. In Python, there are two types of objects, namely **mutable objects** that can be changed and **immutable objects** which cannot be changed. When a function is called with an object as a parameter, mutable objects such as lists or dictionaries are passed by reference, and immutable objects such as strings(str) or integers(int) are passed by value. This does not give the user the ability to choose how to pass variables to functions. This interaction with memory locations, and the user defined dynamic allocation of memory are important concepts to be familiar with in order to create memory efficient programs.\n\n### Encapsulation isn\u2019t safeguarded\n\nData Encapsulation is one of the key features of object oriented programming. In Python, however, the only encapsulation of features within a class is a convention that is to be upheld (putting underscores before the function name). There is no definition of private and public class members, which means that certain members which should actually be immutable can easily be changed by any function in the program. The concepts behind private, public and protected class members are essential for beginners to fully understand, so that they can properly implement data encapsulation.\n\n### The speed isn\u2019t real, and errors show up at the last second\n\nPython is an interpreted language, and therefore runs much slower than a language like C, which is a compiled language. Due to Python\u2019s interpreted nature, each line of code is executed in succession, which means that if there are any errors in the code it won\u2019t show up until the program executes all the lines of code before the line with the bug. This can be frustrating and discouraging for beginners when they are learning the language.\n\n### Parallel Processing is not supported\n\nCPython, which is the standard implementation of Python, has a feature known as the Global Interpreter Lock (GIL), which essentially locks one flow of execution to hold control of the Python interpreter. This means that two separate flows of execution, more commonly known as threads, cannot be run on the same CPU. Since Python programs run as a single process with a single thread of execution, only one CPU core is used, regardless of the number of cores the CPU contains. This increases the runtime of programs that require heavy CPU computation, when compared to languages which support multithreading.\n\n\nIn conclusion, we can all agree that Python has found good use in many fields of computer science, and it is definitely an asset to learn the language in today\u2019s day and age. Nonetheless, a beginner in programming should consider learning a different Object Oriented Programming language like C++ or Java to gain a better grasp on the core concepts of OOPs. Although this is advice from a programming perspective, if the user\u2019s goal is to learn Python only for a specific application like Machine Learning or Data Science, they can go ahead and learn the language to start off.\n\n## References\n\n- [Python Variable Types](https://www.tutorialspoint.com/python/python_variable_types.htm)\n- [Pointers in Python: What's the Point?](https://realpython.com/pointers-in-python/)\n- [Example of Python's Data Encapsulation](https://www.quora.com/Why-do-some-people-say-that-object-oriented-programming-in-Python-is-a-joke/answer/Antonio-Nesic)\n- [Python - Compiled or Interpreted?](https://www.geeksforgeeks.org/python-compiled-or-interpreted/)\n- [What is the Python Global Interpreter Lock (GIL)?](https://realpython.com/python-gil/)",
        "author_name": "Nishant Nayak",
        "slug": "why-python-shouldnt-be-your-first-programming-language"
    },
    {
        "layout": "post",
        "title": "The Elliptic Curve Cryptography",
        "author_github": "rak108",
        "date": "2021-01-04T21:46:44",
        "image": "/assets/img/rakshitavaradarajan.jpg",
        "description": "An insight into the Powerful Public-Key Cryptography Algorithm using Elliptic Curves.",
        "tags": [
            "IEEE NITK",
            "Theoretical Computer Science",
            "ECC",
            "Cryptography"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "rak108",
        "use_math": true,
        "text": "[{% load static %}]()\n## Introduction\n\n**Elliptic Curve Cryptography (ECC)** is one of today's most powerful but least understood public-key encryption methods which is found upon the mathematical concept of elliptic curves. Compared to other algorithms serving the same purpose, ECC allows for **smaller, faster and more efficient keys while providing the same amount of security**. Elliptic curves cryptosystems are applicable for encryption, digital signatures, pseudo-random generators and other tasks.\n\n## Public-Key Cryptography\n\nThe previous paragraph mentions *public-key cryptography*. This concept (also known as asymmetric key encryption scheme) is a required foundation to understand the ECC concept. \n\nModern Cryptography is based on the idea that rather than having a single key performing both encryption and decryption, a pair of keys can be used; one is the public key known to all and the other is the private key, which is kept secret. The keys are designed and generated in a way that they act complementary to one another. Using this system, the sender can encrypt their message using the receiver's public key, and only the receiver who has the respective private key can decrypt the messages. This efficient design was a great step in resolving the issue of compromise of a single shared key, while also enabling authentication of the encrypted messages by the public-key owner.\n\n![Asymmetric Key Encryption Scheme]({% static '/blog/assets/img/ecc/PublicPrivateKeyEncryption.png' %})\n\n\nThe key-pairs are generated while taking into consideration **\"Trapdoor Functions\"**, which are functions that are easy to compute in one direction, but has a special piece of information or secret (called \u201ctrapdoor\u201d) which is required in order to compute the inverse (or reverse the computation). For example, the fundamental concept of the very widely used RSA algorithm is that while it is easy to multiply two prime numbers, it is difficult to perform prime factorization on this product to obtain the component primes. The greater the difference of ease between computing the Trapdoor Function & the difficulty of reversing Trapdoor functions, the more computational effort will be required to derive the private key from the public key and thus, the more secure the cryptographic system. \n\n## Why ECC?\n\nAlgorithms such as RSA and Diffie-Hellman are not very sustainable in the long term. This is because factoring is not the hardest problem out there. Specialized algorithms, which were devised to solely tackle the problem of prime factorization while being faster and less computationally intensive than the naive approach, get more efficient as the size of the number being factored gets larger. Thus, as the numbers get larger, the difference between computing the product of primes & the difficulty of reversing this multiplication to find the component primes decreases.Thus, this calls for utilizing a better cryptosystem having a better trapdoor function.\n\nAnother issue is the increasing availability of resources to decrypt numbers result in the need to increase the size of the keys. This is not suitable in the case of mobile phones or low-powered devices that have a limited amount of computational power. All this pushes us in the direction of adopting a public-key system which has a better Trapdoor. **Cue ECC.**\n\nElliptic Curve Cryptography is one that is based on the algebraic structure of elliptic curves over finite fields. ECC is used for the same reasons as RSA, with the former having the advantages of providing equivalent security with 256-bit keys as a 3072-bit key of the latter (Latter key is 12 times greater in size!). Thus, ECC takes up only around 10% of the storage space and bandwidth as compared to RSA and thus is a perfect fit for resource-constrained systems such as mobile phones, low-powered devices and cryptocurrency networks. \n\n\n## The better Trapdoor Function\n\nAn elliptic curve is a planar algebraic curve defined by an equation of the form:\n \n$${y^2} = {x^3 + Ax + B}$$ &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; where $${4A^3 + 27B^2 \\ne 0}$$\n \nIt is a plane curve over a *finite field* (a field that contains a finite number of elements) which consists of the points satisfying the above equation along with a distinguished point at infinity. \nA field is a set of elements F on which two operators (addition and multiplication) are defined and satisfy basic rules (such as the operators following the distributive law). The real numbers along with the addition and multiplication operators form a field with an infinite set of elements, and on defining a field where the set of elements F is  finite it is called a finite field. An example of such a finite field is the field formed by elements composed of integers mod p when p is a prime number.\n\n![An Elliptic Curve]({% static '/blog/assets/img/ecc/ec.png' %})\n\nElliptic curves have horizontal symmetry, and thus any point on the curve can be reflected over the x axis and the curve remains the same. Another interesting property is that any non-vertical line (different x coordinates) will intersect the curve in at most three places. When two different points on the curve have the same x coordinate and we draw a vertical line through them, we define the point at infinity to be the *third* point at which the line \u201cintersects the curve\u201d. We can imagine this point as lying infinitely far up the y axis, and all vertical lines intersect that point. The point at infinity has no coordinates, and its purpose is to deal with vertical lines. \n\nThe trapdoor function is similar to a mathematical game of pool. Elliptic curves are free of cusps or self-intersections and thus are completely smooth (non-singular). A curve is said to be non-singular if all of it\u2019s points are non singular. Non-singular point is one at which there is a well defined and unique tangent line, and thus a curve composed of completely this is called a non-singular curve. Now as a line between 2 points *always* intersects at a third point, this allows for quick hops around the curve which is computationally easy, and finally results in an endpoint that seemingly has no relation to the starting point, i.e, it is difficult to reverse the process of arriving at that point.\n\n![Dotting]({% static '/blog/assets/img/ecc/method.gif' %})\n\nTaking this example, let *A* be our origin point. If we take points *A* and *B* and draw the line passing through these two points, it will intersect the curve at the third point say *-C*. Now, by reflecting this point across the x-axis on the curve, we get the point *C*. Now, taking the line passing through the origin point *A* and *C* and repeat the process, say *'n'* times.\n\nThis move on 2 points is called the \"dot\" function. In relation to the above example,\n\n> **A dot A = A**\n>\n> **A dot B = C**\n>\n> **A dot C = D** \n\nTo ensure that the dot function produces a line that hits the curve close to the origin and does not go way off out to some extreme without intersecting at a third point, we can define a maximum 'X' value where the line will wrap back around and start from the beginning again.\n\n**This is a great trapdoor function because as long as you know the origin point (*A*) and the number of hops done, it is easy to find the ending point (in our case, *D*). On the other hand, on only knowing the coordinates of the origin point and ending point, it is nearly impossible to find how many hops it took to get there.**\n\nThis is the concept used in ECC. An ECC system can be designed by selecting a curve equation, a prime number as a maximum, and a public point on the curve (the origin point or *A*). A private key is a number *pkey* (in our case, *n*). The public key is the public point *dotted* with itself *pkey* times. Elliptic curve discrete logarithm function refers to the computation of the private key from the public key in an ECC system.\n\nLet us take the example of Alice and Bob communicating. Initially, they agree on using the same curve and a few other parameters. Then, they pick a random point G on the curve. Both Alice and Bob choose secret numbers *a* and *b* respectively. Alice dots the point G by itself *a* times, while Bob does the same *b* times. Both arrive at new points *A=a*G & *B=b*G, and now they exchange the points. Now both of them dot *A* and *B* with *b* and *a* respectively. In this manner, they both have generated the same shared secret key *S=ab*G, and now can securely communicate with the keys generated.\n\n## Conclusion\n\nDespite 30 years of intensive research, there has been no better algorithm found to solve the EC discrete logarithm problem than the current naive approach, and thus the difference between computing the Trapdoor function and reversing it is great. As it is more computationally intensive, it is a stronger system as compared to the other current ones.\n\n**While RSA can probably still continue to be secure by increasing the key length, the trade-off is speed, as well as space. With ECC, using smaller keys that are generated quicker than RSA, the same security can be achieved which is especially a bonus on mobile phones and other less powerful devices.**\n\nIndeed, it is the next generation of public key systems over first generation current systems like RSA based on prime factorization cryptography. Taking into consideration the extent of today's known mathematics, it provides a 10-fold increase in security and acts as the basis of much of the public key cryptography in today\u2019s cryptocurrency world.\n\n\n## References\n\n- [Basic Intro to ECC](https://qvault.io/2020/09/17/very-basic-intro-to-elliptic-curve-cryptography/)\n- [Implementing the Mathematical Curve](https://martin.kleppmann.com/papers/curve25519.pdf)\n- [Elliptic Curve Theory](https://en.wikipedia.org/wiki/Elliptic-curve_cryptography)\n- [How ECC works](https://www.allaboutcircuits.com/technical-articles/elliptic-curve-cryptography-in-embedded-systems/)",
        "author_name": "Rakshita Varadarajan",
        "slug": "the-elliptic-curve-cryptography"
    },
    {
        "layout": "post",
        "title": "Elegant Algorithms using Randomization",
        "author_github": "Dragonado",
        "date": "2021-01-06T23:00:00",
        "image": "/assets/img/",
        "description": "Finding simple and fast solutions using randomized algorithms compared to deterministic algorithms",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Algorithms",
            "Randomization"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "Dragonado",
        "use_math": true,
        "text": "[{% load static %}]()\n## **Elegant Algorithms using Randomization**\n\nRandomized algorithms are those algorithms that make use of something random (for example a random permutation of an array, a random number generator, etc) to calculate the answer fast with certainty or it gives the approximate answer which on an average case is correct. \n\nLet us skip the philosophical/mathematical question of the ability of a computer to generate something truly random and assume that we have an RNG(random number generator) that provides a number between [a,b] with uniform distribution(for all practical purposes).\n\n## **Why should we use Randomized algorithms?**\n\nAfter all deterministic algorithms are so intuitive and predictable. We can understand their time complexities and predict the output quite easily. So what is the need for randomization?\nWe use randomization to come up with faster algorithms (on an average case) or to avoid highly complicated deterministic algorithms. In some cases randomized algorithms are the only feasible solution to some problems.\n\nLet me show you the beauty of Randomization with a few examples:\n\n## **Problem 1:** \n\nProblem Statement: Given an array of integers of length $N$ (where $N$ is even), you are given that there are $\\frac{N}{2}$ integers that are distinct and $\\frac{N}{2}$ integers that are the same (For example {$1,6,2,6,6,3$}, 3 distinct elements and 3 copies). Find the element that repeats $\\frac{N}{2}$ times. \n\n<u>Deterministic Solution:</u> \nYou can run two nested for loops and check for every $(i,j)$ if $arr[i] == arr[j]$ in which case $arr[i]$ would be the answer. The time complexity of this algorithm is $O(N^2)$ which is not very efficient. \nThis can be improved  to $O(N)$with knowledge of hash-maps or hash-tables or even smart partitioning. But in any case it can be proved that any deterministic algorithm takes at least $O(N)$ time to solve this problem. That is because since our program is predictable we can give the array in such a way that it takes at least $\\frac{N}{2}$ steps because the first $\\frac{N}{2}$ integers are the unique ones. \n\n<u>Randomized Solution:</u> \nThe solution is quite elegant and much more simpler AND faster than the deterministic one. The solution is, take any 2 **random** elements of the array. If they are equal then we have found our repeated element, if they are not equal then just repeat the process again. \nThe algorithm is just:\n\n``` c++\nwhile(true){\n\tint i1 = random_number(0,n-1), i2 = random_number(0,n-1);\n\tif(i1 == i2) continue;\n\tif(arr[i1] == arr[i2]){\n\t\tcout << \"Repeated element is: \" << arr[i1] << endl;\n\t\tbreak;\n\t}\n}\n```\n\nThat's it. No hash-map or any advanced concepts. This quite literally the simplest and fastest algorithm to solve the problem. \nIt's surely simple but why is it fast? I mean in theory this algorithm could run forever, imagine it just kept taking the distinct elements. But lets do some maths and find the probability with which our algorithm takes those distinct integers each time. \nNumber of favourable pairs (pairs if chosen our program would end) = $(N/2)\\cdot(N/2-1)$\nTotal number of pairs = $N^2$. \n\nSo the probability that our program would end in the first iteration is,(say) $P$= $\\frac{N \\cdot(N-2)}{4 \\cdot N^2}$ = $\\frac{1}{4} - \\frac{1}{2 \\cdot N}$ \nwhich is greater than $\\frac{1}{5}$ for all $N \\geq 10$, so $P \\geq \\frac{1}{5}$ which implies that the probability that the program does not end on the first step (say) $Q$ = $1-P$, so $Q$ $\\leq 4/5$\n\nSo we have found out that if we only performed one iteration there is a $80$% chance that we would not get our desired answer. Yikes that's extremely bad.  Well whats the probability that we don't get our answer after 2 steps? Its just $Q^2$ because there is no difference between the first and second iteration. and $Q^2 \\leq 16/25$, so we have a $64$% chance that we don't get our answer within 2 steps. Well its still bad but at least its better. \n\nSo let our program run for 100 iterations. Then $Q^{100} \\leq 2\\cdot 10^{-10}$ which is such a small number that for all practical purposes that this will never happen. So our algorithm is so fast that given an array of $2$ million integers any deterministic algorithms would take at least $1$ million steps to find the answer whereas the above one would require at most $100$ (for all practical purposes). \n\nThere is a separate method for denoting and calculating time complexities of randomised algorithms. But they are not straightforward so for brevity's sake I will skip it.\n\n## **Problem 2:** \n\n[Problem Statement](https://codeforces.com/problemset/problem/665/C)\n\nThe solution is greedy. \nAlgorithm: Let $str$ be the string we have inputted and $N$ be the length of the string. Here is the solution below. Only the replace() function needs to be written.  \n\n```c++\nstr.push_back('$'); // adding an extra special character to cover corner cases\nint ans = 0;\nfor(int i = 0; i+1 < N; i++){\n    if(str[i] == str[i+1]){\n\t\tstr[i+1] = replace(str[i],str[i+2]); //coming with the replace() function is the hardest part\n        ans++;\n    }\n}\ncout << ans << endl;\n```\n\n<u>Deterministic Solution:</u>\nWell that seems easy, let's come up with the replace() function that takes 2 alphabets as arguments and returns an alphabet that is not one of the two. Let the two alphabets be $a_1$ and $a_2$ An easy solution is to just return the alphabet  that comes after max($a_1$,$a_2$). So if $a_1 =$'d' and  $a_2 = $ 'x' then we would choose 'y'. \n\nBut wait this wont work if one of them is 'z' as there is no alphabet after 'z'. Ok, to fix this we could choose the alphabet that comes before both of them but that would not work if one of them is 'a'. We can't choose the middle because $a_1$ and $a_2$ could be consecutive. \n\nAll in all, there is definitely an efficient solution that returns us a valid alphabet but the code/logic would be a bit messy. \n\n<u>Randomized Solution:</u> \nJust choose a random alphabet. If the alphabet is equal to one of the two alphabets then choose again, else we are done and return the chosen alphabet. That's it!\n\n```c++\nchar replace(char a1, char a2){\n\tchar ch = a1;\n\twhile(ch == a1 || ch == a2) ch = 'a' + random_number(0,25);\n\treturn ch;\n}\n```\n\nJust 3 lines of code. It definitely simplified the code and logic. What about running time? We can use the same logic we did for the first example and calculate the probability with which our program wont end in 10 steps. Spoiler: The probability is $(\\frac{2}{26})^{10}$ which is even smaller than the previous example. \n\n[Link](https://codeforces.com/contest/1400/submission/103560014) to my submission\n\n## **Problem 3:**\n\n[Problem Statement](https://codeforces.com/contest/1400/problem/A)\n\n<u>Deterministic Solution:</u> \nSince this a constructive problem there are multiple solutions all of which require some smart non-trivial observation.\nFor example one possible answer, is to just print the alternate characters of the given string. This is definitely a simple and fast algorithm, but you need to prove that the answer string you print will always match with at least one of the characters in each substring.  This might take some effort and time on your side to convince others and yourself that this algorithm works. Running time wise, this is the fastest solution. \n\n<u>Randomized Solution:</u> \nChoose a random binary string of length $N$. For each substring of length $N$ of the original string, check if at least one character matches with the chosen random string. If there exists some substring which has no matching character then we just choose another random binary string. If for each substring our string is valid then we output it. \nMake sure you understand the logic of the above algorithm properly, it can be quite tricky to understand the first time you read it. \n\nBut it is obvious that the string produced by this algorithm is always correct . There is no need of proving to anyone of its correctness. But you need to convince yourself that its fast enough. \nCalculating the time complexity is a little more complicated but it can be done. it is of the order $O(N^2)$ and since $N \\leq 50$ we can be sure that the algorithm will give the answer quite quickly. \n\n[Link](https://codeforces.com/contest/1400/submission/103560014) to my submission. \n\nAll the algorithms discussed above fall under a class of randomized algorithms called [Las Vegas Algorithms](https://en.wikipedia.org/wiki/Las_Vegas_algorithm). These are such algorithms that will always give the correct answer but whose running time depends on the randomness of the variables.\n\nThere is another class of algorithms called [Monte Carlo algorithms](https://en.wikipedia.org/wiki/Monte_Carlo_algorithm), where the running time of such algorithms are fixed but the algorithm may or may not give the correct answer because the answer depends on the randomness of our variables. For example it may give the correct answer $80$% of the time. \n\n## Conclusion\n\nNot all problems can be solved using randomization. But do keep a lookout on those that can optimised/simplified using randomness. This just scratches the surface of randomization. \n\nThere are data structures that use randomness called [probabilistic data structures](https://en.wikipedia.org/wiki/Category:Probabilistic_data_structures)(Monte Carlo type) and other data structures like [Treaps](https://cp-algorithms.com/data_structures/treap.html) (Las Vegas type) that depend on randomness for its fast running time. [Quick Sort](https://en.wikipedia.org/wiki/Quicksort) is one of the fastest sorting algorithms because of the property of random numbers. There is also a class of algorithms called [Genetic Algorithms](https://en.wikipedia.org/wiki/Genetic_algorithm)(Monte Carlo type) that simulate survival-of-the-fittest concept to find the optimal answer. \nSo the scope of this topic is quite large and highly practical.\n\n## References\n\n* [Wikipedia](https://en.wikipedia.org/wiki/Randomized_algorithm)\n\n* Computer Algorithms in C++, Computer Science Press, division of W.H. Freeman, New York, September 1996 (with S. Sahni and Sanguthevar Rajasekaran)",
        "author_name": "Chaithanya Shyam D",
        "slug": "elegant-algorithms-using-randomization"
    },
    {
        "layout": "post",
        "title": "An intuition on how RNN and LSTM works",
        "author_github": "rakki-18",
        "date": "2021-01-07T15:00:00",
        "image": "/assets/img/",
        "description": "An analogy between whiteboards and RNN,LSTM",
        "use_math": true,
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "RNN",
            "LSTM",
            "Machine Learning"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "rakki-18",
        "text": "[{% load static %}]()\n##  An intuition on how Recurrent Neural Networks and LSTM works\n\nLet us first try to understand how RNNs work and how they are used to solve sequence modeling tasks that require the model to store the information seen before.\n\n### Working of RNN\n\nAt time step 0, RNN takes the input X<sub>0</sub>,  predicts an output h<sub>0</sub>, and stores the information in the state S<sub>0</sub>. Now, for the next time step, the RNN takes the current input X<sub>1</sub> along with the information from the previous state S<sub>0</sub> and predicts the output h<sub>1</sub> subsequently storing the information from the previous state and the input in S<sub>1</sub>. This process continues until time T. \n\nYou can notice that at some time step t, state S<sub>t</sub> carries all the information seen until then.\n\n![unfolded RNN]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image1.png' %})\n\n### Analogy between RNN and a whiteboard\nNow let us try to draw an analogy between RNN and a whiteboard. \n\nLet us say we want to do some calculation on a whiteboard, which has limited space. At periodic intervals, we see all the information that we have written on the board until then and then figure out the next step and write some more information on to the board. \n\nWe can notice that after some time, we are bound to overwrite the previous information that we wrote because of the limited space of the board. \n\nNow at some time step t, we would have written so much that there would be no way for us to know what we had written at timestep t - k and we also wouldn\u2019t be able to find how much error was caused due to whatever we calculated at timestep t - k.\n\n![whiteboard]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image2.png' %})\n\n We can see how the state S<sub>t</sub> in RNN is similar to a whiteboard with all the information cramped to limited space and how calculating S<sub>t+1</sub>  from S<sub>t</sub> and the input is similar to writing more information on the filled board.\n\nThis gives us intuition about why RNN fails to perform well when there are strong dependencies on information seen long before. \n\nFor example, if we had a sentence, <br> \n```Alice lives in France. She is an artist. She likes to cook. She speaks``` <br> \nand had to predict the next word, we could easily say that it would be ```French``` but it would be difficult for the RNN model to remember the word ```France``` seen long before and hence fails to predict the correct next word.\n\n### Math behind RNN\nLet us now see the actual math that goes behind RNN.\n\n![math behind RNN]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image3.png' %})\n\nAt a time step t, State S<sub>t</sub> and output y<sub>t</sub> is given by,\n![equations]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image6.png' %})\nwhere  U, V and W are the parameter weights, b is the the bias,\nX<sub>t</sub> is the current input, S<sub>t-1</sub> is the previous state,c is the bias, S<sub>t</sub> is the current state and sigma is the \nsigmoid function of x.\n\n\n### Solving efficiently on a whiteboard\nLet us now try to see how we can solve the whiteboard problem to build an intuition about LSTM.\n\nLet us take a scenario where we have to evaluate the expression,  \n![equations]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image7.png' %})\n\non the whiteboard but the whiteboard has space to accommodate only 2-3 steps.\n\nNow instead of writing something like, \n![equations]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image8.png' %})\nWe could just write,\n![equations]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image9.png' %})\n\nLet us call this selective writing where we are writing only specific information instead of writing everything on to the board.\n\nNow, while reading from the board to make the next step, we only need the information about xy and xz but not yz for the next step. So, we can only read whatever is important for the immediate timesteps and we will call this selective reading.\n\nAfter plugging in the values on the board, we get, \n![equations]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image10.png' %})\nWe can see that we don\u2019t need the  xy or xz values anymore and we can erase them from the board instead of keeping everything on the board like before. We are essentially forgetting specific information that we think is not relevant anymore. We will call this step selective forgetting.\n\nWe saw that by using selective read, write, and forget instead of blindly writing and keeping everything, we can solve the whiteboard problem to an extent. \n\n### Analogy between LSTM and the whiteboard\nLet us now see how LSTM is similar to this technique.\n\n![LSTM]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image4.png' %})\n\nWhile passing the information of the previous state S<sub>t-1</sub> to the next state, we don\u2019t pass the whole information like RNN, instead, we introduce a gate and specify how much percent of each value should be passed similar to the selective writing on the board.\n\nAfter taking in information from the previous state and the current input to calculate the current state, we introduce one gate to specify how much should be read from this state similar to selective reading.\n\n\nFinally, instead of keeping all this information in the current state, we introduce one more gate which specifies how much percent of each value should be kept in the state and how much should be discarded similar to the selective forget that we saw above.\n\nIf the same example,<br>\n ```Alice lives in France. She is an artist. She likes to cook. She speaks ``` <br>\n is given to the LSTM model to predict the next word, it would retain the information ```France``` seen long before using gates and would be able to predict the next word as ```French```.\n\n### Math behind LSTM\nWe will now see the mathematical equations behind LSTM,\n\n![math behind LSTM]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image5.png' %})\nThe previous state S<sub>t-1</sub> is passed through an output gate o<sub>t-1</sub> to get the state\nh<sub>t-1</sub>. These are given by, \n![equations]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image11.png' %})\n\nAn intermediate state is then found from the current input X<sub>t</sub> and the previous state h<sub>t-1</sub> which is then passed through the input gate i<sub>t</sub>\n![equations]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image12.png' %})\n\nThe final state S<sub>t</sub> is then found after applying input gate and forget gate f<sub>t</sub> respectively as,\n![equations]({% static '/blog/assets/img/an-intuition-on-how-RNN-and-LSTM-works/image13.png' %})\n\n\n\n\n### Conclusion\nThus we intuitively understand the architecture of LSTMs and RNNs and then connect it with the actual mathematical equations that go behind these models. We also get an intuitive understanding of why LSTM has a better architecture, and how it helps them in performing better than RNN, and solve the problems like long-term dependencies in RNN to an extent.\n\n\n### References\n1. [Deep Learning NPTEL course](https://nptel.ac.in/courses/106/106/106106184/)\n2. [LSTM colah's blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n3. [LSTM tutorial](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21?gi=640b9d603376)",
        "author_name": "Rakshit P",
        "slug": "an-intuition-on-how-rnn-and-lstm-works"
    },
    {
        "layout": "post",
        "title": "A Beginner's Guide to Microservices",
        "author_github": "gaurang2001",
        "date": "2021-05-14T00:00:00",
        "image": "/assets/img/",
        "description": "An overview of an architectural style quickly taking over the world",
        "tags": [
            "Microservices",
            "Cloud",
            "Systems"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "gaurang2001",
        "comments": true,
        "text": "[{% load static %}]()\nWe live in a world that\u2019s constantly growing. There are new applications and services that come up everyday. Existing ones evolve to constantly meet the demands of today\u2019s world and to keep up with the latest trends in technology. This essentially means that there is hardly any time devoted to the maintenance of such applications and services. Nobody likes down-times. If a service is down for sometime, its users would naturally start looking for alternatives, and with such a huge network of service providers available over the internet, they would quite obviously even find one. This is where Microservices comes into the picture.\n\nThis blog delves into the \u201cWhat\u201d and \u201cWhy\u201d of Microservices in detail.\n\n## What are Microservices?\n\nIf you are not very new to the world of The Cloud or Systems, then you would\u2019ve heard the term \u201cMicroservices\u201d quite frequently. Microservices, as the name suggests, is an architectural style that branches out an application into several small independent applications or services. These individual modules are independently built and deployed. Going by the official definition, \n\n> Microservices - also known as the microservice architecture - is an architectural style that structures an application as a collection of services that are\n> - Highly maintainable and testable\n> - Loosely coupled\n> - Independently deployable\n> - Organized around business capabilities\n> - Owned by a small team\n\nThe microservice architecture enables rapid, frequent and reliable delivery of large, complex applications.\n\nLet\u2019s go through each of these points one by one.\n\n1. *Highly maintainable and testable* - Since a huge application is broken down into several chunks, it\u2019s not only easier to spot bugs, but also fix them without getting the entire application down. It is also possible to scale up or scale down a component easily, since each of them is completely independent of each other. Scaling up is making a component larger or faster to handle a greater load. In contrast, scaling down refers to decreasing your system resources.\n2. *Loosely coupled* - Each of the individual components, as mentioned before, are independent and isolated. They cannot communicate with each other, except through APIs. None of the components have any say on the structure of another communication. This gives the developers working on each of the components complete freedom to build the service as they would like it. A new stack can be added to a service to avail larger benefits at the application level.\n3. *Independently deployable* - Each of the components are independently deployed. I had mentioned earlier that it\u2019s possible to scale up or scale down an individual service. This necessarily means that each component has its own set of dependencies. Two components can also work with different versions of the same dependency. This is possible with the use of containers. Each component is containerised along with its dependencies and configurations and independently deployed. (Containerisation simply means that the application, along with all its dependencies, is put into a \u201ccontainer\u201d and is treated as a single unit during deployment)\n4. *Organized around business capabilities* - Microservices introduce a massive change in the organisational structure of a business group. Since the overall goal is a good user experience, development teams are no longer divided into web teams, systems teams, database teams and so on. Rather, they are cross functional teams that work towards fulfillment of one single functionality.\n5. *Owned by a small team* - A cleaner distribution of functionality into microservices allows for more streamlined organisations with smaller teams.\n\nNow that we know what Microservices are, we need to know why they are required in today\u2019s world. But for that, we first need to know what monoliths are.\n\n## What are Monoliths?\n\nIn software engineering, a monolithic application describes a single-tiered software application in which the user interface and data access code are combined into a single program from a single platform. In simple words, it\u2019s an architectural style that does not treat modules independently in an application. The application is deployed as a whole, and all the different components of an application use the same dependencies, etc. It contains a single executable that performs all of the server-side functions for an application. Let me explain with the help of an example.\n\nLet\u2019s consider an e-commerce application. It can have several components to it - a catalogue of products, an ordering system, a payment system, etc. This would mean there are three different containers, one for each service, deployed independently under the Microservices Architecture. Using the Monolithic Architecture, we would need only one container to bundle up the entire application for deployment. \n\n![Monoliths vs Microservices]({% static '/blog/assets/img/microservices/monolith-vs-microservices.png' %})\n\n## Why Microservices?\n\nWe\u2019ve read in detail about what microservices and monoliths are. But why are microservices preferred over monoliths?\n\nLet\u2019s consider a monolithic application with a huge codebase. A small bug somewhere brings the entire application down. It would naturally be difficult to pin-point the location of the bug because of the huge codebase. It also does not make sense to bring the entire application down because of the bug, when the rest of the application is working completely fine. Issues may also arise when scaling up or scaling down the application. All components of the application would have to use the same versions of dependencies. Any conflicts with dependencies can break the entire application.\n\nNow let\u2019s consider the same scenario with Microservices. A small bug in the codebase need not bring the entire application down, since each component is independently deployed. This also means that bugs are easier to trace and fix. Scaling up and scaling down the application is much easier as each component is bundled together with its dependencies and containerised for deployment.\n\n## Conclusion\n\nAlthough microservices have an edge over traditional monoliths due to the aforementioned reasons, it does come with several drawbacks. As each component is deployed independently and communication between components happens only via APIs, there\u2019s the problem of consistency. It also uses up a lot of resources, as there are multiple deployments, and not just one.\n\nHowever, the reason microservices are preferred over traditional monoliths is because of its ability to cleverly deal with bugs, and ease of maintenance. It may take up a lot of resources, but with cloud providers like Google Cloud, AWS, MS Azure providing resources at a nominal price, it is a welcome trade-off. After all, nobody likes down-times, and hence, microservices are the way to go.\n\n### References\n\n- [What are Microservices?](https://microservices.io)\n- [An Overview of Microservices](https://www.tothenew.com/blog/an-overview-of-microservice-architecture-part-i/)\n- [Monolithic Software](https://www.thorntech.com/2017/12/microservices-vs-monoliths-whats-right-architecture-software/)",
        "author_name": "Gaurang Velingkar",
        "slug": "microservices"
    },
    {
        "layout": "post",
        "title": "Neural Style Transfer",
        "author_github": "dharmicksai",
        "date": "2021-05-21T00:00:00",
        "image": "/assets/img/",
        "description": "Neural Style Transfer",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Style Transfer",
            "Deep Learning",
            "Unsupervised Learning"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "dharmicksai",
        "use_math": true,
        "text": "[{% load static %}]()\n# Neural Style Transfer\n \nNeural Style Transfer is the problem of applying the style of an image/painting onto a content image. For Humans the process of painting is seamless. In this blog we will step by step include components into style transfer model, in order to improve the results, first look into the loss functions which capture style representations, next feed-forward networks which produce styled images given style and content image, Then we will look into style embedding and how this can be used to produce new styled images even for unseen paintings.\n \n## Style and Content loss\n \nAs loss function captures the objective of a model, we shall first look into style and content loss functions. When training CNN model for image classification, each layer captures certain features of the input image .the lower layers capture low-level information whereas the higher-level layers capture the high-level content in terms of objects and their arrangement in the input image but do not constrain the exact pixel values. So we can say that the activations of higher layers contain information about the content of an input image. Gram matrix of a layer contains the dot product between different channel activations of a given layer this contains information on correlation of features extracted by different channels of a layer. The gram matrix contains the correlation between activations of different channels of a layer, this is similar to painting where we see a pattern or certain colour play, So we use gram matrix of lower layers as style representation of an input image.\n \nStyle Representation\n\n$$\n\\Large\n G_{ij}^l = \\sum_k F_{ik}^l . F_{jk}^l\n$$\n \n\n \nHere G<sub>ij</sub><sup>l</sup> is the ij<sup>th</sup> entry in the gram matrix of Layer l. F<sub>i</sub><sup>l</sup> is the activations of i<sup>th</sup> channel in l<sup>th</sup> layer.\nWe use the pre trained VGG16 Network for activations\n \n![Style Reconstruction from Gats et al]({% static '/blog/assets/img/Neural-Style-Transfer/VGG_style.png' %})\n \nWe consider the content representation of the content image to bo the target for content loss and we consider the style representation of style image as a target for style loss.\n \nContent Loss given P<sup>l</sup> is target content activation of layer l:\n \n $$\n \\Large\n \\mathcal{L}_{content}(\\vec{p},\\vec{x},l) = \\frac{1}{2}.\\sum_{i,j}(P_{ij}^l-F_{ij}^l)^2\n $$\n\n \nStyle loss given A<sup>l</sup> is target style representationof layer l:\n\n$$\n\\Large\nE_l=\\sum_{i,j}(A_{i,j}^l-G_{i,j}^l)^2\\\\\n\n\\mathcal{L}_{style}(\\vec{a},\\vec{x})=\\sum_{l=0}^L(w_l.E_l)\n$$\n\nHere w<sub>l</sub> is the weight per layer, the weight decreases from lower layers to higher layers\n\n\n \nTotal Loss given content representation $$\\Large \\vec{p}$$, style representation $$\\Large \\vec{a}$$, styled image representation $$\\Large \\vec{x}$$:\n \n$$\n\\Large\n\\mathcal{L}_{total} = \\alpha.\\mathcal{L}_{content}(\\vec{p},\\vec{x})+\\beta.\\mathcal{L}_{style}(\\vec{a},\\vec{x})\n$$\nHere $$\\large \\alpha$$ and $$\\Large \\beta$$ are weights for content and style loss.\n \nThe brute force method to generate the styled image is to take a noise image and calculate its loss from pre-trained image classifier given style and content image and then apply gradient descent on individual pixels of noise image and repeating this process iteratively till we convert noise image to styled image. This method consumes a lot of time and there are no learned parameters.\n \n## Image Transformation Network\n \n![Image Transformation Network from Jonson et al]({% static '/blog/assets/img/Neural-Style-Transfer/Feed_Forward.png' %})\n \nHere the image transformation network learns to map the content image to the style of the painting image, using this network we can generate images of a particular style. The image transformation network consists of Residual blocks and non-residual convolution layers, for upsampling it uses fractional strided convolution. Batch normalisation takes place after every convolution layer, The output layer uses tanh activation to provide output in the range [0-255] The loss network is used to calculate the respective content and style loss given the style and content image. Using gradient dissent the model is trained in order to decrease the loss. The drawback of this is it can be used to generate the style of a single painting and not multiple paintings, so training new weights for new painting style is not practically feasible as it takes a lot of memory. In the conditional instance normalisation, we will make a few changes to this image transformation network using which we can generalise or use the same weights for multiple styles.\n \n## Conditional Instance Normalisation\n \nWe can see that many paintings share some common features like brush strokes etc, conditional instance normalisation helps us to encode style into an embedding and a common image transformation network for multiple styles. To understand about instance normalisation please look at this blog [link](https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8). We see in this [paper](https://arxiv.org/pdf/1607.08022.pdf) that instance normalisation is better than batch normalisation in image transformation network. For using a single image transformation network for multiple styles, we see the work in this [paper](https://arxiv.org/pdf/1610.07629.pdf), where they found a surprising fact about role of normalisation in transformation network, where scaling and shifting of the instance normalized activations can be used to produce different styles. For each style, the activations of instances normalised layer are scaled and shifted differently and these scaling and shifting parameters represent the style as an embedding in space, we can use this property and can find embedding of different styles and during production for different styles, we can just change these shifting and scaling parameters in order to produce different styles, which majority of the weights remaining the same.\n\n$$\n\\Large \nz = \\gamma_s.(\\frac{x-\\mu}{\\sigma}) + \\beta_s\n$$\n \nSo for each style we can train \u03b3<sub>s</sub> and \u03b2<sub>s</sub> which represents the particular style and can be replaced if we wanted different style. this decreasing the cost of memory.\n \n![Applying Instance Normalisation]({% static '/blog/assets/img/Neural-Style-Transfer/Normalisation.png' %})\n \nThe image transformation network details are given in this [paper](https://arxiv.org/pdf/1610.07629.pdf) For implementing multi-style transfer;\n \n![Image Transformation Network Details]({% static '/blog/assets/img/Neural-Style-Transfer/Network_Details.png' %})\n \nThe main drawback of this is we can not generate a styled image for an unseen style.\n \n## Style Prediction Network\n \n![Image Transformation Network along with style prediction network]({% static '/blog/assets/img/Neural-Style-Transfer/style_prediction.png' %})\n \nThe style prediction network takes the style image as input and produces the style embedding(shift and scale parameters) for the particular style. This helps us to generalise to unseen styles. The style prediction network generates the shift \u03b2<sub>s</sub>  and scaling parameters \u03b3<sub>s</sub> which are described in conditional instance normalisation part. On plotting the embedding of style image we can see that similar styles are close to one another. This model is time,space-efficient and also it can generate stylized images for unseen style, this model can be deployed on mobile and desktop apps and would produce amazing results. For more information refer to this [paper](https://arxiv.org/pdf/1705.06830.pdf)\n \n## References\n- [A Neural Algorithm of Artistic Style](https://arxiv.org/abs/1508.06576)\n- [Perceptual Losses for Real-Time Style Transfer and Super-Resolution](https://arxiv.org/abs/1603.08155)\n- [Instance Normalization: The Missing Ingredient for Fast Stylization](https://arxiv.org/abs/1607.08022)\n- [A Learned Representation For Artistic Style](https://arxiv.org/abs/1610.07629)\n- [Normalization Techniques in Deep Neural Networks](https://medium.com/techspace-usict/normalization-techniques-in-deep-neural-networks-9121bf100d8)\n- [Exploring the structure of a real-time, arbitrary neural\nartistic stylization network](https://arxiv.org/pdf/1705.06830.pdf)",
        "author_name": "K Dharmick Sai",
        "slug": "neural-style-transfer"
    },
    {
        "layout": "post",
        "title": "Metamaterials",
        "author_github": "Ananya",
        "date": "2021-01-27T21:29:44",
        "image": "/assets/img/",
        "description": "An introductory post on Metamaterials",
        "tags": [
            "IEEE NITK",
            "Piston",
            "Materials"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "Ananya",
        "text": "[{% load static %}]()\nMetamaterials are engineered materials or artificially structured composite materials that are made to possess properties absent in conventional materials. The internal microstructure of these materials is what makes these stand apart from naturally occurring materials. The properties of these materials depend on the structure rather than the composition of the material. The structure of the material can be tailored and tuned on a microscale level to suit the required application. The material is arranged in a recurring fashion at scales smaller than the wavelength of the waves used in that particular application. Upon precise engineering, the material can be made to interact with waves in a way not observed in bulk materials. Some of the applications of metamaterials:\n\n## 1.\tElectromagnetic metamaterials\n**Negative refractive index** - Refractive index refers to the ability of a material to deflect light. Conventional materials have a positive refractive index. In negative refractive index materials, light bends in a direction opposite to that of the incident beam after entering the medium. The negative value of the refractive index is due to the negative permeability and permittivity of the material and is known as double negative materials (DNG). The structure comprises of periodically arranged split-ring resonators. Unlike regular materials, these materials follow the left-hand rule of electromagnetism. These materials have unusual electromagnetic properties and hence makes a \"perfect lens.\" A metamaterial with a refractive index of -1 can focus light with minimum change in the resolution even if it is the form of a parallel-sided slab.\nImaging below 200nm is not possible using optical methods, but with the help of superlens, which is a metamaterial application, imaging below the diffraction limit, i.e., subdiffraction-limit imaging, is possible. The superlens can be a more cost-effective and high-resolution alternative to electron microscopes.\n ![Figure 1]({% static '/blog/assets/img/metamaterials/figure1.jpg' %})\n*Figure 1: a) Positive Refractive Index material; b) Negative refractive Index material; c) A parallel-sided slab with a Negative Refractive Index behaves as a perfect lens.*\n\n**Optical Cloaking** - Cloaking refers to the usage of metamaterials as an invisibility cloak. Conventional material scatters light and hence is visible to the human eye. An object can be said to be \"invisible\" if it does not absorb, scatter, or reflect any electromagnetic radiation, i.e., it does not disturb the existing electromagnetic field. A cloaking device bends radiation around the object, making it undetectable. Metamaterials can act as cloaking devices over certain electromagnetic ranges. But the application in this field is limited as a fully functional cloaking device in the visible light range has not yet been achieved.\n \n ![Figure 2]({% static '/blog/assets/img/metamaterials/figure2.jpg' %})\n*Figure 2: Left) An object placed in an electric field disturbs the field lines; Right) A cloaked object does not disturb the field lines making it \"invisible.\"*\n\n## 2. Seismic metamaterials\nSeismic metamaterials are earthquake dampeners, which dampen the effect of seismic waves on infrastructure and other objects. They work on a similar principle as the electromagnetic metamaterials, i.e., they make the object invisible to the seismic waves. These materials, when cloaking around an object, deflect the waves away from the object. A densely populated forest is a natural example of a seismic metamaterial. \n ![Figure 3]({% static '/blog/assets/img/metamaterials/figure3.jpg' %})\n*Figure 3: An illustrated representation of a structure being made \"invisible\" to seismic waves with metamaterials*\n\n## 3.\tAcoustic metamaterials \nAs the name implies, acoustic metamaterials can control and manipulate sound waves. Similar to electromagnetic metamaterials, acoustic metamaterials have negative values for bulk modulus and mass density, which does not allow sound wave propagation. This property can be exploited to cancel or concentrate on sound waves. The periodic arrangement of unit cells gives the material new bulk properties, as mentioned above. They can trap and slow down sound waves as used in sound isolation. This method can be widely used for medical purposes. E.g.: - Metamaterials can be utilized to focus high-intensity ultrasound on destroying tumours in inaccessible areas in the body. Acoustic metamaterials are a good substitute for conventional acoustic materials at low-frequency ranges since, at low frequencies, the wavelength is higher than the effective depth of the material, thus reducing the performance of regular materials. Acoustic metamaterials are further being implemented in medical imaging, soundproofing and noise cancellation, sonar dampening, stealth technology, cloaking, and frequency filters.\n ![Figure 4]({% static '/blog/assets/img/metamaterials/figure4.jpg' %})\n*Figure 4: An illustration of an acoustic metamaterial used to trap and slow down sound waves*\n\nResearch in metamaterials is an emerging interdisciplinary field and has tremendous potential in various fields in the forthcoming years. From camouflage and stealth in military operations to noise isolation to even something like the invisibility cloak from Harry Potter can be envisioned with the aid of metamaterials. Something which was only witnessed in science fiction movies and novels is closer to reality than we imagine.",
        "author_name": "Ananya P",
        "slug": "metamaterials"
    },
    {
        "layout": "post",
        "title": "What is Reinforcement Learning capable of?",
        "author_github": "Aryaman2912",
        "date": "2021-05-07T17:41:00",
        "image": "/assets/img/",
        "description": "Applications of reinforcement learning in games, robotics and multi-agent systems",
        "tags": [
            "Reinforcement Learning",
            "Games",
            "Robotics",
            "Multi-Agent Systems"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "Aryaman2912",
        "text": "[{% load static %}]()\nHave you ever been so lazy that you thought, \u201cit would be great if there wes a robot to get me food from the kitchen\u201d? Or have you ever wondered what it would be like if there were robots to do your daily chores for you? Well, the good news is, it is indeed possible to make such an intelligent robot. But if we are to achieve this, reinforcement learning will play a big part in it. In this blog, we will explore the potential of reinforcement learning and how it can be useful in various fields. \n\n## What is Reinforcement Learning?\n\nMachine learning can be broadly classified into three paradigms - supervised learning, unsupervised learning, and machine learning. In supervised learning, we are given input and output data using which we derive a function between the input data and output data. This function is then used to make predictions on sets of unseen input data. Supervised learning is used in computer vision, speech recognition, etc. In unsupervised learning, we are not given the output labels in the training data. Instead, the task is centered around finding patterns and trends in the data. Unsupervised learning is instrumental in clustering, targeted advertisements, etc. The third paradigm is what we are most interested in for this blog. Reinforcement learning is concerned with how an agent takes actions in an environment to maximize the total reward. Reinforcement learning(RL) differs from supervised learning in that it does not need labeled input and output pairs, and it does not need suboptimal actions to be corrected by any human influence. RL is similar to unsupervised learning as the agent tries to find patterns in the environment. However, the goal in RL is not just finding patterns, but also finding an optimal policy that maximizes the reward. This makes it different from unsupervised learning. Also unlike most unsupervised learning problems, we are not given any data apriori, instead, we need to generate it by interacting with the environment.\n\nTypically, we have an agent following a policy/strategy in an environment, using which it takes actions/decisions to improve its cumulative reward over the interaction with the environment. Reinforcement learning tasks are usually modeled as Markov Decision Processes(MDP). This consists of:\n* a set of states for the environment and the agent, S\n* a set of actions for the agent, A\n* probability of transitions from one state to another based on the current state and the action taken\n* a reward function to decide the reward that needs to be assigned on a transition from one state to another when a particular action is taken\n\nThe eventual goal for the agent is to maximize its overall reward. It does this by balancing exploring new states and exploiting already available knowledge.This is popularly known as the [exploration-exploitation dilemma](https://towardsdatascience.com/intro-to-reinforcement-learning-the-explore-exploit-dilemma-463ceb004989).\n\nReinforcement learning can be applied to many fields, including game theory, robotics, multi-agent systems, economics, and many more. Let us now explore a few interesting applications of reinforcement learning.\n\n## Applications\n\n### Games\n\nIn all the applications of RL, we have made the most progress with games. This is partially because the rules are predefined, and there is not much room for unexpected activities happening, unlike in real life. Nevertheless, we can take the learnings and concepts from the successes in games and use them as a foundation in other real-life tasks. In 2016, [Deep Mind's AlphaGo](https://deepmind.com/research/case-studies/alphago-the-story-so-far) defeated the Go world champion Lee Sedol 4-1 in a convincing victory. In the second game of the five-match series, AlphaGo made an unexpected move([Move 37](https://www.wired.com/2016/03/two-moves-alphago-lee-sedol-redefined-future/)) that had a probability of 1 in 10000 of being made by humans. Most people initially thought that the move was bad. But later, this turned out to be an important factor in swaying the game towards AlphaGo\u2019s. So AlphaGo could find moves that humans generally ignored.This gives us an insight into what reinforcement learning and artificial intelligence can offer us as we improve the technology and algorithms. If we can use reinforcement learning for building tools that can help us in real life, those tools will be able to help us resolve problems and issues in a way that might have never been imagined by humanity. On the other hand, in game 4 of the above mentioned series, the machine made some poor moves and ended up losing the game in a way that made AlphaGo look very silly. This shows that we still have a long way to go in making our systems more robust and error-free. Like AlphaGo, there are other machines like [AlphaZero](https://en.wikipedia.org/wiki/AlphaZero) (for chess, shogi, and go) and [AlphaStar](https://en.wikipedia.org/wiki/AlphaStar_(software)) (for the strategy-based video game StarCraft II). Both these machines use deep reinforcement learning techniques and have achieved success to a high degree. It is also worth mentioning [Agent57](https://deepmind.com/blog/article/Agent57-Outperforming-the-human-Atari-benchmark), the deep reinforcement learning agent that plays the Atari57 suite of games. Agent57 has successfully outperformed the human atari benchmark.\n\n### Robotics\n\nRobotics is a more exciting field than games because, unlike gaming, it applies a lot to the real world and the challenges faced in robotics are harder to solve. In principle, RL can enable real-world robots to acquire large repertoires of skills. More importantly, reinforcement learning can allow such systems to continuously improve the proficiency of their skills from experience without much human intervention. Agents trained using reinforcement learning have the potential to adapt to unstructured and very complex environments. This is because, in a real-world scenario, the agent might not know how the environment is going to be, and hence, it has no data. The agent must be able to collect data on its own and then learn to behave accordingly. In such cases, it is not possible to use supervised and unsupervised algorithms as they usually work well when there is enough data available. Reinforcement learning allows the agent to learn to behave in situations accordingly based on its experience.\n\nRL today has mostly been used for simplistic and less cost consuming tasks. This is important to note because trying to apply RL directly to high level tasks might result in extreme damages either in financial terms or for humans. Hence, it is important to master the little things first before moving to more ambitious ideas. [Kormushev et al](https://kormushev.com/papers/Kormushev_Humanoids-2010.pdf) demonstrates a robot that learns archery using reinforcement learning algorithms. [Kormushev et al](http://vigir.missouri.edu/~gdesouza/Research/Conference_CDs/IEEE_IROS_2010/data/papers/1230.pdf) also uses the state-of-the-art EM based PoWER algorithm for learning the pancake flipping task. It is very likely that reinforcement learning will become a very important tool in improving robotic systems because algorithms are always ever improving. [This](https://www.youtube.com/watch?v=O4WV1WP6bTk) video offers a small insight into the work and experiments being conducted on robotic reinforcement learning in UC Berkeley.\n\nA popular and commonly used approach for robot learning is simulation to real-world transfer. In this, an agent is first trained in a simulated environment and then let out into the real world after learning appropriate behaviors. However, the policies trained entirely in simulation may fail to generalize on a robot in the real world. It is hard to simulate the real-world exactly in a simulation because there is always a degree of unpredictability in the real-world which may not be modeled in the simulation. Physical factors like friction, air resistance, humidity, etc are hard to simulate. Even though these can be simulated in good simulators, in real-world, these factors are dynamic which makes training much harder. This gap in an impressive performance in simulation and poor performance in the real world is known as reality gap. Training robots with reinforcement learning directly in the real world eliminates these problems. However, this method comes with its own set of challenges. \n\nChallenges associated with real-world robotic reinforcement learning are:\n* As mentioned earlier, an RL task is usually modeled as an MDP. In such a task, all the states are reset after an episode, and the agent is allowed to attempt the same task again. In a real-world environment, such a reset is not possible.\n* The states are not directly available in a form suitable for applying in an MDP. The agent usually has cameras, motion sensors, etc. The data obtained from it must be converted to a state ideal for MDPs.\n* Typically, rewards are expressed as a function of states or are given by an external supervisor. However, it is more effective if the agent can infer rewards from its sensory inputs.\n\nSolving these challenges is essential to have scalable, efficient, and practical robotic systems.\n\nSo how do we build systems for real-world robotic RL? We need three things:\n\n* **Reset free learning**: Essentially, this means the agent should be able to learn the optimal policy without needing to reset all the states after each episode. This can be done by learning a \u2018perturbation controller\u2019 which changes the agent\u2019s state so that it is never stuck in a particular state for too long, which can happen in some cases, including when it reaches the goal. Also, once the task is completed, the agent is then transferred to a state that has been less explored. \n* **Unsupervised learning for state estimation**: Findings and observations taken from the cameras and other sensors are often hard to use in reinforcement learning and slow the entire process. The algorithms would require data in numerical terms such as coordinates of the agent but this is not easy to extract from cameras and other sensors due to the fact that these are usually higher dimensional. In order to resolve this issue, we use unsupervised learning techniques to condense images into only the necessary features. This can then be fed into the reinforcement learning algorithm for learning the optimal behavior.\n* **Classifier-based rewards**: To minimize external influence in the learning process, we need to have a system that assigns rewards to itself based on a human\u2019s desired outcome. Once these outcomes are given, the agent can self assign rewards based on the probabilities of the agent reaching its goal state from the current state.\n\nOnce these three ingredients are put together, we can have a robotic system that works well without human interference. This system can learn to perform several tasks without instrumentation. Although these systems are currently able to perform limited and simplistic tasks, it is important to note that the concepts laid above are essential to building any robotic systems of any desired complexity. \n\n### Multi-Agent Systems\n\nUntil now, what we saw was restricted to single-agent systems. We had only one agent in the environment, and its goal was to maximize its own reward and find the best policy. If we can succeed in creating intelligent robotic systems using reinforcement learning, it is very likely that we will also be inclined towards creating a system where there are multiple robots focussed on one task or various tasks. This is where the concept of multi-agent systems comes in. Multi-agent systems consist of multiple autonomous entities (artificial agents or humans or both) with computational abilities interacting in an environment with distributed/decentralized information with similar, conflicting, or mixed interests. Typically, these agents refer to software agents. When it comes to reinforcement learning, we attempt to make the agents intelligent and study how agents can collectively learn, collaborate, and interact with each other in an environment. RL in multi-agent systems is also known as Multi-Agent Reinforcement Learning (MARL). MARL is a domain that has existed for many years. But the progress that has been made in deep RL (via powerful and scalable function approximators like neural networks) has catalyzed work and progress in this field. \n\nMARL has a lot of potential for real-world applications. One area of application is [resource allocation and task allocation](https://www.scitepress.org/papers/2017/63934/63934.pdf). MARL can help in devising optimal strategies to divide available resources among different agents/processes. This could potentially offer a new insight into optimally utilizing computing resources. Another application is for [traffic signal control](https://arxiv.org/pdf/1903.04527.pdf). AI-based approaches have been investigated in order to optimize traffic flow in junctions. MARL offers a solution to minimize traffic delays using efficient strategies to allocate time for each lane of an intersection in a traffic network. MARL has also seen improvement in games like Dota, Starcraft,  Go, etc. MARL could also be applied for the task of [energy sharing optimization](https://arxiv.org/pdf/1810.03679.pdf). [Youngwoon Lee et al](https://arxiv.org/pdf/1911.07246.pdf) applied MARL to robotics to assemble IKEA furniture but this direction is currently beyond our scope.\n\nThese applications show not only the power that MARL possesses but also the diverse nature of problems that it can be applied to. However, MARL comes with its own set of challenges and hurdles. \n\nChallenges with MARL:\n* In single-agent RL, the environment reacts (via observations and rewards) in response to just a single agent's actions. However, when there are multiple agents, it depends on the actions of all the agents. This introduces a notion of non-stationarity where if an agent just myopically optimizes its own objective function without taking into account the behavior of the other agents, it generally fails. This is one of the core problems with RL in multi-agent systems and is difficult to tackle.\n* In order to resolve the above issue of non-stationarity, it is possible to have a single policy that can give the actions for all the agents. That way, we can keep the policy of an agent stationary. But in this case, the number of state-action pairs increases exponentially, and hence the algorithm slows down drastically. Also, one of the key aspects of multi-agent systems is that the agents should be able to take actions via decentralized information available to them in the form of their local observations. This gets violated if we have a single global policy that takes actions for all agents. \n\nIn recent years, there have been a few positive results towards solving the above mentioned issues. For example, [Thanh Thi Nguyen et al](https://arxiv.org/pdf/1812.11794.pdf) have offered solutions to the non-stationarity problem using Lenient Deep Q-Network or LDQN. \n\n## What Next for Reinforcement Learning?\nIn this blog, we have seen the applications of reinforcement learning in just three areas. However, reinforcement learning can be applied to a lot more fields, including stocks and finance, medicine, and others. When compared to the other paradigms of machine learning, we can say that reinforcement learning is still in its early stages, and that leaves it with a lot of opportunities that can be explored by scientists and researchers. We have seen in history that given enough time, any problem in science can be overcome if there is enough research going into it. It is no different with RL. We will continue to see a lot of progress in this field, and a future where robots are a part of our everyday lives is waiting for us.\n\n## References\n* [AlphaGo: The story so far](https://deepmind.com/research/case-studies/alphago-the-story-so-far)\n* [The Ingredients of Real World Robotic Reinforcement Learning](https://bair.berkeley.edu/blog/2020/04/27/ingredients/)\n* [Deep Reinforcement Learning for Multi-Agent Systems: A Review of Challenges, Solutions and Applications](https://arxiv.org/pdf/1812.11794.pdf)",
        "author_name": "Aryaman M",
        "slug": "applications-of-rl"
    },
    {
        "layout": "post",
        "title": "Water-Jet Guided Laser Cutting",
        "author_github": "Mallikarjun",
        "date": "2021-02-10T21:46:44",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Piston"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "Mallikarjun",
        "text": "[{% load static %}]()\n## Laser\n\nThe term 'Laser' originated as an acronym for **Light Amplification by Stimulated Emission of Radiation**. The difference between a Laser and other Light sources is the coherence. The spatial coherence of lasers allows them to be focused on a tight spot, enabling applications such as lithography, laser printing, refractive surgery, and of course, Laser Cutting.\n\nLaser cutting is one of the most popular applications of Lasers. Compared to conventional mechanical cutting techniques, laser cutting is more accurate, eco-friendly, and efficient.\n\n## Water-Jet Guided Laser Cutting\n\nIn Water-Jet Guided Laser Cutting, the laser beam used is guided to the workpiece through a high-pressure water jet. The water jet functions like an optical surface, resulting in total internal refraction of the laser due to the difference in refractivity index of water and air. Similar to an optical fiber, the beam is reflected many times on the surface of the jet before reaching the workpiece.\n\n![Diagram for the process]({% static '/blog/assets/img/water-jet-guided-laser-cutting/diagram.png' %})\n\n_Figure 1_\n\nThe nozzle diameter for a typical water jet ranges between 20\u00b5m and 150\u00b5m. The pressure of the filtered, deionized, degassed water can reach 600bar on smaller nozzles. The nozzles are made from diamond or sapphire. Degassed and deionized water is used to minimize energy absorption within the water jet. The laser beam gets reflected at the interface of water and air as it comes into contact with the pressurized water exiting the nozzle. The laser on reaching the workpiece causes it to melt and vaporize. The water jet also assumes a secondary role here, cooling the workpiece to reduce the generation of harmful fumes.\n\nLeading commercial lasers used in materials processing applications include carbon dioxide (CO2) laser, Nd-YAG laser, ytterbium (Yb) doped fiber laser, ytterbium (Yb) doped disk laser, excimer lasers, and high-power diode lasers.\n\nInitially, the waterjet guided laser was developed to reduce the Heat Affected Zone (HAZ). The use of a water jet instead of a conventional assist gas stream led to several additional benefits. The divergence of the beam is reduced, which proves advantageous when cutting metals thicker than 1mm. There is an immediate cooling of the cut, and the high momentum of the water jet efficiently removes the molten materials, resulting in a clean, burr-free cut.\n\nThis technology has many advantages over conventional laser cutting and water jet cutting. Focal optics are not required in this technology. The water jet&#39;s mechanical force is significantly less compared to that of conventional jet cutting (0.1N as opposed to 1-5N), and thus the scathing of the workpiece surface is negligible.\n\n## Limitations of Water-Jet Guided Laser Cutting\n\n1. The laser beam&#39;s high-power density is attenuated dramatically by the water jet, reducing the efficiency of this method, which is particularly visible when cutting thick materials.\n2. It is challenging (and expensive) to obtain degassed and deionized water without air bubbles.\n3. The machining of the nozzle hole has to be extremely precise to avoid the laser ablating the nozzle.",
        "author_name": "Mallikarjun G M",
        "slug": "water-jet-guided-laser-cutting"
    },
    {
        "layout": "post",
        "title": "Gramophone Gaana - Old Is Gold",
        "author_github": "akashbharadwaj2001",
        "date": "2021-05-07T11:13:00",
        "image": "/assets/img/",
        "description": "Music. Who doesn\u2019t love music? Hanging around with headphones on, earphones hooked to the ears and people travelling with volume up is common these days, but these audio devices weren\u2019t a fashion before 20 years.",
        "tags": [
            "IEEE NITK",
            "Diode",
            "Audio recording and reproduction"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "akashbharadwaj2001",
        "text": "[{% load static %}]()\n![image-3]({% static '/blog/assets/img/gramophone-gaana/3.png' %})\n\nMusic. Who doesn\u2019t love music? Hanging around with headphones on,\nearphones hooked to the ears and people travelling with volume up is\ncommon these days, but these audio devices weren\u2019t a fashion before 20\nyears. You might have heard your parents instinctively say \u201cHey, change\nthe plate\u201d when they don\u2019t seem to like the song you are playing only to\nrealize in a moment that it can be done with a single touch. Yes, they\nwere referring to a gramophone when they said \u2018plate\u2019 which you might\nhave seen in old movies. The sensation it created in the 20th century\nand its resurgence in recent times after being replaced by digital\ntechniques makes the study of these outdated audio devices interesting.\n\n## The Basic Idea\n\n![image-1]({% static '/blog/assets/img/gramophone-gaana/1.png' %})\n![image-2]({% static '/blog/assets/img/gramophone-gaana/2.png' %})\n\nSound is nothing but disturbance they create in the\ntravelling medium. The basic idea of the early record players is to\nstore these disturbances mechanically and use them to reproduce the\nsound on a later date. First ever gramophone invented by Thomas Edison\nstored these disturbances in a wooden cylinder wrapped with metal foil\nby the means of tiny grooves whose depth varied in accordance with the\nloudness of sound. While reproducing the sound a lightweight mechanical\nbar like structure traverses over the grooves to produce small\nvibrations which was amplified by a horn.\n\n## Parts of a Gramophone\n\nLater these cylinders were replaced with flat\nvinyl discs, also called as \u2018plates\u2019 to make it compact and the\ngramophone evolved to take the name of record player. A turnable, stylus\nand a tonearm make up the skeleton of a typical record player. As its\nname gives out the clue, turnable is responsible for rotating the vinyl\ndiscs at a desired speed and the tonearm is the bar which terminates\nwith stylus and picks up the microscopic vibrations. Stylus is a needle\nlike structure, made with hard substance like diamond which is in\ncontact with the grooves of vinyl disc.\n\n## Working\n\nWhile recording the sound, stylus is placed on the inner end of\nthe vinyl disc. Sound waves are directed to the tonearm and\nsimultaneously disk is rotated. The radius of the position of the stylus\nis gradually increased so as to fill the entire disc with grooves\nrepresenting the sound. While reproducing the recorded sound, the stylus\nis placed on the disc at the position corresponding to our desired start\nof the sound and rotated with the same speed. The tonearm vibrates due\nto varying depth of grooves and the vibration of the tonearm is\namplified by the horn to produce desired loudness. The original disc is\npressed against an empty disc to create a master copy (negative) making\nit favorable for mass production.\n\n## Conclusion\n\nThough these devices are bulky and have many limitations of\ntheir own over digital, the reason they are making a sudden revival in\nrecent times is due to their openness of sound allowing more features of\nmusic to be heard. The regaining popularity of the device among the\nmusical community is reflected by the drastic increase in the sales of\nvinyl.\n\n## Sources\n\n[How Do Gramophones Work? : Techwalla](https://www.techwalla.com/articles/how-do-gramophones-work)\n\n[How record players and gramophones work - Explain that Stuff](https://www.explainthatstuff.com/record-players.html)",
        "author_name": "Akash S Bharadwaj",
        "slug": "gramophone-gaana"
    },
    {
        "layout": "post",
        "title": "My Two Snowflakes",
        "author_github": "erin-sam-joe",
        "date": "2021-05-07T17:41:00",
        "image": "/assets/img/",
        "description": "There are plenty of resources that say that no two snowflakes are alike. But where do we get this idea from? The universe is really old, with countless planets, surely two of them have been alike\u2026",
        "tags": [
            "IEEE NITK",
            "Piston"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "erin-sam-joe",
        "text": "[{% load static %}]()\nThere are plenty of resources that say that no two snowflakes are alike. But where do we get this idea from? The universe is really old, with countless planets, surely two of them have been alike\u2026\n\nWilson Bentley, an American meteorologist and photographer, worked with snowflakes for 50 years, without getting married, taking detailed photographs of snowflakes and recording their features. He called them \u2018masterpieces of design\u2019, but there doesn\u2019t seem to be any design in any of them, but that does not make them any less amazing.\n\n![image1]({% static '/blog/assets/img/my-two-snowflakes/image1.png' %})\n\n\nEvery plate, every branch, every needle on a needle on a needle, all of those details, are what is called emergent properties. This is complexity that is based on very simple rules, and for snowflakes, those rules go back to the basic laws of physics. In the air, or in a liquid, water molecules are zipping around, bouncing off each other and every else trillions of times per second, and we have no way of knowing where they are or what direction they are facing at any moment. As we remove heat, it gets colder, and those water molecules start to slow down, eventually their atomic attraction, the actual hydrogen bonds between water molecules take over,  and they settle into order. Sounds complicated, but we just call that \u2018freezing\u2019. \n\n![image2]({% static '/blog/assets/img/my-two-snowflakes/image3.png' %})\n\nThe structure of a snowflake can be found in just 6 water molecules. It is known that  the angle between any two hydrogen is about 105 degrees. For some of those water molecules, the other hydrogen is behind them. Just like that, we\u2019ve uncovered the six-fold symmetry of a snowflake crystal. That crystal starts as a tiny speck of dust, or pollen, which catches water vapor out of the air and eventually forms the simplest of snowflake shapes: tiny hexagons called diamond dust and then randomness kicks in. Till we get the intricate and delicate structures that we know and love. \n\n![image3]({% static '/blog/assets/img/my-two-snowflakes/image2.png' %})\n\nDepending on temperature and humidity, and a variety of factors that are not fully understood yet, those simple hexagons can give rise to seemingly infinite shapes. Each snowflake will travel through different air current and bump into different water molecules. But, in 1998, Nancy Knight claimed to find two identical snowflakes and they do look very much alike. But they cannot be alike and physics tells us why. \n\nWe know that water molecules are made of two hydrogen atoms and an oxygen atom, but not every hydrogen is created equally. If we go back to the Big Bang, we find out that out of every million or so hydrogen atoms created, a couple hundred of them, instead of just being a proton and an electron, are holding on to a neutron. This is the isotope of hydrogen called deuterium. In Earth\u2019s water, even in you, about one in 3,000 molecules will be holding onto deuterium instead of hydrogen. Out of the millions of molecules that make up a snowflake, a lot of them will be holding onto deuterium. Even identical looking snowflakes are not the  same. \n\nNow you could love a snowflake just because it\u2019s pretty,  but it doesn\u2019t take away from its beauty that it was sculpted by chance and physics. They\u2019re ordered, but they\u2019re created in disorder, every random branch re-tells their history, that singular journey that they took to get here, and most of all they\u2019re fleeting and temporary.",
        "author_name": "Erin Sam Joe",
        "slug": "my-two-snowflakes"
    },
    {
        "layout": "post",
        "title": "ConvNets: What, Why and How?",
        "author_github": "shreyasrao114",
        "date": "2021-05-14T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Diode",
            "Machine Learning",
            "Deep Learning"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "shreyasrao114",
        "text": "[{% load static %}]()\n![img-1]({% static '/blog/assets/img/convnets-what-why-how/img1.jpg' %})\n\nA Convolutional Neural Network is a Deep Learning algorithm which can take an input image, assign learnable weights to various aspects/objects in the image and be able to differentiate one from the other.\n\nIt is based on the convolution mathematical operation. Convolution layers consist of a set of filters that are just like two-dimensional matrices of numbers. The filter is then convolved with the input image to produce the output. In each convolution layer, the filter slides across the image to perform the convolution operation. The main agenda of the convolution operation is matrix multiplication of the filter values and pixels of the image, and the resultant values are summed to get the output.\n\n## But why ConvNets over traditional neural networks?\n\n### Reason 1: Images are Big\n\nImages used for Computer Vision problems nowadays are often 224x224 or larger. Imagine building a neural network to process 224x224 colour images: including the 3 colour channels (RGB) in the image, that comes out to 224 x 224 x 3 = 150,528 input features! A typical hidden layer in such a network might have 1024 nodes, so we&#39;d have to train 150,528 x 1024 = 150+ million weights for the first layer alone. Our network would be huge and nearly impossible to train.\n\n### Reason 2: Positions can change\n\nIf you trained a network to detect dogs, you&#39;d want it to be able to a detect a dog regardless of where it appears in the image. Imagine training a network that works well on a certain dog image, but then feeding it a slightly shifted version of the same image. The dog would not activate the same neurons, so the network would react completely differently! Moreover, objects in images are made up of small, localized features, like the circular iris of an eye or the square corner of a piece of paper. It seems wasteful for every node in the first hidden layer to look at every pixel.\n\n## So let&#39;s look at the architecture of a CNN model. It typically comprises of three types of layers\n\n### 1. Convolutional layers\n\n![img-2]({% static '/blog/assets/img/convnets-what-why-how/img2.jpg' %})\n\nIn the convolutional layer, the weight kernel moves to the right with a certain stride value till it parses the complete width. Moving on, it hops down to the beginning (left) of the image with the same stride value and repeats the process until the entire image is traversed.\n\nConventionally, the first convolutional layer is responsible for capturing the low-level features such as edges, colour, gradient orientation, etc. With added layers, the architecture adapts to the high-level features as well, giving us a network which has the wholesome understanding of images in the dataset, similar to how we would.\n\nTo capture features in the borders, the images are typically augmented with zeros along the four edges. This is known as zero padding.\n\n![img-3]({% static '/blog/assets/img/convnets-what-why-how/img3.png' %})\n\n### 2. Pooling layers\n\nJust like convolutions, pooling is done to reduce the dimensions of the feature map in order to decrease the computational power required to process the data. The pooled values are further passed into the activation function (sigmoid, ReLU etc.) and input to the next layer.\n\n![img-4]({% static '/blog/assets/img/convnets-what-why-how/img4.png' %})\n\nThere are two types of pooling, namely Max pooling and Average pooling. In max pooling, the maximum value in a kernel is pooled out whereas in average pooling the mean of the kernel is pooled out for further processing.\n\n### 3. Fully connected layers\n\nAfter going through the above processes in multiple layers, the model is able to understand features of various complexity from the training data. Finally, we flatten the final output and feed it to a regular Neural Network for classification.\n\n![img-5]({% static '/blog/assets/img/convnets-what-why-how/img5.jpg' %})\n\nThe overall training process can be summarised as follows:\n\n1. We initialize all filters and weights with random values\n\n2. The network takes a training image as input, goes through the forward propagation step (convolution, ReLU and pooling operations along with forward propagation in the Fully Connected layer) and finds the output probabilities for each class.\n\n3. It calculates the total error at the output layer using a loss function like binary cross-entropy, mean-squared error, triplet loss, etc.\n\n4. Finally, it uses backpropagation to calculate the _gradients_ of the error with respect to all weights and uses _gradient descent_ to update them to minimize the output error.The weights are adjusted in proportion to their contribution to the total error.\n\nNow, let&#39;s visualize the layers of a ConvNet. Consider the following model taken from one of the sources mentioned below:\n\n![img-6]({% static '/blog/assets/img/convnets-what-why-how/img6.png' %})\n\nThe input image contains 1024 pixels (32 x 32 image) and the first Convolution layer (Convolution Layer 1) is formed by convolution of six unique 5 \u00d7 5 filters with stride 1 on the input image.\n\nConvolutional Layer 1 is followed by Pooling Layer 1 that does 2 \u00d7 2 max pooling with stride 2 separately over the six feature maps in Convolution Layer 1.\n\n![img-7]({% static '/blog/assets/img/convnets-what-why-how/img7.png' %})\n\nPooling Layer 1 is followed by sixteen 5 \u00d7 5 convolutional filters with stride 1 that perform the convolution operation. This is followed by Pooling Layer 2 that does 2 \u00d7 2 max pooling.\n\nFinally, we have the fully connected layers as follows\n\n![img-8]({% static '/blog/assets/img/convnets-what-why-how/img8.png' %})\n\n1)120 neurons in the first fully connected layer\n\n2)100 neurons in the second fully connected layer\n\n3)10 neurons in the third and final fully connected layer. This is the output layer. Note that the pixel corresponding to the digit 8 is the brightest which means the model classifies the hand written digit as 8 with highest probability.\n\nHere are some popular CNN Architectures:\n\n1. LeNet-5:\n\n![img-9]({% static '/blog/assets/img/convnets-what-why-how/img9.png' %})\n\n1. AlexNet:\n\n![img-10]({% static '/blog/assets/img/convnets-what-why-how/img10.png' %})\n\n1. VGG-16:\n\n![img-11]({% static '/blog/assets/img/convnets-what-why-how/img11.png' %})\n\n## Conclusion\n\nConvNets are a trending topic today in both, academia as well as industry. They are used for a plethora of tasks like object detection and recognition, semantic segmentation, medical imaging and many more.\n\nIn this blog, I have tried to explain the fundamental concepts of convolutional neural networks with substantial simplicity. We have seen the types of layers used in ConvNets, some popular CNN architectures and visualized different layers from a model. Hope it was informative!\n\nReferences:\n\n[https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/](https://www.analyticsvidhya.com/blog/2018/12/guide-convolutional-neural-network-cnn/)\n\n[https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/](https://ujjwalkarn.me/2016/08/11/intuitive-explanation-convnets/)\n\n[https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)",
        "author_name": "M Shreyas Rao",
        "slug": "convnets"
    },
    {
        "layout": "post",
        "title": "What is the future of power generation?",
        "author_github": "Pramod5-CIV",
        "date": "2021-05-14T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Piston",
            "Power Generation",
            "Civil"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "Pramod5-CIV",
        "text": "[{% load static %}]()\n**The following statements are something we all have been reading and learning as the truth:**\n\n- Fossil fuels, such as coal and oil, are dangerous pollutants, and anyway we are running out of them.\n- Nuclear power is so dangerous that it cannot safely be used; indeed, the nuclear facilities already in existence represent such a mortal danger that they should be shut down.\n- But there is one hope: power derived from the sun and winds. These are infinite in quantity, or at least indefinitely great; and they are also safe and clean. All we need is a few years in which to develop this kind of power, and our energy needs will be taken care of.\n\nOnly the first of these three statements is true, with some qualifications. The second and third statements are false, although it is popular to believe that they are true.\n\nDiscussing sources of energy and what would be the future power generation method?\n\n## 1) Fossil fuels\n\nThe fuels that caused the great industrial revolution, the fuels which accelerated the scientific progress of mankind, making our lives better, have also been the cause of polluting the environment. CO2, nitrous oxide, etc that are released during burning of fossil fuels are the major contributors to global warming while also leading to air pollution. Electricity and Heat Production occupy a major chunk of fossil fuel usage. Since they are non renewable and the pace at which we are already using it up they are likely to be over in the coming years. With such problems that pose from fossil fuels there\u2019s a need to find better alternatives for energy production.\n\n## 2) Renewable Energy\n\nAre renewable energy green and clean as believed?\n\n### a) Solar energy\n\nDirect Energy from the fusion reactor of the solar system, solar energy might look promising with new technology being developed, but is it true?\n\nThe major problem with solar power (in fact every renewable power plant) is that the energy generated isn\u2019t constant and varies based on the amount of sunlight that reaches the earth\u2019s surface i.e. based on night, clouds, heavy rains etc. These will cause hindrances in power generation and the effects of these are already being displayed in places like California and Germany which rely mostly on renewable energy generation, increase in price rise has been seen after the implementation since California pays the neighbouring states to use electricity when in excess and has to again buy electricity when electricity generation is low just to maintain the grid. As a result maintaining the grid is pretty difficult and hence large storage batteries are required which also raise the price. Another problem is the land occupied by the solar farms, the amount of land occupied by solar farms are huge compared to that of nuclear or fossil fuel power plants. On an estimate Solar and wind farms require between 400 and 750 times more land than nuclear and natural gas plants. Batteries also require space and hence the area needed might increase significantly.\n\nAnother major problem is the environmental concerns, the materials used in solar panels especially the toxic heavy metals can\u2019t be recycled much, as a result they are being dumped in 3rd world countries once their lifetime is completed. As per reports by Chinese and Japanese experts, recycling and manufacturing solar panels from used ones is much costlier than making it all new. Wild-Life Conservationists argue that animals are losing their habitats from solar farms which is also a concerning issue (desert tortoises number has reduced over the years in California\u2019s desert solar farms as of today many experiments are being done to allow the tortoises to dwell in the solar farms and make it their habitat).\nSolar energy for low energy demanding works or a few household purposes are better but for large scale energy generation, as of now they are inefficient and economically inviable.\n\n### b) Wind Energy\n\nThis, as explained, has problems similar to solar energy, apart from fluctuating and needing large scale place they also have materials that are not viable for recycling and are being dumped in 3rd world countries, they are also causing death of endangered bird species. \n\n### c) Hydro-electric power plants\n\nAmong the 3 major renewable sources this has higher efficiency in energy generation, can be used to prevent floods and also used to direct water for irrigation and drinking water purposes. But they too have serious environment concerns since they are generally built in active earthquake centres, loss of habitat of animals and people during construction etc., but the advantages outweigh the problems since they can generate higher amount electricity, protection from floods, water for drinking and irrigation etc. which lead to greater economic development while also causing lesser pollution, hence among renewable energy Hydro-electric power plants are more preferable for centralised distribution of power. But the major drawback is that they can be implemented in regions having rivers with high water flow and valleys.\n\nThus among renewable energy for centralised power distribution hydro electric power generation is better while for decentralised power roof top solar panels might be better off. \n\n## 3) Nuclear energy\n\nThe most dreaded form of energy production, with nuclear bombings in Japan and the cold war\u2019s race to develop more nuclear weapons, Chernobyl and Fukushima disasters and a lot of misinformation floating around, this might look like a taboo that mankind should never venture into and would be better if we shift our focus from the method, but is that true?\n\nNuclear fission power plants\u2019 mortality rate per thousand terawatt hour is the least among all forms of energy production, produces the highest amount of energy per space occupied, the amount of waste produced per energy produced is also less. It has also cut down the cost of electricity production while the generation itself is mostly constant. The radioactive materials are handled with much more care and since the quantity produced per energy generation is less, with many new technologies being developed to reduce the life of toxic radioactive materials, it can be easily contained since the quantity generated itself is less in the process.\n\n<figure style=\"text-align: center;\">\n<style>\n.img {\n    display: inline-block !important;\n    width: 30%;\n    margin: 0 !important;\n}\n@media only screen and (max-width: 768px) {\n    .img {\n        width: 80%;\n    }\n}\n</style>\n<img src=\"/blog/assets/img/future-of-power-generation/image1.png\" alt=\"image1\" class=\"img\">\n<img src=\"/blog/assets/img/future-of-power-generation/image2.png\" alt=\"image2\" class=\"img\">\n<img src=\"/blog/assets/img/future-of-power-generation/image3.png\" alt=\"image3\" class=\"img\">\n</figure>\n\nThe 3 images represent the amount of clean energy used vs the cost that households pay, we see that Germany which is leading in usage of renewable energy still has a lot of coal power plants due to the variable nature of solar and wind power generation while also the price for household usage is rising.\n\nThere are a few problems such as using technology used in Nuclear Energy development for nuclear weapon production but new technology transfer ensures it\u2019s intrinsically proliferation resistant. Thorium which is abundant in many nations can be used to reduce monopoly, strict procedures in every part of a nuclear power plant from building to maintenance to shut down have to be discussed and framed out such that events such as Chernobyl and Fukushima don\u2019t occur again. But still comparing the data from the table below a nuclear power plant has a lesser mortality rate than other means. The initial investment required to build the power plant is high and the time taken to do is also more.\n\n![result]({% static '/blog/assets/img/future-of-power-generation/image4.png' %})\n\n## Conclusion\n\nComparing all the data we can say that nuclear power plants are the way for a green environment and cleaner energy. But does that mean every form of energy has to be discarded and only nuclear energy has to be developed and used, the answer would be no. There are several cases where fast deployment and generation of energy is needed where solar and wind energy can be used, Space exploration is one such example. Houses and other buildings can install solar panels to generate excess energy (if they can economically do so). Developing better analysis models we can predict the structural stability of dams and also build dams or hydro electric projects in a way to reduce disasters due to earthquakes. Research on fossil fuels is needed too so that as long as we are using them we create less amount of pollution. \n\nBut for a centralised distribution of energy, a less polluting, environment friendly and cheaper source of energy which can lead to greater economic development, nuclear energy is the best choice that Earth has as of now.\n\n### Sources \n- [If solar panels are so clean why do they produce so much toxic waste?](https://www.forbes.com/sites/michaelshellenberger/2018/05/23/if-solar-panels-are-so-clean-why-do-they-produce-so-much-toxic-waste/?sh=f100a49121cc)\n- [Why renewables can't save the planet](https://www.forbes.com/sites/michaelshellenberger/2019/09/04/why-renewables-cant-save-the-climate/?sh=1541fd463526)\n- [Nuclear power our best option](https://fee.org/articles/nuclear-power-our-best-option/)\n- [Safest sources of energy](https://ourworldindata.org/safest-sources-of-energy)\n- [Deaths per TWH for all energy sources](https://www.nextbigfuture.com/2008/03/deaths-per-twh-for-all-energy-sources.html)\n- [What is the safest energy for the future](https://www.altenergymag.com/article/2020/03/what-is-the-safest-energy-for-the-future/32904)\n- [Death rate by energy source](https://www.statista.com/statistics/494425/death-rate-worldwide-by-energy-source/)\n- [World's safest energy source](https://www.visualcapitalist.com/worlds-safest-source-energy/)\n- [Desert Tortoise Ecology and Renewable Energy Development](https://www.usgs.gov/centers/sbsc/science/desert-tortoise-ecology-and-renewable-energy-development?qt-science_center_objects=0#qt-science_center_objects)",
        "author_name": "Pramod Sunil Parande",
        "slug": "future-of-power-generation"
    },
    {
        "layout": "post",
        "title": "CAN Bus Protocol",
        "author_github": "sahaj7100",
        "date": "2021-05-21T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Diode",
            "Communication Protocol"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "sahaj7100",
        "text": "[{% load static %}]()\nThe CAN Bus protocol stands for Controller Area Network Protocol, which is a two wired*,* bidirectional serial bus communication method. It was designed to perform in harsh environments such as in the automotive industry, but now it has emerged as a go-to network technology in industrial automation as well as in other applications. CAN define the data link and physical layer of the Open Systems Interconnections (OSI) model, thus providing a low-level networking solution for high-speed in-vehicle communication. CAN Bus is primarily used in embedded systems and provides fast communication among microcontrollers up to real-time requirements, thus eliminating the need for the much more expensive and complex technologies.\n\n## History\n\nThe controller Area Network (CAN) was originally developed by Bosch in 1986 for in-vehicle networks. Before that, automotive manufactures connected electronic devices in a vehicle using point-to-point wiring systems. Point-to-point wiring systems were those in which all the electronic components had their data line wired individually. As manufacturers began using more and more electronics in vehicles the wiring harnesses became more heavy and expensive. So they replaced the primitive point-to-point wiring system with in-vehicle networks.\n\nAs the CAN communication protocol had high immunity to electrical interference and the ability to self - diagnose and repair data errors, the automotive industry readily adopted the CAN Bus Protocol, and later in the year 1993 it became the international standard known as ISO 11898. Since 1995 many Higher Layer Protocols (HLPs) were developed for and standardized on CAN.\n\nToday, CAN is found in almost every market while developments are continuing in the area of HLPs to support the needs of existing and new developers.\n\n## What makes CAN Bus special?\n\n- **Simple Design** - CAN provide a cost effective robust communication network. Due to the simple design, the ECU has only a single CAN interface rather than analog and digital inputs to every device in the system thus it reduces error, cost, and weight of the wiring.\n\n- **Broadcast Communication** - The CAN protocol uses a peer-to-peer network which means that every electronic module or ECU connected to the Bus can receive and transmit data to all other modules or ECUs and sees all the transmitted messages, thus the message can take multiple routes to get to the destination. This feature adds to the robustness of the CAN protocol as even if one module on the CAN bus fails, it does not necessarily cause the failure of other modules.\n\n- **Priority Id System**- Every message transmitted through the CAN Bus has a priority Id, So in case if two or more nodes in the Bus tries to transmit messages at the same time the message with higher priority is considered first and the other message is processed after this.\n\n- **Cyclic Redundancy Check**- The CAN Bus protocol uses a cyclic redundancy code(CRC) in each of its message frames. Thus this helps in identifying the errors in the transmitted frame's content and after error identification that particular message frame is disregarded by all nodes.\n\n- **Fully Centralized**- The CAN Bus provides a single point of communication with all the ECUs enabling central diagnostics, data logging, and configuration.\n\n  ![img-1]({% static '/blog/assets/img/can-bus-protocol/img1.jpg' %})\n\n## Structure of CAN Frames\n\nCommunication over the CAN Bus is done via CAN frames. The original ISO laid out what is called a Standard CAN. Standard CAN uses an 11-bits identifier for different messages, later CAN was modified and the identifier was extended to 29-bits which is known as Extended CAN.\n\n### Standard CAN\n\n![img-2]({% static '/blog/assets/img/can-bus-protocol/img2.jpg' %})\n\nThe meaning of the bit fields of the above figure is -\n\n- SOF - The single Start of Frame(SOF) bit marks the start of the message and is used to synchronize the node on a bus after being idle.\n- Identifier - The 11- bit Identifier establishes the priority of the message, the lower the binary value, the higher is its priority.\n- RTR - The single remote transmission request (RTR) bit is dominant when the information is required by another node. All the nodes present on the CAN Bus receive this request but the identifier determines the specific node. The responding data is also received by all the nodes and can be used by any node interested.\n- IDE - A dominant single Identifier extension(IDE) bit means that a standard CAN identifier with no extension is being transmitted.\n- r0 - Reserved bit\n- DLC - the 4-bit data length code(DLC) contains the number of bytes of data being transmitted.\n- Data - Up to 8 bytes of application data can be transmitted.\n- CRC - The 16-bit cyclic redundancy check (CRC) contains the checksum (number of bits transmitted) of the preceding application data for error detection.\n- ACK - After every node receives an accurate message it overwrites this recessive bit by a dominant bit indicating an error-free message has been sent. ACK is 2 bits long one bit is the acknowledge bit and one is a delimiter.\n- EOF - This end-of-frame (EOF), 7-bit field marks the end of a CAN frame and disables bit-stuffing, indicating a stuffing error when dominant. When 5 bits of the same logic level occur in succession during normal operation, a bit of the opposite logic level is stuffed into the data.\n- IFS - This 7-bit interframe space (IFS) contains the time required by the controller to move a correctly received frame to its correct position.\n\n### Extended CAN\n\n![img-3]({% static '/blog/assets/img/can-bus-protocol/img3.jpg' %})\n\nAs shown above, the Extended CAN is the same as Standard CAN with the addition of - \n\n- SRR - The substitute remote request (SRR) bit replaces the RTR bit in the standard message location.\n- IDE - A recessive bit in the identifier extension (IDE) indicates that more identifier bits follow. The 18-bit extension follows IDE.\n- r1 - Following the RTR and r0 bits an additional reserve bit has been included ahead of the Data Length Code bit.\n\n## How CAN Communication Works\n\nCAN data transmission uses a lossless bitwise arbitration(the process in which two or more CAN controllers agree on who is to use the bus) method of contention resolution. This arbitration method requires all nodes on the CAN network to be synchronized to sample every bit on the CAN network at the same time. The CAN Bus consists of two wires CANH and CANL, when the controller sends a stream of bits, these are complemented and placed on the CANH line. The CANL line is always the complement of CANH.\n\nThe CAN specifications use the terms \"dominant\" bits and \"recessive\" bits, where dominant is a logical 0 and recessive is a logical 1. The idle state is represented in the CAN Bus is by the recessive level. If one node transmits a dominant bit and another node transmits a recessive bit then there happens a collision and the dominant bit wins. This means there is no delay to the higher-priority message, and the node transmitting the lower priority message automatically attempts to re-transmit six-bit clocks after the end of the dominant message. This makes CAN very suitable as a real-time communications system.\n\nThe exact voltages for a logical 0 or 1 depend on the physical layer used, but the basic principle of CAN requires that each node listens to the data on the CAN network including the transmitting node itself. If a logical 1 is transmitted by all transmitting nodes at the same time, then a logical 1 is seen by all of the nodes, including both the transmitting node and receiving node. If a logical 0 is transmitted by all transmitting nodes at the same time, then a logical 0 is seen by all nodes. If a logical 0 is being transmitted by one or more nodes, and a logical 1 is being transmitted by one or more nodes, then a logical 0 is seen by all nodes including the node transmitting the logical 1. When a node transmits a logical 1 but sees a logical 0, it realizes that there is a collision and a high priority is being sent thus it quits transmitting. By using this process, any node that transmits a logical 1 when another node transmits a logical 0 re-transmits the message six-bit clocks after all the high priority message ends and the CAN frame bit-stream continues without error until only one node is left transmitting. Since the 11 (or 29 ) bit identifier is transmitted by all nodes at the start of the CAN frame, the node with the lowest identifier transmits more zeros at the start of the frame, and that is the node that wins the arbitration or has the highest priority.\n\n## CAN Message Types\n\nFour different types of frames can be transmitted through a CAN Bus - \n\n- **The Data Frame**- The data frame comprises the Arbitration Field, the Data Field, the CRC Field, and the Acknowledgment Field. The Arbitration Field contains an 11-bit identifier and the RTR bit, which is dominant for data frames. Next is the Data Field which contains zero to eight bytes of data, and the CRC field which contains the 16-bit checksum used for error detection. Last is the Acknowledgment Field.\n- **The Remote Frame**- The intended purpose of the remote frame is to help in the transmission of data from another node. The remote frame is similar to the data frame, with two important differences. First, this type of message is specifically marked as a remote frame, and secondly, there is no data.\n- **The Error Frame**- The error frame is a special message that violates the formatting rules of a CAN message. It is transmitted when a node detects an error in a message and causes all other nodes in the network to send an error frame as well. The original transmitter then automatically retransmits the message. \n- **The Overload Frame**- The overload frame is similar to the error frame in terms of the format, and it is transmitted by a node that becomes too busy. It is primarily used to provide for an extra delay between messages.\n\n## Conclusion\n\nThis blog introduced CAN Bus Protocol. To summarize CAN Bus is a robust serial communication network protocol that is suitable to work in harsh working environments. CAN uses a differential signal, which makes it more resistant to noise, along with a priority arbitration scheme for non-destructive message transmission.\n\n## References\n\n- [https://www.microchip.com/stellent/groups/SiteComm_sg/documents/DeviceDoc/en558265.pdf](https://www.microchip.com/stellent/groups/SiteComm_sg/documents/DeviceDoc/en558265.pdf)\n- [https://en.wikipedia.org/wiki/CAN_bus](https://en.wikipedia.org/wiki/CAN_bus)\n- [https://www.csselectronics.com/screen/page/simple-intro-to-can-bus/language/en](https://www.csselectronics.com/screen/page/simple-intro-to-can-bus/language/en)\n- [https://www.allaboutcircuits.com/technical-articles/introduction-to-can-controller-area-network/](https://www.allaboutcircuits.com/technical-articles/introduction-to-can-controller-area-network/)\n- [https://www.ni.com/en-in/innovations/white-papers/06/controller-area-network--can--overview.html](https://www.ni.com/en-in/innovations/white-papers/06/controller-area-network--can--overview.html)\n- [https://www.ti.com/lit/an/sloa101b/sloa101b.pdf](https://www.ti.com/lit/an/sloa101b/sloa101b.pdf)\n- [https://sewelldirect.com/blogs/learning-center/understanding-can-bus-and-can-loggers](https://sewelldirect.com/blogs/learning-center/understanding-can-bus-and-can-loggers)",
        "author_name": "Sahaj Saxena",
        "slug": "can-bus-protocol"
    },
    {
        "layout": "post",
        "title": "Once upon a time in Fiction",
        "author_github": "varshini_c",
        "date": "2021-05-21T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Piston"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "varshini_c",
        "text": "[{% load static %}]()\nWe often assume that the technology we rely on came from the minds of genius inventors determined to solve a problem. While most original concepts are inspired by the need to improve the way something was done, some ideas spring from the imaginations of creative people who have little or no expertise in design and are armed only with the ability to imagine worlds beyond the constrictions of the here and now. These persons, in turn, inspire those who possess the necessary proficiency to convert these ideas into reality. \n\nMany of the inventions we use every day are inspired by the stories written many years before they were possible. Here are some of such designs that started as works of fiction and eventually made their way into our everyday lives.\n\n## Voice Control\n\n![Robot]({% static '/blog/assets/img/once-upon-a-time-in-fiction/image2.png' %})\n\nVoice-controlled robots such as R2-D2 from Star Wars (pictured) were a mainstay of science fiction long before Apple unveiled its Siri software. While the technology had been in development as far back as the 1950s, it wasn\u2019t until Stanley Kubrick\u2019s 1968 film 2001: A Space Odyssey showed a talking and responsive computer, HAL, that people began to imagine its true possibilities. Voice control systems first emerged during the 1950s and 1960s, when Bell Laboratories designed its \u2018Audrey\u2019 system, which could recognize digits spoken by a single voice. IBM took things a step further with its \u2018Shoebox\u2019 machine that could understand 16 English words. By the turn of the century, consumer electronics firms started to integrate such technology into their products. \n\n## Chess Playing Computers\n\nA game as complicated as Chess is renowned for necessitating patience, forethought, and strategic thinking. The idea of creating a machine capable of challenging humans in chess has been around for centuries, including several hoaxes in the 18th and 19th centuries that hid human players inside seemingly intelligent machines.\n\n![Chess Playing Computers]({% static '/blog/assets/img/once-upon-a-time-in-fiction/image4.png' %})\n     \nHowever, it wasn\u2019t until 2001: A Space Odyssey showed the supercomputer HAL playing chess against a human opponent that software companies were inspired to design programs capable of taking on the world\u2019s best chess players. After many failed attempts to match the fictional feat, IBM\u2019s Deep Blue supercomputer defeated World Chess Champion Garry Kasparov in 1997.\n\n## Mobile Phones\n\nOften cited as one of the most forward-thinking television shows in history \u2013 and the cause of embarrassment for many grown men, with their in-depth knowledge of alien languages, uniforms, and starship registrations \u2013 Star Trek imagined an early form of the mobile phone. The \u2018Communicator\u2019 used by Captain Kirk and his crew inspired mobile phone inventor Martin Cooper to design a portable cellular phone in the early 1970s. Cooper\u2019s somewhat clunky design has been refined over the years, and it is now an essential part of almost every human being\u2019s life. \n\n![Mobile Phones]({% static '/blog/assets/img/once-upon-a-time-in-fiction/image3.png' %})\n     \nHe has said that the inspiration he took from Star Trek was clear: \u201cThat was not fantasy to us, that was an objective.\u201d\n\n## Organ Transplants\n\nSome have cited Frankenstein as the first science fiction novel. In Mary Shelley\u2019s 1818 book, scientist Victor Frankenstein uses parts of a dead body to build a man, bringing him to life with a powerful electric current. While such an extreme feat has yet to be achieved, transplanting body parts from one human to another has been possible for some time. Transplants have been attempted for centuries, but the ability to make the host bodies accept the donor\u2019s organs has only come about in the last 100 years.\n\n![Organ Transplants]({% static '/blog/assets/img/once-upon-a-time-in-fiction/image6.png' %})\n    \t\t\t\t\nDr. Richard H Lawler of Chicago achieved the first successful kidney transplant in 1950, and in 1967 Christian Barnard performed the first heart transplant in South Africa. It is thought Shelley\u2019s story had a huge influence on the idea of organ transplantation, and her idea of using electricity to animate a body can be seen as a foreshadow of the defibrillator. These innovations have proved vital in saving lives all over the world.\n\n## 3D Printers\n\n![3D Printer]({% static '/blog/assets/img/once-upon-a-time-in-fiction/image5.png' %})\n\nUnsurprisingly, science fiction staple Star Trek presaged more than one modern invention. On the Enterprise, \u201cReplicators\u201d could print a copy of just about anything the crew needed in a matter of seconds. Experimenting with liquid photopolymers and ultraviolet light, Chuck Hull figured out in 1983 how to turn printing liquid-solid when the light hit it, creating a solid 3D model of a computer design. Since then, people have been able to 3D print everything from jet engines to human ribs. \n\n## Atomic Power\n\n![Nuclear Plant]({% static '/blog/assets/img/once-upon-a-time-in-fiction/image1.png' %})\n\nHG Wells first mentioned the idea of atomic power in his novel, The World Set Free, in 1913. This vision of Wells\u2019 inspired Hungarian-German-American physicist Leo Szilard. Twenty years later, he patented the idea of a nuclear reactor along with Enrico Fermi. While this invention led to the atomic bomb, Wells envisioned a greater use for atomic power than a bomb.                                         \n\nThese were the advanced ideas of the 19th and 20th centuries that have been successfully realized recently. I am personally looking forward to the next 50-100 years where we will bring to reality the fantasies suggested by Marvel, Steven Spielberg, Christopher Nolan, and others. We might have a fully functional Iron Man suit (I say fully because a rudimentary model already exists), a Thor\u2019s Hammer that could decide your \u2018worth,\u2019 A Time Machine (finally!). For all we know, the next dream holiday could be a Jurassic Park in place of Disney Land. The only thing we need to bring these \u2018futuristic\u2019 schemes to life is a bit of technology, lots of time, and a group of eccentric people who genuinely believe that **\u2018THINGS ARE ONLY IMPOSSIBLE UNTIL THEY\u2019RE NOT.\u2019**",
        "author_name": "Varshini C",
        "slug": "once-upon-a-time-in-fiction"
    },
    {
        "layout": "post",
        "title": "An Introduction to Matrix Exponentiation",
        "author_github": "AnirudhAchal",
        "date": "2021-05-28T00:00:23",
        "image": "/assets/img/",
        "description": "Introductory Blog to Matrix Exponentiation",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "DP",
            "Matrix Exponentiation",
            "Competitive Programming",
            "Algorithms"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "AnirudhAchal",
        "text": "[{% load static %}]()\nMatrix Exponentiation is one of the most used techniques in advanced competitive programming. The concept of matrix exponentiation in its most general form is very useful in solving questions that involve calculating the nth term of a linear recurrence relation in time of the order of log(n). It is especially useful in converting a linear O(n) dynamic programming solution into a O(log(n)) solution. \n \nTo understand this better, let us consider a very simple example of finding the nth Fibonacci number. This problem can be very easily solved using a linear recurrence. As we all know, by definition of the fibonacci series, F<sub>n</sub> = F<sub>n - 1</sub> + F<sub>n - 2</sub>. Consider the code below that calculates the nth fibonacci number. \n\n``` python\ndef get_fibonacci(n):\n\n    if n <= 0: return -1 # Throw error : Invalid value of n\n\n    if n == 1: return 0\n    if n == 2: return 1\n\n    dp = [0] * (n + 1) # Initializing Array\n\n    # Setting base case\n    dp[1] = 0 \n    dp[2] = 1\n\n    for i in range(3, n + 1):\n        dp[i] = dp[i - 1] + dp[i - 2]\n\n    return dp[n]\n\n```\n\nThis solution is an iterative dp that runs in linear time ie. O(n). Note that this solution has a space complexity of O(n), but it can be easily converted into O(1) space.\n\nNow at first there does not seem to be any straightforward way to improve the time complexity of this solution. This is where matrix exponentiation comes into the picture. Our goal is to obtain a recurrence relation of the form F<sub>n</sub> = P * F<sub>n - 1</sub>  where P is a constant matrix and F<sub>n</sub> and F<sub>n - 1</sub> are matrices.  Let us see what happens if we obtain such a relation. \n\nF<sub>2</sub> = P * F<sub>1</sub> \n\nF<sub>3</sub> = P * F<sub>2</sub> \\\nF<sub>3</sub> = P * P * F<sub>1</sub> \\\nF<sub>3</sub> = P<sup>2</sup> * F<sub>1</sub> \n\nF<sub>4</sub> = P * F<sub>3</sub> \\\nF<sub>4</sub> = P * P<sup>2</sup> * F<sub>1</sub> \\\nF<sub>4</sub> = P<sup>3</sup> * F<sub>1</sub> \n\n.\\\n.\\\n.\n\nFn = P<sup>n - 1</sup> * F<sub>1</sub>\n\nThis is a very helpful relation. We have got the nth term of the series in terms of the base matrix F<sub>1</sub>. <b>Note:</b> This base matrix need not always be n = 1.\n\nYou must already be knowing that x<sup>n</sup> can be calculated in O(log(n)) time using binary exponentiation where x and n are integers. Refer to the code below in case you need a refresher on how that is done. If you have never heard of binary exponentiation, go through [this](https://cp-algorithms.com/algebra/binary-exp.html) article before continuing.\n\n``` python\ndef power(x, n):\n\n    result = 1\n\n    while n > 0:\n\n        if n % 2 != 0:\n            result *= x\n\n        n = n // 2\n        x = x * x\n\n    return result\n    \n```\n\nA very similar function can be implemented to calculate P<sup>n</sup> in O(log(n) * m<sup>3</sup>) time where P is a square matrix, n is an integer and m is the dimension of P (ie. P is an m x m matrix).  \n\n``` python\ndef matrix_power(P, n):\n    m = len(P) \n\n    # Initializing m x m identity matrix\n    R = [[1 if i == j else 0 for i in range(m)] for j in range(m)] \n\n    while n > 0:\n\n        if n % 2 != 0:\n            R = matrix_multiply(P, R)\n\n        n = n // 2\n        P = matrix_multiply(P, P)\n\n    return R\n\n\ndef matrix_multiply(A, B):\n\n    n = len(A)\n    m = len(A[0])\n    q = len(B)\n    r = len(B[0])\n\n    if m != q:\n        return -1 # Throw error : Incompatible\n\n    # Initialzing m x m zero matrix\n    R = [[0 for i in range(r)] for j in range(n)] \n\n    for i in range(n):\n        for j in range(r):\n            for k in range(m):\n                R[i][j] += A[i][k] * B[k][j]\n\n    return R\n\n```\n\n'matrix_multiply' is a function that runs in O(n<sup>3</sup>). Considering that, the overall time complexity of calculating P<sup>n</sup> is O(log(n) * m<sup>3</sup>).\n\nNow let us get back to the original question. That is, calculating the nth fibonacci number. What we need to do is to get the matrices F<sub>n</sub> and P. \n\nWe consider F<sub>n</sub> to be:\n\n```\nFn = |  fn  | \n     | fn_1 |\n\nwhere fn is the nth fibonacci number and fn_1 is the (n-1)th fibonacci number.\n```\n\nNow we need to find P such that\nF<sub>n</sub> = P * F<sub>n - 1</sub> \\\nUsing f<sub>n</sub> = f<sub>n - 1</sub> + f<sub>n - 2</sub> and f<sub>n - 1</sub> = f<sub>n - 1</sub> and with the help of basic linear algebra we see that:\n\n```\nP = |1 1|\n    |1 0|\n```\n\nWe will consider the base matrix to be:\n\n```\nF2 = |f2| = |1|\n     |f1|   |0|\n\nThe base matrix is Fn when n == 2.\n```\n\nNow we can easily see F<sub>n</sub> = P<sup>n - 2</sup> * F<sub>2</sub>. As shown early P<sup>n - 2</sup> can be calculated in O(log(n) * m<sup>3</sup>). Here m = 2. Therefore the time complexity of this solution is O(log(n) * 2<sup>3</sup>) which is O(log(n)). The code for this is given below.\n\n``` python\ndef get_fibonacci_matrix_exp(n):\n    if n <= 0: return -1 # Throw error\n    if n == 1: return 0\n \n    F2 = [[1],\n          [0]]\n \n    P = [[1, 1],\n         [1, 0]]\n \n    Pn_2 = matrix_power(P, n - 2) # Calculating P^(n-2)\n    Fn = matrix_multiply(Pn_2, F2) # Fn = P^(n-2) * F2\n\n    return Fn[0][0] \n\n```\n\nThis is much more efficient solution and scales much better for very large values of n.\n\nNow using this, try to solve the following problem in logarithmic time.\n\n<b>Q. Given a 3 x N rectangle, determine how many ways can we tile the rectangle using 1 x 3 and 3 x 1 tiles. </b>\n\nLike before, first we will come up with a O(n) solution and obtain a linear recurrence relation. In this problem dp<sub>n</sub> = dp<sub>n - 1</sub> + dp<sub>n - 3</sub>. If it is unclear how we obtain this, I suggest you read [this](https://www.geeksforgeeks.org/count-number-of-ways-to-fill-a-n-x-4-grid-using-1-x-4-tiles/) article. We need to express this relation as DP<sub>n</sub> = P * DP<sub>n - 1</sub> , where P, DP<sub>n</sub> and DP<sub>n - 1</sub> are all matrices.\nHere we take DP<sub>n</sub>, P and base matrix DP<sub>3</sub> (n == 3) as:\n```\nDPn = | dpn |\n      |dpn_1|\n      |dpn_2|\nwhere dpn is the answer when for when N = n.\n\nP = |1 0 1|\n    |1 0 0|\n    |0 1 0|\n\nDP3 = |2|\n      |0|\n      |0|\n```\n\nIt is easy to see that, DP<sub>n</sub> = P<sup>n -3</sup> * DP<sub>3</sub>. As shown before, this can now be solved in O(log(n) * m<sup>3</sup>) where m is now 3. The code for this is given below.\n\n``` python\ndef get_tiling_count(n):\n    if n <= 0: return -1 # Throw error : Invalid value of n\n    if n == 1: return 0\n    if n == 2: return 0\n \n    DP3 = [[2],\n           [0],\n           [0]]\n \n    P = [[1, 0, 1],\n         [1, 0, 0],\n         [0, 1, 0]]\n \n    Pn_3 = matrix_power(P, n - 3) # Calculating P^(n-3)\n    DPn = matrix_multiply(Pn_3, DP3) # DPn = P^(n-3) * DP3\n\n    return DPn[0][0] \n\n```\n\nHere are some other problems that you can try solving to practice this concept.\n\n- [Tiling 4x4](https://www.geeksforgeeks.org/count-number-of-ways-to-fill-a-n-x-4-grid-using-1-x-4-tiles/)\n- [Tetrahedron](https://codeforces.com/problemset/problem/166/E)\n\n### References\n\n- [https://codeforces.com/blog/entry/67776](https://codeforces.com/blog/entry/67776)\n- [https://www.youtube.com/watch?v=eMXNWcbw75E](https://www.youtube.com/watch?v=eMXNWcbw75E)",
        "author_name": "Anirudh Achal",
        "slug": "an-introduction-to-matrix-exponentiation"
    },
    {
        "layout": "post",
        "title": "Neural Landers",
        "date": "2021-05-28T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Piston"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "viba-r-udupa",
        "text": "[{% load static %}]()\n> \u201cWhen I meet God, I am going to ask him two questions: Why relativity? And why turbulence? I really believe he will have an answer for the first.\"\n-Horace Lamb\n\nLanding any aircraft smoothly is difficult. It is an energy-management challenge. The aircraft must make the transition from a vast open sky to a relatively constricted environment. It is subject to strong crosswinds increasing the turbulence. There is complex turbulence created as the airflow from the wing/rotor bounces off and interacts with the ground. We do not have a good enough understanding of turbulence yet, and pilots should have excellent judgment skills to make a smooth landing. Coming to autonomous flight, it is a very challenging task to develop a system to account for the turbulence and make the right adjustments to land smoothly.\n\n## Understanding Ground Effects\n\nThe ground effect is a phenomenon that causes reduced induced drag when hovering close to the ground.  Below is an image of a section of a wing.\n\n![img-1]({% static '/blog/assets/img/neural-landers/image1.png' %})\n\nA wing generates lift by deflecting incoming airflow downward. According to Bernoulli's principle, the region above the airfoil has high-speed winds at low pressure compared to the low speed-high pressure winds below the airfoil. This difference in pressure causes lift. The high-pressure air below the wing tries to rush to the low-pressure region above the wing, but the wing comes in between, obstructing it, except at the wingtips, which causes the airflow below the wings to spill around the wingtips above the wings, forming a twisting vortex. This can be visualized in the image below.\n\n![img-2]({% static '/blog/assets/img/neural-landers/image2.png' %})\n\nThe formation of the vortices increases the downwash. The lift generated is perpendicular to the direction of relative wind flow. And with increased downwash, the lift vector tilts backward, and its horizontal component is called induced drag, as seen in the image below. This reduces the net lift force and increases the drag on the aircraft.\n\n![img-3]({% static '/blog/assets/img/neural-landers/image3.png' %})\n\nWhen an aircraft is closer to the ground, the downwash is restricted as the ground is a rigid surface. As a result, even wingtip vortices are restricted. This causes the airflow below the wing to be nearly horizontal, and the lift vector now is not much tilted back. As a result, the induced drag is reduced, which means less power is required to maintain the same airspeed. However, due to an increase in lift, cushioning effect is experienced by the aircraft. The aircraft tends to float and takes longer to land. This effect is noticeable when the aircraft is at a height equal to its wingspan.\n\n![img-4]({% static '/blog/assets/img/neural-landers/image4.png' %})\n\nIn fixed-wing aircraft, a solution is to have Winglets, which are vertical extensions of the wingtips to reduce the vortices and the associated induced drag. Below image shows the effects of winglets on vortices.\n\n![img-5]({% static '/blog/assets/img/neural-landers/image5.png' %})\n\nHowever, if we talk about rotorcraft, using winglets is not a very efficient idea. This is because the propellers are rotating at a very high RPM. The winglet would be acting as a cantilever beam, which would cause instability and material failure due to fatigue. The ground effect in helicopters is shown in the image below and is more complicated for drones.\n\n![img-6]({% static '/blog/assets/img/neural-landers/image6.png' %})\n\n## Neural Landers\n\nDue to complex interactions of the rotors and the wind flow with the grounds, which can cause flight instability, the automatic landing of a drone is risky. Typically drones wobble and slowly move towards the dock until power is cut off and then drop the remaining distance to the ground, which is about 15-20cm of free fall.\n\n![img-7]({% static '/blog/assets/img/neural-landers/image7.png' %})\n\nUnmanned Aerial Vehicles require high precision control of aircraft positions during takeoff and landing. Artificial Intelligence experts from Caltech\u2019s center for autonomous systems and technologies have used Deep Neural Networks(DNN) to develop a system that aids the UAVs to learn how to land efficiently.\n\nThe system created is a learning-based controller that tracks the position and speed of the drone and modifies its landing trajectory and rotor speed accordingly to achieve the smoothest possible landing. The existing systems are mathematical models based on steady flow assumption, while it is never the case in practice. Integral and adaptive control methods give delayed feedback.\n\nTo incorporate a Machine Learning approach to the system's development, a lot of data is required, for which, they used a giant array of 1296 computer fans to generate different sorts of winds and repeatedly landed the drone to study the controllers' performance. Neural networks are a series of algorithms that detect underlying relationships between data. DNN is used as a feedforward prediction term in the controller; that is, the disturbances are measured and accounted for before they have time to affect the system. The input for the neural network is knowledge about the drone's location, velocities, and angular rate, along with the landing location. The output is the additional force required to make the drone land smoothly at the desired location. The result can sometimes be widely varying predictions or condition shifts. To compensate for this, spectral normalization of data is used. The output is used to separately control each of the rotors to adjust the thrust and correct the trajectory to make a smooth landing. Below is an image that was taken during the testing of the neural lander.\n\n![img-8]({% static '/blog/assets/img/neural-landers/image8.png' %})\n\nWe know that landing is the trickiest maneuver for any aircraft. Even more so for vertical take-off and landing(VTOL) aircraft as apart from ground effects, the side winds can make it difficult to control its orientation perfectly.  The deep neural controller will significantly contribute to solving these challenges. Future uses of these could be to effectively incorporate this controller into the aircraft to make landings smoother and can be used for 3D mapping, search-rescue missions, and delivery services.",
        "author_name": "Viba R Udupa",
        "slug": "neural-landers"
    },
    {
        "layout": "post",
        "title": "The Revolution called DDR",
        "date": "2021-05-28T00:00:00",
        "image": "/assets/img/",
        "description": "If you have ever done some decent amount of research before buying any phone, laptop, graphics card, prebuilt PC, or even built your PC you would have come across terms such as LPDDR, DDR, or GDDR when the product's RAM or VRAM is concerned. All of these terms have DDR in common them, but what is DDR?",
        "tags": [
            "IEEE NITK",
            "Diode",
            "DDR",
            "RAM",
            "Digital Systems"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "krutideepanpanda",
        "text": "[{% load static %}]()\n## DDR\n\n### DDR SDRAM\n\n![img1]({% static '/blog/assets/img/revolution-called-ddr/Fig-2-Figure-Clocking-1-300x89.jpg' %})\n\nDDR memory stands for **Double Data Rate memory**, this is actually called DDR SDRAM but since it has become so widespread we just say DDR. To understand how this works and why is this so important lets go back to simple memory components such as latches. Latches and Flip -Flops store data at either the rising or falling edge of the clock cycle, and this was the case with the initial **SDRAM (Synchronous Dynamic Random Access Memory)**. \n\nWhat makes DDR SDRAM so different is that it does the same functions as SDRAM but at both rising and falling edges of the clock cycle. Doing so effectively doubled the transfer rate (measured in transfers per second). The first generation of DDR memory had transfer rates between **266-400 MT/s (Mega Transfers per Second)**. All these RAMs have some prefetch buffer associated with them which acts as a cache for the RAM. DDR had a 2bit prefetch buffer, which is double that of SDRAM. We usually see DDR memory as DDR4 in laptops and desktops. This is the 4th generation of DDR memory. \n\n### DDR2\n\nThe second-generation DDR2 memory could operate the external bus at twice the speeds of its previous generation while maintaining the same internal clock. It had a prefetch buffer of 4bits.\n\n### DDR3\n\nA major drawback of DDR2 was that it consumed a lot of power than DDR1 causing it to heat up frequently. This was dealt with DDR3, it consumed 40% less power than DDR2 and added 2 extra functions of **ASR (Automatic Self-Refresh)** and **SRT (Self-Refresh Temperature)** which allowed for the memory to control its refresh rate according to temperature variations. DDR3 has a prefetch buffer of 8bits.\n\n### DDR4\n\nDDR4 is the current generation of DDR memory and has even lower operating voltages and higher transfer rates. DDR4 has something called **Bank Groups**, it has 4 bank groups allowing to process 4 data in a clock cycle. DDR4 also offers better signal integrity due to functions such as DBI (Data Bus Inversion), CRC (Cyclic Redundancy Check), and CA parity.\n\n### Bank groups\n\nPrevious generational improvements were brought about by increasing but having too high of a pre-fetch is also not ideal. As per the figure below, if the trends were to continue then DDR4 would have had a 16N prefetch. This isn't ideal so what we do is divide it into 2 8N groups, each such group is considered to be a bank group.\n\n![img2]({% static '/blog/assets/img/revolution-called-ddr/prev.jpg' %})\n\n![img3]({% static '/blog/assets/img/revolution-called-ddr/present.jpg' %})\n\nA comparison of all the generations of DDR memory is shown in the table below. We can see that through various generations DDR becomes faster and more power-efficient.\n\n![img4]({% static '/blog/assets/img/revolution-called-ddr/ddr-table.png' %})\n\n## LPDDR\n\nIn layman's terms, LPDDR can be considered as DDR but for mobile devices where power consumption is a major factor. Unlike PCs, mobile devices don't have the liberty to draw as much power as they need. They are restricted to manage with whatever the battery can provide while also ensuring that they don't gulp it all at once, instead they have to sip it slowly to maintain good battery life for the device.\n\nLPDDR4 is the mobile equivalent of DDR4 memory, with a few key differences. LPDDR4 has dual 16-bit channels resulting in a 32-bit total bus per DIMM (DIMM or Dual In-line Memory module could be just considered as RAM ). In comparison, DDR4 has 64-bit channels per DIMM. At the same time, LPDDR4 has a wider prefetch than DDR4. This means that **to consume low power LPDDR4 sacrifices some of the bandwidth**, this results in battery life of anywhere between 8-10 hours. However, the real-world battery life depends on how it is implemented and varies from device to device. LPDDR4 is in no way slower than DDR4 and data rates of 3200 MT/s aren't hard to come by.\n\n## GDDR\n\nGDDR stands for Graphics Double Data Rate and can be seen in VRAMs of graphics cards, with the standards such as GDDR6, GDDR6X, and GDDR5 being pretty common in modern cards. In design, it similar to DDR but has several differences that make GDDR have much better performance. \n\n**Differences between GDDR and DDR -**\n\n- GDDR has a wider memory bus as compared to DDR and thus has a higher bandwidth\n- GDDR can request as well as receive in the same clock cycle unlike DDR\n- GDDR is much more power-efficient and gives off less heat. This allows for keeping higher performance modules with just a simple cooling solution\n\nThe latest standard of GDDR is GDDR6X and is significantly faster than previous generations. Usually, when we send or receive data we do so in two power levels 1 or 0, but GDDR6X uses four power levels which are 00, 01, 10, and 11. This doubles the amount of data present in each cycle. Each 8Gb (1GB) chip of GDDR6X memory has the bandwidth to transfer 84GB/s. \n\n## Resources\n\n- [What is DDR (Double Data Rate) Memory and SDRAM memory?](https://www.microcontrollertips.com/understanding-ddr-sdram-faq/)\n- [What is the difference between SDRAM, DDR1, DDR2, DDR3 and DDR4?](https://www.transcend-info.com/Support/FAQ-296)\n- [What is GDDR?](https://www.computerhope.com/jargon/g/gddr.htm)\n- [What is the Difference Between DirectX 11 and DirectX 12?](https://www.hardwaretimes.com/what-is-the-difference-between-ddr4-and-lpddr4-lpddr4x/)\n- [What is DIMM?](https://www.enterprisestorageforum.com/storage-hardware/what-is-dimm.html)\n- [What is GDDR6X?](https://www.technipages.com/what-is-gddr6x)\n- [DDR4 Bank Groups](https://www.synopsys.com/designware-ip/technical-bulletin/ddr4-bank-groups.html)",
        "author_name": "Kruti Deepan Panda",
        "slug": "the-revolution-called-ddr"
    },
    {
        "layout": "post",
        "title": "The First 20 Hours",
        "date": "2021-06-04T00:00:00",
        "image": "/assets/img",
        "tags": [
            "IEEE NITK",
            "Piston"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "yuv-b",
        "text": "[{% load static %}]()\n<center><img src=\"/blog/assets/img/first-20-hours/image1.jpg\" width=\"500px\" alt=\"image-1\"></center>\n\nThe amount of time to acquire and master a skill is approximated to be 10,000 hours. But these approximations were taken by studying top athletes, artists, musicians, etc. at the top of the game. They were one of the best in the world.\n\nSo it is a misconception that you need that amount of time to get *good* at something. Good is a relative word. It is idiosyncratic, it simply means that it depends on the individual. So *good* for a professional whose life revolves around a skill (you get what I mean) and for a person for whom it is just another tool in the shed or a hobby is completely different. Many of us don't need to have mastery over a skill as a professional does. Hence the term *good enough*!\n\nHaving said that, the main reason that it takes so much time for a professional to get the mastery is because as you get better, even smaller improvements need a lot of time to be attained. This is explained visually in the infamous **learning curve** graph. So it takes a lot less time than the 10,000 hours to get good. According to the speaker its only 20 hours.\n\nSo the first 20 hours if managed effectively will make you get good at that skill. There are 4 simple steps to manage these hours properly.\n\n### 1. Deconstruct the skill\n\nFigure out what you want to be able to do once you get good. Then break the skill down into manageable chunks. Classify parts which are important to get the skill and practice them first. When you practice the important ones first and spend more time on them you improve faster.\n\n### 2. Learn enough to self correct\n\nPractice and to know are two different things. There may be things you need to know before you start to practice a skill. You need to have some knowledge to self correct when you go wrong. Here what goes wrong is that people focus on the wrong thing. For example, when you want to learn programming, reading a whole book on programming and then starting to practice programming is not the right way. It leads to procrastination and may even get you overwhelmed. So read just enough that you know what you're doing and practice then repeat the process.\n\n### 3. Remove practice barriers\n\nThis is obvious. Remove distractions while you practice. Focus on what is in front of you. It just takes a little bit of willpower to get rid of them. Short periods of focused practice or learning, even 15-20 min will improve you much faster than an hour of distracted mess.\n\n### 4. Practice for at least 20 hours\n\nWhen learning something new there is always the feeling of frustration. We feel stupid not being able to do what we intend to. So commit yourself to practice for at least 20 hours so you overcome that period of frustration, stick to the practice and reap the rewards eventually.\n\nThese are the four simple ways to guide your first 20 hours into learning a new skill.\n> The major barrier to learning something new is not an intellectual problem but an emotional one.\n\nWe are scared to feel stupid, we do not like to feel that. So we unconsciously decide its better off not learning it at all. So put 20 hours into learning anything. That feeling of stupidity will eventually go off. And after those 20 hours when you feel something different, some sense of achievement of drastic improvement, it surprisingly motivates you to get even better than *good*.",
        "author_name": "Yuvasankar B",
        "description": "",
        "slug": "first-20-hours"
    },
    {
        "layout": "post",
        "title": "Image Steganography",
        "author_github": "Jimmy622",
        "date": "2021-06-04T00:00:00",
        "image": "/assets/img/",
        "description": "The technique of hiding text, images, audio inside an image.",
        "tags": [
            "IEEE NITK",
            "Steganography",
            "Encryption Techniques"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Jimmy622",
        "text": "[{% load static %}]()\nData or information is a very crucial resource to us, therefore it's our responsibility to protect its confidentiality and integrity. While sending a message or information, unintended users might try to tamper with the message or might use its content for their good, or other unintended purposes, therefore we need some data hiding techniques to hide some confidential messages. The importance of data hiding techniques comes from the fact that there is no reliability over the medium through which the information is sent. In this blog, we will go through one of the data hiding techniques i.e., Image Steganography.  \n\n## Importance of Image steganography\n\nSteganography is a data hiding technique, which is used to hide information such as text, images, videos, etc. inside another image or video. Image Steganography is the technique of hiding text, images, audio inside an image. \n\nSo why do we need steganography when we have various encryption techniques to do the same? Let's take an example to understand that, suppose we want to encrypt a string \"IEEE is an International Organization\", the encrypted message may look something like \"351@sbttr\\_$ee#^erh@\\_re8rry$#\" or something similar to this based on the encrypting algorithm. The encrypted message often looks like something that makes no sense to the reader without decrypting it. This draws unnecessary attention and prevents us from hiding the presence of a string in it. Sometimes it's necessary to hide even the presence of encrypted information because no matter how strong an encrypting algorithm is, it can be cracked, or it might be necessary because of restrictions on encrypting a message in some countries. In such situations, steganography comes to our rescue. By using this data hiding technique, we can hide the presence of a message in it. We can see its example in the following two images, the first image is a host image, and the second image is the image obtained after encrypting the string \"IEEE is an International Organization\". The difference between the host and encrypted message is not visible to naked eyes. \n\n![img]({% static '/blog/assets/img/image-steganography/host1.jpg' %})\t\t\t![img]({% static '/blog/assets/img/image-steganography/encoded1.jpg' %})\n\nLet\u2019s take another example which shows the hiding of an image within an image![img]({% static '/blog/assets/img/image-steganography/host2.jpg' %})\t\t![img]({% static '/blog/assets/img/image-steganography/lena.jpg' %})\t\t![img]({% static '/blog/assets/img/image-steganography/encoded2.jpg' %})\n\nThe first image is the host image which is used to hide the hiding image i.e., that is the second image in this scenario. The third image is the encrypted image which is the image obtained after hiding image2 in image1 using steganography techniques. \n\nSteganography can be combined with encryption techniques to make a better algorithm.\n\nVarious Image Steganography Techniques: \n\nThere are a lot of image steganography techniques, which are based on LSB implementation, FFT, and CNN. Common FFT-based transformations are Discrete Cosine Transformation (DCT), Quaternion Fourier Transformation (QFT), Discrete Fourier Transformation (DFT), Discrete Wavelet Transformation (DWT), and the Discrete Hadamard Transformation (DHT). In the LSB-based implementation, we make suitable changes in the LSB of each pixel value of the image to hide the data. In the FFT-based implementation, suitable changes are made in the magnitude of the host image's DFT to hide the information. This is because it is well known that for many images, the phase of the Fourier transform is more important than the magnitude. In this blog, we will discuss the implementation of image steganography using the LSB technique. \n\nIn computers, we represent every color as a combination of three fundamental colors i.e., red, green, blue. For instance, indigo color in computers is expressed as (75,0,130), where 75,0,130 represents the respective intensities of red, green, and blue channels.  \n\nWe know that a digital image can be expressed as a multidimensional matrix. Consider a color image of width m and height n. Then the order of the matrix will be (n\u00d7m\u00d73). The third dimension of the matrix represents the number of channels in the image. The following images will help visualize an image as a three-dimensional matrix. \n\n![img]({% static '/blog/assets/img/image-steganography/channel.png' %})\n\n![img]({% static '/blog/assets/img/image-steganography/lsb2.png' %})\n\nIn a 24-bit RGB image, each channel pixel has 8 bits (1byte). So, the pixel intensities vary between 0-255 if we consider the data type to be uint8. The last bit of the bit is called LSB. The binary value of each channel's intensity for gray color is ( [01001011], [00000000], [10000010] ). If we change the LSB of each channel's intensity and make it ( [01001011], [00000000], [10000010] ), this little change in the color won't be visible to the naked eye. This idea forms the basis of image steganography. Suppose we want to hide a text message inside an image, then we first convert the message into binary and then take a single bit of the message, replace the LSB of the pixel value of the host image with the message bit. The following code is a simple way to do the same, \n\n![img]({% static '/blog/assets/img/image-steganography/code1.png' %})\n\nThe code can be suitably modified to add an option of password for encoding as well as for decoding. For decoding the message from the encoded message, we take the LSB of the binary pixel intensities of the encoded image, stack them to make a binary message. This binary message is then converted into an 8-bit long byte, which represents a single decoded character. The following code is a simple way to do the same,\n\n![img]({% static '/blog/assets/img/image-steganography/code2.png' %})\n\nSimilarly, we can write a code for hiding images within an image. One can consider, images as a 2-Dimensional array and text as a 1-dimensional array. This similarity can be used to expand the existing code to hide images.",
        "author_name": "Garvin Pokhra",
        "slug": "image-steganography"
    },
    {
        "layout": "post",
        "title": "State Management in Flutter",
        "author_github": "ikjot-2605",
        "date": "2021-06-04T00:00:00",
        "image": "/assets/img/",
        "description": "Did reading about state management from the plethora of blogs out there just confuse you? Let me set it straight once and for all...",
        "tags": [
            "Flutter",
            "State Management",
            "Development",
            "Good Practices"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "ikjot-2605",
        "text": "[{% load static %}]()\n## Let's start by getting one thing clear.\n\n### **What do you mean by \"state\"?**\n\nEssentially, the state of the app, or a particular widget in the app, is *just information*, that is used to render the UI. This means that any change in the UI was caused by a change in the state of the application.\n\n<center><img src = \"/blog/assets/img/State-Management-In-Flutter/image1.png\" height = 225 alt = \"What is a State?\"></center>\n<hr>\n\n## **Alright, but what is state management?**\n\nState management is nothing but a means to handle the multiple states your application can have. But to go further here, we need to understand the difference between Ephemeral State and Application State.\n<center><img src = \"/blog/assets/img/State-Management-In-Flutter/image2.png\" height = 225 alt = \"State Management\"></center>\n\n<hr>\n\n## **Ephemeral State vs Application State**\nYou can think of the Ephemeral state as the state that is localized to a particular, very small part/widget in the application. <br>For example, a password text field may have two states:\n\n1. Hide Password\n2. Show Password\n\nAs you can probably guess, the state that may be controlling this particular widget is extremely localized and can be handled easily with a setState() method.\n\n**This is an ephemeral state.**\n\nOn the flip side, if you think of another more large-scale feature, such as implementing a dark mode, the state that will be working with this, will have a large-scale impact throughout the application, and hence it cannot be handled with a vanilla setState() method.\n\n**This is an Application state, and this is where the concept of requiring a state management system comes in.**\n<hr>\n\n## **Okay, so how does state management help?**\nWe will answer this by looking at the pros and cons of using **setState()** and **Flutter BLoC**(a popular state management plugin)\n\n## **<u>setState()</u>**\n\n### **Pros of setState()**\n\n1. Very easy and straightforward to understand.\n2. A great way to handle the Ephemeral State.\n\n**Per the docs**: Ephemeral State is sometimes called UI State or local State. It\u2019s the State you can neatly contain in a single widget.\n\n### **Cons of setState()**\n\n1. Not a good way to handle things involving app State (Global State and parts of the State you want to persist between sessions).\n2. Using setState() all over an app can become a maintenance nightmare very quickly because your State is scattered all over the place\n3. Usually used within the same class as the UI code, mixing UI and business logic, which breaks clean code principles. In a tiny app, this is no big deal but it becomes a concern quickly when you have more than just a couple of screens. You can separate the logic with a little effort, but there are better ways to handle State Management once an app grows beyond a few pages.\n4. From my personal experience, your app will quickly become laggy and slow if you use a lot of setState() method. This is because it **rebuilds the entire widget tree** each time it is called.\n\n<hr>\n\n## **<u>Flutter BLoC(Business Logic Component)</u>**\n\n### <u>BLoC is an architectural state management pattern created and used by Google</u>\n\nBLoC stands for Business Logic Components, and it\u2019s much more of an architecture than setState(); it has even been compared to MVVM (Model, View, View Model). Unlike the others, BLoC makes use of Streams and it\u2019s often used with Provider ( another state management plugin ), which is often used as a way of exposing the BLoC for the UI.\n\nBut what does it do? Anything you want. Maybe you fed it a String that said \u201cSmith\u201d and the logic in the BLoC was made to return a list of everyone in your contacts list with that last name. Maybe the BLoC gets fed the number of clock ticks that have passed since an animation started and the BLoC\u2019s job is to calculate the position of your bouncing ball based on how long ago you pressed the button. You can make it do whatever you want, it\u2019s an architecture, a method of handling State\u2026 what you do with it is entirely up to you.\n\n<center><img src = \"/blog/assets/img/State-Management-In-Flutter/image3.png\" height = 500 alt = \"BLoC Flow\"></center>\n\n### **Pros of BLoC**\n\n1. Easy to separate UI from logic\n2. Easy to test code\n3. Easy to reuse code\n4. Good Performance\n\n### **Cons of BLoC**\n\n1. Technically, you need to use streams in both directions, creating a lot of boilerplate. However, many people cheat this by using streams only from the backend to the UI, but when events occur they\u2019re simply calling functions directly instead of feeding those events into a sink.\n2. More boilerplate code, but it's worth it due to the extensive reusability. \n\n<hr>\n\n## **Conclusion**\n\n### In this article we discussed\n\n 1. What is state?\n 2. What is state management?\n 3. What are the two types of state?\n 4. Comparison of two popular yet very different techniques to implement state management.\n\n### There are other options as well which are beyond the scope of this article like Redux, Flutter Hooks, etc.\n\n<hr>",
        "author_name": "Ikjot Singh Dhody",
        "slug": "state-management-in-flutter"
    },
    {
        "layout": "post",
        "title": "The Impact of F1 on the World",
        "date": "2021-06-18T00:00:00",
        "image": "/assets/img",
        "tags": [
            "IEEE NITK",
            "Piston"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "Kaustubh",
        "text": "[{% load static %}]()\nIn general, F1 and motorsport have a long history of inventing and implementing technologies that eventually trickles down into road cars. Active suspension, traction control, carbon fibre construction, advanced aerodynamics and semi-automatic paddle gearboxes all began in the technology race that is Formula 1, not to mention the endless tyre and safety technology that continues to shape and give knowledge to today's motor industry.\n\n## On The Road\n\nThe 2014 F1 regulations that require teams to use a V6 Hybrid engine has directly resulted in pushing forward engine technology, with the power units reaching a thermal efficiency of over 50 percent, making it one of the most efficient internal combustion engines ever. F1 hybrid power units are not just extraordinarily efficient; they also have made considerable contributions to battery technology. The same learnings that delivered improved power for racing have been applied to improving fuel consumption in the road-going environment. This has also been directly applied into Mercedes-AMG's Project One hypercar and Aston Martin's Valkyrie. Hypercars aside, Mercedes uses hybrid technology derived from its F1 car in models such as the S-Class.\n\n![img-1]({% static '/blog/assets/img/impact-of-f1/image1.jpg' %})\n\nEven though F1 isn't an Autonomous Racing series, its race cars are an ideal testbed for certain technologies vital to autonomous driving due to the sheer amount of data that is processed locally on the car and sent to the garage.\n\nMeanwhile, KERS introduced back in 2009 to harness braking energy and unleash it on the track, is now used not only in hybrid cars, but also in buses, helping make cities greener.\n\n![img-2]({% static '/blog/assets/img/impact-of-f1/image2.jpg' %})\n\nThese were the more modern technologies applied to cars. Many such innovations, especially from the 1980s and 1990s, can be seen not only in high-end cars, but in everyday vehicles too. Suspension technology, traction control, Engine control units, Automotive Aerodynamic Devices, Paddle shifters and many more were perfected by F1 in and can be seen in nearly every modern-day car.\n\n![img-3]({% static '/blog/assets/img/impact-of-f1/image3.jpg' %})\n\n## Changing Lives\n\nA Formula One car is probably one of the most connected cars in the world. A single car is embedded with hundreds of sensors that measure all kinds of things \u2013 from temperatures to forces. Engineers can extract the most out of this if that data is available in real-time; however, it's limited by a control unit that's common for all teams. This results in teams trying to transfer this data to the garages through the pit stops, while also making sure that they don't have to sacrifice track time. In 2017, F1 teams began using 5G technology, which allowed them to transmit this data wirelessly at speeds that match the sport. When the car is within four meters of the garage, it can transmit a GB of data in less than five seconds.\n\nSimilar to the garage, modern hospitals and medical research institutes too face the challenge of transferring large amounts of data from devices such as X-Ray machines and have implemented a similar solution through various F1 technology partners.\n\nThis technology is not a standalone Formula One product, though. Qualcomm has developed it for the consumer market and used F1 as a high-speed R&D environment, putting its product to the ultimate test. In the future, it is very likely that this technology will be seen in various everyday devices.\n\nThe same technology is already being implemented by McLaren Applied Technologies to connect roads, rails and underground transportation.\n\n![img-4]({% static '/blog/assets/img/impact-of-f1/image4.jpg' %})\n\nF1 Engineering is not all a world of high \u2013 tech -Williams Advanced Engineering has worked with its F1 partners as part of a total benefits package for sponsors. Working with Unilever, Williams F1 managed to incorporate its technology, believe it or not, into the washing powder sector! It worked with Unilever on management methods and teamwork but also on engineering specifics like a drying tower for soap suds. Williams has also adapted F1 front wing designs to help increase the efficiency of supermarket refrigerators.\n\nIn 2020, when there was a dire need for respiratory devices such as ventilators, seven F1 teams answered the UK government's call for assistance by setting up 'Project Pitlane'. Under 'Project Pitlane', teams used their unique ability to respond rapidly to technological challenges and sped up the production of various medical devices. Mercedes and the University College of London had also developed a breathing device that could be mass-produced.\n\n![img-5]({% static '/blog/assets/img/impact-of-f1/image5.jpg' %})\n\n## Environmental Impact\n\nF1 over the years, especially over the past few years, has piled up a bad rapport for not being or not doing enough to be environment friendly. In response to this, F1 brought in regulation changes in 2014 that required teams to use a V6 Hybrid engine instead of the fuel-guzzling V8 engines. But this isn't enough, which is why they've unveiled 'Countdown to Zero'. F1 is on a mission to have a net zero carbon footprint by 2030 by adapting the sport towards a more sustainable means.\n\nF1 is working closely with the FIA on advancing the development of sustainable fuels. In the short term, they plan on increasing the percentage of biological components in the fuel and to completely move to a sustainable fuel over time. F1 has now created a second-generation biofuel variety as part of its push, which has been exclusively refined from bio-waste that is not intended for human or animal consumption. F1's engine manufacturers have been asked to test and validate the fuel to prove that the technology is feasible for racing. The racing contributes to only a small per cent of the total carbon emissions, while the main culprits are the team factories and transportation.\n\nIn 2020, F1 accelerated plans to introduce a more sustainable way of broadcasting our races, known as remote production. This has resulted in a 34% reduction in technical cargo and a 36% reduction in travelling staff sent to every race. They're also moving on to more fuel-efficient means of transport such as railways and sea freights. They've also transitioned into 100% renewable electricity for all of its UK offices. But this isn't something new to them. Mclaren F1 has been carbon neutral since 2011!\n\nDespite all the measures taken, there will inevitably be emissions, which is why they'll offset this through robust and verifiable biological and technical sequestration programmes.",
        "author_name": "Kaustubh",
        "description": "",
        "slug": "impact-of-f1"
    },
    {
        "layout": "post",
        "title": "LSTM Models",
        "author_github": "Predator4hack",
        "date": "2021-06-18T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE-NITK",
            "Diode",
            "Machine Learning",
            "Deep Learning",
            "LSTM"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Predator4hack",
        "text": "[{% load static %}]()\nWhat\u2019s the first thing that comes to your mind when you think of speech recognition? Alexa, Google assistant, Cortana, right? It's quite reasonable; we are fascinated by machines that can understand us. But have you ever wondered how these things even work?\nAs you read this article, you\u2019ll get the basic idea along with the maths used behind this.\n\nWhen analyzing human voice, a very important aspect are filters, which constitute a selective frequency transmission system that allows energy through some frequencies and not others. Currently, the standard way of preprocessing audio is to compute the short-time Fourier transform (STFT) with a given hop size from the raw waveform. This gives us a tridimensional arrangement called a spectrogram that shows the frequency distribution and intensity of the audio as a function of time.\n\nOnce we have the spectrogram, it is possible to propose a neural network that is able to handle command recognition while still keeping a small footprint in terms of number of trainable parameters. This leads to our main topic of the article, LSTM models.\nBut before going to that, let's just look at its predecessor model, Recurrent Neural Network (RNN).\n\n## **Recurrent Neural Networks**\n\nHumans don\u2019t start their thinking from scratch every second. As you are reading this essay, you understand each word based on your understanding of previous words. You don\u2019t throw everything away and start thinking from scratch again. Your thoughts have persistence.\nTraditional neural networks can\u2019t do this, and it seems like a major shortcoming. Recurrent neural networks address this issue. They are networks with loops in them, allowing information to persist.\n\n![image-1]({% static '/blog/assets/img/lstm-models/png1.jpg' %})\n\nRecurrent neural networks (RNNs), are a class of neural networks that allow previous outputs to be used as inputs while having hidden states. It can be thought of as multiple copies of the same network, each passing a message to a successor. For each timestamp t, the activation a<t> and the output y<t> are expressed as follows:\n\n![image-2]({% static '/blog/assets/img/lstm-models/png2.jpg' %})\n\nwhere Wax , Waa , Wya , ba , by are the coefficients that are shared temporally and g1 , g2 activation functions.\n\n## **The Problem of Long-Term Dependencies**\n\nRNNs were designed to connect the previous information to the present task, but unfortunately, as the gap grows, RNNs become unable to connect the information.\n\n![image-3]({% static '/blog/assets/img/lstm-models/png3.png' %})\n\nThe reason behind this behavior is vanishing and exploding gradients. Due to this,  it is difficult to capture long-term dependencies because of multiplicative gradients that can be exponentially decreasing/increasing with respect to the number of layers.\n\nThankfully, LSTMs don\u2019t have this problem!\n\n## **LSTM Networks**\n\nLong Short Term Memory (LSTMs) are explicitly designed to avoid the vanishing and exploding gradient phenomenon.\nThey have internal mechanisms called gates that can regulate the flow of information.\nLike RNNs, LSTMs also have a chain-like structure, but the repeating module has a different structure. Instead of having a single neural network layer, there are four layers that interact in a very special way.\n\n![image-4]({% static '/blog/assets/img/lstm-models/png4.png' %})\n\nFirst of all, let\u2019s just try to get comfortable with the notation we\u2019ll be using.\n\n![image-5]({% static '/blog/assets/img/lstm-models/png5.png' %})\n\nIn the above diagram, the pale yellow boxes represent a neural network layer. The pink circles represent pointwise operations, like vector addition/subtraction. Lines merging denote concatenation, while a line forking denotes its content being copied and the copies are passed to different locations.\n\n## **The Core Idea Behind LSTMs**\n\nThe core concept of LSTMs are the cell states and its various gates. One can treat cells like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions which helps to retain info even in a very long chain. It\u2019s very easy for information to just flow along it unchanged.\n\n![image-6]({% static '/blog/assets/img/lstm-models/png6.png' %})\n\nGates provides a way to optionally let information through. They\u2019re composed of a sigmoid neural network layer and a pointwise multiplication operation. First, we\u2019ve the \u201cforget gate\u201d. This gate decides what information should be kept or thrown away. Data from the current input is passed through the sigmoid function. Values come out to be 0 and 1. The values that are closer to 0 means to forget and closer to 1 means to keep.\n\n![image-7]({% static '/blog/assets/img/lstm-models/png7.png' %})\n\nNext, we\u2019d like to make a decision on what new information we\u2019re going to store within the cell state. This has two parts. First, a sigmoid layer also called the \u201cinput gate layer\u201d decides which values we\u2019ll update. Next, a tanh layer creates a vector of the latest candidate values, C~t, that would be added to the state. In the next step, we\u2019ll combine these two to make an update to the state.\n\n![image-8]({% static '/blog/assets/img/lstm-models/png8.png' %})\n\nIt\u2019s now time to update the old cell state, Ct\u22121, into the new cell state Ct. This is the new candidate value, scaled by how much we decided to update each state value.\n\n![image-9]({% static '/blog/assets/img/lstm-models/png9.png' %})\n\nFinally, we have the output gate which will output a  filtered version of the cell state. First, we have a sigmoid layer which decides the parts of the cell state we\u2019re going to output. Then, we put the cell state through tanh which ensures to keep the values between 1 and -1, and then, multiply it by the output of the sigmoid gate, so that we can get desirable output.\n\n![image-10]({% static '/blog/assets/img/lstm-models/png10.png' %})\n\n## **Applications of LSTMs**\n\n### ***Language Modelling and predictions***\n\nIn this method, the likelihood of a word in a sentence is considered. The probability of the output of a particular time-step is used to sample the words in the next iteration(memory). In Language Modelling, input is usually a sequence of words from the data and output will be a sequence of predicted word by the model. While training we set xt+1 = ot, the output of the previous time step will be the input of the present time step.\n\n### ***Speech Recognition***\n\nA set of inputs containing phoneme(acoustic signals) from an audio is used as an input. This network will compute the phonemes and produce a phonetic segments with the likelihood of output.\n\n### ***Machine Translation***\n\nIn Machine Translation, the input is will be the source language(lets say Hindi) and the output will be in the target language(lets say English). The main difference between Machine Translation and Language modelling is that the output starts only after the complete input has been fed into the network.\n\n## **Conclusion**\n\nWe have learned the basic idea behind RNN and LSTM, how LSTM manages the shortcomings of RNN and their use in speech recognition. LSTMs are a very promising solution to sequence and time series related problems. Within a few dozen minutes of training can generate nice looking descriptions of images, Image Captioning.  LSTM\u2019s and GRU\u2019s are also used in state-of-the-art deep learning applications like speech synthesis, natural language understanding, etc.\n\nIf you\u2019re interested in going deeper, here are links to some fantastic resources that can give you a different perspective in understanding LSTM\u2019s and GRU\u2019s. This post was heavily inspired by them.\n\n## **References**\n\n1. [https://colah.github.io/posts/2015-08-Understanding-LSTMs/](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n2. [https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#overview](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks#overview)\n3. [https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n4. [https://www.youtube.com/watch?v=8HyCNIVRbSU](https://www.youtube.com/watch?v=8HyCNIVRbSU)\n5. [https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0](https://towardsdatascience.com/the-fall-of-rnn-lstm-2d1594c74ce0)\n6. [https://analyticsindiamag.com/overview-of-recurrent-neural-networks-and-their-applications/](https://analyticsindiamag.com/overview-of-recurrent-neural-networks-and-their-applications/)",
        "author_name": "Chandan Kumar",
        "slug": "lstm-models"
    },
    {
        "layout": "post",
        "title": "Trying-to-understand-Trie",
        "author_github": "DeltaCube23",
        "date": "2021-06-18T00:00:00",
        "image": "/assets/img/",
        "description": "An Overview of the Trie Data Structure",
        "tags": [
            "IEEE-NITK",
            "CompSoc",
            "Algorithms",
            "Data Structures"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "DeltaCube23",
        "text": "[{% load static %}]()\n## Introduction\n\nA trie is a rooted tree that maintains a set of strings. Each string in the set is stored as a chain of characters that starts at the root. If two strings have a common prefix, they also have a common chain in the tree. Consider the following set of strings {canal, candy, there, the} and let us make an example trie from these.\n\n![Example Trie]({% static '/blog/assets/img/Trying-to-understand-Trie/image1.png' %})\n\nThe * character in the node means that a string in the set ends at that node. Such a distinction is needed because a string may be a prefix of another string like \u201cthe\u201d is of \u201cthere\u201d.\n\nThe 2 basic operations of a trie is search and insert :\n\n**Search** - We can check if a trie contains a string of length n in O(n) time because we can just follow the corresponding chain starting from the root and suppose a character in that string is not present in the trie then it means that string does not exist.\n\n**Insert** - We can also insert a string of length n in O(n) time by initially following the chain till the longest matching prefix available and then adding nodes as necessary. So in the end the extra nodes that we will have to insert are just the ones not in the longest matching prefix.\n\nThe **size** of a trie is equal to the number of distinct prefixes of all the strings in the set. The **height** of a trie is defined as the length of the longest string in the set.\n\n## Applications\n\nTrie is an incredibly useful data structure for many real life applications which are based on string processing and handling. Some examples are given below:\n\n**Auto Complete** - One could say autocomplete is the most useful application of trie. It is a process through which the software predicts the rest of the word a user is typing based on the string prefix. It helps to optimize search engine results and also improves the user experience. Various software applications such as web browsers, command line interpreters, email, database query tools and others use autocomplete. To improve the quality of the suggested words we can store additional information like a key value and prefer those words with the same prefix that have a history of being used more often, this is similar to browser history.\n\n**Spell Checkers** - Trie is used to store the data dictionary and can check for the word in the dictionary and if it is not present then generate potential valid suggestions of words that can be constructed instead. Then sort these suggestions with the higher priority ones on top and present it to the user.\n\n**DNA Sequencing** - It is used to store and process large DNA sequences and search for matching patterns present in it. Since this process requires significant computation and memory a trie is quite an efficient data structure for this.\n\n## PseudoCode\n\n### **Trie Node Structure**\n\n```cpp\nstruct TrieNode\n{\n    struct TrieNode *children[26];\n    bool is_end;\n    TrieNode()\n    {\n        is_end = false;\n        for(int i=0; i<26; i++)\n        children[i] = NULL;\n    }\n};\n\nstruct TrieNode *root;\n```\n\n### **Insert Operation**\n\n```cpp\nvoid insert(string key)\n{\n    struct TrieNode *pwalk = root;\n    \n    for(int i = 0; i < (int)key.length(); i++)\n    {\n        int index = key[i] - 'a';\n        if(!pwalk->children[index])\n        pwalk->children[index] = new TrieNode();\n        pwalk = pwalk->children[index];\n    }\n    \n    pwalk->is_end = true;\n}\n```\n\n### **Search Operation**\n\n```cpp\nbool search(string key)\n{\n    struct TrieNode *pwalk = root;\n    \n    for(int i = 0; i < (int)key.length(); i++)\n    {\n        int index = key[i] - 'a';\n        if(!pwalk->children[index])\n        return false;\n        \n        pwalk = pwalk->children[index];\n    }\n    \n    return pwalk->is_end;\n}\n```\n\n## Improving efficiency of a solution using Trie\n\nConsider the problem where we need to find the maximum value xor pair in an array of numbers.\n\nThe Naive approach would be to calculate the xor value of all pairs of numbers in the array and then find the maximum value out of it. However this is not very efficient as the time complexity is **O(N^2)**.\n\nHere we can use trie to store the binary representation of each number in the array and then we search for each number the maximum xor value we can get by taking the path opposite to the value of that bit in the number. Suppose the current bit is 1 then we look for a path with the bit set to 0 and we take the path with the opposite bit as long as it exists. In this manner for each number we will end up with the maximum value xor pair it can form and then take the maximum of these values.\n\n## Comparison with other data structures\n\nTrie and hash tables are used in many similar applications. There are many advantages of a trie over a hash table :\nThere are no collisions of different keys in a trie.\nBecause of the above property the worst case look up time in a trie is better ( O(length) ) than hash table which can be ( O(n) ) due to collisions. Best case for both is the same ( O(length) ).\nTrie provides an alphabetical ordering of entries.\nTrie does not require buckets unless a single key is associated with more than one value.\n\nThere are some drawbacks also for trie in comparison to hash table:\nThey require more memory for storing strings.\nWhen keys are floating point numbers it can lead to long chains that are not really meaningful. However this can be handled by using a variant of trie called bitwise trie.\n\nHere is a list of some resources to get a better understanding of trie and also some common interview problems.\n\n- [Maximum Xor Subarray](https://www.geeksforgeeks.org/find-the-maximum-subarray-xor-in-a-given-array/)\n- [Trie Explained](https://www.hackerearth.com/practice/data-structures/advanced-data-structures/trie-keyword-tree/tutorial/)\n- [Good Insight](https://medium.com/underrated-data-structures-and-algorithms/trie-data-structure-fd9a2aa6fcb8)",
        "author_name": "Adithya Rajesh",
        "slug": "trying-to-understand-trie"
    },
    {
        "layout": "post",
        "title": "Concrete: The Way You've Not Seen It",
        "author_github": "Harshavardhan",
        "date": "2021-07-05T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Concrete"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "Harshavardhan",
        "text": "[{% load static %}]()\n## Introduction\n\nConcrete, as we know it today, is a mixture of cement, water, and some aggregates along with it. After Portland Cement was patented in 1824, concrete came out to the world as one of the most useful materials ever made by human beings. Now, it's leading the construction industry from the front by being the most durable building materials. It provides good fire resistance, and structures made of concrete have a long service time. But, to go along with the endless list of positives of concrete, there are a few negatives as well. Let's find out more.\n\n## History of Concrete\n\nThe traders may have built the first concrete structures in oases of Southern Syria around 6500 BC. These traders kept the concrete as dry as possible as excess water weakens the concrete. By 700 BC, they figured out how to build kilns to produce material for the construction of concrete floors, rubble-wall houses, underground waterproof cisterns, etc. These cisterns were kept secret and probably were why these people were able to thrive in the deserts of Syria.\n\nThe Egyptians, later in-3000BC, used gypsum and lime mortars in building the pyramids. The bedding material used in the Great Pyramid of Giza was 500000 tonnes of lime mortar. The mortar was used for casing stone which is the visible surface of the pyramid. Around the same time, the Chinese were able to build the Great Wall of China using a particular kind of cement which was also used in boat building.\n\nLater came the Romans, who are considered the masters of cement and concrete. By 200BC, the Romans were building very successfully with material like cemented rubble. They have created many beautiful structures like the Pantheon, Colosseum, and numerous arches and domes.\n\n## The Rise of the Concrete Industry\n\nConcrete is the most used human-made material ever. It's the second most consumed substance after water. Every second, over 3000 bathtubs of concrete are used for construction. Concrete production has increased more than 30 folds since 1950 and almost four-folds since 1990. China used more concrete in 2011-2013 than the USA in the 20th century. All these numbers say concrete is one of the essential materials in the modern-day. It is the foundation of modern development, roof for billions of people, strengthening us to fight the natural calamities, and providing structure to significant buildings like schools, hospitals, etc.\n\nSo, why are we speaking these numbers? Why are we calculating the amount of concrete which is helping humans in such a notable fashion? It is because one of the chief nature of the concrete is to harden and degrade dangerously slowly. Then why is it less discussed than plastic?\n\n## Concrete against Nature\n\nIt's because it's not getting found in the stomachs of animals, its traces aren't found in our blood, we don't see it in every other trash.\n\nNot degrading is not the only problem with concrete. Concrete is a dominant contributor to global warming and other climate crises. If concrete were a country, it would be the third most CO\u2082 producer only after the USA and China. The main ingredient in concrete is cement, which is produced by a kiln which requires a tremendous amount of fossil fuels which in turn contributes to CO\u2082 emission. This is not the only way in which the concrete industry produces CO\u2082.\n\nHalf of the CO2 emissions in the production of concrete come from the reaction that breaks up the calcium carbonate into CO and CO\u2082, and the other half from the fossil fuels required to heat the kilns and transport the materials. Concrete accounts for about 8% of total CO\u2082 emission. This might not sound much compared to other industries such as transportation, electricity, agriculture, but researchers have already come up with a few gripping solutions to this concrete situation.\n\n## Solutions\n\nIt's almost unbelievable sometimes that with some research and innovation, what feat we can achieve. One example is CarbonCure's concrete. Its innovative system injects CO\u2082 into the concrete as it's being mixed. When the concrete hardens, those otherwise harmful emissions are sequestered forever before it even enters our atmosphere. CarbonCure converts the CO\u2082 into a mineral which itself helps to strengthen the material. So, now, we will be able to use less cement in their mixture and still get the same strength. With its innovative technology, CarbonCure has saved over a hundred thousand tonnes of CO\u2082 produced. The best thing about CarbonCure is, it comes at the same cost as the standard concrete which we use!\n\nThere are multiple other alternatives such as ashcrete, greencrete, mycelium, etc. But these materials are new and can't account for the same strength that concrete promises.\n\n## Conclusion\n\nWith everything said, there are multiple arguments against concrete, and many of which show us why using concrete is necessary. But we must control concrete from growing into a highly polluting industry like plastic. The recent increase in concrete consumption and the predicted increase in consumption in coming years only acknowledge the fact. CarbonCure and other innovations such as this can make the burden less on nature to sustain.",
        "author_name": "Harshavardhan S",
        "slug": "concrete-the-way-youve-not-seen-it"
    },
    {
        "layout": "post",
        "title": "Holographic Memory",
        "author_github": "somaanilkumar",
        "date": "2021-07-05T00:00:00",
        "image": "/assets/img/",
        "description": "An introduction to holographic memory",
        "tags": [
            "IEEE NITK",
            "Diode",
            "Data Storage"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "somaanilkumar",
        "text": "[{% load static %}]()\n# Holographic Memory\nThe word holography might fill your mind with 3-D images, but what the\nterm essentially means is to record a unique photographic image of\nsomething that can be stored and later retrieved. The definition of\nholography hints at the storage of data. Instead of using the surface to\nstore data (like C.D.s), holographic data storage aims to use the entire\nvolume of a medium to store data (three-dimensional data storage).\nAlthough holographic data storage technology is as old as that of the\ninternet, it hasn't proved indispensable for data storage. That might\nseem concerning to someone aware of its technological advantages, but\nthe internet took its own sweet time (nearly 30 years) before it gained\nwidespread popularity. It's all about the timing of a particular\ntechnology, as in the case of the internet, and for now, it seems that\nholographic data storage hasn't yet found its purpose. But, given the\nadvancements in other technology fields, such as big data and\nbiotechnology, it seems inevitable that holographic data storage will be\nthe standard to aid these advancements.\n\nHenry Caulfield (the principal scientist at the Sperry Rand Research\nCenter in the U.S.A.) and electrical engineers Richard A. Soref and\nDonald H. McMahon filed the first patent (U.S. patent \\#3,635,538) for\nholographic data storage are credited with pioneering holographic\nmemory. The beauty of holographic memory is its ability to store large\namounts of data in a tiny amount of volume. If the cost of data storage\nper volume in holographic memory does come down, as it did in the case\nof hard disks, it would be revolutionary given how businesses and\ngovernments alike will have access to large amounts of data at a low\nprice. More importantly, it could benefit most users as computing is\nbecoming more ubiquitous and an integral part of their lives. It's hard\nto predict the future, but it seems clear that humanity aims to merge\nwith computers to enhance itself, which demands a lot of processing and\ndata about the human body (an incredibly large data source).\n\n<figure class=\"image\" style=\"text-align: center; color: gray;\"><img src=\"/blog/assets/img/holographic-memory/image1.png\" alt=\"image1\">A holographic storage device developed by InPhaseTechnologies (one of the companies leading in holographic storage device manufacture)</figure>\n\nAt the core of holographic memory creation is the interference of light,\nwhich is used to store an object's properties. Here is how it works:\n\nAs is the case with any data storage device, two processes are\nconsidered: \n- how data is written to storage  \n- how data is read.\n\n**Data Write:**\n\nIn the data writing process, a laser beam is generated and split into\ntwo beams called the data beam and the reference beam done using a beam\nsplitter (basically a half-silvered mirror). The data beam is then\nincident on the object to be encoded. You might ask, how can binary data\nbe represented as an object? This is done by representing 1s and 0s as\nlight and dark patterns on an LCD. The reference beam follows a path\ndifferent from that of the data beam. It travels without interacting\nwith anything in its path. The two resultant beams are then incident on\nthe recording medium, typically a lithium niobate crystal, where they\ninterfere and create an interference pattern stored in the crystal as\ndata. By changing the reference beam's angle, multiple data pages are\nstored in the same volume of the recording medium, which gives rise to\nthe enormous data storage capacity of holographic memory. U.V. light can\nbe used to erase the data stored in a crystal.\n\n<figure class=\"image\" style=\"text-align: center; color: gray;\"><img src=\"/blog/assets/img/holographic-memory/image2.png\" alt=\"image2\">The image above shows the data read process. The beam on the right is the reference beam, and that on the left is the data beam. The cuboid is the recording medium (usually lithium niobate crystals)</figure>\n\n\n**Data Read:**\n\nThe data read process involves the use of just the reference beam. An\nappropriate angle of the reference beam is selected depending on the\ndata page required to be chosen. The reference beam is then fired on the\nselected page, which recreates the data stored at that particular\nlocation, which is then converted into binary data, which can then be\nprocessed.\n\n<figure class=\"image\" style=\"text-align: center; color: gray;\"><img src=\"/blog/assets/img/holographic-memory/image3.png\" alt=\"image3\">The image above shows the data read process. Only the reference beam is used</figure>\n\nAlthough in its early stages of development to enable mass usage,\nholographic memory has a vast potential to affect a wide variety of\nusers. It has several advantages:\n\n1. On examining the data read process and comparing it with that of a CD\nor other contemporary storage devices, it can be observed that\nholographic data read process can read a huge amount of data at a time\ncompared to other storage devices. This is beneficial to both store and\naccess high-quality digital multimedia, which will provide a smooth\nexperience to the user.\n2. The high capacity and long durability of holographic memory\n(typically 50 years) make it highly desirable for medical archiving.\nD.N.A. has enormous amounts of data. Scientists working on genetic\nengineering desire high-density storage media and for long periods,\nwhich are then processed by computers. It also favors corporations and\ngovernments, which usually store and process tremendous amounts of data.\n3. Holographic memory has an added advantage of low power consumption,\nwhich improves the portability of these devices.\n\nConsidering all these advantages, it might seem evident that holographic\nmemory might be in widespread use with a few years. But there are\nseveral barriers to be overcome to achieve this. This also explains\ntheir non-availability for commercial use in today's world, considering\nthey were invented nearly half-a-century ago. These include the high\ncost and high sensitivity of these machines. They will not be adopted as\nlong as the price of holographic data storage drops. The project undertaken by the tech giant Microsoft is worth mentioning in this regard. Microsoft's project HSD aims at harnessing the power of massive data storage capacities that Holographic Memory can provide. This project seems to be a result of the enormous rise in cloud storage requirements. Microsoft Azure and Microsoft Research Cambridge are working to make Holographic Data Storage a reality. Microsoft is just one of the many companies and\nresearchers working to overcome the issues faced with Holographic Data Storage and bring them into the\nmarket. It is thus unclear if holographic memory will render current data storage devices\nobsolete. But, as history speaks for itself in the case of great\ntechnologies, patience and timing might result in holographic memory\nhaving a significant impact on the future of technology.\n\n# References:\n\n1. [Lasers, Technology, and Teleportation with Prof. Magnes](https://pages.vassar.edu/ltt/?p=998)\n2. [Processes used in holographic memory and images](https://www.youtube.com/)\n3. [Holographic storage dynamics in lithium niobate: theory and experiment](https://www.osapublishing.org/josab/ViewMedia.cfm?uri=josab-13-11-2513&seq=0&guid=3ba83cfd-d0a1-8422-199d-0ddca4145945)\n4. [Holographic Data Storage](https://en.wikipedia.org/wiki/Holographic\\_data\\_storage)\n5. [Project HSD: Holographic Storage Device for the Cloud](https://www.microsoft.com/en-us/research/project/hsd/)",
        "author_name": "Soma Anil Kumar",
        "slug": "holographic-memory"
    },
    {
        "layout": "post",
        "title": "Introduction to Audio Processing in NLP",
        "author_github": "spectre900",
        "date": "2021-07-05T00:00:00",
        "image": "/assets/img/",
        "description": "An introduction to the various techniques used for processing audio in NLP.",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Audio Processing",
            "NLP"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "spectre900",
        "text": "[{% load static %}]()\n## Introduction\n\nAny machine learning task involves three steps - data collection, training and evaluation. However, for training a machine learning model, we cannot use raw data. We need to pre-process the data to some suitable form and extract features which can be used for training the model. In the case of NLP, audio signals are used as data, hence one must be familiar with the various processing techniques specific to the audio data types. One of the state-of-the-art tools for extracting features from an audio signal is MFCCs (Mel-frequency cepstral coefficients), hence we will be specifically focusing on MFCC feature extraction.\n\n\n## Useful Techniques and Terminologies\n\n\n#### Fourier Transform\n\nFourier transform is an important tool used in signal processing, it is a mathematical transform that converts a time-domain signal into a frequency domain signal. When we calculate a Fourier transform, we begin with a function of time, f(t), and through mathematical decomposition, we produce a function of frequency, F(&omega;). When discrete signals are involved, Discrete Fourier Transform (DFT) is used, which is normally computed using the so-called Fast Fourier Transform (FFT). Here is a sample Python code to calculate FFT of an audio clip using the SciPy library. You can refer to this [link](https://en.wikipedia.org/wiki/Fast_Fourier_transform) to know more about FFT.\n\n```\nimport numpy as np\nfrom scipy.io import wavfile as wav\nfrom scipy.fft import fft\n\nrate, data = wav.read('audio.wav')\nfft_out = fft(data)\nprint(fft_out)\n```\n\n#### Spectrum and Cepstrum\n\nTwo important features in audio processing are Spectrum and Cepstrum. Both Spectrum and Cepstrum are closely related to each other.\n\n1. A spectrum is the Fourier transform of a signal, hence a spectrum is the frequency domain representation of a time-domain audio signal.\n2. A cepstrum is defined as the Fourier transform of the logarithm of the spectrum. This results in a signal that's neither in the frequency domain nor in the time domain. The domain of the resulting signal is called the quefrency. You can refer to this [link](https://en.wikipedia.org/wiki/Cepstrum) to know more about cepstrum and quefrency.\n\nThe reason for converting signals into their frequency domain is related closely to the biology of the human ear. The cochlea is a portion of the inner ear that looks like a snail shell and is a fluid-filled part with thousands of tiny hairs that are connected to nerves. The shorter hairs resonate with higher frequencies and the longer hairs resonate with lower frequencies. Since our ears are frequency analyzers, decomposing audio signals into frequency domain seems like a logical approach to extract features from it.\n\n\n#### The Mel-Frequency Scale\n\nThe human hearing is highly selective to lower frequencies ( < 1000 Hz) and this keeps decreasing as the frequency increases. Mel-Frequency Scale is a kind of psycho-acoustic scale, derived from a set of experiments on human subjects. It has high resolution at lower frequencies and the resolution keeps decreasing as the frequency increases, hence it helps to simulate the way human ears work.\n\n## Mel-frequency cepstral coefficients (MFCC)\n\nOne popular audio feature extraction method is the Mel-frequency cepstral coefficients (MFCC). They are coefficients that collectively make up an MFC (Mel-frequency cepstrum). In the MFC, the frequency bands are equally spaced on the Mel scale, which approximates the human auditory system's response more closely than the linearly-spaced frequency bands used in the normal cepstrum. Hence MFCCs can represent human audio signals more effectively as compared to ordinary cepstrum or other transformations.\n\n\n#### Steps to calculate Mel-frequency cepstral coefficients\n\n1. Break the signal into overlapping frames.\n2. Apply FFT to get the frequency Spectrum.\n3. Apply Mel Filter banks.\n4. Take the logarithm.\n5. Apply FFT to get the Mel-frequency cepstral coefficients.\n6. Keep the first 13 coefficients and discard the rest.\n\n\n#### Calculating MFCC in Python\n\nHowevere there are many wonderful libraries in python which can do all this in a single line of code. One such library is **Librosa**. The code for generating MFCC for a sample audio clip is given below.\n\n```\nimport librosa\nfrom librosa import display\nimport matplotlib.pyplot as plt\n\ny, sr = librosa.load(librosa.util.example_audio_file()) # reading audio clip\n\nlibrosa.display.waveplot(y, sr=sr)                      # plotting audio clip\nplt.title('Audio Wave Plot')\nplt.show()\n\nmfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)     # calculating the first 13 MFCCs\n\nlibrosa.display.specshow(mfccs, x_axis='time')          # plotting the MFCCs\nplt.colorbar()\nplt.title('MFCC')\nplt.tight_layout()\nplt.show()\n```\n\nThis is the wave plot of the sample audio clip.\n\n![Wave Plot of Sample Audio Clip]({% static '/blog/assets/img/Audio-Processing-NLP/wave.png' %})\n\nThis is the plot of the first 13 MFCCs of the sample audio clip.\n\n![MFCC Plot of Sample Audio Clip]({% static '/blog/assets/img/Audio-Processing-NLP/mfcc.png' %})\n\n## References\n\n[Fourier Transform](https://en.wikipedia.org/wiki/Fourier_transform)\n\n[Spectrum and Cepstrum](https://agilescientific.com/blog/2012/3/23/the-spectrum-of-the-spectrum.html)\n\n[Mel-frequency cepstral coefficients](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum)",
        "author_name": "Pratham Nayak",
        "slug": "introduction-to-audio-processing-in-nlp"
    },
    {
        "layout": "post",
        "title": "Alpha Beta Pruning",
        "author_github": "pranavdv",
        "date": "2021-07-11T00:00:00",
        "image": "/assets/img/",
        "description": "Overview of the Alpha Beta Pruning optimization technique on regular Minimax",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Artificial Intelligence",
            "Algorithms",
            "Game Theory"
        ],
        "categories": [
            "Compsoc"
        ],
        "github_username": "pranavdv",
        "text": "[{% load static %}]()\nAmong the many advancements computer science has made in the past couple of decades, Artificial Intelligence (AI) is the most widely known. You might have simply heard about J.A.R.V.I.S from the Avengers or smart assistants like Siri or Alexa. All of them use AI and machine learning algorithms to work as well as they do. It turns out that we can get a glimpse into one such algorithm - particularly the one used by computers to beat you at chess, connect 4, tic-tac-toe and more. It\u2019s called the **Minimax Algorithm** and you can implement this by yourself very quickly. In fact, you can optimize this algorithm to run faster and more efficiently using a technique called **Alpha Beta Pruning** in just a few extra lines of code. Let\u2019s see how it works.\n\n## Basic Gameplay - The Minimax Algorithm\n\nTwo player turn based games like chess, tic-tac-toe, backgammon etc, when played against a computer are based on the **Minimax Algorithm**. This algorithm uses backtracking to find the optimal move at any point in the game by assuming that the opponent also plays optimally. The two players are sometimes called the **maximising** and **minimising players**. The objective of the maximising player is to get the largest possible positive score or the smallest possible negative score based on board evaluations. On the other hand, the minimising player tries to get the largest possible negative score or smallest possible positive score. These scores indicate the player having the upper hand at that instant - positive for maximising player and negative for minimizing player.\n\nThe way these scores are evaluated differ in technique and complexity based on the game. For something simple like tic-tac-toe (where there aren\u2019t too many positions to consider) we could exhaustively search all patterns until the game ends. Then the winning games could have score 1, losing games score -1 and games that draw get a score of 0. For complicated games like chess, this evaluation is a lot more involved and the number of moves is too large for us to search all positions until the game ends. So the evaluations are done considering several other metrics.  \n\nSince this is a backtracking based algorithm, it tries all possible moves from a given board position while assuming that the opponent is always going to play the best move they can. The number of moves the algorithm **looks ahead** is the **depth of the recursion**. At the end of the process, the algorithm picks the best board evaluation and makes a move intending to arrive at it. The various intermediate boards are evaluated along the way and the scores make up the **decision tree** of the algorithm.\n\nTo understand what this tree looks like, consider the example of **Connect 4** - a popular two player turn based game - being evaluated using minimax. Here\u2019s a visual representation of the boards and the moves explored by the algorithm at the very beginning of the game :\n\n![Decision Tree of Connect4]({% static '/blog/assets/img/Alpha-Beta-Pruning/Connect4_Minimax.png' %})\n\nInstead, if we assign scores for each board evaluation, it could look like this:\n\n![Board Evaluated Decision Tree]({% static '/blog/assets/img/Alpha-Beta-Pruning/Decision_Tree.png' %})\n\nAs you can see, the algorithm tries all the moves it can make, and then all subsequent moves the opponent can make, repeatedly until it reaches the depth of its recursion. Then it picks the maximum of the branches for maximising player and minimum of the branches for minimising player until the backtracking is complete and returns the calculated value and the move/branch corresponding to it.\n\nHere is the pseudocode for the algorithm:\n\n```\nfunction minimax(node, depth, maximizingPlayer) is\n    if depth = 0 or node is a terminal node then\n        return the score/evaluation of node\n    if maximizingPlayer then\n        best_score := \u2212\u221e\n        for each child of node do\n            branch_score := minimax(child, depth-1,False)\n            best_score := max(best_score, branch_score)\n        return best_score\n    else (* minimizing player *)\n        best_score := +\u221e\n        for each child of node do\n            branch_score := minimax(child, depth-1,True)\n            best_score := min(best_score, branch_score)\n        return best_score\n```\n\n## The Need to Optimize - Alpha Beta Pruning\n\nYou might have already noticed that while this exhaustive search for the best result is indeed more or less foolproof, it is very time consuming and often excessive. The reason for this is because there are some moves that are so glaringly bad or some which are obviously the best ones possible and they become apparent much before the depth of recursion is reached. Unfortunately the algorithm in its current implementation cannot recognize such moves and continues its search.\n\nTo improve this, we use an optimization technique called **Alpha Beta Pruning**. This optimization can tell the algorithm when it has found a really good or bad move so that it can stop exploring further or to simply ignore evaluating that sequence of moves. The algorithm gets its name because it manages to shorten or **prune** unnecessary branches of the decision tree.\n\nAfter pruning, the tree might look something like this :\n\n![Decision Tree of Connect4 after pruning]({% static '/blog/assets/img/Alpha-Beta-Pruning/Connect4_Pruned.png' %})\n\nOnce again, in terms of board evaluation, it could look like this:\n\n![Board Evaluated Decision Tree after pruning]({% static '/blog/assets/img/Alpha-Beta-Pruning/Pruned_tree.png' %})\n\nThe values seen in grey are not evaluated - those are the branches that the algorithm decided as unnecessary to explore.\n\nThis is the pseudo code for the implementation :\n\n```\nfunction alphabeta(node, depth, \u03b1, \u03b2, maximizingPlayer) is\n    if depth = 0 or node is a terminal node then\n        return the score/evaluation of node\n    if maximizingPlayer then\n        best_score := \u2212\u221e\n        for each child of node do\n            branch_score := alphabeta(child, depth-1, \u03b1, \u03b2, False)\n            best_score := max(best_score, brach_score)\n            \u03b1 := max(\u03b1, best_score)\n            if \u03b1 \u2265 \u03b2 then\n                break (* \u03b2 cutoff *)\n        return best_score\n    else\n        best_score := +\u221e\n        for each child of node do\n            branch_score := alphabeta(child, depth-1, \u03b1, \u03b2, True)\n            best_score := min(best_score, branch_score)\n            \u03b2 := min(\u03b2, best_score)\n            if \u03b2 \u2264 \u03b1 then\n                break (* \u03b1 cutoff *)\n        return best_score\n```\n\nThe variables alpha and beta are used to denote the most favourable board evaluations for the maximising and minimising player respectively. Whenever the maximum score that the minimizing player is assured of becomes less than the minimum score that the maximizing player is assured of, the maximizing player need not consider further descendants of this node, as they will never be reached in the actual play.\n\n## Some Shortcomings of the Optimization\n\nThis optimization gives a significant boost to the standalone Minimax Algorithm but is not always bound to work. Sometimes, if the moves explored by the algorithm are just in a **bad order**, then it might come across one of those really good or bad moves towards the end at that level of recursion. Another unfortunate drawback of this algorithm is the fact that it **cannot remember** previously evaluated boards. Often we can arrive at a certain board position by performing the moves in different orders. But since the algorithm does not keep track of these boards, it re-evaluates them. In such cases, Alpha Beta Pruning hasn\u2019t managed to reduce the number of boards that need to be evaluated.\n\n## Improvements and Conclusion\n\nAs you can see, even with this optimization, the algorithm isn\u2019t quite perfect. However, we can improve this algorithm further if we choose the order in which moves are explored wisely. For example, in chess, moves that capture pieces may be examined before moves that do not, and moves that have scored highly in earlier passes through the decision tree analysis may be evaluated before others. We can also consider storing some recent board evaluations (sometimes there are too many to store at once so we only consider the last few). With these, and several other improvements, this algorithm can be modified to give some fantastic results.\n\n## References\n\n[More on Minimax Algorithm](https://ieee.nitk.ac.in/blog/minimax-algorithm/)\n\n[Alpha Beta Pruning - Wikipedia](https://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning)\n\n[More code - Javapoint](https://www.javatpoint.com/ai-alpha-beta-pruning)\n\n[Some theory and exercises - UCLA Recitation](http://web.cs.ucla.edu/~rosen/161/notes/alphabeta.html)\n\n[More examples - Cornell Recitation](https://www.cs.cornell.edu/courses/cs312/2002sp/lectures/rec21.htm)",
        "author_name": "Pranav DV",
        "slug": "alpha-beta-pruning"
    },
    {
        "layout": "post",
        "title": "Bionics, Biomimetics and Robotics",
        "author_github": "Shobuj-Paul",
        "date": "2021-07-11T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Diode",
            "Robotics",
            "Automation"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Shobuj-Paul",
        "text": "[{% load static %}]()\nLet us get the boring part out of the way and define those three terms in the title. Biomimetics is when we look at how biological systems work and try to replicate the same movements and systems with electromechanical components. Bionics is when we integrate these artificial systems with human bodies. Robotics is when we take mechatronics and try to make it smart. Why do we do that? It could be for a multitude of reasons. To fix disability due to a lost limb is a good example. Or to provide extra strength and support in situations that require heavy work like construction. Or to help patients with paralysis walk and allow them to have a fulfilling life.\n\nThe first thing to consider is locomotion in robots, which could be on four-legs (like the Boston Dynamics spot robot), on two-legs (Atlas by Boston Dynamics) or maybe just even hopping on a single leg.\n\n![Quadruped Robot]({% static '/blog/assets/img/bionics-and-biomimetics/Four_Legged_Robot.jpg' %})\n\nAll of these motions are usually simulated by simplified versions, which could be easily analysed called \"templates\". \n\nBiped walking uses a template called the rimless wheel, and the template for a limb in running motion is a single spring. These templates seem arbitrary but are very useful in analysing the internal dynamics of the systems and approximating them for designing controllers for such systems.\n\n![Rimless Wheel]({% static '/blog/assets/img/bionics-and-biomimetics/Rimless_Wheel.png' %})\n\nThe genesis of more intelligent and more powerful sensors helps robot-sensing to become more effective. Another factor is more sophisticated machine learning and deep learning algorithms, which help to make the robots smarter and also helps in computer vision which again improves robot sensing and provides it with a sense of \"sight\". We can also use computer vision in applications such as mapping and path planning which are very important for robotics because these algorithms are how robots can move around without crashing into objects. Another advancement in bionic technology that is still in its infancy is the neural control of robots.\n\nSoft Robotics is a sub-field that focuses on the use of polymeric \"muscles\" to create systems that more closely resemble biological systems than just electromechanical contraptions. For example, Micro UAVs can be created using this kind of tech, which resemble insects in nature and can manoeuvre increasingly dangerous environments. There is a lot of research scope in these field as such systems are usually under-actuated and require other methods to work than traditional actuators. Another research area in biomimetics is non-linear control engineering since most jointed systems are non-linear for large angles and require more sophisticated control and state estimation techniques. \n\n![Control System Diagram]({% static '/blog/assets/img/bionics-and-biomimetics/Control_System.png' %})\n\n## References \n1. Robotics: Mobility MOOC by UPenn on Coursera\n2. [J. Oehlke, M. A. Sharbafi, P. Beckerle and A. Seyfarth, \"Template-based hopping control of a bio-inspired segmented robotic leg,\" 2016 6th IEEE International Conference on Biomedical Robotics and Biomechatronics (BioRob), 2016, pp. 35-40, doi: 10.1109/BIOROB.2016.7523595](https://doi.org/10.1109/BIOROB.2016.7523595).",
        "author_name": "Shobuj Paul",
        "slug": "bionics-and-biomimetics"
    },
    {
        "layout": "post",
        "title": "Paradox Free Time Travel",
        "description": "A guide to various paradoxes in Time Travel and their possible solutions",
        "date": "2021-07-11T00:00:00",
        "image": "/assets/img",
        "tags": [
            "IEEE NITK",
            "Piston"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "virenvarma007",
        "text": "[{% load static %}]()\n> \u201cPeople like us, who believe in physics, know that the distinction between past, present, and future is only a stubbornly persistent illusion.\u201d \u2013 Albert Einstein.\n\nIt\u2019s a worry of physics enthusiasts who believe in the notion of \u201cTime Travel\u201d everywhere. What if they go back in time and alter the course of time, like preventing one's parents from meeting or killing their grandfather? Such a time-traveling act could prevent them from ever being born or divert them from ever attempting to go back in time. Therefore, they would have never existed or gotten the opportunity to travel back in time in the first place. This is called a Paradox...The solution you ask for?? Well, there is none, that's why it\u2019s a Paradox. Before diving deep into the various Paradoxes in Time Travel and their possible solutions let's look at the two broad categories of Time Travel Paradoxes:\n\n1. Closed Causal Loops, such as the Predestination Paradox and the Bootstrap Paradox, involve a self-existing time loop in which cause and effect run in a repeating circle but is also internally consistent with the timeline\u2019s history. Assuming the one in the show \u201cDark\u201d where each character goes into the past with the intent of stopping an action from happening but ends up being a part of why that was caused.\n2. Consistency Paradoxes, such as the Grandfather Paradox, and other similar variants such as The Hitler paradox, and Polchinski\u2019s Paradox, generate several timeline inconsistencies related to the possibility of altering the past. Take the example of the movie \u201cBack to the Future\u201d where the protagonists repeatedly travel to the past and the future to alter the course of time and actions made by certain people at certain points in time and can return to their new future.\n\nOne way to solve a paradox is by stating \u201cTime travel isn\u2019t possible at all\u201d. Although the very notion of traveling to the past violates one of the most fundamental premises of physics, and that of causality, there is still nothing in Einstein\u2019s Theories to rule out the possibility of Time Travel. So why not ponder over such a reality as an exercise in logical reasoning?\n\n1. **The Predestination Paradox** occurs when the actions of a person traveling back in time become part of past events, and may ultimately cause the event he is trying to prevent to take place........ This results in a \u2018temporal causality loop\u2019 wherein assuming there is an Event 1 in the past which influences Event 2 in the future (time travel to the past) which then causes Event 1 to occur, with this circular loop of events ensuring that history is not altered by the time traveler proving our previously stated point that any attempts to stop something from happening in the past will simply lead to the cause itself, instead of preventing it from happening. This paradox inferences that things are always destined to turn out the same way and that whatever has happened must happen. But this also raises questions as to can there be Free Will when it comes to becoming a Time Travel or if you are just a part of an enormous predetermined Time Loop.\n2. **The Bootstrap Paradox** is when an object, person, or piece of information sent back in time results in an infinite loop where the object has no discernible origin and exists without ever being created. A materialistic thing may degrade post several cycles or Time Loops and may wither off the loop but a piece of information is always an excellent constant to picture a scenario like this. It is also known as an Ontological Paradox, as ontology is a branch of philosophy that deals with the nature of being, or existence. Now Assume A Scientist traveling back in time and giving himself the knowledge of how to build a machine that can allow him to travel through time and space which he then goes on to use to return to the past and pass on the information to himself again for would create a bootstrap paradox involving information, as the information would have no true point of creation or origin.\n3. Now the Paradox you\u2019ve been waiting for: **The Grandfather Paradox** concerns \u2018self-inconsistent solutions\u2019 to a timeline\u2019s history caused by traveling back in time. For example, if you traveled to the past and killed your grandfather, you would never have been born and would not have been able to travel to the past \u2013That is a paradox. Assuming you did decide to kill your grandfather because he created a dynasty that destroyed the world. You figure if you ended his life before he meets your grandmother then the whole family line .including you. will cease to exist and the world will be saved from the heinous acts of tyranny but that wouldn\u2019t work considering you wouldn\u2019t have been born to travel back in time to commit the murder.\n\nNow Solution: Assuming You go back in time, walk up to your Grandfather before he met your grandmother, and point a revolver at his head and pull the trigger......and Boom! The deed is done. You return to what you think is \"The Present\" but you never existed here. Everything about you has been erased, including your family, friends, home, possessions, bank account, and history. You\u2019ve entered a timeline where you never existed. Scientists put forward the possibility that you have now created an alternate timeline or entered a parallel universe.\n\nThe Parallel Universe Theory states that every time you attempt to Travel forwards or backward in time you are not really traveling to a reality on your earth but a parallel reality where things are absolutely identical to actions taken on your world. This can answer a lot many questions and rule out the later mentioned Butterfly effect from the equation. Consider it\u2019s a parallel universe and it can open up the possibility of Coexistence where you can live in a world where your parents or any memory of you doesn't exist or in other words the effect lives on even after removing the cause.\n\n**Butterfly Effect**: Perhaps the most important theory in time travel states that a small change in one state of a deterministic nonlinear system can result in large differences in a later state. Let's see the story of one Paleontologist:\n\nA Paleontologist, with the help of a Time Machine, travels back to the Jurassic Period to get photographs of the various dinosaurs and species. He knows he can\u2019t take samples so he just takes magnificent pictures from the fixed platform that is positioned precisely to not change anything about the environment. His assistant is about to pick a long blade of grass, but he stops him and explains how nothing must change because of their presence. They finish what they are doing and return to the present, but everything is gone. They reappear in a wild world with no humans, and no signs that they ever existed. They fall to the floor of their platform, the only man-made thing in the whole world, and lament \u201cWhy? We didn\u2019t change anything!\u201d And there on the heel of the scientist\u2019s shoe is a crushed butterfly.\n\nThus, Even changing anything as small as moving a leaf or killing one small butterfly can cause drastic ripples in the fabric of time and the overall effect of decision it has on the lives of the people let alone be the action of killing one's Grandparents before their parents are conceived.",
        "author_name": "Viren Varma",
        "slug": "paradox-free-time-travel"
    },
    {
        "layout": "post",
        "title": "CubeSats",
        "author_github": "nakshatra-gopi",
        "date": "2021-07-17T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Diode",
            "Space",
            "Systems Engineering"
        ],
        "categories": [
            "diode"
        ],
        "github_username": "nakshatra-gopi",
        "text": "[{% load static %}]()\n![image1]({% static '/blog/assets/img/cubesats/image1.jpg' %})\n\nLet\u2019s start off with what \u201cCubeSats\u201d are, and how they are different from other small satellites. Any satellite that weighs less than 300 kg is called a small satellite, but a CubeSat must adhere to more specific size, shape, and weight. The aim of CubeSats is to reduce design and deployment costs and make satellites more affordable for students, commercial bodies or space agencies to work with. You might have come across a bunch of high schoolers on the news making a lightweight, cheap and workable satellite. If these satellites happen to measure 10cm x 10cm x 11cm and weigh 1.33 kg, this would be a 1U CubeSat. Where U is the standard CubeSat unit, CubeSats may be 1.5U, 2U, 3U, etc.\n\nCubeSats have various applications. They are useful for education and research, conducting experiments and demonstrations in space, climate monitoring and disaster response, connectivity for IoT, communications, earth monitoring, etc. NASA launched a CubeSat as part of its CAPSTONE mission in 2020 to test the planned orbit for the Lunar Gateway, a space station that would orbit the moon. The images sent by ISRO's Cartosat-2 series satellite will be useful for more accurate cartography and land mapping, coastal land regulation, road monitoring, water distribution, etc. QuakeSat by Quakefinder California helps scientists improve earthquake detection. NASA's GeneSat 1 is conducting bizarre biology experiments in outer space to study the genetic changes that occur on exposure to the space environment. Beyond helping space enthusiasts take their first step into rocket science, CubeSats have wide and varied applications.\n\nLet's have a brief look at what makes up a CubeSat. A CubeSat consists of many subsystems such as (a) flight computer, (b) electrical power system, (c) altitude control, (d) sensors, (e) radio and other components of choice, and (f) the structure.\n\n![image2]({% static '/blog/assets/img/cubesats/image2.png' %})\n\nBefore looking into what these systems are and how they work, it is imperative to know \u2013 what do you want your CubeSat to do? Statistics show that it takes *months* just to develop your concept.\n\nAfter you are done with planning, how you build your satellite ultimately depends on your budget. While cheaper than other satellites, they are still not exactly affordable for an ordinary college kid to make. A CubeSat kit costs 11,000 USD ignoring payload and launch costs! \n\nWith some research, you will find that a basic CubeSat would have a computer unit containing the memory, and the command centre. It can also have components such as an internal temperature monitor, real-time clock for time keeping, etc. \n\nThe power system uses solar battery and would require you to know your \u201cpower budget\u201d, i.e., how much power the various components on your avionics board consumes, and thus calculate how much minimum power is required to get your CubeSat running.\n\nThe altitude control system is responsible for the stable orientation of the CubeSat in its orbit, generally consisting of magnetic field and torque controllers.\n\nSensors like gyroscope, magnetometer, temperature sensors, etc. can be used to retrieve data about the environment. Depending on what you want your CubeSat to do, you could add on a multitude of other more specific sensors.\n\nA radio system is required for communication purposes, connecting the CubeSat through a communication satellite to a ground station. A GPS to track its current location and time can also be added.\n\nCubeSats, despite being continuously studied and improved upon for nearly three decades since their first development in the 1990s, with nearly 2000 launched as of 2020, there is still tremendous scope for research. CubeSats are essential, not just for remote sensing, but also for emerging communications technology, internet of space things, and various other applications.",
        "author_name": "Nakshatra Gopi",
        "slug": "cubesats"
    },
    {
        "layout": "post",
        "title": "A beginner's guide to Cloud Computing",
        "author_github": "srujangit123",
        "date": "2021-07-17T00:00:00",
        "image": "/assets/img/",
        "description": "Introduction to cloud computing",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Cloud"
        ],
        "categories": [
            "Compsoc"
        ],
        "github_username": "srujangit123",
        "text": "[{% load static %}]()\n## What is Cloud?\n\nSimply put, cloud computing provides storage, computing resources, networking and other services without direct management by the user.\n\nThe existence of such user-accessible cloud resources is possible because of the presence of data centers.\n\nA data center refers to the place where the servers/computers are located and the place of computation in the cloud. These servers communicate with each other using the local network.\n\nEvery cloud provider manages several data centers across the world. This kind of distribution of servers allows for reduced latency by routing the client requests to the geographically closest data center by determining his region and zone.\nEvery region contains a certain number of zones. Users can use the cloud service through any of these regions.\n\nData centers will be working 24 x 7 so that the users of the cloud don\u2019t have to worry about managing their own servers, scaling or failures since the servers are distributed. So the website will keep running even if one of the server instances goes down.\nOnce the user buys a virtual machine by the cloud provider, he will be given an IP address of the virtual machine so that they can access it by **SSH**ing into that virtual machine.\n\n## How does it work?\n\n![Virtual machines]({% static '/blog/assets/img/Intro-to-cloud/image1.png' %})\n\nA virtual machine is an emulated computer system created using software(like virtual box). This is done by using hypervisors. Virtual machines use the resources of the physical system. They allow the user to run applications with various software dependencies in an isolated way so that developers don\u2019t have to worry about managing dependencies. A single physical computer can run multiple VMs. They can be created using softwares like Virtual Box.\nVirtualization is one of the core ideas for the working of cloud computing.\nEach machine will have an hypervisor installed in them. Basically hypervisors are used to manage the virtualization in the machine. So to access a machine in the cloud, the user generally has to enter the hardware requirement they need and also the location of the machine they want (zone). Using this the cloud service provider will look up their data centers to find a suitable computer meeting the requirements of the request. This is done by an internal tool which is specific to the provider. The changes are continuously managed by the cloud providers. Once a particular machine is found, it will install the dependencies and the utilities user has specified and creates a connection for the user. Virtual space is allocated on the physical machine for the virtual machine and these user-specified applications.. This allows multiple users to access and utilize the resources of the same machine but have their activities completely isolated from each other on the hardware level. In this way, Virtual machines are really important for the functioning of the cloud.\nCloud client/UI is basically where users select some of their software requirements. Everything else once they enter is done by the cloud provider.\n\n## Uses of cloud computing\n\nEasy scaling/elasticity: When a company gets heavy traffic at some time, in the traditional developer-owned data center they need to scale their service horizontally to keep up with the traffic. In the cloud this is done easily using auto scaling features provided by the cloud. This makes the developers work on new ideas instead of managing servers.\n\n### Pay for what you use\n\nThe cloud service is elastic. The number of servers running in the cloud depends on the traffic. When there is usual traffic, the company doesn't need a lot of servers. So, it is auto scaled down as required. This helps the client by cutting down on unnecessary server maintenance costs.\n\n### Performance\n\nThe servers will be upgraded to the latest generation of computing hardware when required by the cloud service provider. This is another layer of convenience to the client, since they don't need to worry about operating on outdated hardware.\nCloud has benefits over a single corporate datacenter, including reduced network latency for the applications.\n\n### Easy and fast deployments\n\nWith the cloud, you can expand your application to new regions and deploy globally in minutes. It\u2019s easier to expand the business all over the world with cloud instead of building data centers in all the regions and hiring people to manage them.\n\n### Cloud native apps\n\nCloud computing is used for building cloud native technologies and approaches like kubernetes for auto-scaling, microservices architecture, devOps etc.\nCI/CD : Whenever a developer commits and pushes changes to the repository, the CI system will rebuild the branch and run all related test cases to verify that new changes won\u2019t break the existing application and functionalities. Cloud resources can automatically scale up and down based on CI/CD workloads.\n\n### DevOps\n\nThe set of practices that combines software development and operations. Sometimes the development team and operations team work together for the entire software development lifecycle from developing, deploying to operations.\nCI/CD is one of the best practices devops teams implement.\n\nDevOps automation is becoming cloud-centric. Most cloud providers support DevOps systemically on their platform, including continuous integration and continuous development tools.\nThis tight integration lowers the cost associated with on-premises DevOps automation technology, and provides centralized governance and control for a sound DevOps process.\n\n## Cloud Services\n\n![Cloud services]({% static '/blog/assets/img/Intro-to-cloud/image2.jpg' %})\n\n### Infrastructure as a Service(IaaS)\n\nThis is the least abstracted service out of all the three. In this the user rents the resources, servers and then does the task of connecting the servers by themselves.\nIaaS users manage applications, runtime, OSes, middleware, and data. However, cloud providers manage the servers, networking, virtualization, and storage.\nExample: [Google Cloud Platform](https://cloud.google.com)\n\n### Platform as a Service(PaaS)\n\nThis is usually used by developers to code, build and run their code. This is done by using containerization/virtualization technology in the cloud. The servers, storage, and networking are managed by the cloud provider while the developers can maintain management of the applications. Developers can also use some of the APIs provided by the cloud.\nExample: [Repl.it](https://repl.it), [Heroku](https://www.heroku.com/)\n\n### Software as a Service(SaaS)\n\nSaaS eliminates the need of the IT staff to install applications and their dependencies on each individual computer. SaaS applications are hosted as web apps. Users of SaaS applications don\u2019t have to install them in order to use it.\nExample: [Salesforce](https://salesforce.com), [Google workspace](https://workspace.google.com/)\n\n## Conclusion\n\nCloud is really useful for startups by giving virtual data centres and a lot of other benefits mentioned above. It marked the beginning of a new era in the field of IT. Users are gradually moving from conventional computing to cloud computing. Using this technology, people with great business ideas will no longer need to spend a lot of money to buy computers but rather focus on building the business.\nCloud computing is still at its beginning stage and there is still so much potential it can have owing to the ongoing research and development.",
        "author_name": "Srujan Bharadwaj",
        "slug": "intro-to-cloud-computing"
    },
    {
        "layout": "post",
        "title": "To Infinity and Beyond",
        "author_github": "Pranshu-S",
        "date": "2021-07-17T00:00:00",
        "image": "/assets/img/",
        "description": "A read on future of space exploration",
        "tags": [
            "IEEE NITK",
            "Piston",
            "Space"
        ],
        "categories": [
            "piston"
        ],
        "github_username": "Pranshu-S",
        "text": "[{% load static %}]()\nEver since the dawn of space exploration in 1957, humankind has seen an unprecedented amount of growth in the domain of space exploration. With the innovation of new and advanced space technologies, both the government and private companies are competing to make space travel more accessible and affordable for the future, but let us ask the right questions. What is the future of space exploration?\n\n## Shoot for the Stars, Reach the Moon\n\nFor more than 50 years since the first moon mission, we have continued to push the boundaries of knowledge to accomplish feats unlike ever before. With the current plans of NASA to achieve the goal of an initial human landing by 2024 in its Artemis Mission, we are moving forward to setting new records in space exploration every decade.\n\nArtemis I or formerly regarded as Exploration Mission-1, will be one of the most significant challenges to NASA's deep space exploration systems and the first in a series of increasingly complex missions. It will lay the foundation steps for human space exploration and force us to make new technological and scientific advancements.\n\nWe are not just limited to just reaching the Moon; plans for developing an Artemis Base Camp at the lunar South Pole are also underway. These planned Base camps would include all the future one can imagine - such as a lunar terrain vehicle, a habitable mobility platform, a lunar foundation habitation module, power systems, and many more.\n\n## The Red Planet\n\nAfter taking notes from the Artemis program, we will begin preparing for the next major feat in space exploration - Mars Colonization. NASA's Space Launch System rocket and the renowned Orion spacecraft will be the backbone to build the Gateway and set up transportation for astronauts to and from Earth. SpaceX is not far behind the space race either; The Starship program proposed by Elon Must will act as a pioneer for a very cost-friendly and effective solution to space travel. It will provide a fully reusable transportation system designed to carry both crew and cargo to Earth orbit, the Moon, Mars, and beyond.\n\nStarship will be the world's most powerful launch vehicle ever developed, with the ability to carry over 100 metric tonnes to Earth orbit. Adding to its capabilities, Starship will also feature tanker vehicles capable of allowing Starship to refill in low-Earth orbit prior to departing for Mars, enabling the transport of up to 100 tons to Mars.\n\n## Crossing Horizons\n\nThe dream of crossing horizons and reaching the stars is closer to reality than ever before. The renowned NASA Voyager 1 has made very important and significant strides in space exploration. The probe has broken through the sun's magnetic bubble to be the first and only probe to touch the interstellar wind. Voyager 2 is not that far behind either.\n\nIt goes without a doubt that space exploration will continue to serve as the world's leading hub for researchers to conduct cutting-edge research and technology development. Moreover, it will also facilitate the growth of other technologies in transportation domains, materials engineering, and many more, leading us to a better and brighter tomorrow. Cheers to science and discovery!",
        "author_name": "Pranshu Shukla",
        "slug": "to-infinity-and-beyond"
    },
    {
        "layout": "post",
        "title": "An Introduction to IoT",
        "author_github": "v-kartikeya",
        "date": "2021-07-30T00:00:00",
        "image": "/assets/img/",
        "description": "An article describing what IoT is and why there is a certain buzz around it, its applications, advantages, and disadvantages",
        "tags": [
            "IEEE NITK",
            "Diode",
            "Internet of Things"
        ],
        "categories": [
            "diode"
        ],
        "github_username": "v-kartikeya",
        "text": "[{% load static %}]()\n# A Look into The Internet of Things\nRecently, there has been much buzz around the topic Internet of Things.\nIn this post, let us know what IoT is and why there is a certain buzz around it, its applications, advantages, and disadvantages.\n\n## What is IoT\n\nIoT is an extensive network where any device with an Internet Protocol (IP) address is connected to the internet or to each other to automate tasks and reduce human intervention. Anything that has a sensor attached to it and can transmit data from one object to another object or people with the help of the internet is known as an IoT device. IoT devices include wireless sensors, software, actuators, and computer devices. All smart devices come under IoT. Smartwatches, health monitors, wireless trackers, smart home security systems, and speakers like Alexa are some of the devices that come under the Internet of Things.\n\n## Reason for the speculation around IoT\n\nMany tasks can be improvised, monitored, performed remotely using IoT. From checking burnt calories to performing delicate surgeries, from equipping smart cars to smart homes to smart cities, from tracking poultry to running agricultural machinery, there is a wide range of applications for IoT making our lives more effortless than ever. Thus it creates a lot of promising opportunities in this field.\n\n![iot]({% static '/blog/assets/img/introduction-to-iot/iot.png' %})\n\n## Applications\n\nIoT has a wide range of applications in various domains such as medicine, engineering, governance, agriculture, etc. Some of the applications include\n\n### Agriculture\nOperating farm machinery, tracking livestock and produce, monitoring the crop field are some of the uses in this domain\n\n### Healthcare\nWearable devices like fitness bands, heart rate monitors, blood pressure monitors can be used for personalized health monitoring. IoT can also be used to diagnose diseases by using various sensors.\n\n### Engineering\nIoT can be used in this domain to ease the process of governing the machines, improving field testing, etc.\n\n### Governance\nIoT can be helpful in tracking, monitoring traffic and activities and could send valuable data in improvising the governance.\n\n### Hospitality\nIoT can be helpful for monitoring information in the hospitality industry. It can also ease and automate processes like keeping track of guests and using keys by guests, etc., by integrating the guests' devices.\nThese are few applications in the wide range of applications in the field of IoT.\n\n## Advantages of the Internet of Things\n\n### Automation\nAutomation is one of the main advantages of IoT. IoT enables machine-to-machine communication without human intervention, thus saving time and increasing efficiency.\n\n### Information\nUsing IoT, we can gather and source much information that could be used to make better decisions.\n\n### Convenience\nAutomation makes our tasks easier and more convenient\n\n## Disadvantages\n\n### Privacy\nAs everything is connected, by breaching into a part of the network, all the data belonging to an individual or an organization can be exposed.\n\n### Security concerns\nSeveral devices make use of unencrypted network services, making it easy for an interceptor to breach the network.\n\n### Compatibility\nThere is no one standard for the devices because operation between components from two different manufacturers becomes a challenge.\n\n### Complexity\nEven though IoT seems simple on the outside, the complexity of operations performed inside is high. A faulty design in one of the components in a sub-system may cause the entire system to fail and, in some cases, damage the equipment.\n\n### Dependence on the Internet\nEverything on IoT depends on the internet. If the internet becomes unavailable for some reason, the devices are rendered useless to a large extent.\n\n## Conclusion\n\nIn conclusion, the Internet of Things makes our lives a lot easier, convenient, and automated. It has a wide range of applications, and thus field promises many career opportunities. However, limitations to IoT that are few but severe and, when overcome, would make the future reach the present faster.\n\n## References\n\n- [What is IoT](https://www.aeris.com/in/what-is-iot/).\n- [Possible Limitations of IoT](https://www.ndimensionz.com/2016/04/29/internet-of-things-iot-possible-limitations/)\n- [What is an IoT Device](https://www.hcltech.com/technology-qa/what-is-an-iot-device)\n- [IoT in Agriculture](https://www.cropin.com/iot-internet-of-things-applications-agriculture/)\n- [IoT in Engineering](https://technosofteng.com/applications-of-internet-of-things-iot-in-engineering/)\n- [Internet of Things](https://www.esds.co.in/blog/what-is-internet-of-things/#sthash.Tn0dBHyW.dpbs)\n- [Advantages and Disadvantages of IoT](https://www.linkedin.com/pulse/advantages-disadvantages-internet-things-iot-tommy-quek/)",
        "author_name": "V Kartikeya",
        "slug": "introduction-to-iot"
    },
    {
        "layout": "post",
        "title": "Introduction to Kubernetes Architecture",
        "author_github": "sravanireddy1102",
        "date": "2021-07-30T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Kubernetes"
        ],
        "categories": [
            "Compsoc"
        ],
        "github_username": "sravanireddy1102",
        "text": "[{% load static %}]()\nThis blog post will give you a basic idea of what Kubernetes is,its applications and infrastructure.\n\nLet's take a look at how application development and deployment has changed over time.\n\n![container_evolution]({% static '/blog/assets/img/Introduction-to-Kubernetes-Architecture/containerevolution.png' %})\n\n### **Traditional deployment era**\n\nEarly on, organizations ran applications on physical servers. There was no way to define resource boundaries for applications in a physical server, and this caused resource allocation issues. For example, if multiple applications run on a physical server, there can be instances where one application would take up most of the resources, and as a result, the other applications would underperform. A solution for this would be to run each application on a different physical server. But this did not scale as resources were underutilized, and it was expensive for organizations to maintain many physical servers.\n\n### **Virtualized deployment era**\n\nAs a solution, virtualization was introduced. It allows you to run multiple Virtual Machines (VMs) on a single physical server's CPU. Virtualization allows applications to be isolated between VMs and provides a level of security as the information of one application cannot be freely accessed by another application.\n\nVirtualization allows better utilization of resources in a physical server and allows better scalability because an application can be added or updated easily, reduces hardware costs, and much more. With virtualization you can present a set of physical resources as a cluster of disposable virtual machines.\n\nEach VM is a full machine running all the components, including its own operating system, on top of the virtualized hardware.\n\n### **Container deployment era**\n\nContainers are similar to VMs, but they have relaxed isolation properties to share the Operating System (OS) among the applications. Therefore, containers are considered lightweight. Similar to a VM, a container has its own filesystem, share of CPU, memory, process space, and more. As they are decoupled from the underlying infrastructure, they are portable across clouds and OS distributions.\n\n## Containers have become popular because they provide extra benefits, such as\n\n* **Agile application creation and deployment**:\nIncreased ease and efficiency of container image creation compared to VM image use.\n* **Continuous development, integration, and deployment**:\nprovides for reliable and frequent container image build and deployment with quick and easy rollbacks (due to image immutability).\n* **Dev and Ops separation of concerns**:\ncreate application container images at build/release time rather than deployment time, thereby decoupling applications from infrastructure.\nObservability not only surfaces OS-level information and metrics, but also application health and other signals.\n* **Environmental consistency across development, testing, and production**: Runs the same on a laptop as it does in the cloud.\n* **Cloud and OS distribution portability**:\n Runs on Ubuntu, RHEL, CoreOS, on-premises, on major public clouds, and anywhere else.\n* **Application-centric management**:\nRaises the level of abstraction from running an OS on virtual hardware to running an application on an OS using logical resources.\n* **Loosely coupled, distributed, elastic, liberated micro-services**:\napplications are broken into smaller, independent pieces and can be deployed and managed dynamically \u2013 not a monolithic stack running on one big single-purpose machine.\n* **Resource isolation**: predictable application performance.\n\nNow that we have understood what container is and what VM is.\n\n**Let's understand why we use Kubernetes.**\n\nContainers are a good way to bundle and run your applications. In a production environment, you need to manage the containers that run the applications and ensure that there is no downtime. For example, if a container goes down, another container needs to start. Wouldn't it be easier if this behavior was handled by a system?\n\nThat's how Kubernetes comes to the rescue!. It takes care of scaling and failover for your application, provides deployment patterns, and more.\n\nThe components on the master server work together to accept user requests, determine the best ways to schedule workload containers, authenticate clients and nodes, adjust cluster-wide networking, and manage scaling and health checking responsibilities. We will look at each of the individual components in the master node and try to understand their functionalities.\n\n![architecture]({% static '/blog/assets/img/Introduction-to-Kubernetes-Architecture/architecture.png' %})\n\nAs you can see in the diagram, there are a lot of terms that you might not understand. I will explain it one by one.\n\n## Master\n\nMaster is the controlling element or brain of the cluster.Master has 3 main components in it:\n\n### API Server\n\nThe application that serves Kubernetes functionality through a RESTful interface and stores the state of the cluster.\n\n### Scheduler\n\nScheduler watches API server for new Pod requests. It communicates with Nodes to create new pods and to assign work to nodes while allocating resources or imposing constraints.\n\n### Controller Manager\n\nComponent on the master that runs controllers. Includes Node controller(basically checks if desired number of nodes are active), Endpoint Controller, Namespace Controller, etc.\n\n## Worker Nodes\n\nThese machines perform the requested, assigned tasks. The Kubernetes master controls them. There are 4 component inside Nodes:\n\n### Pod\n\n All containers will run in a pod. Pods abstract the network and storage away from the underlying containers. Your app will run here.\nThis is how we deploy pods on the node.\n\n```yaml\n#pod.yaml file\napiVersion: v1\nkind: Pod\nmetadata: \n   name: site\n   labels:\n      app: my-app\nspec:\n    containers:\n    - name: front-end\n      image: nginx\n      ports:\n      - containerPort: 80\n    - name: reader\n      image: \n      ports: sravani/php-ngnix:v1\n      - containerPort: 88\n```\n\n### Kubelet\n\nKubectl registering the nodes with the cluster, watches for work assignments from the scheduler, instantiate new Pods, report back to the master.\n\n### Container Engine\n\nResponsible for managing containers, image pulling, stopping the container, starting the container, destroying the container, etc.\n\n### Kube Proxy\n\nResponsible for forwarding app user requests to the right pod.\nI\u2019m not going to describe the detailed concept here, cause it will lead to a boring situation.\nYou can read the official documentation for more details information.[Click here](https://kubernetes.io/docs/home)\n\n## References\n\n[Introduction to Kubernetes](https://www.digitalocean.com/community/tutorials/an-introduction-to-kubernetes)\n\n[Kubernetes Documentation](https://kubernetes.io/docs/home/)",
        "author_name": "Sravani Katasani",
        "slug": "introduction-to-kubernetes-architecture"
    },
    {
        "layout": "post",
        "title": "Revolutionising the Wheel",
        "author_github": "Monal",
        "date": "2021-07-30T00:00:00",
        "image": "/assets/img/",
        "description": "Superelastic tires and their part in tomorrow's future",
        "tags": [
            "IEEE NITK",
            "Piston"
        ],
        "categories": [
            "piston"
        ],
        "github_username": "Monal",
        "text": "[{% load static %}]()\nWith scientific advancements in space technologies, the need for more robust and efficient components is on the rise. To meet the demands of smoother and more stable rides in exoplanet missions such as Mars exploration, scientists are on the verge of reinventing our very concept of tires. Enter Superelastic Tires - a new research domain that inherits from the idea of Spring Tires, presented by NASA Glenn and Goodyear - inspired by the Apollo lunar tires.\n\nSpring tires introduced a new concept of airless tires from what we see every day. These off breeds consist of several coiled steel wires woven into a flexible mesh providing the tires with the capabilities to support high loads while also conforming to the terrain, making it suitable for operations in unknown and complicated terrains we might expect from Martian or Lunar missions.\n\nBut what differs between Spring tires and the superelastic tires developed by NASA? To answer this, we'll need to look into the revolutionizing concept of \"Shape Memory Alloys.\"\n\n## A Metal that remembers - NITINOL\n\nNiTiNOL is an alloy of nickel and titanium, developed by William Beuhler in the laboratories of the US Navy in the 1960s. The exact ratio between nickel and titanium is 55% and 45%, respectively. Apart from their excellent superelasticity, durability, and corrosion-resistant properties, these alloys feature a very peculiar shape memory concept.\n\nThese alloys can be deformed when cold but return to their pre-deformed shape when heated to a specific cutoff temperature. This property is in virtue of the temperature-induced phase transformation that reverses deformation. When Nitinol is below a particular temperature, it has a crystal structure called martensite. Its crystal structure is arranged in such a way that it can accommodate deformation very easily. When stress is applied, the grains deform and align to absorb the pressure best.\n\nLike the dislocation movement in other metals, this deformation is permanent in Nitinol, without external energy providing the energy needed to revert backward. Still, Nitinol can get that energy from heat. Upon heating, the Nitinol forms austenite, an ordered and regular crystal structure, which effectively resets the crystal structure, and when the Nitinol cools again, the Nitinol remembers its original.\n\nIt is this use of shape memory alloys which results in a tire that can withstand excessive deformation without permanent damage due to their abilities of undergoing high strain as load-bearing components, instead of typical elastic materials.\n\n## Nitinol en route\n\nUsing shape memory alloy as stiffening elements can also increase the load-carrying capacity of the tire. The Superelastic Tire fabricated from these allows traction superior to conventional pneumatic tires. It also eliminates both the possibility of puncture failures and running \"under-inflated,\" thereby improving automobile fuel efficiency and safety. Furthermore, the tire design does not require an inner frame that simplifies and lightens the tire/wheel assembly.\n\nTo reduce the amount of energy transferred to the vehicle during high deformation events, the Glenn Superelastic Tire can be made to soften causing increased deflection. Besides, the use of shape memory alloys in radial stiffeners instead of springs provides even more load-carrying potential and improved design flexibility. This type of compliant tire would allow for increased travel speeds in off-road applications paving way for a safer and better tomorrow.",
        "author_name": "Monal Singh",
        "slug": "revolutionising-the-wheel"
    },
    {
        "layout": "post",
        "title": "An Introduction to Disjoint-set Data Structures",
        "author_github": "ZerothKing",
        "date": "2021-08-02T00:00:00",
        "image": "/assets/img/",
        "description": "A practical introduction to Disjoint-set Data Structures",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Disjoint-set",
            "Data Structures and Algorithms"
        ],
        "categories": [
            "Compsoc"
        ],
        "github_username": "ZerothKing",
        "text": "[{% load static %}]()\n# <span style=\"text-decoration:underline;\">Introduction</span>\n\nA **disjoint-set data structure**, also called a **union\u2013find data structure** or **merge\u2013find set**, is a data structure that stores a collection of **[disjoint](https://en.wikipedia.org/wiki/Disjoint_sets)** (non-overlapping) sets. Equivalently, it stores a **[partition of a set](https://en.wikipedia.org/wiki/Partition_of_a_set)** into disjoint subsets.\n\n# <span style=\"text-decoration:underline;\">Capabilities</span>\n\nThis data structure provides the following capabilities. We are given several elements, each of which is a separate set. A DSU(Disjoint set union) will have an operation to combine any two sets, and it will be able to tell in which set a specific element is. The classical version also introduces a third operation, it can create a set from a new element.\n\n# <span style=\"text-decoration:underline;\">Basic Interface</span>\n\nThus the basic interface of this data structure consists of only three operations:\n\n* **make_set(v)**- creates a new set consisting of the new element **v**\n* **union_sets(a, b)** - merges the two specified sets (the set in which the element **a** is located, and the set in which the element **b** is located)\n* **find_set(v)** - returns the representative (also called leader/parent) of the set that contains the element**v**. This representative is an element of its corresponding set. It is selected in each set by the data structure itself (and can change over time, namely after **union_sets** calls). This representative can be used to check if two elements are part of the same set or not. **a** and **b** are exactly in the same set, if **find_set(a) == find_set(b).** Otherwise they are in different sets.\n\n# <span style=\"text-decoration:underline;\">How it works</span>\n\nWe will store the sets in the form of **trees**: each tree will correspond to one set. And the root of the tree will be the representative/leader of the set.\n\nIn the beginning, every element starts as a single set, therefore each vertex is its own tree.\n![Image_being_discussed]({% static '/blog/assets/img/Disjoint-set-data-structures/1.png' %})\n\n Then we combine the set containing the element 1 and the set containing the element 2. Then we combine the set containing the element 3 and the set containing the element 4. And in the last step, we combine the set containing the element 1 and the set containing the element 3.\n\nFor the implementation this means that we will have to maintain an array **parent** that stores a reference to its immediate ancestor in the tree.\n\n# <span style=\"text-decoration:underline;\">Implementation</span>\n\nAs I said, all the information about the sets of elements will be kept in an array **parent**.\n\nTo create a new set (operation **make_set(v)**), we simply create a tree with root in the vertex **v**, meaning that it is its own ancestor.\n\nTo combine two sets (operation **union_sets(a, b)**), we first find the representative of the set in which **a** is located, and the representative of the set in which **b** is located. If the representatives are identical, that we have nothing to do, the sets are already merged. Otherwise, we can simply specify that one of the representatives is the parent of the other representative - thereby combining the two trees.\n\nFinally the implementation of the find representative function (operation **find_set(v)**): we simply climb the ancestors of the vertex **v** until we reach the root, i.e. a vertex such that the reference to the ancestor leads to itself. This operation is easily implemented recursively. Here is the implementation -\n\n```cpp\nvoid make_set(int v) {\n    parent[v] = v;\n}\n\nint find_set(int v) {\n    if (v == parent[v])\n        return v;\n    return find_set(parent[v]);\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b)\n        parent[b] = a;\n}\n```\n\nThe time complexity of this algorithm is **O(N)** where N is the size of the set of elements.\n\n# <span style=\"text-decoration:underline;\">Improving the efficiency of the algorithm</span>\n\n## <span style=\"text-decoration:underline;\">Path Compression</span>\n\nPath compression is useful for speeding up the **find_set** function.\n\nIf we call **find_set(v)** for some vertex **v**, we actually find the representative **p** for all vertices that we visit on the path between **v** and the actual representative **p**. The trick is to make the paths for all those nodes shorter, by **setting the parent of each visited vertex directly to p**. Here is the implementation -\n\n```cpp\nint find_set(int v) {\n    if (v == parent[v])\n        return v;\n    return parent[v] = find_set(parent[v]);\n}\n```\n\nThe simple implementation does what was intended: first find the representative of the set (root vertex), and then in the process of stack unwinding the visited nodes are attached directly to the representative. The complexity has been reduced to **O(NlogN).**\n\n## <span style=\"text-decoration:underline;\">Union by Rank</span>\n\nThink about what happens when we join the larger tree to the smaller tree while merging the two sets. Clearly this can lead to trees with chain length of **O(N)**. So if we always join the smaller tree to the larger one, we can significantly reduce the time complexity.\n\nIn this approach, we use the size of the tree as the rank and we join the tree with lower rank to the one with the higher rank. Here is the implementation -\n\n```cpp\nvoid make_set(int v) {\n    parent[v] = v;\n    rank[v] = 1;\n}\n\nvoid union_sets(int a, int b) {\n    a = find_set(a);\n    b = find_set(b);\n    if (a != b) {\n        if (rank[a] < rank[b])\n            swap(a, b);\n        parent[b] = a;\n        rank[a] += rank[b];\n    }\n}\n```\n\n# <span style=\"text-decoration:underline;\">Time Complexity</span>\n\nIf we combine the two optimisation techniques that we discussed, the time complexity comes out to be **O(\u03b1(n))**, where **\u03b1(n)** is the inverse Ackermann function, which grows very slowly. In fact it grows so slowly, that it doesn't exceed 4 for all reasonable n. So we can say that we will reach **nearly constant** time queries. This is one of the reasons why a disjoint-set is so powerful.\n\n# <span style=\"text-decoration:underline;\">Applications</span>\n\n## <span style=\"text-decoration:underline;\">Connected Components in a graph</span>\n\nThis is one of the obvious applications of DSU.\n\nFormally the problem is defined in the following way: Initially we have an empty graph. We have to add vertices and undirected edges, and answer queries of the form (a,b) - \u201care the vertices a and  b in the same connected component of the graph?\"\n\nHere we can directly apply the data structure, and get a solution that handles an addition of a vertex or an edge and a query in nearly constant time on average.\n\nThis application is quite important, because nearly the same problem appears in **Kruskal\u2019s algorithm for minimum spanning tree** and DSU helps in reducing it\u2019s time complexity.\n\n## <span style=\"text-decoration:underline;\">Job Sequencing problem</span>\n\nGiven a set of n jobs where each job i has a deadline **d_i** >= 1 and profit **p_i** >= 0. Only one job can be scheduled at a time. Each job takes 1 unit of time to complete. We earn the profit if and only if the job is completed by its deadline. The task is to find the subset of jobs that maximizes profit.\n\n**Note** : If you try to solve this greedily the time complexity will be O(n^2) but DSU makes it much faster.\n\nAll time slots are individual sets initially. We first find the maximum deadline of all jobs. Let the max deadline be m. We create m+1 individual sets. If a job is assigned a time slot of t where t => 0, then the job is scheduled during [t-1, t]. So a set with value X represents the time slot [X-1, X].\n\nWe need to keep track of the greatest time slot available which can be allotted to a given job having deadline. We use the parent array of Disjoint Set Data structures for this purpose. The root of the tree is always the latest available slot. If for a deadline d, there is no slot available, then root would be 0.\n\n# <span style=\"text-decoration:underline;\">Conclusion</span>\n\nWe can clearly say that DSU are one of the most efficient data structures and are really helpful for various algorithms. They are efficient and use small amount of memory. I hope that this article will help you quickly familiarise yourself with DSU as it can help you in solving various problems quickly and efficiently.\n\n# <span style=\"text-decoration:underline;\">References and further reading</span>\n\n1. [en.wikipedia.org/wiki/Disjoint-set_data_structure](http://en.wikipedia.org/wiki/Disjoint-set_data_structure)\n2. [en.wikipedia.org/wiki/Ackermann_function](http://en.wikipedia.org/wiki/Ackermann_function)\n3. [https://www.topcoder.com/community/competitive-programming/tutorials/disjoint-set-data-structures/](https://www.topcoder.com/community/competitive-programming/tutorials/disjoint-set-data-structures/)\n4. [https://en.wikipedia.org/wiki/Kruskal%27s_algorithm](https://en.wikipedia.org/wiki/Kruskal%27s_algorithm)",
        "author_name": "Sanjkeet Jena",
        "slug": "an-introduction-to-disjoint-set-data-structures"
    },
    {
        "layout": "post",
        "title": "CPUs Vs GPUs: Allies or Foes?",
        "author_github": "rahul-magesh",
        "date": "2021-08-02T00:00:00",
        "image": "/assets/img/",
        "description": "CPUs are the brains of a computer handling connections and all the processes involved in running your computer. While this remains true even today, one doesn't attain a full picture of the basic processing in a modern computer.",
        "tags": [
            "IEEE NITK",
            "Diode",
            "Computer Architecture"
        ],
        "categories": [
            "diode"
        ],
        "github_username": "rahul-magesh",
        "text": "[{% load static %}]()\n# CPUs Vs GPUs: Allies or Foes?\n\nCPUs are the brains of a computer handling connections and all the processes involved in running your computer. \n\nWhile this remains true even today, one doesn't attain a full picture of the basic processing in a modern computer.\n\n![See the source image](https://media.gettyimages.com/photos/microprocessor-with-human-brain-picture-id175545547)\n\nOver the past decade, another rapidly improving component of a modern computer which is just as important or even more so is the GPU.\n\n## So, what is a GPU?\n\n![See the source image](https://content.presspage.com/uploads/1369/1920_shutterstock-562179643.jpg?10000)\n\n**I'm not a gamer so I don't need GPUs**.\nThis is a common misconception that is spread among less technically-inclined people.\nOne might hear that \"GPUs are a waste of money and only used for gaming.\"\nThis is definitely not the complete story and a classic case of over-simplification.\nA GPU or GRAPHICS PROCESSING UNIT is used for displaying video output and will be found on all modern devices capable of the same.\n\n## iGPU vs dGPU: What do you have?\n\nIntegrated GPUs (iGPUs) share die-space with your processor and are found baked into most desktop processors and all mobile processors.\nYes, your thin and light laptop with an Intel CPU, contrary to common myths, has a GPU too. \n\n![See the source image](https://i.stack.imgur.com/OpaCC.jpg)\n\nIntel HD or Iris graphics are among the common names for Intel's iGPUs. \nEven the Adreno on your fancy Android phone is an iGPU baked into your Snapdragon processor\n\nDedicated GPUs(dGPUS) are physically separate from the die-space of your CPU and communicate with the CPU via PCIE lanes. \n\nDue to having a physically separate die (this allows the manufacturers to have a higher degree of freedom while choosing the die size, no. of cores and other factors), we see larger variations in power draw as well as performance.\n\n![See the source image](https://www.dsogaming.com/wp-content/uploads/2020/09/NVIDIA-RTX3090-feature-672x372.jpg)\n\n![Table 1]({% static '/blog/assets/img/cpu-vs-gpu/table1.png' %})\n\n## PCIE LANES\n\nPCIE or Peripheral Component Interconnect Express, determines the devices that are attached or plugged into the motherboard. It identifies the links between each device, creating a traffic map and negotiates the width of each link. \n\nIn simple terms, one could describe them as highways with a direct connection from your computer parts to your CPU. The lanes can also be visualized as highway lanes. Higher bandwidth for more lanes, just like more traffic can flow on more lanes. \n\nSince dedicated GPUs are bandwidth-heavy compared to other components like USBs or storage drives, we usually allocate them with 8 or 16 lanes. \n\nHere is a small table, containing the maximum bandwidth of various PCIE generations. \n\n![See the source image](https://th.bing.com/th/id/OIP.CwTzWuoeMSoB9SkG4AtxzgHaBy?pid=Api&rs=1)\n\n## PARALLEL PROCESSING\n\nNow, let's have a look at the fundamental differences in the processing approach of a GPU compared to a CPU.\n\n![Table 2]({% static '/blog/assets/img/cpu-vs-gpu/table2.png' %})\n\nA CPU consists of a few powerful cores (relatively speaking) with a lot of cache memory that is good at handling sequential tasks but not as good at conducting multiple tasks simultaneously. A GPU on the other hand consists of many more simpler cores which are individually weaker and capable of fewer instructions than a CPU core but together can perform a task optimized for parallel processing much faster than a CPU.\n\nThis makes them ideal for graphics, where textures, lighting and the rendering of shapes have to be done at once to keep images flying across the screen. \n\n***I suggest that you take a look at this short video for an interesting depiction of the idea presented above.***\n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/-P28LKWTzrI\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n\n## Artificial Intelligence\n\nAI has also made rapid strides thanks to the parallel processing nature of GPUs.\n\nThey have become key to a technology called \u201cdeep learning.\u201d Deep learning pours vast quantities of data through neural networks, training them to perform tasks too complicated for any human coder to describe.\n\n## VRAM\n\nVRAM or Video Random Access Memory acts as a buffer between the CPU and the GPU. It stores the data of all the images and videos the GPU generates. It is also referred to as a frame buffer. \n\nIf the system does not have a sufficient amount of VRAM, the textures and images you are trying to load can cause GPU to overflow on the system\u2019s RAM. This will cause poor performance due to the increased latency.\n\nMore VRAM is better but may not necessarily indicate better performance between different GPUs.\n\n## CONCLUSION\n\nWe can certainly see that GPUs are here to stay.\n\nHowever, due to having better performance than CPUs only in optimized or specific scenarios, one can conclude that the two complement each others' weaknesses rather than outright replacing the other.\n\n#### <u>Resources:</u>\n\n- [What's the Difference Between a CPU vs a GPU?](https://blogs.nvidia.com/blog/2009/12/16/whats-the-difference-between-a-cpu-and-a-gpu/)\n- [PCIe lanes explained - how many do you need on a motherboard? (cotscomputers.com)](https://cotscomputers.com/blog/pcie-lanes/)\n- [What is VRAM? Video RAM Explained (geeksgyaan.com)](https://www.geeksgyaan.com/2019/12/vram-explained.html)",
        "author_name": "Rahul Magesh",
        "slug": "cpu-vs-gpu"
    },
    {
        "layout": "post",
        "title": "Learning Fluid Mechanics: From Living Organisms to Machines",
        "author_github": "bdiptesh99",
        "date": "2021-08-02T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Piston"
        ],
        "categories": [
            "piston"
        ],
        "github_username": "bdiptesh99",
        "text": "[{% load static %}]()\nBirds, bats, insects, fish, whales, and other aquatic and aerial life-forms perform remarkable feats of fluid manipulation, optimizing and controlling their shape and motion to harness unsteady fluid forces for agile propulsion, efficient migration, and other exquisite maneuvers. The range of fluid flow optimization and control observed in biology is breathtaking and has inspired humans for millennia. How do these organisms learn to manipulate the flow environment?\n\nTo date, we know of only one species that manipulates fluids through knowledge of the Navier\u2013Stokes equations. Humans have been innovating and engineering devices to harness fluids since before the dawn of recorded history, from dams and irrigation to mills and sailing. Early efforts were achieved through intuitive design, although recent quantitative analysis and physics-based design have enabled a revolution in performance over the past hundred years. Indeed, physics-based engineering of fluid systems is a high-water mark of human achievement. However, there are serious challenges associated with equation-based analysis of fluids, including high dimensionality and nonlinearity, which defy closed-form solutions and limit real-time optimization and control efforts. At the beginning of a new millennium, with increasingly powerful tools in machine learning and data-driven optimization, we are again learning how to learn from experience.\n\n![image-1]({% static '/blog/assets/img/fluid-mechanics-from-living-to-machines/image1.png' %})\n<center>Figure: First example of learning and automation in experimental fluid mechanics: Rechenberg\u2019s (1964) experiments for optimally corrugated plates for drag reduction using the Galtonbrett (Galton board) as an analog random number generato</center>\n\n![image-2]({% static '/blog/assets/img/fluid-mechanics-from-living-to-machines/image2.png' %})\n\n## Challenges and Opportunities for Machine Learning in Fluid Dynamics\n\nFluid dynamics presents challenges that differ from those tackled in many applications of ML, such as image recognition and advertising. In fluid flows it is often important to precisely quantify the underlying physical mechanisms in order to analyze them. Furthermore, fluid flows exhibit complex, multiscale phenomena the understanding and control of which remain largely unresolved. Unsteady flow fields require algorithms capable of addressing nonlinearities and multiple spatiotemporal scales that may not be present in popular ML algorithms. In addition, many prominent applications of ML, such as playing the game Go, rely on inexpensive system evaluations and an exhaustive categorization of the process that must be learned. This is not the case in fluids, where experiments may be difficult to repeat or automate and where simulations may require large-scale supercomputers operating for extended periods of time.\n\nI believe that this nonexhaustive list of challenges need not be a barrier; to the contrary, it should provide a strong motivation for the development of more effective ML techniques. These techniques will likely impact several disciplines if they are able to solve fluid mechanics problems. The application of ML to systems with known physics, such as fluid mechanics, may provide deeper theoretical insights into algorithms. We also believe that hybrid methods that combine ML and first principles models will be a fertile ground for development.",
        "author_name": "Diptesh Banerjee",
        "slug": "fluid-mechanics-from-living-to-machines"
    },
    {
        "layout": "post",
        "title": "Brain-Machine Interfaces",
        "author_github": "achintya-k-n",
        "date": "2021-08-09T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Diode",
            "Medical Electronics"
        ],
        "categories": [
            "diode"
        ],
        "github_username": "achintya-k-n",
        "use_math": true,
        "text": "[{% load static %}]()\n# Brain-Machine Interfaces\n\nWith Artificial Intelligence and assisted technologies improving as rapidly as they are, many scientists and tech pioneers feel that AI might become 'better' than humans sooner than we think (some even say that they can take over the world!). But would that be the case if our minds could directly connect with AI, robots and other minds? Maybe we need to become cyborgs to stay relevant in an AI ruled age and this is exactly what Brain-Machine Interfaces hope to bring to the table.\n\nAs the name suggests, Brain-Machine Interfaces (also called Brain-Computer Interfaces) are devices that enable users to interact with computers by means of brain activity (in layman terms, you can do stuff just by thinking about it). BCIs are capable of a lot of things that would otherwise seem impossible to say a common man of the 20th century. For instance, they can replace lost functions (speaking, moving), they can restore functionality in organs by stimulating certain nerves and they can also enhance and improve general human physical capabilities (better reflexes and awareness). \nSo when did this, seemingly revolutionary technology come to the fore? \nResearch and innovations in this field date back as far as 1924 when Hans Berger discovered electrical signals being emitted from the brain and the subsequent development of the EEG as a result. Berger analyzed the interrelation of his EEG wave diagrams with some brain disorders and found that EEG opened up completely new possibilities for research on human brain activity. Not only were opportunities opened up in the field of research but also early diagnosis of various disorders and diseases which were earlier thought to be 'untreatable and incomprehensible' was possible now. \n\nNow, let us try to define a proper, rigid classification basis for BCIs to better understand the content that follows,\nMainly, BCIs have a broad classification into 3 categories, namely,\n#### a) Invasive BCI\nThis kind of BCI require surgery to implant various electrodes and sensors in the brain (yes, way inside the skull). This provides great accuracy in readings however the disadvantages are obviously the side-effects of such surgerical procedures. In some cases, scar tissues can form as well, which defeats the whole purpose of invasive BCI by weakening the signals.\n#### b) Partially Invasive BCI\nThese BCIs also require surgery and implants inside the skull but these reside outside the grey matter in the brain. Risk of scar tissue formation is lesser but this is at the cost of reduced accuracy but is still better than non-invasive BCI where the bone tissue interferes greatly with the brain signals.\n#### c) Non-invasive BCI\nThis is the safest type of BCI with no scary implants in the brain but of course, accuracy is greatly reduced. Most research and publications are based on these BCI. They have relatively poor spatial resolution and cannot effectively use higher-frequency signals because the skull dampens signals, dispersing and blurring the electromagnetic waves created by the neurons. The best BCI for a particular user depends on various parameters.\n\nThere are many kinds of BCIs out there but most of them grapple with both or either of these 2 questions namely: How do I recieve relevant information from the brain? and How do I send relevant information to the brain? Now, this is basically what neurons do and the BCI industry wants to somehow replicate this artificially, which is far easier said than done. There are roughly 100 billion neurons in the brain and 50 times more glial cells (they hold the nerves together and aid neurotransmission and maintain the chemical stability of the brain) and all these neurons are connected to many other neurons so it is safe to say that 'complicated' doesn't even begin to describe it. \n![img]({% static '/blog/assets/img/brain-computer-interface/img8.jpg' %})\n\n So how do BCIs tackle (or hope to tackle) all of this? Well, they do the best that can be done with the tools available today. Firstly, let us look at some of the tools BCIs use and their basic features.\n### 1) fMRIs\nFunctional MRIs are devices which use magnetic fields to generate images concerning blood flow. Blood flow indicates which parts of the brain is recieving blood and some superficial analyses can be done on the brain and its activities. \n![img]({% static '/blog/assets/img/brain-computer-interface/img3.jpg' %})\nHowever, the resolution of fMRIs is pretty low (we can't know where exactly as fMRIs are taken of the entire brain). Nevertheless, since fMRIs have good scale and have been instrumental in understanding which parts of the brain influence what kind of functions.\n### 2) EEGs\nEEG (electroencaphalography) basically puts electrodes to your head (specifically, on the scalp) and records the electrical activity in various regions of the brain and is very useful in determining how the brain uses electrical signals to transmit data. \n![img]({% static '/blog/assets/img/brain-computer-interface/img4.jpg' %})\nThough this has better resolution than fMRIs, it is still very low. Considering the complexity of the brain, it barely scratches the surface in terms of resolution but it does help us understand the electrical patterns of the brain for carrying out certain tasks like, \n![img]({% static '/blog/assets/img/brain-computer-interface/img4a.jpg' %})\n### 3) ECoG\nIn ECoG (electrocorticography), electrodes are inserted under the scalp and similar circuits as EEG are used for the detection of electrical activity in the brain. This has much better resolution but it is kind of invasive to the user (it might seem very invasive but this is just an incision on the scalp, the following ones are much worse).\n![img]({% static '/blog/assets/img/brain-computer-interface/img5.jpg' %})\n### 4) Low Field Potential\nIn LFPs, thin needle-like electrodes (made of silicon wafers or other materials from the integrated circuits industry) are stuck 1 or 2 mm in the cortex (cortex makes up for 80% of the brain, it is the whole jelly-like squiggly mass which makes up most of your brain).Though this has incredibly high resolution, it is horrible when it comes to scale and invasiveness. There have been further innovations in this fields, for example, Multielectrode arrays which use the same idea as LFPs but 100 electrodes are situated next to one another so this improves the scale of the device. These electrodes are of course, very thin and sharp as they are of the size and scale of cells.\n![img]({% static '/blog/assets/img/brain-computer-interface/img6.jpg' %})\n### 5) Single-Unit Recording\nSingle-Unit Recording also uses thin electrodes but their tips are ultra thin and the resistance values are made to be super high. Thus it will only achieve connection when it gets really close to a neuron and other noise won't be heard due to the high resistance wall. This gives the best resolution possible however it is very invasive and scale is practically null. Some new designs of the SUR take it to the next level using a technique called Patch Clamp where it clamps on to a membrane patch of the neuron using a glass tip which is located at the tip of the needle instead of an electrode (this also takes advantage of the high resistance value of the patch clamp tip).\n![img]({% static '/blog/assets/img/brain-computer-interface/img7.png' %})\n\nThis can give us much better readings and values. Also, since it physically touches the neuron, this can be improvised to stimulate the neuron.\nThere are also other designs like Sharp Electrode Recording which pierces the neuron and the tip is so sharp that, a membrane forms around this tip and this allows us to effectively read the voltage values on the neuron and also stimulate it as and when required.\n\nThat was about it on the tools used in BCIs, there are various BCIs out there today, so let's check out some of them,\n\n## BCI with LOCOMOTORY OUTPUTS\nOne of the most exciting application of these tools is to implement a BCI device would be those which can be controlled by the user's thoughts. Firstly, let's look at those controlled by the Motor Cortex which has huge implications especially for disabled people. Now, bringing about movement in people who can't move is a very challenging task. First, the brain signal of such a person will have to be interpreted and then carried out physically. For this, the user will have to be trained to visualize moving their appendages as vividly as possible and later our device needs to incorporate Machine Learning algorithms to successfully learn to relate these brain signals to that of arm or leg movement. This algorithm can then be integrated with a software and connected to a robotic arm which can carry out these functions for the user. Similar concepts can be used to enable disabled people to type, use computers, write, etc.. (provided such an algorithm can be developed successfully)\nCheck out this video where a quadriplegic woman uses BCIs to carry out locomotory tasks: \n\n<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/QRt8QCx3BCo\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>\n\n## BCI with SENSORY INPUTS\nThese can really be a revolutionary innovation for people whose senses have been impaired. One of the most basic and primitive forms of these BCIs are Cochlear Implants. Cochlea is an essential part of the hearing system in mammals. Now, for an average person, sound waves enter the ear and pass through the eardrums and several other pathways (Cochlea being one of them) and then pass on to the Auditory Nerves which recieve an electrical input of these sound waves. In some cases, these hearing pathways might be damaged whereas the Auditory nerve might still be intact, in such a scenario, the Cochlear implants can do the 'hearing' for the subject and output electrical signals into the auditory nerves, enabling the subject to hear again to some extent. Similar processes can be carried out to create artificial eyes but it is far more challenging since the visual system is much more sophisticated but it is definitely possible to do so by using 2 camera-like devices as eyes in theory.\n\nA more detailed flowchart for carrying out these algorithms can be seen below,\n![img]({% static '/blog/assets/img/brain-computer-interface/img1.jpeg' %})\n\nAs you can see, AI/ML and Signal Processing plays a huge role in this and can be used to solve many problems in this domain but the problems are very, well, problematic. For instance, there will be loads of noise (unless you are okay with patch-clamping) for proper signal processing and creating ML models to understand brain waves amidst all of the noise (remember, our brain does a whole lot of things simultaneously which also becomes part of the noise) is a challenge of its own.\n\n## Convolutional Neural Networks and BCIs\nCNN is a type of AI neural network based on the visual cortex of the mammalian brain. It has the capacity to learn the appropriate features from the input data automatically by optimizing the weight parameters of each filter through the forward and backward propagation steps in order to minimize the classification error. Using such Deep learning frameworks for BCI applications is advantageous in many ways, it requires minimal pre-processing as optimal settings are learned automatically, feature extraction and classification are integrated into a single structure and optimized automatically. Moreover, Convolutions are integrated in a slide-show manner which can retain temporal information while extracting features from the CNN. However, it is still very difficult to find reliable patterns in brain wave functions due to reasons mentioned before (the brain is the biggest baddest multitasker out there making pattern recognition hugely complicated).\n![img]({% static '/blog/assets/img/brain-computer-interface/img2.jpeg' %})\n\nThus, it is safe to conclude that BCIs have a long way to go but decent inroads have been made in recent times which can surely be built upon in the near future (well, they are already being worked on..eg. Neuralink.. to know more, click [here](https://en.wikipedia.org/wiki/Neuralink)).\n\n## References:\n- [Brain-Computer Interface](https://en.wikipedia.org/wiki/Brain%E2%80%93computer_interface)\n- [This Is a Computer on Your Brain](https://www.wired.com/2006/07/this-is-a-computer-on-your-brain/)\n- [Neuralink and the Brain\u2019s Magical Future](https://waitbutwhy.com/2017/04/neuralink.html#part3)\n- [A Beginner's Guide to Brain-Computer Interfaces and CNNs](https://towardsdatascience.com/a-beginners-guide-to-brain-computer-interface-and-convolutional-neural-networks-9f35bd4af948)\n- [How Brain-computer Interfaces Work](https://computer.howstuffworks.com/brain-computer-interface.htm)",
        "author_name": "Achintya K N",
        "slug": "brain-machine-interfaces"
    },
    {
        "layout": "post",
        "title": "Where's the Bot Now?",
        "author_github": "ramyashri1887",
        "date": "2021-08-09T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Diode",
            "Robotics"
        ],
        "categories": [
            "diode"
        ],
        "github_username": "ramyashri1887",
        "text": "[{% load static %}]()\nHave you ever wondered how life was way before GPS was introduced.I remember taking an OLA ride to a place back in 2013s when google map was not incorporated into the OLA app, and both the driver and I were new to the location and although we were right in front of the location , we didn't know where exactly we were!!!.\n\nWe kept driving around the same location for over 30 min only to find that we were at the right location way before. The bill amounted to 1000 and I was devastated. **So answering the question of what we did before GPS was mounted in our devices. We got lost!** Now you get the essence and importance of Localisation. GPS is a life saver but is not the most reliable technology for indoor localisation.By the end of this article you'll know the answers for all the Ws of localisation. Have a basic intuition of different models and filters used for localisation.\n\n## WHAT'S LOCALISATION FIRSTLY ?\n\nLocalisation provides an answer to the question \"Where's the robot now\"?\n\nSo by localisation we mean estimating the position and the orientation of an \"object\" in a given coordinate frame.So essentially it is the building block for all navigation systems.\n\nThe \"object\" could be a robot navigating through an environment or a self-driving car or a drone, it could be anything.\n\nLocalisation is often performed given a map of the environment.\n\n### MAPS ARE EVERYWHERE IN ROBOTICS\n\nMaps are required for most of the robotic tasks like path planning and localisation . The map could be represented in various forms as below.\n\n#### 1) Feature based maps\n\nAn image based representation using coordinates of known landmarks or features.\n\n![img]({% static '/blog/assets/img/localization-robotics/feature-map.png' %})\n\n\n#### 2) Occupancy grid maps\n\nAn image based representation of the environment in terms of occupied and unoccupied regions. White pixels represent unoccupied regions and black pixels represent occupied regions.\n\n![img]({% static '/blog/assets/img/localization-robotics/grid-map.png' %})\n\nLet's say you are new to the NITK campus and have to navigate to the swimming pool inside the campus. You are given a map of the campus alongside. How do you know where you are exactly with respect to the map ?\n\nYou use your vision(Sensor) to perceive the environment around you and compare it with the landmarks provided in the map to find that you are close to the sports complex.You also figure out that to reach the swimming pool you have to walk 3000m forward and take a right and move another 200m/s. You begin to move with a uniform speed of 5m/s.\n\nYou have your sensors(Your vision), you have a model with you and hence you can estimate your current location with respect to the sports complex using a simple equation x=vt.\n\nGreat job, that's what a robot does, a mobile robot equipped with sensors to monitor its own motion (e.g., wheel encoders and inertial sensors) can compute an estimate of its location relative to where it started if a mathematical model of the motion is available.This is called **DEAD RECKONING**.But dead reckoning is not a truly reliable process due to sensor noises etc.\n\nSo , Errors in dead reckoning are corrected using environment perception i.e. by correlating the information from sensors( **Laser range** finders, **ultrasonic sensors** , **Cameras** are most common sensors used for obtaining range and bearing measurements to landmarks) with the map available.\n\nSo as we saw the most important components of a localisation problem are the **motion model** describing the motion of the bot using simple kinematics equations and the **sensor model** which is the relationship between the observations from the sensors and the location of the robot in the map.\n\nFor instance ,the kinematic equation governing your motion is a simple x=vt. similarly,\n\nthe equations governing the motion of the **differential drive robot** are given by\n\n\ud835\udc65\u0307 (\ud835\udc61)=(\ud835\udc63(\ud835\udc61) + \ud835\udeff\ud835\udc63(\ud835\udc61)) cos(\ud835\udf19\ud835\udc5f (\ud835\udc61))\n\n\ud835\udc66\u0307 (\ud835\udc61)=(\ud835\udc63(\ud835\udc61) + \ud835\udeff\ud835\udc63(\ud835\udc61)) sin(\ud835\udf19\ud835\udc5f (\ud835\udc61))\n\n\ud835\udf19\u0307 (\ud835\udc61) = \ud835\udf14(\ud835\udc61) + \ud835\udeff\ud835\udf14(\ud835\udc61)\n\nWe can have various sensor models based on the sensors mounted and the representation of the environment; either feature based map or grid occupancy based map and obtain location estimate based on the landmark observation.\n\nThe sensor based positioning includes WiFi based positioning systems, Bluetooth systems,Infrared systems,proximity based or Acoustic based (Sound followers)\n\nThe main caveat is ,\n\n## WHO TO LISTEN TO? \n\nThe estimate based on sensor information or the estimate from the kinematic model (motion information)?\n\nThe answer is **Bayesian filtering**.\n\nIt takes into account the uncertainties involved in both the measurement models and obtains an estimate of the robot location and the associated uncertainty! Extended Kalman filter (EKF) and particle filter provide complaint approximations to Bayesian filtering. Both are recursive probabilistic frameworks.\n\n## EKF vs PARTICLE FILTER\n\nGLOBAL LOCALISATION (initial position of the bot is unknown) cannot be performed using EKF\n\nEKF can't be employed if the representation of the map is an occupancy grid.\n\nEKF assumes Gaussian probability distribution, while Particle filters can handle arbitrary probability distributions.\n\n## THE TAKEAWAY\n\nLocalisation is highly relevant in robotics, autonomous driving cars, computer vision, achieving intelligent connectivity,even smartphone requires pose information to make use of certain services\n\n## FURTHER READING\n\n### Implementation of Mapping and localisation Algorithms on ROS\n\n1. [ROS NAVIGATION IN 5 DAYS #3 - Robot Localization](https://www.youtube.com/watch?v=NANc8CkGI2U)\n2. [ROS Developers LIVE-Class #49: How to Map & Localize a Robot (ROS)](https://www.youtube.com/watch?v=RknTTpga64s)\n\n### Maths behind Extended Kalman filters ,Particle filters and Monte Carlo localisation Algorithm\n\n1. [Particle Filters and Their Applications](https://web.mit.edu/16.412j/www/html/Advanced%20lectures/Slides/Hsaio_plinval_miller_ParticleFiltersPrint.pdf)\n2. [Monte Carlo Localization Algorithm](https://www.mathworks.com/help/nav/ug/monte-carlo-localization-algorithm.html)\n\n## BIBLIOGRAPHY\n\n1. [Robot Localization: An Introduction](https://onlinelibrary.wiley.com/doi/full/10.1002/047134608X.W8318).\n2. [Particle Filter and Monte Carlo Localization (Cyrill Stachniss, 2020)](https://www.youtube.com/watch?v=MsYlueVDLI0&amp;list=PLgnQpQtFTOGSeTU35ojkOdsscnenP2Cqx&amp;index=13)",
        "author_name": "P Ramyashri",
        "slug": "localization-robotics"
    },
    {
        "layout": "post",
        "title": "Playing Blackjack Optimally with Reinforcement Learning",
        "author_github": "Shreya301",
        "date": "2021-08-09T00:00:00",
        "image": "/assets/img/",
        "description": "An insight into Reinforcement Learning methods and how they work on blackjack",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Reinforcement Learning"
        ],
        "categories": [
            "compsoc"
        ],
        "github_username": "Shreya301",
        "use_math": true,
        "text": "[{% load static %}]()\n## Introduction to Reinforcement Learning\n\nReinforcement learning(RL) is a branch of machine learning which draws from **the way humans learn to do tasks**. For anyone unfamiliar with Pavlov, he is a physicist popularly known for his work in conditioned response. He trained a dog such that every time he rang a bell, he would immediately give the dog food. The dog soon learned to salivate at the sound of the bell. The same foundation to Jim\u2019s mint prank in The Office for those who get the reference xD.\n\n![Pavlov's Experiment]({% static '/blog/assets/img/blackjack/pavlov.jpg' %})\n\nReinforcement learning (or RL) takes Pavlov\u2019s experiment one step further. Suppose the new experiment was that when Pavlov rang each bell out of a set of them, the dog would have to perform some corresponding trick to receive his treat. The dog, through trial and error, would eventually observe and learn the correct action to take for each bell to get as many treats as he can. This is essentially how dog-training and, you guessed it, RL in its basic form works!\n\nFor a quick insight into the technical basics of RL, _it is a setup where an agent (our dog) takes an action in some environment and then receives observations (the different ringing bells) and reward (yummy treats) from that environment_. The environment consists of **states** (in this case, what bell is ringing), and in each state, the agent takes an **action** to get to another state. While the next state and reward received are out of the agent\u2019s control or full prior knowledge, what it can do is learn from those findings to take better actions. The fundamental objective is to maximize the cumulative expected reward, also called expected return. Depending on the factor \u03b3, also called the discounting factor, the amount of importance given to future rewards may be reduced. \u03b3 decides how short-sighted or far-sighted the agent is, i.e: how much it prioritizes reward it receives in the near future compared to what it would receive later.\n\n![RL basic idea]({% static '/blog/assets/img/blackjack/rl.jpg' %})\n\nWhat is referred to as the agent\u2019s **policy** can be thought of as a strategy while playing a game. More formally, it\u2019s **a function that maps a state to an action or a distribution over actions**. We also define an **action-value function**, Q that gives us a measure of how good it is to take a particular action in each state, or to know how favorable each action is in a state of the game. There is also a **state-value function** which tells you how good it is to be in each state. It is the expected reward that is obtained by following a given policy, starting from that state.\n\nAll this sounds very similar to a human playing a game, and you\u2019re right, it really is. The usual concept of points conveniently plays the role of reward. So, let\u2019s understand it better by playing the casino card game of blackjack.\n\n## Blackjack rules\n\n### Here are the basic rules\n\nHere all face cards have the value of 10. The game begins with two cards dealt to both dealer and player. One of the dealer\u2019s cards is face up and the other is face down. If the player has 21 immediately (an ace and a 10-card), he wins unless the dealer also has the same, in which case the game is a draw. If the player does not have 21, then he can request additional cards, one by one (hits), until he either stops (sticks) or exceeds 21 (goes bust). If he goes bust, he loses; if he sticks, then it becomes the dealer\u2019s turn. _The dealer hits or sticks according to a fixed strategy without choice: he sticks on any sum of 17 or greater, and hits otherwise_. If the dealer goes bust, then the player wins; otherwise, the outcome \u2014 win, lose, or draw \u2014 is determined by whose final sum is closer to 21. Note that in the game, an ace could count as either a 1 or 11, you generally count it as 11 unless your sum exceeds 21 in which you reduce it to 1 which prevents you from going bust. If the player holds an ace that he could count as 11 without going bust, then the ace is said to be usable.\n\n## Model free methods\n\nBefore we get into actually playing the game, more like watching the agent learn to play, bear with some more technical aspects that will give us a better understanding of what\u2019s happening under the hood.\n\nHere model-free essentially means that you, or rather, the agent doesn\u2019t know the exact dynamics (state-transition probabilities) of the environment. What that means here is that you don\u2019t have prior knowledge in Blackjack what the next state will be when you choose to hit or stay. If you did, then playing wouldn\u2019t really be much of a task and casinos world-wide would go bankrupt. All you\u2019d need to do is look through this knowledge to figure out what to do in each state to go to the most favorable next state to essentially find the optimal policy, one of the ways this can be done is through dynamic programming. But in blackjack, and most other interesting applications, you don\u2019t know what could happen next so you need to play and figure it out with experience. To kick off this experience we follow some arbitrary policy, but one that lets us explore states so that we don\u2019t overlook some advantageous ones that we may deem unfavorable  at first. This is often referred to as the **exploration-exploitation dilemma**. Exploitation is the right thing to do to maximize the expected reward on the one step, but exploration may produce the greater total reward in the long run. There are algorithms that deal with this. We accomplish this through \u03f5-greedy policies, ones that will mostly take the optimal action and learn from what happens next, but with a probability of \u03f5, will explore the consequences of a random action so as to learn more about each state eventually and ensure that we reach the global optimum solution.\n\nOur task really comes down to two tasks we can repeat over and over again to reach our optimal policy: we should be able to evaluate the current policy we are following and secondly, use this evaluation to improve our current policy. Formally, this is known as Generalized Policy Iteration (GPI).\n\n![GPI]({% static '/blog/assets/img/blackjack/GPI.png' %})\n\n## Let\u2019s Play!\n\n### Monte Carlo method\n\nThe term Monte Carlo is often used more broadly for any estimation method that relies on repeated random sampling. In RL Monte Carlo methods allow us to estimate values directly from experience, from sequences of states, actions and rewards.\nWe use this as a way to evaluate the policy. Here we sample sequences of episodes: State, action, reward generated by following the policy till termination of the episode and the back propagate from the end to update the value of each state in the episode. The value of each state is the expected return from that state which can be calculated incrementally.\n\nAs an example, this is the evaluation of a policy that chooses to stick only on values of 20 or 21 (source: Sutton and Barto)\n\n![Monte Carlo evaluation]({% static '/blog/assets/img/blackjack/mc-eval.jpg' %})\n\nThe game plan is to initialize our policy such that it\u2019s stochastic which means that all actions have a possibility of being selected (here we just assign all probabilities to the same value or randomly initialize the policy). We evaluate the current policy using Monte Carlo evaluation and then we can easily improve this current policy but acting greedy with respect to it (we actually act \u03f5 greedily). Basically, look at the values corresponding to the current state in the Q-function and choose the action that maximizes the value. With this new policy, we re-compute the Q-table and the process goes on.\nMathematically, going in reverse from the time step of termination of the episode, the return for each state-action pair is computed according to the following formula, with future rewards being discounted by a factor of \u03b3 raised to the power of how much later they are received.\n$$\nG_t=R_{t+1} + \\gamma R_{t+2}+...+ \\gamma ^{T-1}R_T\n$$\nNow the values are updated according to :\n$$\nQ(S,A)=Q(S,A)+ \\alpha (G_t-Q(S,A))\n$$\nSo the value is moved alpha (learning rate) times the error between what was previously computed to be the expected return and the return we actually received. To compute the returns, the value must be computed in reverse from the end of each episode. The backup diagram looks something like this:\n\n![MC backup]({% static '/blog/assets/img/blackjack/mc-backup.png' %})\n\nThe optimal policy learnt by this algorithm is as follows:\n\n![Optimal policy from Monte Carlo]({% static '/blog/assets/img/blackjack/pistar.png' %})\n(source: Sutton and Barto)\n\nThe agent we trained for 500,000 episodes with decaying epsilon value learnt the following policy:\n\n![Monte Carlo policy]({% static '/blog/assets/img/blackjack/mc-pol.JPG' %})\n\nAt a glance, it seems in the case of a usable ace, our agent learns a policy very similar to the HIT17 policy of our dealer. Since we can\u2019t outright judge the quality of the policy, we observe the player playing 1000 rounds against the dealer and note the statistics.\n\nwins = 434.0<br>\ndraws = 89.0<br>\nlosses = 477.0<br>\n\nHowever, we still can\u2019t manage to beat the dealer but that isn\u2019t too concerning here, since HIT17 is a pretty great policy to begin with and blackjack, like most casino games, is rigged to be in favor of the dealer. Regardless, if our agent learned  a policy even slightly better than HIT17, it would have had more wins than losses.\n\n#### DRAWBACKS OF MC\n\nMonte Carlo methods inherently assume that the task you are using it on is episodic since it does wait till termination to update the Q-values. So, this algorithm can\u2019t be applied to continuing tasks (for example, a personal assistant robot, its task doesn't really have finite episodes and never ends). Additionally, say eventually on termination you got a big  negative reward; this would affect the Q-values of all the states that came before in the episode, so the credit or blame isn\u2019t assigned properly to certain states. While this may be fine for short episodic tasks like blackjack, for longer-horizon ones, it would be better to use temporal difference methods. Of these methods, here we will discuss Q-learning.\n\nBefore moving on, do note that blackjack is one of the cases where Monte Carlo works well given the nature of the environment. The game is episodic, each episode is short and the reward only appears at the end of the episode.\n\n### TEMPORAL DIFFERENCE METHODS\n\nTemporal Difference (TD) methods **bootstrap**. What that fancy term means is that at each time step, it updates the values of each state by looking at the value of the next state (TD(0)) and working its way back. There are 3 methods under this: sarsa, Q-learning and expected-sarsa, of which we will assess Q-learning here.\nHere is a visual of what state or action values each method considers for backups\n\n![backup diagrams]({% static '/blog/assets/img/blackjack/backups.png' %})\n\n### Q-learning\n\nHere we use a similar initialization for Q-values for each state and action. We sample episodes and at each time step, the agent chooses an action \u03f5-greedily, observes the reward from the next state and backs up the value for the current state-action pair. In Q-learning, in each step of each episode we sample, after choosing the action A and observing the reward R and next state S', the action-value corresponding to S,A is updated according to the formula:\n$$\nQ(S,A)=Q(S,A)+ \\alpha [R+ \\gamma max(Q_a(S',a))-Q(S,A)]\n$$\nThe difference between this and our previous monte carlo update is in the target for update. Previously it was the sampled return which was calculated in reverse from episode termination. Here the target is the reward received + the discount factor times the maximum value from the next state, out of all the possible actions from that state. This way, for each update, what is considered is the maximum over the action values of the next state. The backup diagram looks like this:\n\n![Q-learning backup]({% static '/blog/assets/img/blackjack/q-backup.png' %})\n\nWe once again train our agent on 500,000 episodes and it learns the following policy:\n\n![Q-learning policy]({% static '/blog/assets/img/blackjack/q-pol.JPG' %})\n\nAfter 1000 games based on the above policy:\n\nwins = 420.0<br>\ndraws = 92.0<br>\nlosses = 488.0<br>\n\nFrom these statistics, it does seem that our Q-learning policy didn\u2019t perform as well as Monte Carlo; that could perhaps benefit from more rounds of training or tweaking the parameters.\nRegardless, temporal difference methods are usually more efficient and converge quicker since it updates the Q-values at every step and not just when an episode ends.\n\n#### DRAWBACKS OF TD\n\nTemporal difference learning is faster but less stable and may converge to the wrong solution. We estimate the Q-values only from the current reward, and use our own estimate of the values of the next state to predict the future reward. While this is faster, our targets are based on our estimation itself, making it unstable. Moreover, our estimate starts off arbitrarily wrong, so we might train toward wrong targets and might converge to a wrong solution.\n\n## What next?\n\nThere\u2019s much more to learn of reinforcement learning and here we have seen a bit of the basics and watched the algorithms learn to play a relatively simple game. Achievements like AlphaGo were accomplished through even more complex methods and also using the brilliant idea of letting the agent play against itself to get better (self-play), which has played an important part in beating human level performance.\n\nThere are other similar applications where things get more complex, for instance, the number of states itself could be massive and computing values is infeasible. There are many methods to deal with this and various other situations. A lot of the work done in RL is relatively recent and much research is still on-going into the possibilities of its application.\n\nThere\u2019s loads to explore (and exploit :p). **Here are some resources to get you started**:\n\n- [Book by Sutton and Barto](http://incompleteideas.net/book/the-book-2nd.html)\n- [A blog that covers the methods mentioned here](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html)",
        "author_name": "Shreya Namath",
        "slug": "playing-blackjack-optimally-with-reinforcement-learning"
    },
    {
        "layout": "post",
        "title": "Amateur Radio",
        "author_github": "Moha-Mankad",
        "date": "2021-08-16T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Diode"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Moha-Mankad",
        "text": "[{% load static %}]()\n![image-1]({% static '/blog/assets/img/amateur-radio/image1.jpg' %})\n\nWe are all fascinated by how our voice travels through the atmosphere by electromagnetic waves and reaches its destination. We humans have built transmitters and receivers suitable to our needs.\n\n## **What is Amateur Radio and who are these Amateurs?**\n\nAmateur Radio operators are a group of people interested in wireless communication technology worldwide. According to the Indian Wireless Telegraphs (Amateur Service) Rules, 1978, Amateur Radio or HAM radio is 'A service of self-training, inter-communication and technical investigations carried on by amateurs, that is, by persons duly authorized under these rules interacted in radio technique solely with personal aim without pecuniary interest'. Unlike commercial radio broadcasting, where we tune our radios to receive information, amateur radio is a two-way exchange of information.\n\n## **What can you gain from this?**\n\nDespite the availability of other communication facilities amateurs run their own wireless communication network with entirely personal effort. Some operators also build their wireless transceivers from scratch and transmit their voice throughout the world to be then replied to by their fellow operators. This a very special hobby where HAMs operate their wireless transceivers for joy and excitement of carrying out a worldwide wireless communication. They learn important communication engineering concepts by doing. Their conversation covers diverse topics like Astronomy, Literature, Climate Change, Wildlife, STEM fields. Exchange of information on business, commerce, religion, and politics are not allowed.\n\n## **So, can anyone broadcast information?**\n\nAmateur radio is one of the few hobbies in the world which requires a government license. You cannot broadcast information on air without a license. It is given by the Wireless Planning and Coordination (WPC) Wing of the Ministry of Communication, New Delhi in India. You have to qualify for a written examination that covers the basics of electronics radio theories and national and international rules and regulations of amateur radio communication in radiotelephony (voice) and Morse Code to obtain the license. The eligibility to appear for examination is 12 years. License is given for 4 grades. Advanced, Grade I License, Grade II License Grade II License (Restricted Grade). Out of these only Restricted Grade license can be obtained without any knowledge of Morse code. While for getting the license for other grades, the candidate must show proficiency in sending and receiving Morse code according to international standards. The speed of transmitting and receiving Morse code should be 5 WPM (Words per Minute) for Grade I license.\n\nEach Amateur is allotted a separate number known as a call sign. By looking at the call sign other amateurs can say that from which country does a person belong. These are allotted by International Telecommunication Union (ITU). Prefix 'VU' is used in call signs of Indian operators.\n\n## **Okay, but what is the real-life use apart from all the fun?**\n\nAlong with sharing and acquiring knowledge, amateur radio operators have been very helpful in re-establishing destroyed communication during natural calamities. An example: In December 2004, a point in the ocean near Indonesia became an epicenter of a major Earthquake which then resulted in a Tsunami. The effect was seen on the Andaman and Nicobar Islands of India also. Communication to islands was destroyed so it was very difficult to carry out the relief work. In those circumstances, a HAM operator from New Delhi Bharati Prasad, who was on a tour to the islands came forward with her Amateur Radio set and helped establish communication. Like this example, many times Ham operators have been useful.\n\n## References\n\n1. [Government website for HAM radio](https://vigyanprasar.gov.in/science-communication-programs/ham-radio/)\n2. [Wikipedia](https://en.wikipedia.org/wiki/Amateur_radio)\n3. Safari Magazine",
        "author_name": "Moha Mankad",
        "slug": "amateur-radio"
    },
    {
        "layout": "post",
        "title": "DP on Trees",
        "author_github": "vaibhavpuri32",
        "date": "2021-08-16T00:05:00",
        "image": "/assets/img/",
        "description": "Introduction to DP on Trees",
        "tags": [
            "IEEE NITK",
            "CompSoc"
        ],
        "categories": [
            "Compsoc/Diode/Piston"
        ],
        "github_username": "vaibhavpuri32",
        "text": "[{% load static %}]()\n## **Introduction**\n\nDP on Trees is a very useful Technique in problem-solving,\nThe prerequisite for learning this is a very basic understanding of Trees, vectors, and DP.\nBefore directly diving into the topic, We will discuss some topics in brief, they are:\n\n- Difference between graphs and Trees.\n- Adjacency List\n- DFS.\n- Dynamic Programming.  \n\n## **Difference between Graphs and Trees**\n\nTrees are a Subset of Graphs, like graphs they  are  represented by nodes and edges, a popular shout is \u201c any graphs with n nodes and n-1 edges is a tree \u201c , it\u2019s not  completely correct, here is an example to contradict it,  \n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/1.png' %})\n\nIn the above example we have 4 nodes and 3 edges , yet it is not a tree.\n\nSo, what\u2019s the complete definition, the complete definition can be\n\u201cA Tree is a connected graph with no cycles\u201d.\nA Tree is a connected graph without any cycle, such graphs can be thought of suspended from the node.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/2.png' %})\n\nbut any graph which satisfies the above criteria can be converted into a tree, for example\n\nThe following diagram does not looks like a conventional tree\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/3.png' %})\n\nBut it can easily be converted into a tree by pivoting it at any node let\u2019s pivot it along node 2\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/4.png' %})\n\n## **Adjacency list**\n\nWe have seen how a tree looks like, now let\u2019s see how we can store a graph in memory when we are given with edges between the nodes.\nThere are many ways to do it, but the easiest way to do it is using the adjacency list, which is an array of vectors.\nFor each node, we have a vector, which stores all the adjacent nodes(nodes which are directly connected to it by an edge).\n\nIf nodes a and b are connected by an edge then a must be present in the adjacency list of b, and vice-versa.\n\nLet\u2019s take an example for the following tree,\n\nThe adjacency list of the graph is:\n\n- 1 --> 2 , 3, 4\n- 2 --> 1, 5\n- 3 --> 1, 6, 7\n- 4 --> 1\n- 5 --> 2, 8\n- 6 --> 3\n- 7 --> 3\n- 8 --> 5\n\nFor a tree with n nodes , there are a total of 2n - 2 elements in an adjacency list in total because we will have exactly n-1 edges and\n\nAnd each edge connects 2 nodes.  \n\nLet\u2019s write a C++  code for making adjacency list of a Tree.\n\n```cpp\n#include<bits/stdc++.h>\nusing namespace std;\n\nint main()\n{\n\n    int nodes, edges, i, j; \n\n    cout<<\u201d\\nEnter the Number of nodes and edges: \u201c;\n\n    cin >> nodes >> edges;                //We read number of nodes and edges.\n\n    vector<int> adj[nodes + 1];\n\n    cout << \"\\nEnter the edges: \";\n\n    for (i = 1; i <= edges; i++)\n\n    {\n\n        int x, y;\n\n        cin >> x >> y;                    \n\n        adj[x].push_back(y);            //If x is adjacent to y which means that y is also adjacent to x \n\n        adj[y].push_back(x);\n\n    } \n\n    for(i=1 ;  i<=nodes; i++)             // We go through each node and print its adjacency list.\n\n    {  cout<<\"The adjacency list of node \"<<i<<\" is : \";\n\n        for(j=0; j<adj[i].size(); j++)        \n\n       {     cout<<adj[i][j]<<\"  \"; \n\n       } \n\n      cout<<\u201d\\n\u201d;\n\n    }\n\n    return (0);\n\n}  \n```\n\n<span style=\"text-decoration:underline;\">Input: </span>\n\nEnter the number of nodes and the edges: 8 7\n\nEnter the edges: 1 2  1 3  1 4  2 5  3 6  3 7 5 8\n\n<span style=\"text-decoration:underline;\">Output</span>:\n\n- The adjacency list of node 1 is: 2 3 4\n- The adjacency list of node 2 is: 1 5\n- The adjacency list of node 3 is: 1 6 7\n- The adjacency list of node 4 is: 1\n- The adjacency list of node 5 is: 2 8\n- The adjacency list of node 6 is : 3\n- The adjacency list of node 7 is : 3\n- The adjacency list of node 8 is: 5\n\nNote that no list can be empty because in tree every node is connected to at least one node.\n\n## **DFS (Depth First Search)**\n\nTraversing a Graph means visiting each node of a Graph.\n\nThere are 2 main ways of traversing a graph:\n\n- DFS (depth first search)\n- BFS (breadth first search)\n\nWe will be discussing DFS only on Trees , not on Graph.\n\nFor DFS on Graph you can refer to this link:[https://www.geeksforgeeks.org/depth-first-search-or-dfs-for-a-graph/](https://www.geeksforgeeks.org/depth-first-search-or-dfs-for-a-graph/)  \n\nWe will only discuss DFS because it is required for understanding DP on Trees.  \n\n- Parent-child convention:\n For understanding DFS or any topic related to Tree, we use this convention, in this, we fix a root node and then we add children to it, then if the child also has a child we call it grandchild of root node and child of the node which is connected to.  \n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/6.png' %})\n\nIn the above example node, 1 is the root node, node 2 and 3 are children of root 1, node 4 and 5 are children of root 2 and grandchildren of root 1, and similarly, 6 is the child of root 3 and grandchild of root 1.\nIt is often misunderstood that a node cannot have more than 2 children.  \nOnly in a Binary Tree, we can have at most 2 children, but in general, it\u2019s ok if a node has more than 2 children.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/7.png' %})\n\nSo in the above figure , if we do dfs of root node 1, the order will be\n\n1 \u2192 2 \u2192 5 \u2192 3 \u2192 4\u2192 6\u2192 7 \u2192 8  \n\nSo overall in DFS we finish all the children before moving to the next child, and this is true for every node we visit.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/8.png' %})\n\nFirst, 1 is Parent Node and node 2 is its child, so before visiting 3 we have to complete 2.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/9.png' %})\n\nSimilarly, now node 2 is its parent node and root 4 and its children have to be completed before we visit 5.\nLet\u2019s take an example:  \n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/10.png' %})\nFirst we visit the root node.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/11.png' %})\nThen we move on to its first child.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/12.png' %})\nThen we visit the child of node 2.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/13.png' %})\nafter we are done with all the children of node 2 we move onto node 3.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/14.png' %})\nSince there are no children of 3 we move to the next child which is 4.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/15.png' %})\n  then we move to first child of 4   That is node 6.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/16.png' %})\nThen we complete the node 7.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/17.png' %})\n   and we finish it with node 8.  \n\nNow let\u2019s write code for it, in which we have the edges(using the adjacency list program ) and print the order in which the nodes are visited.  \n\nHere we define a function DFS which accepts the adjacency list,\n the node on which we are performing DFS, and the parent of the node.\n\n```cpp\n void Dfs(int v,vector<int> adj[], int p)\n\n{\n\n    cout << v << \" \";     //We print the current node\n\n    int i;\n\n    for (i = 0; i < adj[v].size(); i++)\n\n    {\n\n        if (adj[v][i] != p)    // so that we don't print the parent node \n\n        {\n\n            Dfs(adj[v][i], adj, v);  //Now our current node becomes the   parent node , and the child of it                                becomes the main node(v)                                   \n\n        }                                                          \n\n    }\n\n}  \n```\n\n<span style=\"text-decoration:underline;\">Input:</span>\n\nEnter the edges: 1 2\n\n2 5\n\n1 3\n\n4 8\n\n1 4\n\n4 6\n\n4 7\n\n<span style=\"text-decoration:underline;\">Output:  </span>\n\n1 2 5 3 4 8 6 7\n\nNow let us see how the code works :\n\n```cpp\ndfs(1, 0)\n\n    dfs(2, 1)\n\n         dfs(5, 2)\n\n         end\n\n    end\n\n    dfs(3, 1)\n\n    end\n\n    dfs(4, 1)\n\n        dfs(6, 4)\n\n        end\n\n        ....\n\n    end\n\nend\n\n```\n\n## <span style=\"text-decoration:underline;\">Dynamic Programming (DP)</span>\n\nDynamic Programming is a very powerful technique, it is a huge topic in itself, for a more detailed overview of DP you can visit\n[https://www.geeksforgeeks.org/dynamic-programming/](https://www.geeksforgeeks.org/dynamic-programming/).\nTo put it in a nutshell Dynamic Programming is mainly an optimization over plain recursion, it helps us to reduce both space and time complexity(For learning more about time Complexity please refer to [https://www.geeksforgeeks.org/understanding-time-complexity-simple-examples/](https://www.geeksforgeeks.org/understanding-time-complexity-simple-examples/) ).\nThe idea is to store the results of previous subproblems so that we do not have to re-compute them when needed later. Though it does not sound very groundbreaking, it can convert an exponential function to a linear one, let\u2019s consider some examples\n\n Find the nth term of Fibonacci  .\n\nIf you don\u2019t know what Fibonacci series is please refer:  \n\n[https://www.geeksforgeeks.org/program-for-nth-fibonacci-number/](https://www.geeksforgeeks.org/program-for-nth-fibonacci-number/)\n\nNow we know the following property of a fibonacci number\n\nF[n] = F[n-1] + F[n-2]      , n>2\n\nF[1] = 0\n\nF[2] = 1\n\nNow the first approach which comes to mind is the recursion\n\n```cpp\n int fib(int n)   //To find the n fibonacci number \n\n{ if(n==1)\n\n   return(0); \n\n   if(n==2)\n\n   return(1); \n\n   else \n\n   return(fib(n-1) + fib(n-2));\n\n}  \n```\n\nThe recursion is very straight forward, but the time complexity of program is  exponential [https://www.geeksforgeeks.org/time-complexity-recursive-fibonacci-program/](https://www.geeksforgeeks.org/time-complexity-recursive-fibonacci-program/)\nNow let\u2019s calculate the same with DP\n\nLet\u2019s declare an array of integer fib[100] in which the i index will contain the ith term of Fibonacci series,\nNow the first 3 values can be assigned manually fib [0]=0 as there is no 0th term.\nFib[1] =0 as the first term is 0.\nFib[2] =1 as second term is 1.\n\nNow run a basic for loop from index 2,\n\n```cpp\nint fib(int n)\n\n{ int i,fib[100]; \n\n   fib[0]=0; \n\n   fib[1]=0;\n\n   fib[2]=1; \n\n    for(i=3; i<=n; i++)\n\n    fib[i]=fib[i-1]+fib[i-2]; \n\n    return(fib[n]);\n\n}   \n```\n\nSo basically we calculate the value of any term from the previous two terms, we do this in a linear time hence time complexity is 0(n).\nPlus we have one more advantage if we are given with x queries of\nTerms, for example: find the nth term of Fibonacci series for the following terms: x1, x2, x3, x4, ...xn.\nIf we go by recursion the answer will take even more time, but solving it with DP, we can do it in linear time just going through the for loop once, how?\nAll we need to find out the largest among x1,x2,....xn.  \nAnd we can then find Fib[max(x1,x2,x3\u2026.)], and all other values will be calculated among the way,\nEx: let\u2019s say im given with 5 queries : 3, 8, 17, 27, 21.  \nNow, all we need to calculate is Fib[27] because values of Fib[17], Fib[21], Fib[3], Fib[8], will be calculated along the way.\n\nEx: let\u2019s say im given with 5 queries :  3, 8, 17, 27, 21.  \n\nNow all we need to calculate is  Fib[27] because values of Fib[17], Fib[21], Fib[3], Fib[8], will be calculated along the way.\n\nLet us consider one more example :\n\nWe have an array of n integers and we have to find the maximum subarray(continuous block of integers having maximum sum), the problem is only interesting if we take negative integers into consideration otherwise it is a no brainer, and length of block is not fixed.\n\nFor example, consider the following array\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/18.png' %})\n\nIf we check manually the greatest sum is from index 1 to index index 5 which is\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/19.png' %})\n\n2 + 4 + (-3) +5 +2 = 10.  But it is impossible to check it manually for larger array, so how do we  go about it?\n\nThe first approach which comes to mind is  fixing the first element\n\nAnd then calculating all possible cases with remaining elements, store the maximum result and then change the first element and repeat the process.  \n\nUnderstanding it with diagrams\n\nLet\u2019s fix \u201ca\u201d at index 0 and then lets shift \u201cb\u201d first b will start with index \u201c0\u201d, this is the case when the maximum subarray is the first element itself\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/20.png' %})\n\nThe sum is -1\n\nThen let\u2019s shift b to index 1\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/21.png' %})\n\nthe sum will be a to b that is -1 +2 = 1.\n\nNow shift b to right once more\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/22.png' %})\n\nSum is 1+ 4 = 5.\n\nKeep shifting it, till you reach the end\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/23.png' %})\n\nSum is 5 +(-3) = 2\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/24.png' %})\n\nSum is 2+ 5 = 7.  \n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/25.png' %})\n\nSum is 7 +2 = 9.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/26.png' %})\n\nSum is  9 +(-5) = 4\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/27.png' %})\n\nSum is 4 +2 = 6.\n\nNow b has reached the end, note the maximum sum = 9\n\nNow when b reaches the last index shift a to right by 1 index\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/28.png' %})\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/29.png' %})\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/30.png' %})\n\nKeep going till the b reaches its end then again shift a by 1 index to the right, finally stop when we have reached the following stage:\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/31.png' %})\n\nAnd note the maximum value noted throughout the process, its code is very straight forward\n\n```cpp\nint maxsub(int a[], int n)\n\n {   int i,j; \n\n     int best = 0;\n\n      for (i = 0; i < n; i++)\n\n    {\n\n        int sum = 0;\n\n        for (j = i; j <= n; j++)\n\n        {\n\n            sum = sum + a[j];\n\n            best = max(best, sum);\n\n        }\n\n    } \n\n    return best;\n\n}\n```\n\nWe assign best to the minimum value possible, then we repeat the above process.\nThe time complexity of this method is O(n^2).\n\nNow surprisingly, we can also solve this in O(n) approach using DP, Let\u2019s see how,\nThe idea is to calculate for each position, the maximum sum of the subarray that ends at that position, after that the answer is a maximum of those sums.\n\nNow consider the subproblem of finding the maximum-sum subarray that ends at position k , there are 2 possibilities\n\n- The Subarray only contains the element at position k\n- The subarray consists of a subarray that ends at position k-1, followed by the element at position k.\n\n In the latter case, since we want to find a subarray with the maximum sum, the subarray that ends at position k-1 should also have the maximum sum, thus we can solve the problem efficiently by calculating the maximum subarray sum for each ending position from left to right.  \n\nExample\n\nInitially we assign sum =0 and best =0 (which will store our answer) and consider the same array\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/32.png' %})\n\nLet's write the code first and then we will discuss more,\n\n```cpp\nint Maxsub(int a[], int n)\n\n{\n\n    int k, i, sum = 0, best = 0; \n\n     for (k = 0; k < n; k++)\n\n    {\n\n        sum = max(a[i], a[i] + sum + a[i]);\n\n        best = max(sum, best);\n\n    } \n\n    return best; \n\n}\n```\n\nWhen we iterate for the first time, sum =0,  a[i]=-1, a[i]+sum =-1 , so either way the value of sum becomes -1, and the best values remains the same 0, it means that any subarray ending at index i=0, will have a maximum sum of -1,then for the second time   i=1, a[i] = 2, and sum+a[i] = 1, so sum will now take maximum of those values which is 2, and best value will also become 2, means the maximum subarray that ends at index 1, can have a sum of 2, now when we iterate for the 3rd time, a[i] =4, sum + a[i] = 6, so sum will take maximum of value which is 6 and best will also be equal to 6, again which means maximum subarray that ends at index 2 has sum of 6, we can go on till the last index and find the maximum subarray sum which is value of best.\n\n## **<span style=\"text-decoration:underline;\">DP on Trees </span>**\n\nNow that we have covered every prerequisite topics, in brief, we will move on to DP on Trees. Let\u2019s directly dive into some problems,\nThe first problem is very standard:\n\nGiven a Tree with n weighted nodes(nodes which contain any finite value), calculate the maximum sum of the node values along a path from root to any leaves without re-visiting any node\n\nNow first let us understand the problem,\nWe are at the root node, we need to go to any leaf node, to which the sum of nodes visited along the way is maximum, now if we think carefully, we can only visit a node in only one way, and to visit a specific leaf node , there is only one way , hence we will calculate the sum of elements along the way , this would become clear with the following diagram.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/33.png' %})\n\nNow if we come out of node\n\n11 -->  3 + 1 +1+4 = 9\n\n12 -->  5 +1+1+3 =10\n\n13  --> 9 + 9 + 2 + 3 = 23\n\n14 -->  8 + 9 + 2 + 3 = 22\n\n8 -->  1+10+3 = 14\n\n9 -->  5 + 10 + 3 = 18\n\n10 -->  3 + 10 + 3 = 16  \n\nHere since the tree is small , we can manually calculate,but if the trees are large, it is impossible to do it manually, so we will try to solve it using DFS and Dp,\n\nLet\u2019s Write the complete Program for it\n\n```cpp\n#include <bits/stdc++.h>\n\nusing namespace std;\n\nint dfs(int v, vector<int> adj[], int p, int a[])\n\n{\n\n    int i, max = 0;\n\n    for (i = 0; i < adj[v].size(); i++)\n\n    {\n\n        if (adj[v][i] != p)\n\n        {\n\n            int temp = dfs(adj[v][i], adj, v, a);\n\n            if (temp > max)\n\n                max = temp;\n\n        }\n\n    }\n\n    return (max + a[v]);\n\n}\n\nint main()\n\n{\n\n    int nodes, i, edges;\n\n    cin >> nodes>>edges;\n\n    int a[nodes + 1];\n\n    cout << \"\\nEnter the values in the node: \";\n\n    for (i = 1; i <= nodes; i++)\n\n        cin >> a[i];\n\n    vector<int> adj[nodes + 1];\n\n    cout << \"Enter the edges: \";\n\n    for (i = 0; i < edges; i++)\n\n    {\n\n        int x, y;\n\n        cin >> x >> y;\n\n        adj[x].push_back(y);\n\n        adj[y].push_back(x);\n\n    }\n\n    cout << dfs(1, adj, 0, a) << \"\\n\";\n\n    return (0);\n\n}\n```\n\nNow Let\u2019s go through the program,  \n\nThe main function is almost the same as the adjacency list one, except here we have some values,  stored in the node itself, so here we pass 4 things to the function, the root node, the adjacency list, parent of node(which is 0 for root node), and an array of values in the nodes.\n\nNow, the main logic of the problem is that we first go to  the leaves nodes,  and then we start moving upward and each time, we move up , we pass the maximum value that can be obtained from the children of the root to its root then the root returns the maximum value plus the value in the root to its parent node, this process continues till we reach the root node.\n\nNow Let\u2019s Trace the code\n\n```cpp\ndfs(1,adj,0,a)\n\n    dfs(2,adj,1,a)\n\n      dfs(5,adj,2,a) \n\n         dfs(11,adj,5,a)\n\n         returns  4 to dfs(5,adj,2,a),setting the max value to 4\n\n         End dfs(11,adj,5,a)\n\n         dfs(12,adj,5,a) \n\n         returns  5 to dfs(5,adj,2,a),changing  the max value to 5\n\n         End  dfs(12,adj,5,a) \n\n      returns  (5+1(it\u2019s own value)) to dfs (2,adj,1,a)\n\n     End dfs(5,adj,2,a) \n\n     dfs(6,adj,2,a)\n\n     Returns 3(it\u2019s only value)  to dfs(2,adj,1,a)\n\n     End\n\n    returns 6 + 1(it\u2019s own value ) to dfs(1,adj,0,a)    \n\n    dfs(3,adj,1,a) \n\n    .\n\n    .\n\n    .\n\n    .\n\n    .\n\n    .\n\n\n    Till we get the maximum sum from the 3 children, and we finally add the value of node 1 (3) to it.  \n```\n\nNow there is one more interesting property, we can find this value for any node present on the tree, how?\nBecause we can suspend a tree from any node in the tree, here is an example of suspending it from node 5\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/34.png' %})\n\nWe can still calculate the maximum sum ,all we need to do is to call dfs(5,adj,0,a) instead of dfs(1,adj,0,a) from the main function !!!\n\nNow Let\u2019s take one more example\n\nGiven a Tree with n weighted nodes(with values attached to them),\n\nchoose a subset of nodes such that no two adjacent nodes(i.e. nodes connected directly by an edge) are chosen and sum of values of the chosen nodes is maximum.\n\nNow, the problem might sound a bit complex, if we think the problem breaks down to ,  choosing nodes which are not directly connected by an edge, we cannot choose both the child and parent in every child parent node, this problem is very similar to\n1-D array problem where we are given an array A1,\u2009A2,\u2009...,\u2009AN; we can't choose adjacent elements and we have to maximise sum of chosen elements,  Here the approach will be very similar to the earlier question( maximum subarray ), either the element will be included or it won\u2019t be included.\nThe final formula will be\n\n```python\ndp[i] = max(dp[i-1],a[i]+dp[i-2]),   i <= i<= n.\n```\n\n This gives us some basic idea about the problem, but unlike the array problem, we don\u2019t have direct adjacent elements, the best way will be defining our state to which subtree we are solving for. For defining subtrees we need to root the tree first. Say, if we root the tree at node 1 and define our dp[v] as the answer for subtree of node V, then we will keep moving up till we reach node 1.\nNow, similar to the array problem, we have to decide on including node V in our subset or not. If we include node V, we can't include any of its children(say v1, v2, ..., vn), but we can have the grandchildren of node v, and if we don\u2019t include v we can include\nAll the children of node v,\nLet\u2019s say we are at node v, to calculate the maximum from the leaf nodes, we will again have to travel from bottom to node v, and then decide whether we want to consider the value of node v, and then we pass the value to the parent node of v, Here is the code\n\nFor this we have to consider 2 arrays, we need the adjacency list and also the value in the nodes and parent of node, let\u2019s assume they are present globally,\n\n```cpp\nvector<int> adj[100];\nint dp1[100], dp2[100];\nvoid dfs(int V, int p, int a[])  //a is the value in the nodes \n{\n\n    \n    int sum1 = 0, sum2 = 0;\n    for (auto v : adj[V])\n    {\n        if (v == p)\n            continue;\n        dfs(v, V);\n        sum1 = sum1 + dp2[v];\n        sum2 = sum2 + max(dp1[v], dp2[v]);\n    }\n\n    dp1[V] = a[V] + sum1;\n    dp2[V] = sum2;\n} \n\nThe final answer will be the maximum value of dp1[1] and dp[2]. \n```\n\n## **<span style=\"text-decoration:underline;\">Useful Resources</span>**\n\n- [Dynamic Programming on Trees - GFG](https://www.geeksforgeeks.org/dynamic-programming-trees-set-2/)\n- [Dynamic Programming on Trees (or Tree DP) - CommonLounge](https://www.commonlounge.com/discussion/8573ee40c4cb4673824c867715a5bc7b)  \n- [Applications of DP on Trees in CP - Youtube](https://www.youtube.com/watch?v=PgvJbi2cJSw)\n- [FAMTREE - Codechef](https://www.codechef.com/problems/FAMTREE)",
        "author_name": "Vaibhav Puri",
        "slug": "dp-on-trees"
    },
    {
        "layout": "post",
        "title": "From 1G to 5G",
        "author_github": "Sujay1101",
        "date": "2021-08-16T00:00:44",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Diode"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Sujay1101",
        "text": "[{% load static %}]()\nFirst mobile phones had to be packed in a *briefcase!* Mobile technology has come a long way. It will play a crucial role in technologies of the future like *IoT* and *autonomous vehicles!* Let us look at the evolution from 1G to 5G\n\n## WHAT ARE CELLULAR NETWORKS AND WHY ARE THEY IMPORTANT?\n\n![Cells]({% static '/blog/assets/img/from-1g-to-5g/Cells.png' %})\n\nNetworks are a group of connected devices. A common network architecture has all devices connected by 'links'. When the devices can move from one place to another these 'links' are *wireless*. The networks where the end devices are connected to the network wirelessly are called cellular networks. Mobile phones are classic examples of cellular networks. In the future, IoT devices and autonomous vehicles shall form a cellular network.\n\n![Cellular - Network]({% static '/blog/assets/img/from-1g-to-5g/Cellular-Network.jpg' %})\n\nTo provide service over a land area, the total land area is divided into '*cells*'. Cells are conventionally chosen as *hexagons*. Each cell contains at least one *base station* (aka mobile tower). All the base stations are connected to the *core network*. The core network is like the *brain* of the network. It controls the information flow\n\n## WHAT HAPPENS WHEN YOU TALK TO SOMEONE ON YOUR PHONE?\n\nYour voice is converted into *digital data* by your handset. This digital data is replicated in a *radio wave* by changing the wave's particular characteristics such as phase, frequency, or amplitude. These waves are radiated by the handset. A very *small* part reaches the antenna of the nearest base station. From here the signal is sent to your *mobile operator's (Idea, Jio, etc.) core network* via the transmission link. It transfers the call to your friend's mobile operator's core network, which directs the call to the nearest base station to your friend. This base station radiates radiowaves which reach your friend's handset. The earpiece inside converts signal to audio and finally, your friend can hear you!\n\nMobile data works similarly. Instead of transferring a call to the friend's mobile operator's core network, the core network would access the internet and send information back to you.\n\n*Roughly every 10 years, the industry decides to make this process faster, more reliable, more secure and, increase the capacity of networks to serve more devices. These standards of performance are better known as 'generations' or simply 'G'. 5G is the latest set of standards.*\n\n## IMPORTANT TERMINOLOGY\n\n### CHANNEL\n\nThe space between a mobile phone and the base station. It carries the radiowaves. The channel is very *unpredictable*. It is continuously changing and consists of everything around us including us! Communication through the channel is possible in a specific frequency range called the *spectrum*.\n\n### BANDWIDTH OF NETWORK\n\nThe range of frequencies with which mobile phones and base stations have agreed to communicate is called the *bandwidth* of the network. Since the channel is capable of carrying a certain range of frequencies without distortion, the bandwidth has a limit. The number of devices using the network is increasing. Hence each generation uses available bandwidth more efficiently and finds ways of increasing bandwidth.\n\n### PEAK DATA RATE\n\nThe theoretical maximum rate at which mobile phones can send information on the network, or receive information from the network. Each generation increases the peak data rate.\n\n### THROUGHPUT\n\nThe actual rate at which mobile phones send and receive information from the network is called the throughput. It is generally much less than the peak data rate and, depends on several factors.\n\n## 1G\n\nThe first mobile network, now called 1G, started in Japan in 1979. Unlike today's cell phones which use digital methods to store and transmit information, devices of this generation used *analog signals* to store and transmit information. The peak data rate was only *2.4 Kbps*.\n\n<img src=\"/blog/assets/img/from-1g-to-5g/Motorola Dyna TAC1-G phone.png\" alt=\"Motorola Dyna TAC1-G phone\" style=\"zoom:67%;\"/>\n\n\u200bMotorola DynaTAC weighed a little less than a *kilo* and took *10 hours* to charge!\n\n## 2G\n\n2G mobile networks began in Finland in 1991. They were based on the *GSM* (Global System for Mobile Communications) standards. The major change in this generation was the use of *digital* methods to store and send information. This allowed calls and text encryption, SMS (Short Messaging Service), picture messaging and, MMS (Multimedia Messaging Service). Equipment all around the world was manufactured according to the GSM standard. This introduced uniformity in network technology across regions. Hence roaming became possible. The peak data rate was *50 Kbps*.\n\n<img src=\"/blog/assets/img/from-1g-to-5g/Nokia3210(2-G phone).png\" alt=\"Nokia3210(2-G phone)\" style=\"zoom:67%;\" />\n\nNokia 3210 sold about 160 million units\n\n## 3G\n\nThe first pre-commercial 3G network was launched by NTT DoCoMo in Japan in 1998. It was based on *WCDMA* (Wideband Code Division Multiple Access) which uses the spectrum more efficiently. This was a natural evolution from GSM. It gives a peak data rate of *2Mbps* for *stationery* users and *384Kbps* for moving users. The biggest success of 3G was the introduction of *mobile data*. The arrival of smartphones in the early part of this generation increased the usage of 3G mobile data. New applications of 3G were mobile TV, GPS location tracking and, video calls.\n\n## 4G\n\nThe growth in mobile data was so fast that the industry started planning a major new network upgrade based on Internet Protocol(IP). The new technology, *Long Term Evolution (LTE)* became known as 4G. The first network was launched at the end of 2009. Within 4 years there were more than 250 others, making it the fastest-growing generation. It uses *OFDM(Orthogonal Frequency Division Multiplexing)* technology which is far more spectrum efficient. It has a peak data rate of *400Mbps* and a typical throughput of a good 4G network will be *10s of Mbps* for moving users and *100s of Mbps* for stationary users. High Definition online TV, High Definition online gaming, videoconferencing became possible with 4G.\n\n![image-1_sized_50]({% static '/blog/assets/img/from-1g-to-5g/image-1_sized_50.jpg' %})\n\nAs more people get access to smartphones and the IoT network expands, *billions* of devices shall need access to a mobile network. That is where 5G comes in.\n\n## 5G\n\n5G differs from 4G in three important aspects, *bandwidth*, *latency*, and *peak data rate*.\n\n### Bandwidth\n\n4G uses frequencies less than 3GHz. 5G uses each bit of spectrum. *Low bands* below 1GHz, *mid bands* with frequency in 1GHz to 6GHz and, *high bands* beyond 30GHz. Waves in high bands have wavelengths of the order of a millimeter. Hence 5G uses *mm-wave technology*. There is a problem with using frequencies beyond 30GHz. The high-frequency radio waves lose significant energy on encountering an obstacle like a wall or tree. These waves are *short range*. One possible solution is to use more base stations.\n\n### Latency\n\nLatency means 'delay'. Time taken by your phone to show results of a google search is one kind of latency in a network. The core network controls the flow of information. It is the 'intelligence' of the network. For faster decisions, this intelligence is *distributed*. Information relevant to a device is stored close to the device. This is called *edge cloud* and *edge computing*. Edge cloud can be used when low latency is necessary and for latency tolerant applications core network will be accessed. Unfortunately, latency is a very unpredictable parameter. Even with 5G, there shall be wide variations in the latency depending on the quality of the network.\n\n![qc_onq_lowlatency_gif_final]({% static '/blog/assets/img/from-1g-to-5g/qc_onq_lowlatency_gif_final.gif' %})\n\n### Peak data rate\n\nThe peak data rate of 5G will be *20Gbps*. This is about 50 times the peak data rate of 4G. For typical 5G network throughput could be in Gbps.\n\nCurrently, 5G has been deployed at a few places in South Korea, the USA and, other countries. Although several organizations are working towards realizing the full potential of 5G, the success of 5G also depends upon government regulations. There is no doubt that 5G holds a lot of promise for the future. From *virtual reality* and *autonomous vehicles* to *industrial internet* and *smart cities* 5G will be at the heart of the future of communications!\n\n## REFERENCES\n\n- What happens when you talk to someone on your phone? - [Introducing Radio Spectrum](https://www.gsma.com/spectrum/wp-content/uploads/2017/04/Introducing-Radio-Spectrum.pdf) from [gsma.com](https://www.gsma.com)\n- 1G, 2G, 3G, 4G - [Key differences between 3G, 4G, 5G](https://www.verizon.com/about/our-company/5g/difference-between-3g-4g-5g) from [verizon.com](https://www.verizon.com/)\n- [Introducing Radio Spectrum](https://www.gsma.com/spectrum/wp-content/uploads/2017/04/Introducing-Radio-Spectrum.pdf) from [gsma.com](https://www.gsma.com)\n- 5G - [Q&A on 5G](https://www.qualcomm.com/invention/5g/what-is-5g#:~:text=5G%20will%20bring%20wider%20bandwidths,Gbps%20throughput%2C%20and%20low%20latency.) from [qualcomm.com/invention](https://www.qualcomm.com/invention/5g)\n- [Latency solutions](https://www.qualcomm.com/news/onq/2019/05/13/how-5g-low-latency-improves-your-mobile-experiences#:~:text=The%20current%20architecture%20is%20a,is%20changing%20now%20with%205G.&text=Edge%20computing%20reduces%20latency%20because,to%20the%20cloud%20and%20back.) from [qualcomm.com/news](https://www.qualcomm.com/news/onq/2019/05/13/how-5g-low-latency-improves-your-mobile-experiences#:~:text=The%20current%20architecture%20is%20a,is%20changing%20now%20with%205G.&text=Edge%20computing%20reduces%20latency%20because,to%20the%20cloud%20and%20back.)\n- [Introducing Radio Spectrum](https://www.gsma.com/spectrum/wp-content/uploads/2017/04/Introducing-Radio-Spectrum.pdf) from [gsma.com](https://www.gsma.com)",
        "author_name": "Sujay Chuttar",
        "slug": "from-1g-to-5g"
    },
    {
        "layout": "post",
        "title": "Different Ways to console.log for Better Clarity",
        "author_github": "tim-nitk",
        "date": "2021-08-16T00:00:00",
        "image": "/assets/img/",
        "description": "Helps in effective debugging + spices up your boring JS console command line",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Javascript",
            "Debugging"
        ],
        "categories": [
            "Compsoc"
        ],
        "github_username": "tim-nitk",
        "text": "[{% load static %}]()\n## Here are 5 ways you can ninjafy your console logging skills\n\n### **1. console.log( 'COLORED_TEXT' )**\n\nYou will have to use `%c` with each occurrence accompanied by an argument that expresses the styling that you desire\n\n```javascript\nconsole.log(\n  '%c Object A instantiated %c before B !!  ',\n  'background: white; color: red', \n  'background: red; color:white'\n);\n```\n\nNote that you could use any CSS property under the sun into as an argument. In case of the above string this is how it renders out\n\n![console color](https://dev-to-uploads.s3.amazonaws.com/i/ytw800vc5jjemxkmvjq1.png)\n\n### **2. console.table( ARRAY_OF_OBJECTS )**\n\nUse this when you want to print an **array of objects**\nFor example if you want to print this :\n\n```javascript\nconst arrayOfBooks = [\n  { title: 'Heart of Darkness', author: 'Joseph Conrad' },\n  { title: 'A Walk in the Woods', author: 'Bill Bryson' },\n  { title: 'Rich Dad Poor Dad', author: 'Robert Kiyosaki' }\n];\n```\n\nthen y'all know what `console.log(arrayOfBooks)` does\n\n![console log](https://dev-to-uploads.s3.amazonaws.com/i/ji0931s47gdlqwt3c7nn.png)\n\nBut if you instead use:\n\n```javascript\nconsole.table(arrayOfBooks)\n```\n\nyou'll get the following output:\n\n![console table](https://dev-to-uploads.s3.amazonaws.com/i/pjrv2rf8ngrvtvn0k95i.png)\n\nIsn't it at least 300 times nicer and easier to infer what the array is ?\n\n### **3. console.image( 'URL_OF_IMG' )**\n\n![console image](https://camo.githubusercontent.com/835e3c41004fae89bb9061405d78cada32aa6783/687474703a2f2f692e696d6775722e636f6d2f68763670776b622e706e67)\n\n**HOLD TIGHT FOLKS !** Before you leave to try this out yourself in the console let me tell you that this one is **NOT** natively available to Javascript in the browser\n\nYou will have to first load this JS resource from the CDN using the below script tag :\n\n```html\n<script src='https://raw.githubusercontent.com/adriancooney/console.image/master/console.image.min.js'></script>\n```\n\nFor more details on \u261d\ufe0f , refer [this link](https://github.com/adriancooney/console.image). Obviously the project isn't maintained anymore (the last commit is like 6 years ago) because there isn't really anything more to `console.image` :)\n\n**BONUS :** You get `console.meme` included in the CDN to make something like this :\n\n![meme console](https://camo.githubusercontent.com/0f25ac62249194f32edbd54c912ae2952aa02f1a/687474703a2f2f692e696d6775722e636f6d2f4f646f564d44532e706e67)\n\nAnd the format for that as per their Github Readme is:\n\n```javascript\nconsole.meme(upper text, lower text, meme type|url, width, height)\n```\n\n### **4. console.warn( YOUR_MESSAGE )**\n\nYou can use this to sort of indicate log messages that show the devs it's not really something that breaks the project but good to fix it in the future commits\n\n```javascript\nconsole.warn('Image Kirk_0932.jpg dimensions are slightly off and its causing a small part to be hidden from the user')\n```\n\nand here is a screenshot of how \u26a0\ufe0f WARNING messages look like inside the console\n\n![warning](https://dev-to-uploads.s3.amazonaws.com/i/a1km5vxno0i8rlemo873.png)\n\n### **5. console.time() to Test Your API**\n\nYou can keep track of how much time api calls take to fetch data right in the console. You can use this to find out average time and if you think it *suxx,* you can bug your backend dev ;P\n\nSo pass in the same label `'API_TEST'` to `time` and `timeEnd` functions for it to work.\n\n```javascript\nconsole.time(\"API_TEST\");\n\nconst fiftyTests = Array.from(\n     { length: 50 }, \n     () => fetch('https://jsonplaceholder.typicode.com/todos/1'));\n\nfor(const prom of fiftyTests) {\n  const resp = await prom;\n  const json = await resp.json();\n  console.count('Fetched ');\n}\n\nconsole.timeEnd(\"API_TEST\");\n```\n\nNow you can see the time it takes to make api calls 50 times one - after - the - other printed in your console.\n\n![console time](https://dev-to-uploads.s3.amazonaws.com/i/oyefq9cmajnmissrikur.png)\n\nYou can now divide it by 50 to get the average time the API takes to respond.\n\n**\u26a0\ufe0f Don't use *Promise.all()* because it will simultaneously await all promises and tell you once everything has resolved which defeats our purpose**",
        "author_name": "Tilak",
        "slug": "javascript-console-better-debugging"
    },
    {
        "layout": "post",
        "title": "A future called Quantum",
        "author_github": "SidDsloth",
        "date": "2021-08-23T00:00:44",
        "image": "/assets/img/",
        "description": "A dive into the basics of Quantum Computing and its impact on various fields.",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Quantum Computing"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "SidDsloth",
        "use_math": true,
        "text": "[{% load static %}]()\n## Introduction\n\n>*Life is not just made of a good(1) or a bad(0) experience it is a superposition of such experiences.* $\\qquad \\qquad \\qquad \\quad \\quad$ --**Manav**\n\nThen why do we only think and hear of binary when it comes to information transfer in computers? Presenting to you a new field, called Quantum Computing which works on the superposition of two bits you constantly hear about the 0 and the 1 to give a qubit.\n\nIn this realm we believe that the information is stored in \\|$\\varphi$> a ($\\alpha$\\|0>+$\\beta$\\|1>) state wherein $\\alpha ,\\beta$ are the respective coefficients of \\|0> and \\|1> ( \\|0> is the matrix $\\begin{bmatrix}1\\\\\\0 \\end{bmatrix}$ and \\|1> is the matrix $\\begin{bmatrix}0\\\\\\1 \\end{bmatrix}$).\n\nLike the common binary gates which exist, drawing parallel in this field we have the **Quantum Gates**.\n\ni) **Quantum NOT Gate(X)**:\n\nNOT \\|0> = \\|1>\n\nNOT \\|1> = \\|0>\n\nTherefore X is basically the matrix: $\\begin{bmatrix}0 & 1\\\\\\1 & 0\\end{bmatrix}$\n\nii) **The Hadamard Gate(H)**:\n\nH \\|0> =$\\quad\\frac{\\vert0>+\\vert1>}{\\sqrt 2}$\n\nH \\|1> =$\\quad\\frac{\\vert0>-\\vert1>}{\\sqrt 2}$\n\nTherefore H is basically the matrix: $\\frac{1}{\\sqrt 2}\\begin{bmatrix}1 & 1\\\\\\1 & -1\\end{bmatrix}$\n\niii) **Controlled NOT Gate(CNOT)**:\n  ![CNOT]({% static '/blog/assets/img/A-future-called-quantum/cnot-1x.png' %})\nThis gate is such that the implementation of the NOT is controlled by a control qubit( Filled Dot) while the unfilled dot is the target qubit. If the control qubit is set to 1, as in the states of |10> and |11>, then it flips (i.e. NOTs) the target qubit or else it does nothing.\nTherefore:\n\nCNOT \\|00>=\\|00>\n\nCNOT \\|01>=\\|01>\n\nCNOT \\|10>=\\|11>\n\nCNOT \\|11>=\\|10>\n\nFrom these basic quantum gates, the various AND, OR or XOR gates can be implemented.\n\n**Measuring a Qubit :**   If I give you a qubit without telling you $\\alpha$ and $\\beta$ is there a way for you to figure it out? Well the answer is *NO* !\nThe amplitudes are thought as hidden information. So information extraction works by *measurement in the computational basis*. Suppose a qubit is in the state $\\alpha\\;$\\|0>+$\\beta\\;$\\|1>, when you measure this qubit in the computational basis it gives you a classical bit of information(0 or 1): it gives you the outcome \\|0> with probability $\\alpha^2$ and the outcome \\|1> with probability $\\beta^2$.\n\n## Now why is this field INTERESTING and what are its PROSPECTS?\n\n### **Quantum Networking**\n\nQubits are being used for creating secrets keys using random strings of 0s and 1s which can be used to encode classical information, this application being called quantum key distribution (QKD).\nQKD involves one party, say Alice, sending qubits to Bob, who measures the qubits. Only for certain types of measurements will Bob get the same value that Alice encoded in the qubits. Alice and Bob can compare notes over a public channel to figure out what those measurements are, without actually sharing the qubit values. Using those private values they can create a secret shared key to encrypt classical messages. If an intruder were to intercept the qubits, Alice and Bob could detect the intrusion, discard the qubits and start over until no one is tapping their quantum channel.\n![Meme]({% static '/blog/assets/img/A-future-called-quantum/qc.jpg' %})\nTheoretically more is expected than QKD however qubits encoded using a photon\u2019s polarization large amounts of quantum information is problematic. Photons get absorbed or scattered. Fortunately, there is a more robust way to exchange quantum information\u2014via another property of quantum systems, called entanglement.\n\nWhen two particles or quantum systems interact, they can get entangled. Once entangled, both systems are described by a single quantum state, so measuring the state of one system instantly influences the state of the other. Imagine two network nodes, Alice and Bob, each made of some isolated bit of matter. Such \u201cmatter nodes\u201d can become entangled with each other via a process that involves the exchange of entangled photons. Using entangled matter nodes, Alice can exploit her share of the entanglement to send an entire qubit to Bob, without actually transmitting a physical qubit, making the transfer foolproof and secure.\n\n### **Quantum Cryptography**\n\nIn this future basically the one thing cryptography relies on that is the lack of computation power of classical computers is not true anymore. Hence longer keys are the first line of defense against quantum encryption. Longer keys will make encryption slower and more costly, however the key length will have to increase substantially to stay ahead. Another option is to use symmetric encryption for the messages themselves, then use asymmetric encryption just for the keys.\n\n### **Quantum Machine Learning**\n\n*Now why is this arena brimming with excitement?*\n\nAI systems are given large amounts of data to analyse and classify. Quantum computers will give the edge in accessing more computationally complex feature sp1aces. They would also perform feature scaling on complex data structures.\nRight now qubits struggle to remain in a quantum state for more than a few hundred microseconds even in a highly controlled laboratory environment however need to remain longer for calculations.\nRecently Google even published a paper on ***Quantum Neural Networks***, highlighting the design of it in a world where such a computer would exist.\n\n**Humans too are currently in that superposition state(\\|success>) where $\\alpha$ of the world is preparing for a post-quantum world while the other $\\beta$ is still working in making it a reality!**",
        "author_name": "Manav Singhal",
        "slug": "a-future-called-quantum"
    },
    {
        "layout": "post",
        "title": "Cloud Computing in Detail",
        "date": "2021-08-23T00:05:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Diode"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Rohanrao02",
        "text": "[{% load static %}]()\nCloud computing is a computing paradigm, where a large pool of systems is connected in private or public networks, to provide dynamically scalable infrastructure for application, data, and file storage. In the simplest terms, cloud computing is storing data and programs over the Internet instead of storing them in a computer's hard drive. \"The cloud\" refers to servers that are accessed over the Internet, and the software and databases that run on those servers. Cloud servers are located in data centers all over the world. By using cloud computing, users and companies don't have to manage physical servers themselves or run software applications on their machines.\n\nCloud computing is a practical approach to experience direct cost benefits. It can transform a data center from a capital-intensive set up to a variable priced environment. Cloud computing customers do not own the physical infrastructure, rather they rent the usage from a third-party service provider i.e., consume services and pay only for the resources that they use.\n\n## Cloud Computing Models\n\nCloud computing is offered in three different service models which each satisfy a unique set of business requirements. These three models are known as Software as a Service (SaaS), Platform as a Service (PaaS), and Infrastructure as a Service (IaaS).\n\n### Software as a Service (SaaS)\n\nSoftware as a Service offers applications that are accessed over the web and are not managed by your company, but by the software provider. It is a software licensing and delivery model in which software is licensed on a subscription basis and is centrally hosted. It is sometimes referred to as \"on-demand software\", and was formerly referred to as \"software plus services\" by Microsoft. SaaS applications are also known as Web-based software, on-demand software, and hosted software.\n\nIn this model, a complete application is offered to the customer, as a service on demand. A single instance of the service runs on the cloud & multiple end users are serviced. There is no need for upfront investment in servers or software licenses on the customers\u2019 side, while for the provider, the costs are lowered since only a single application needs to be hosted & maintained. Today SaaS is offered by companies such as Google, Salesforce, Microsoft, Zoho, etc.\n\n### Infrastructure as a Service (IaaS)\n\nInfrastructure as a service (IaaS) is a form of cloud computing that provides virtualized computing resources over the internet. They are online services that provide high-level APIs used to dereference various low-level details of underlying network infrastructure like physical computing resources, location, data partitioning, scaling, security, backup, etc.\n\nIaaS provides basic storage and computing capabilities as standardized services over the network. Servers, storage systems, networking equipment, data center space, etc. are pooled and made available to handle workloads. The customer would typically deploy his software on the infrastructure. Some common examples are Amazon, GoGrid, 3 Tera, etc.\n\n### Platform as a Service (PaaS)\n\nPlatform as a Service is halfway between Infrastructure as a Service (IaaS) and Software as a Service (SaaS). It offers access to a cloud-based environment in which users can build and deliver applications without the need of installing and working with IDEs (Integrated Development Environments, which are often very expensive).\n\nHere, a layer of software, or development environment is encapsulated & offered as a service, upon which other higher levels of service can be built. The customer has the freedom to build his applications, which run on the provider\u2019s infrastructure. To meet manageability and scalability requirements of the applications, PaaS providers offer a predefined combination of OS and application servers, such as LAMP platform (Linux, Apache, MySQL, and PHP), restricted J2EE, Ruby, etc. Google\u2019s App Engine, Force.com, etc are some of the popular PaaS examples.\n\n## Public, Private and Hybrid Clouds\n\nEnterprises can choose to deploy applications on Public, Private or Hybrid clouds. Cloud Integrators can play a vital part in determining the right cloud path for each organization.\n\n### Public Cloud\n\nThe public cloud is defined as computing services offered by third-party providers over the public Internet, making them available to anyone who wants to use or purchase them. It uses the standard cloud computing model to make resources -- such as virtual machines, applications, or storage -- available to users remotely. Public cloud services may be free or offered through a variety of subscription or on-demand pricing schemes, including a pay-per-usage model.\n\nExamples: Amazon Elastic Cloud Compute(EC2), Google App Engine, Blue Cloud by IBM, and Azure services Platform by Windows.\n\n### Private Cloud\n\nA private cloud is a server, data center, or distributed network wholly dedicated to one organization. Private clouds are built exclusively for a single enterprise. They aim to address concerns on data security and offer greater control, which is typically lacking in a public cloud. There are two variations to a private cloud:\n\n- **On-premise Private Cloud:** On-premise private clouds, also known as internal clouds are hosted within one\u2019s own data center. This model provides a more standardized process and protection but is limited in aspects of size and scalability. IT departments would also need to incur the capital and operational costs for the physical resources. This is best suited for applications that require complete control and configurability of the infrastructure and security.\n- **Externally hosted Private Cloud:** This type of private cloud is hosted externally with a cloud provider, where the provider facilitates an exclusive cloud environment with a full guarantee of privacy. This is best suited for enterprises that don\u2019t prefer a public cloud due to sharing of physical resources.\n\nExamples:  HP Data Centers, Microsoft, Elastra-private cloud, and Ubuntu\n\n## Hybrid Cloud\n\nHybrid cloud is an IT infrastructure that connects at least one public cloud and at least one private cloud, and provides orchestration, management, and application portability between them to create a single, flexible, optimal cloud infrastructure for running a company\u2019s computing workloads. Hybrid cloud architectures help organizations integrate their on-premises and cloud operations to support a broad spectrum of use cases using a common set of cloud services, tools, and APIs across on-premises and cloud environments.\n\nExamples: Amazon Web Services (AWS), Microsoft Azure, or Google Cloud Platform (GCP)\n\n## Benefits of Cloud Computing\n\n- **Flexible and Scalable:** One of the major benefits of cloud computing for any business which has opted for cloud computing can increase or decrease the bandwidth as per requirement. The cloud is not constricted to low bandwidths, and unlike the conventional in-house technical infrastructure, it is flexible enough to allow business owners to engage in last-minute changes and still have the ease of work.\n- **Reduced Infrastructure Cost:** Moving to the cloud reduces the cost of managing and maintaining servers. Cloud computing eliminates the need to invest in servers and software. Cloud also reduces the overhead costs like IT Staff, power, data storage, and bandwidth.\n- **Mobility:** The omnipresence of the cloud marks mobility as its prime feature. With the help of cloud connection, companies can connect remotely over an array of devices like smartphones, iPad, and Laptops with ease. Remote accessibility enables quick turnaround time, instant solutions, and constant connection. It is perfect for freelancers, remote employees, organizations with offices in different locations and different sectors.\n- **Automated Updates on Software:** In cloud computing, the server suppliers regularly update your software including the updates on security, so that you do not need to agonize on wasting your crucial time on maintaining the system. With automatic software updates and cyclic upgrades, users can now focus their time & energy on work.\n- **Enhanced Collaboration:**  Cloud computing guarantees ease of access and remote accessibility, which makes it an instant hit to the technical world. Cloud applications enhance collaboration by authorizing diverse groups of people to virtually meet and exchange information with the help of shared storage. Such capability helps in improving the customer service and product development and also reducing the marketing time.\n\n## Cloud Computing Challenges\n\nWith all these benefits come the challenges of Cloud Computing. Data Security is a crucial element that warrants scrutiny. Enterprises are reluctant to buy an assurance of business data security from vendors. They fear losing data to competition and the data confidentiality of consumers. In many instances, the actual storage location is not disclosed, adding to enterprises\u2019 security concerns.\n\nIn some of the European countries, Government regulations do not allow customer's personal information and other sensitive information to be physically located outside the state or country. To meet such requirements, cloud providers need to set up a data center or a storage site exclusively within the country to comply with regulations. Having such an infrastructure may not always be feasible and is a big challenge for cloud providers.\n\nEvery time a company moves data from the internal storage to a cloud, it is faced with being compliant with industry regulations and laws. For example, healthcare organizations in the USA have to comply with HIPAA (Health Insurance Portability and Accountability Act of 1996), public retail companies have to comply with SOX (Sarbanes-Oxley Act of 2002), and PCI DSS (Payment Card Industry Data Security Standard).\n\nCloud computing marks the beginning of a new era in information and communication technology as it brings with it an evolutionary paradigm that can change how computing was done. In conclusion, cloud computing is a recent technological development that has the potential to have a great impact on the world. It has many benefits that it provides to its users and businesses. It reduces operating costs by spending less on maintenance and software upgrades and focuses more on the businesses.\n\nBut there are other challenges cloud computing must overcome. These challenges include security, privacy and reliability of data, consistency in the availability of services, interoperability issues due to unstandardized application programming interfaces which are unique to different service providers, issues in assessment and implementation of cloud computing, high costs of data transfers, and bugs in large-scale distributed systems.\n\nCloud computing as a recent technology is still at a nascent stage of its development. There is still so much potential that can be realized due to ongoing research and development in this regard.\n\n## References\n\n- [Understanding cloud computing](https://www.redhat.com/en/topics/cloud)\n- [Cloud Service Models](https://www.javatpoint.com/cloud-service-models)\n- [Types of Cloud](https://www.geeksforgeeks.org/types-of-cloud/)\n- [Examples of Hybrid Cloud Deployment Models](https://newrelic.com/blog/best-practices/hybrid-cloud-deployment-models-examples#:~:text=Hybrid%20cloud%20solutions%20combine%20public,with%20an%20internal%20private%20cloud.)\n- [Cloud Computing Risks, Challenges & Problems Businesses Are Facing](https://www.datapine.com/blog/cloud-computing-risks-and-challenges/)",
        "author_name": "Rohan Rao H J",
        "slug": "cloud-computing-in-detail"
    },
    {
        "layout": "post",
        "title": "I2C",
        "date": "2021-08-23T00:05:00",
        "image": "/assets/img/",
        "description": "I2C",
        "tags": [
            "IEEE NITK",
            "Diode"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "BhaskarVyas",
        "text": "[{% load static %}]()\n## **Introduction**\n\nThe I<sup>2</sup>C bus was developed in the early 1980's by Philips Semiconductors. Its original purpose was to provide an easy way to connect a CPU to peripheral chips in a TV-set. Its name, Inter-Integrated Circuit, literally explains its purpose: to provide a communication link between Integrated Circuits.\n\n![alt_text]({% static '/blog/assets/img/i2c/I2C1.jpg' %})\n\n## **How does it work**\n\nI<sup>2</sup>C requires two digital lines: Serial Data line (SDA) to transfer data and Serial Clock Line (SCL) to keep the clock. Each slave has a unique 7bit (generally) address in a system so that up to 128 devices can communicate using only two wires. Since the system is synchronous, so data sent will depend on the clock signal.\n\nThere is a specific condition(or \u201cStart\u201d condition), to initiate communication between master and slave, has to be engaged. After that master sends 8bit sequence which consists of 7bit address of slave from which it needs to communicate and 8th bit to indicate whether master will read from the slave (logic 1) or write to it (logic 0). For the next bit through data, line is sent by slave, which determine that it acknowledged it (logic 0) or not (logic 1). After that, the internal register address of slave memory\u2019s is sent through the data line. This is necessary because of a slave which can be a multivariable storing sensor needs to send a particular value of data which is stored in that internal register. After that, again, there is a requirement for acknowledgement bit. After the whole addressing, data transfer sequence begins, which depends on Reading/Write bit sent just after the slave address. After successful completion of data transfer, stop bit will be sent through the data line and this communication between master and slave will be ceased.\n\n![alt_text]({% static '/blog/assets/img/i2c/I2C2.jpg' %})\n\nTo appreciate the working of protocol here, let us consider a case of communication between multiple independent components. What if the slaves do not know who the master is? What if multiple masters are requesting from the same slave? What happens if a master requests data from a slave that for some reason, has become nonfunctional? In these scenarios, the whole system will collapse within a blink of an eye. It is the principle of I2C which prevents collapse and ensures smooth functioning of the system.\n\n### Advantages\n\n* Simple Hardware standard\n* Simple protocol standard\n* Easy to add/remove functions or devices (hardware and software)\n* Simpler PCB: Only two traces required to communicate between devices\n* Very convenient for monitoring applications\n* Fast enough for all \u201cHuman Interfaces\u201d applications\n* Displays, Switches, Keyboards\n* Control, Alarm systems\n\n## **Application**\n\nOk, so that\u2019s it for theory, now let\u2019s see where we can implement it. Let us consider the case of a system which requires lots of pin connection with a microcontroller, which is typically Arduino Uno. So sometimes we need to cascade two Arduinos due to pin shortage or sensor working or whatever the reason. To ensure the working of cascading, I2C comes into the picture.\n\nA4 pin of Arduino Uno board is SDA and A5 is SCL, so we have to connect these pins of one Arduino to another. Also, as per I2C protocol, we need to define one Arduino as master and another one as a slave for which we can use Arduino IDE with \u2018wire\u2019 library for I2C.\n\n![alt_text]({% static '/blog/assets/img/i2c/I2C3.png' %})\n\nIn the code of master Arduino Uno, there will be Wire.begin() function, which will work as start condition as mentioned earlier. Now for selecting a slave device, there is function Wire.beginTransmission(X), in which X is the address of slave, which will begin transmission with slave. Wire.write() is used for writing data to slave memory and Wire.read() is used for reading values from slave. Wire.endTransmission() will behave as stop condition to cease communication till next time.\n\nIn the code of slave Arduino Uno, similarly, there will be Wire.begin(X) where X would be the address of it. read() and write() can be used whenever there is a need to read or write data through the data line. onReceive() and onRequest() functions are used by the slave to specify a new function that is called when whether master sent data or master requested data.\n\n```cpp\n//Demonstration of wire library\n// Master arduino as reader\n#include <Wire.h>\nvoid setup()\n{\n  Wire.begin();        // join i2c bus\n  Serial.begin(9600);\n}\nvoid loop()\n{\n  Wire.requestFrom(3, 8);    // request 8 bytes from slave device #3\n  while(Wire.available())    \n  { \n    char c = Wire.read(); // receive a byte as character\n    Serial.print(c);         \n  }\n  delay(500);\n}\n```\n\n```cpp\n// Slave arduino as receiver\n#include <Wire.h>\nvoid setup()\n{\n  Wire.begin(3);                // join i2c bus with address #3\n  Wire.onReceive(receiveEvent); // register event\n  Serial.begin(9600);           \n}\nvoid loop()\n{\n  delay(100);\n}\nvoid receiveEvent(int howMany)\n{\n  while(1 < Wire.available()) \n  {\n    char c = Wire.read(); // receive byte as a character\n    Serial.print(c);    \n  }\n  int x = Wire.read();    // receive byte as an integer\n  Serial.println(x);       \n}\n```\n\n## Resources and Going Further\n\n* [Wikipedia Article on ](http://en.wikipedia.org/wiki/I%C2%B2C)I<sup>2</sup>C - Not great, but not a terrible place to start.\n* [Standards Doc](http://www.nxp.com/documents/user_manual/UM10204.pdf) - Phillips Semiconductor became NXP a few years back; this is the official standards doc for IC.\n* I<sup>2</sup>C [primer](http://www.i2c-bus.org/i2c-bus/) - The official primer on I<sup>2</sup>C and related technologies.",
        "author_name": "Bhaskar Vyas",
        "slug": "i2c"
    },
    {
        "layout": "post",
        "title": "AI Against the Pandemic",
        "author_github": "anirudh1221",
        "date": "2022-06-23T00:00:00",
        "description": "Our intelligence is what makes us human, and AI is an extension of that quality",
        "tags": [
            "IEEE NITK",
            "Blog",
            "AI AGAINST THE PANDEMIC"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "anirudh1221",
        "text": "[{% load static %}]()\n#  AI AGAINST THE PANDEMIC \n\nArtificial Intelligence, in today\u2019s world has proved to be an area of ever-growing applications to assist humans in their day to day activities. As demand of AI increases in healthcare, it will vital role in keeping us healthier and safer. AI has proved to be useful to improve the conditions of healthcare facilities such as managing medical records and other data, analyzing CT scans and X-rays and recommending appropriate treatment paths, drug creation, precision medicine, and digital consultation. With all these successful applications of AI already in existence, we can be sure to see many more in the near future.\n\nIn today\u2019s situation where the COVID-19 has been officially, by the World Health Organization (WHO), declared as a global pandemic posing threat to the world human population, it is believed that AI can play a huge role to predict and minimize it\u2019s spread and adversity. Till date AI has been used during this pandemic to track traveler data across the globe to be able to predict where the novel coronavirus could show up next. AI includes High-resolution computer-generated simulation which allows scientists to study and interpret large disease-related data sets such as the COVID-19 to learn more about how they spread.\n\n A greater understanding of these phenomena can empower the community to respond far more rapidly to attacks and outbreaks. AI is already being used in hospitals to an extent. For example the Tampa General Hospital in Florida used AI to detect fever in visitors through facial scan. AI is also helping doctors at the Sheba Medical Centre in Israel to predict complications such as respiratory failure or sepsis in COVID-19 patients. Google\u2019s DeepMind division used its latest AI algorithms and its computing power to understand the proteins that might make up the virus, and published the findings to help others develop treatments. BenevolentAI uses AI systems to build drugs that can fight the world\u2019s toughest diseases and is now helping support the efforts to treat coronavirus, the first time the company focused it\u2019s product on infectious diseases. Within weeks of the outbreak, it used it\u2019s predictive capabilities to propose existing drugs that might be useful.\n\n## Application of artificial intelligence in COVID-19 disease management\n![image](https://user-images.githubusercontent.com/78913275/160179536-9e5786e5-f7aa-4e88-812c-cd85dfc93808.png)\n\nUnprecedented pace of efforts to deal with the COVID-19 pandemic scenario is leveraged by massive information and computer science (AI). Various offshoots of AI are utilized in many disease outbreaks earlier.\nAI has achieved success in the identification of disease clusters, prediction of the future outbreaks, monitoring of cases, diagnosis of COVID-19, disease management by resource allocation, mortality risk and pattern recognition for studying the disease trend. \n\n**AI in prediction & tracking**\n\nAI can be used for predicting the spread of virus and developing warning systems earlier by extracting information from various social media platforms, news websites and provide information about the vulnerable regions and for prediction of morbidity and mortality. Bluedot identified a cluster of pneumonia cases and predicted the outbreak and geographical location of the COVID-19 outbreak based on available data using machine learning.\n\n**AI in early diagnosis**\n\nAI was used for the detection of COVID-19 cases from CT scan pictures and chest x-ray . Researchers have developed a deep learning model referred to as COVID-19 detection neural network (COVNet), for differentiating between COVID-19 and community-acquired respiratory disease supported visual second and 3D options extracted from meter chest CT scan. Singh et al. developed a completely unique deep learning model mistreatment Multi-Objective Differential Evolution and convolutional neural networks for COVID-19 designation employing a chest CT scan. COVID-ResNet was developed using automatic and discriminative learning rate and progressive image resizing performed higher than COVID-Net in identification of COVID-19.\nA case study used AI-based classifiers to predict the outcome of RT-PCR results of COVID-19 cases using just 16 simple parameters derived from complete blood profile. \n\n![image](https://user-images.githubusercontent.com/78913275/160179554-244e3be0-3925-4a90-b12f-ee6c7a1c9060.png)\n\n**AI in genomics**\n\nRandhawa et al. produced a method for accurate classification of SARS-CoV-2 genomes by applying various machine learning aglorithms on identified genomic signatures.\n\nMany applications on the internet can be used to track the symptoms and curing process of many patients without creating chaos because with AI we can be assured that they use error free mechanisms. AI can also be used to study the nature of the novel virus and this could be a huge forward step to building the vaccination which is a guaranteed hope for the cure and prevention of this life-threatening virus.\n\nArtificial Intelligence not solely tested to be a promising hand throughout this worldwide pandemic but will continue to be so in all the upcoming healthcare challenges. It will play a huge role in all advancements in medicine, medical treatments and diagnosis and drug developments. AI was found to be even more accurate than human experts in COVID-19 diagnosis and drug discovery. We  would be needing larger datasets for training AI models and a legal framework and ethical considerations for sharing data before AI takes the forefront in diagnosis and other areas. \n\n## Resources\n \n* [https://economictimes.indiatimes.com/magazines/panache/the-world-wasnt-prepared-for-covid-19-in-future-ai-will-curb-the-next-pandemic/articleshow/74776209.cms?from=mdr](https://economictimes.indiatimes.com/magazines/panache/the-world-wasnt-prepared-for-covid-19-in-future-ai-will-curb-the-next-pandemic/articleshow/74776209.cms?from=mdr)\n* [https://novatiosolutions.com/10-common-applications-artificial-intelligence-healthcare/](https://novatiosolutions.com/10-common-app)\n* [https://www.forbes.com/sites/bernardmarr/2020/03/13/coronavirus-how-artificial-intelligence-data-science-and-technology-is-used-to-fight-the-pandemic/#4a87daef5f5f](https://www.forbes.com/sites/bernardmarr/2020/03/13/coronavirus-how-artificial-intelligence-data-science-and-technology-is-used-to-fight-the-pandemic/#4a87daef5f5f)\n* [https://www.ncbi.nlm.nih.gov/pmc/articles/PMC769286/](https://www.ncbi.nlm.nih.gov/pmc/articles/PMC769286/)",
        "author_name": "Anirudh Singh Solanki",
        "slug": "ai-against-pandemic"
    },
    {
        "layout": "post",
        "title": "Electromagnetic Suspension",
        "author_github": "ABHILASHBHARADWAJ-K",
        "date": "2022-06-23T00:00:00",
        "description": "How cool is it to replace a set of shock absorbers with a pair of Electromagnets! How is that even possible? It must have been a revolutionary idea!",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Electromagnetic Suspension"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "ABHILASHBHARADWAJ-K",
        "comments": false,
        "text": "[{% load static %}]()\n# Electromagnetic Suspension\n\n## Abstract\n\nWhen a current passes through a wire, a magnetic field around that wire is generated. The strength of the generated magnetic field is proportional to the current through the wire. When a wire is coiled, this generated magnetic field is concentrated through the center of the coil. The strength of this field can be greatly increased by placing a ferromagnetic material in the center of the coil. This field is easily manipulated by passing a varying current in the wire. Therefore, a combination of permanent magnets with electromagnets is an optimal arrangement for levitation purposes.\n\n![image]({% static '/blog/assets/img/Electromagnetic-suspension/Electromagnetic_suspension_1.jpg' %})\n\n![image]({% static '/blog/assets/img/Electromagnetic-suspension/Electromagnetic_suspension_2.jpg' %})\n\n## The beginning\n\nThe audiophiles at Bose have been working on a new type of electromagnetic car suspension system that can sense the road, maintain the vehicle perfectly, and deliver an ultra-smooth ride for decades. Despite how amazing the early recordings were, \"Project Sound\" was deemed too heavy and expensive to use on real production vehicles.\n\n## Background\n\nBose eventually sold the technology to ClearMotion, an active suspension startup whose CEO recently told Autocar that very few major automakers are interested in bringing its latest version to market. In reality, the smooth-sailing technology, now known as the \"digital chassis system,\" is set to debut in 2019 as a \"low-volume vehicle\". It's worth noting how promising Bose's Project Sound appeared to be when it was initially launched in the mid-2000s after more than two decades of development.\n\nIt was an unprecedented notion developed by \u201cAmar Bose\u201d, who was known for both his love of eccentric passion projects and his hatred for potholes. The motors would extend and retract in response to data from the road sensors, which needed to be evaluated and acted on in milliseconds. These days, active suspension systems are quite popular, but this strategy pushed it to a new degree of control. As a result, the car would be able to stay perfectly flat in corners, making bumpy roads virtually imperceptible to the driver, and even \"lift\" a wheel to avoid a pothole. The best part is that it can actually hop over obstacles.\n\n## Setbacks\n\nAlthough it may seem impossible, the business had a working prototype running underneath an ancient Lexus LS 400 by 2005. It almost appears artificial if you haven't seen it in action. Unfortunately, Bose never got Project Sound out of the prototype stage, and the global financial crisis that hit a few years later was the final nail in the coffin. ClearMotion dropped the electric motor concept in favor of using Bose's road sensing and control software on its own active valve damper development. Shak Avadhany, the company's CEO, calls the technology \"proactive\" rather than \"active.\"\n\n## Resources\n\n* [https://en.wikipedia.org/wiki/Electromagnetic_suspension](https://en.wikipedia.org/wiki/Electromagnetic)\n* [https://www.extremetech.com/extreme/259042-bose-sells-off-revolutionary-electromagnetic-suspension](https://www.extremetech.com/extreme/259042-bose-sells-off-revolutionary-electromagnetic-suspension)\n* [https://www.thedrive.com/news/20996/the-crazy-bose-magic-carpet-car-suspension-system-is-finally-headed-for-production](https://www.thedrive.com/news/20996/the-crazy-bose-magic-carpet-car-suspension-system-is-finally-headed-for-production)\n* [https://www.motorauthority.com/news/1116747_after-30-years-bose-developed-suspension-tech-will-go-into-production](https://www.motorauthority.com/news/1116747_after-30-years-bose-developed-suspension-tech-will-go-into-production)",
        "author_name": "Abhilash Bharadwaj",
        "slug": "Electromagnetic-suspension"
    },
    {
        "layout": "post",
        "title": "Merkle Trees and their application in Git",
        "author_github": "advaithcurpod",
        "date": "2022-06-23T15:16:00",
        "image": "assets/img/",
        "description": "",
        "tags": [
            "Merkle Trees",
            "Git"
        ],
        "categories": "Compsoc",
        "github_username": "advaithcurpod",
        "text": "[{% load static %}]()\n# **Merkle Trees and their application in Git**\nIn computer science, Merkle trees are a hash-tree data structure used for data validation and synchronization.\n\nLets break this down. It is essentially a tree data structure but instead of storing the data (like we do when we want to represent a heirarchial data), merkle trees are special trees which store the hash values of data. By hashes, I am talking about cryptographic hashes. Before getting into merkle trees, what they are and how they work, it is important to understand what hashes are and how they work.\n\n## **Hashes**\nHashing is simply the process of passing some data through a formula that produces a result. Well then, what is sooo speacial about it?? Well..\n\n![Hash Function]({% static '/blog/assets/img/merkle-trees/hash_functions.jpg' %})\n\nHash functions are one way mathematical functions that convert a given piece of data into another form (like encoded data). It is this one way property that makes it unique. It acts as a \u201cdigital fingerprint\u201d for a piece of data. Hash functions are designed in a way that there is very little probability that another piece of data will also result in the same hash. Another interesting and useful property of hash functions is that they are pseudo random, i.e., even a small change in the data (say addition or removal of a letter) will result in a completely new and a random looking string, which makes it nearly impossible to reverse enginner the hash to get back the input data. This is really useful in cryptography when you want to check for the integrity of data.\n\nIf you want to play around with hashes, check this out!!\n[SHA 256 hash calculator](https://www.movable-type.co.uk/scripts/sha256.html)\n\n\n## **Merkle Trees**\nAfter construction, a Merkle Tree looks something like this:\n![Merkle Tree]({% static '/blog/assets/img/merkle-trees/merkle-tree.png' %})\n\na, b, c, and d are some data elements (files, public/private keys.. etc) and H is a hash function.\n\nEach node is created by hashing the concatenation of its \u201cparents\u201d in the tree. For eg: H(ab) is formed by concatenating its \"parents:, H(a) and H(b).\n\nNote: In general and in most use cases, merkle trees are binary trees, but it is not a restriction.\n\nThe tree is constructed by taking nodes at the same height, concatenating their values, hashing the result and recursively performing this operation until the root is reached.\n\nOnce built, data can be audited (checked for) using ***only the root hash logarithmic time*** to the number of leaves (also called a Merkle-Proof). When we are auditing data, what we are essentially doing is checking if a given piece of information is a part of the merkle tree or not? It works by recreating the branch containing the piece of data from the root to the piece of data being audited. In the example above, if we wanted to audit c (assuming we have the root hash), we would need to be given H(d) and H(H(a) + H(b)). We would hash c to get H(c), then concatenate and hash H(c) with H(d), then concatenate and hash the result of that with H(H(a) + H(b)). If the result was the same string as the root hash, it would imply that c is truly a part of the data in the Merkle Tree.\n\n## **Applications**\nEver since it was introduced in 1979, Merkle Trees have gotten a lot of attention. They are used in many systems that use distributed architectures like\n- Blockchain\n- Git\n- BitTorrents\n- IPFS\n\nIn many peer-to-peer (P2P) systems, individuals need to be able to request data from untrusted peers with some proof that what those peers sent them is part of the real content they requested. Torrents are a very good example of this, where in another peer would provide the piece of data, and you need to be sure of the integrity of the data you recieved.\n\nIn the merkle tree example shown above, what this means is that if you asked for \"c\", you rebuild the merkle path and check if your result is the same as the root hash. If it is, you can be really confident of the integrity of  the data. If you\u2019re concerned about the security of this approach, recall that in a hash function it is computationally very difficult (nearly impossible) to find some x such that H(x) = H(c). This means that if the the root hash is correct, you can be really confident that the data you recieved is indeed what you asked for!!\n\nIn this blog, I will mainly focus on the use case of Merkle Tree in Git.\n<br>\n\n## ***Version Control, Git & Merkle Trees***\nIf you aren't familiar with git, you can learn it from [here](https://www.freecodecamp.org/news/what-is-git-and-how-to-use-it-c341b049ae61/)\n\nGit maintains version history by maintaining a hash table in the .git folder. Hash of the object is the key, and the content is the value. A git object could be a commit, a tree, or a blob.\n\n![Git Internal working]({% static '/blog/assets/img/merkle-trees/git-working.jpeg' %})\nIn our discussion on merkle trees, we will mainly focus on the lower half the above diagram.\n\nEvery file in git is stored as a blob (binary large object). Blog is a file like object, with immutable raw data. If 2 files have the same content, then their hashes will be the same, so no new blob will be created even if the 2 files are in 2 different directories. \n\nEvery commit object has 2 reference pointers, one pointing to its parent (previous) commit and the other referencing its merkle tree root hash.\n\n<!-- ![Git Versioning]({% static '/blog/assets/img/merkle-trees/git-versioning.jpeg' %}) -->\n\nThis merkle tree hash is computed by hashing all its \"parent\" nodes. The term \"parent\" might be a little misleading, but it is important to understand that 2 blobs come together (hashes are concatenated) and a new hash of the concatenated string becomes the hash of the merkle tree. So the direction is from bottom to up and not the other way.\n\n![Merkle Tree Git]({% static '/blog/assets/img/merkle-trees/merkle-tree-git.png' %})\n\n### ***How Merkle Tree helps in versioning***\nAs discussed earlier, blobs are raw and immutable contents, you will never find a replica of a blob in memory. To understand this, let us consider an example...\n\nIn the diagram ,blob1 in the second half of the diagram is **NOT** recreated, it refrences the earlier blob in memory. This is really useful in versioning because it helps save space as another complete copy of the repo need not be created, only the ones that are changed are reflected along the merkle path.\n\n![Git Versioning]({% static '/blog/assets/img/merkle-trees/versioning.png' %})\n\nThis way merkle trees are used in git to efficiently track and store versions.\n<br><br>\n\n## **Use case in Blockchain**\nIn the case of cryptocurrencies, if someone claims that in some transaction another peer paid them, how can a node on the network verify that transaction really happened? One option is that the node could store the entire history of every transaction that has ever occurred. But this is not a good approach as it would lead to wastage of space and time and is simply not practical for large systems. Merkle Trees are a solution to this proble. By creating a Merkle Tree out of the transaction data in each block, transactions can be audited in logarithmic time instead of linear time. This opens the door for, say bitcoin clients, to save space by only storing the root of the Merkle Tree (will be able to verify any transaction in a block with just this one hash value). Not needing to store every transaction that has ever happened in the history of Bitcoin is of huge value!\n\nThe applications of Merkle Trees are indeed numerous, and their utilization in any particular domain could be the subject of an entire blog post. I hope this served as a reasonable introduction.\n\nIf you have any more questions or just want to chat, feel free to contact me at advaithcurpod@gmail.com or ping me on Twitter: [@Advaith79630527](https://twitter.com/Advaith79630527)\n\n## **References**\n1. [An introduction to Merkle Trees](https://medium.com/hackernoon/merkle-tree-introduction-4c44250e2da7)\n2. [Git Internals](https://www.youtube.com/watch?v=P6jD966jzlk)\n3. [All about Merkle Trees](https://www.codementor.io/blog/merkle-trees-5h9arzd3n8)\n4. [Merkle Tree in Bitcoin blockchain](https://medium.com/blockchain-stories/the-tale-of-merkle-tree-in-bitcoin-blockchain-2c5fa5a298f7)\n5. [How Git works](https://github.com/kdakan/How-Git-Works)",
        "author_name": "Advaith Prasad Curpod",
        "slug": "merkle-trees-and-their-application-in-git"
    },
    {
        "layout": "post",
        "title": "Autonomous Underwater Vehicle",
        "author_github": "Anannnyyaa",
        "date": "2022-06-30T00:00:00",
        "description": "We've all heard about UAVs in our daily lives, haven't we? Do you, however, know what 'AUV' stands for? An autonomous underwater vehicle is a term used to describe a vehicle that can move and navigate underwater independently without the need for human input.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Autonomous Underwater Vehicle"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "Anannnyyaa",
        "comments": false,
        "text": "[{% load static %}]()\n# Autonomous Underwater Vehicle\n\n## Abstract\n\nThe term \"autonomous underwater vehicle\" (AUV) also refers to an uncrewed underwater vehicle. AUVs can be used for underwater survey tasks such as identifying and mapping submerged wrecks, rocks, and impediments that could pose a hazard to commercial and recreational vessels' navigation.\n\nIt completes its survey mission without the assistance of an operator. When a mission is finished, the AUV will return to a pre-programmed position to download and process the data.\n\n![image]({% static '/blog/assets/img/auv/image1.png' %})\n\n## How does an AUV work?\n\nAUVs are surveillance robots that are pre-programmed with mission specifications before being deployed. AUVs can drift, glide, or accelerate themselves through the water depending on their design. Non-propelled AUVs (drifters or gliders) either hover in the water column or glide up and down by changing their buoyancy. When compared to propelled AUVs, these AUVs consume little to no power, are significantly less agile, and are often deployed on missions that last weeks to months. Propelled AUVs may go faster and manoeuvre more easily than non-propelled AUVs, but they have a shorter battery life and are often used for missions lasting many hours to days. There are several other sophisticated battery management systems that allow an AUV to spend more time underwater instead of constantly recharging its batteries. They are also equipped with ultra-capacitors, which provide a significant upgrade to the power system.\n\n## Positioning of AUV\u2019s\n\nAUVs, unlike other robotic systems, cannot use global navigation satellite systems. They use GPS navigation and can only get a GPS signal when on the surface. When it is underwater, it uses its last known GPS position to compute its movement, which is measured by an on-board inertial navigation system that monitors the AUV's velocity, acceleration, and rotation. This method of navigation is known as \"dead reckoning.\" Doppler velocity logs may also be used to determine the robot's speed in relation to the seabed, but only when the robot is not traveling too high from the bottom. A pressure sensor on the vehicle may be used to calculate the AUV's depth. The AUV may rise to the surface often to acquire a new GPS signal and transmit data, or it may spend the whole voyage below, depending on the mission. These robots are fitted with numerous sensors so as to define their accurate positions. It's worth noting that the AUV, as a maritime technology, relies on something called the Underwater Acoustic Positioning System, which utilizes the ship's or naval vessel's GPS to drive it further into the ocean.\n\n## Applications\n\n-   AUVs are used by oil drilling and subsea drilling businesses to examine the right oceanic area for their needs.\n-   AUVs are being employed as a marine technology to see whether there are any booby-traps hidden in the ocean bottom or bed.\n-   When it comes to deploying AUV's for study, this includes searching for any damage to the undersea ecology on the oceans or sea floor.\n    \n\n![image]({% static '/blog/assets/img/auv/image2.webp' %})\n\n\n## Disadvantages\n\n-   AUVs cannot be operated everywhere and has limited range.\n-   They can be influenced by strong currents.  \n-   They are less suited for areas that are heavily populated due to acoustic interference, collision risk, and entanglement. \n-   It is economically costly.  \n-   If they are not managed carefully, they may also contribute to the e-waste in the water.\n    \n\n## Future advancements\n\nNew AUVs with improved hovering, extended endurance, extreme depth, or quick response capabilities, as well as the development of new sensors, will broaden the range of geochemical parameters that can be monitored in the future, which might be of interest to marine geoscience. Machine learning is also being used to evaluate the probability of AUV loss as well as how advanced sampling devices are being integrated into autonomous vehicles.\n\n  \n\n## Conclusion\n\nIn today's world, a range of vehicles such as SUVs and ATVs are employed for a number of functions. But, among all of these vehicles, there is none that compares to an AUV in terms of the features it offers. An autonomous underwater vehicle is a significant breakthrough that has occurred in recent years, and it merits suitable acknowledgment and credit.\n\n## Resources\n\n- [What is the difference between an AUV and an ROV?](https://oceanservice.noaa.gov/facts/auv-rov.html#:~:text=AUV%20stands%20for%20autonomous%20underwater,for%20commercial%20and%20recreational%20vessels)\n- [https://www.sciencedirect.com/topics/earth-and-planetary-sciences/autonomous-underwater-vehicle](https://www.sciencedirect.com/topics/earth-and-planetary-sciences/autonomous-underwater-vehicle)\n- [https://www.marineinsight.com/types-of-ships/everything-you-ever-wanted-to-know-about-autonomous-underwater-vehicle-auv/](https://www.marineinsight.com/types-of-ships/everything-you-ever-wanted-to-know-about-autonomous-underwater-vehicle-auv/)\n- [https://transmitter.ieee.org/auvs-how-autonomous-underwater-vehicles-protect-oceans-and-divers/](https://transmitter.ieee.org/auvs-how-autonomous-underwater-vehicles-protect-oceans-and-divers/)",
        "author_name": "Anannya Rao",
        "slug": "auv"
    },
    {
        "layout": "post",
        "title": "Rise of Robotics in medicine",
        "author_github": "apurva-20",
        "date": "2022-06-30T00:00:00",
        "description": "Robots are revolutioninzing the practice of medicine. Today,medical robots are well known for their roles in surgery.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Rise of robotics in medicine"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "apurva-20",
        "text": "[{% load static %}]()\n# RISE OF ROBOTICS IN MEDICINE\n\nThe Healthcare industry is always on the quest for new methods to develop and innovate. Robots have come a long way from being science fiction to assisting in surgical procedures. They help in improving efficiency and precision in the healthcare industry.\n\nThere are different generations of surgical robots-    \n\n## The first-generation robots and their first surgeries\n\n ![image](https://user-images.githubusercontent.com/84237640/160172994-911b7e69-bc05-4401-8aa0-1cba85259bd2.png)\n \nIn 1985, PUMA (Programmable Universal Machine for Assembly) - a robotic arm, assisted with a neurological biopsy for the first time. Computed tomography (CT) guided the robot while inserting a needle into the brain. Earlier, this procedure was subjected to errors due to shivers or tremors in human hands during the needle placement. But, PUMA conducted the operation successfully.\nOriginally, PUMA was a standard industrial robotic arm. Thus the first-generation robots were not designed primarily for medical purposes but modified to perform medical tasks.\n\nFew of the first-generation robots were declared \"autonomous\" by the U.S. Food and Drug Administration (FDA), which meant they could carry out some procedures entirely by themselves without a surgeon's guidance. In 1992, ROBODOC was the first FDA-approved robot. ROBODOC was used to move the femur during hip replacement surgeries.\n\n\n![image](https://user-images.githubusercontent.com/84237640/160173403-c56b8222-949d-4370-809a-2d66ac961a5f.png)\n\n \nWith many such robots coming into the medical field, surgeons wanted robots to be collaborative assistants and not replacements. This thought gave rise to the second generation of surgical robots.\n\n## Second-generation robots\nSecond-generation robots work in a master-slave configuration. The robot (slave) translates the surgeon's (master) hand, wrist, and finger movements in this configuration. The most popular one of this generation is \"The da Vinci Surgical Robot.\" The FDA first approved it in 2000.\n\n![image](https://user-images.githubusercontent.com/84237640/160173742-19c46967-a962-4a1e-a619-be28673e1af1.png)\n\n \n\n## How does da Vinci Surgical Robot work?\nThe four interactive arms of the robot hold tools like scalpels, scissors, and graspers. The doctor seated near the master console controls the arms. The instruments move on a small scale inside the patient's body based on the hand movements of the surgeon. The camera and light source in one of the arms guide the surgeon during surgery. \n\nWhen the surgeon moves his head away from the console, the activities of the da Vinci Surgical System suddenly stop. Therefore any severe problem in surgery is prevented.\n\n\n## Why Robots?\n**Robots can be smaller or larger than humans -**\nConsider the picture below. A surgeon's hand has the farthest reach of 10cm, while that of a small robotic device is about 1cm.\n\n![image](https://user-images.githubusercontent.com/84237640/160173827-8d8010e7-64c9-4da8-8229-f53b855f39d9.png)\n\nSince robots can be smaller than humans, they can operate on patients through smaller incisions. Smaller incisions cause minor damage to body tissues. With less tissue damage, it is more likely for patients' recovery time to increase.\n\nFurthermore, it is easy to support and lift patients with the assistance of giant robots compared to the effort that a health worker puts in.\n\n**Eliminate human error and provide higher accuracy -**\nRobots can perform precise movements and be present with patients for as long as necessary. \nUnlike human beings, robots are tireless, and their \"hands\" never quiver.\nThey can be used for simple repetitive tasks during the surgery, while humans can take up the crucial functions.\n\n**Lesser infection risk.**\n\n**Better utilization of information from medical scan images and test report data -**     \n\nMedical images serve as a rich source of patients' data. Surgeons get different geometrical information about what is happening inside a patient's body. Though surgeons and other medical experts utilize this information amazingly, robots can use the information in a better way. Robots can analyze virtual boundaries around delicate organs and restrain themselves from venturing into them from the images.\n\n\n## Key challenges and developments\nGaining the trust of many patients who are doubtful about undergoing a robot-assisted surgery is essential. Attempts are on the go to use robots for emotional support. With sophisticated technologies coming up in the future, biomedical engineering will continue to find better ways to handle various medical procedures.\n\n## References\n1.\t[https://online-engineering.case.edu/blog/medical-robots-making-a-difference](https://online-engineering.case.edu/blog/medical-robots-making-a-difference)\n2.\t[https://aabme.asme.org/posts/robot-assisted-surgery](https://aabme.asme.org/posts/robot-assisted-surgery)\n3.\t[http://bme240.eng.uci.edu/students/10s/sgupta1/Developments.html](https://bme240.eng.uci.edu/students/10s/sgupta1/Developments.html)",
        "author_name": "Apurva S",
        "slug": "rise-of-robotics"
    },
    {
        "layout": "post",
        "title": "Web design in a developer's arsenal",
        "author_github": "not-shoyo",
        "date": "2022-06-30T14:22:00",
        "image": "/assets/img/",
        "description": "An intro to Web-Design\u2019s increasing encroachment in a Web-Developer\u2019s workspace",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Web Design"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "not-shoyo",
        "text": "[{% load static %}]()\n# **Blast to the past**\n\nWebsites weren\u2019t always as smooth and easy to use. Infact, up until a few years ago, you could not even create the majority of the most common layouts seen on today's internet. A look at the past can teach us a lot, from the internet culture back then to the type of websites used to share and propagate this culture.\n\n## Theme: Fun\n\nWebsites in the 2000s and even a little into the 2010s were all about the developers having fun and really just expressing themselves through their websites. Sure, most of the time the clients for the website probably had a say in the matter, but in the beginning, the clients were the same people that were making the websites.\n\n![Old Website]({% static \"/blog/assets/img/Web-design-in-a-developer's-arsenal/old_website_1.png \"Unprofessional looking website with bad color contrast and random animations and gifs\"\" %})\n\n_Unprofessional looking website with bad color contrast and random animations and gifs_\n\nAn interesting and funny point to note about the time was the ubiquitous use of \u201cthe pop-up man\u201d. It's funny due to how prevalent this feature was despite how annoying and unprofessional it made the website look. This was a key feature throughout the era, where developers would do things on their website, simply because they could. All that is to say that websites of the past used to be a motley assortment of all the available patterns and structures available to them, to try and make something that would attract attention and draw clicks and traffic to the page.\n\n![Poor Navigation On A Website]({% static \"/blog/assets/img/Web-design-in-a-developer's-arsenal/poor_navigation_1.jpg \"Weird or awkward positioning of navigation which left users lost in the website\"\" %})\n\n_Weird or awkward positioning of navigation which left users lost in the website_\n\nUnfortunately, this also meant that it was very common for users to find themselves stranded on a webpage with nowhere else to go and needing to use the back arrow button on their browser to trace back their steps. As you can probably guess, this heavy emphasis on stylish, fancy, and attention-grabbing designs often left a lot of room for improvement with respect to functionality and uniformity within a website.\n\n## Technology as the hindering factor\n\n![Layout created using table]({% static \"/blog/assets/img/Web-design-in-a-developer's-arsenal/table_layouts_1.jpg \"Technological constraints faced by past developers to create a simple layout\"\" %})\n\n_Technological constraints faced by past developers to create a simple layout_\n\nIt's hard to be super critical when revisiting such old pages as you have to take into consideration the era they are from and the technology that was available to them. Much of the popular layout formats or even the idea of easy-to-use interactivity tools were not even conceived back then. This meant that developers often found themselves pushing the boundaries of their imaginations to even be able to mimic simple structures.\n\n![Layout created using Flexbox]({% static \"/blog/assets/img/Web-design-in-a-developer's-arsenal/flex_layouts_1.jpg \"easiness of rendering identical layouts using modern technology like flexbox\"\" %})\n\n_easiness of rendering identical layouts using modern technology like flexbox_\n\nThe example shown here illustrates how a web developer from way back would have tried to implement such a complex layout using tables (which as we all know are meant to show a tabular form of data) while nowadays we can easily build such layouts with a mix of flexbox and CSS grids. This is one of many such instances where it's easy to correlate the bad design choices and website look and feels of the time to simply a lack of technology that could keep up with the developers.\n\n# **Modern (?) websites**\n\nLuckily nowadays, you don't have to go on the internet to see a mess of tables and poorly designed websites with even poorer graphics. However, Due to the widespread nature of the internet and how easy it is for someone to make a website for anything nowadays, it is still possible to find poorly designed web pages. Keep in mind, these webpages do NOT look bad, they are simply designed poorly, which is a distinction we will explore later.\n\n## Boundless options & the absence of structure\n\nDevelopers nowadays suffer from a very peculiar problem: They have way too many options to choose from. Now although that may sound counterintuitive, it makes sense when you take a step back and think about what a website is really used for\u2026 and once you realize the multitude of uses for a website, that is when you are truly stumped. It is an unfortunate assumption that most engineers or web developers consider themselves less creative than their counterparts, as a result of which, parsing through such a wide array of options with no end in sight of a perfect website can be overwhelming for most.\n\nAnother thing that holds back modern websites is the belief that one has to be creative to be good at web design. This results in web developers often putting structured approaches to any problem (as they would in other fields) on the backburner when working on a website's design. The lack of such a clear cut structure, compounds the vast array of options, resulting in some very dismal looking websites at their worst, and even at their best, just websites that are confusing to go through and use.\n\n# **UX & UI**\n\n> Design is not just what it looks like and feels like, design is how it works - _Steve Jobs_\n\nTwo terms familiar to most people even remotely interested in web development are - User Interface (UI) and User Experience (UX). However, there seems to be a huge gap between people that have heard these terms vs people that understand and implement them. To keep it short, the first part of Steve Jobs\u2019 quote describes what most people would define as UI, while the quote as a whole, gives you a picture of what UX really is. Both literally and in practice, UX is what you get when you combine the UI with functionality (or rather, apply the correct UI to the appropriate functionality).\n\n![Comparison of bad (left) and good (right) UX]({% static \"/blog/assets/img/Web-design-in-a-developer's-arsenal/UX_good_vs_bad.png \"Coloring of the call to action button gives two different messages to the user regarding the importance and finality of the action\"\" %})\n\n_Coloring of the call to action button gives two different messages to the user regarding the importance and finality of the action_\n\n## UX specifics\n\nTo provide something different from the usually conceptual and vague explanations of UX and UI, here are some specific pointers to keep in mind when designing:\n\n- What is the website for\n  - Depending on whether the website is made for selling a product, a service, entertainment, or even within products, the type of product itself can determine the styling, design and overall feel of a website.\n- Sticking to tried and tested page layouts and components\n  - It just so happens that humans tend to resonate more to familiar patterns and are usually wary about or stunned by changes in their environments. Applying similar logic to web design, if a client of a website requires trust and easy belief from the consumers. No matter how good it looks, it is best suited to go for a familiar layout as opposed to something that would make them second guess their purpose on the page\n- Know where the focus should be\n  - In most sites where the end goal is for a consumer to subscribe or sign in through some form, it is best to make that section stand out the most and place it in the user's path of action\n- Simple is more\n  - Cluttering your page with a lot of information more often than not leads to lesser information actually getting through to the end-user. Instead pre parsing your information and sticking to only displaying what's required and effective for the client should be the priority.\n- Images, Icons & quotes\n  - All three being double edged swords, when used in the right places and types of websites, can completely blow new life into the page. However done wrong, (like placing icons on a website with a minimalist theme) can completely destroy the flow of the website.\n\n# **Developers \u2248 Designers**\n\nAs mentioned earlier, the number one thing inhibiting developers from adopting good design practices is the belief that one has to be conventionally creative to perform well in such a field. However, the very fact that engineers can be web developers proves that one can simplify even this \u201ccreative\u201d task down to a science and recognize patterns to implement logic to make objectively good designs. In today's day and age where many new specializations are popping up, there is a good chance that in a few years, that profession of code-heavy web developers as we now know it will no longer exist. This isn't such a big surprise as the work of a web developer looks way more different than it did 5 years ago. This should serve as a push to obtain design skills as a developer since even if code heavy developer jobs may cease to be, the design inputs of humans can still very much be useful and valued.\n\nStepping back from the speculation for a bit, it is no secret that having more skills under one\u2019s belt is a boon, and web development being one of the easiest skills to pick up, having good design skills is what sets up the elite from the rest",
        "author_name": "Akheel Muhammed",
        "slug": "web-design-in-a-developer's-arsenal"
    },
    {
        "layout": "post",
        "title": "Electro Kinetic Road",
        "author_github": "gauravKumar",
        "date": "2022-07-07T00:00:00",
        "description": "The electro-kinetic road is a method to generate electricity by harnessing the kinetic energy of automobiles that drive over the ramp",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Electro Kinetic Road"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "gauravKumar",
        "text": "[{% load static %}]()\n# Electro Kinetic Road\n\n## Abstract\n\n\u201cElectro Kinetic Road\u201d is a topic which involves both civil and circuital engineering discipline. It\u2019s latest innovation which will really going to impact future in dealing problem related to energy problem.\n\n## Introduction\n\nAs we all notice the number of vehicles on roads how they are increasing day by day? They also require a lot of energy to run either in any forms of sources. We always try to look some sustainable development so here\u2019s a new technology in civil engineering of electro-kinetic pavement design of roads. Just think when roads will start returning the kinetic energy what it gets from the vehicle on its pavement. The pavement is designed in such manner that it has some electric storing elements like piezoelectric and transducer in pavement\n\n## Working\n\nWhen vehicle passes through roads, they apply stress on the pavement generally the heavy trucks apply heavy stress value. Mechanical energy exerted by traffic, especially heavy trucks, can be converted into electricity in several ways. The mechanical stress from traffic can be captured by piezoelectric material, or causes relative movement in electromagnetic generator. Also, significant solar energy absorbed by pavement can be harvested using photovoltaic cells, heat flux, or thermoelectric material. Even geothermal energy can be collected with a heat pump and underground thermal energy storage. As research done by Dr. Hao Wang (Rutgers, Civil and Environmental Engineering) and his collaborator, Dr. Ahmed Safari (Rutgers, Materials Science and Engineering) has published many papers related to that.\n\nHave you ever wondered that when a car passes a road breaker most of kinetic energy is wasted as heat in it .By conserving this kinetic energy and generating electricity, we can supply this energy to the urban and remote areas for lightning purposes.\n\nAnother technology apart from above mentioned is Road Power Generation (RPG) which is designed to capture waste and kinetic energy from all devices. This device converts the kinetic energy of the vehicles into electric energy. This is done by pressure plates installed on the ramp. These ramps are connected to shaft which again rotate turbine kind of thing.\n\n##  Advantages\n\n- By using this technology, we can reduce scarcity of electricity to some extent.\n- We always talk about green energy and this is one of them which is making waste as useful.\n- Smart Roads are going to be future roads.\n\n## Current disadvantages\n\n- Since this is a new technology and expensive so it\u2019s little uneconomical to install specially for developing and undeveloped nation\n- These things and technology are to set up under the pavement, so already built road you have to scrap, set and then built again so it sounds uneconomical.\n \n## Conclusion\n\nDigital wave is everywhere. World has been taken over by digital wave. \u2018Smart roads\u2019 or \u2018Smart highways\u2019 are the road to the future so its really a good innovation in the field of energy saving. In up coming days it will progress rapidly, even some nation already and still many researches going on. It will sort  out energy problem to certain extent.\n\n## Recourses\n\n- [https://en.wikipedia.org/wiki/Electro-kinetic_road_ramp](https://en.wikipedia.org/wiki/Electro-kinetic_road_ramp)\n- [http://www.ijarse.com/images/fullpdf/1523548414_292IJARSE.pdf](http://www.ijarse.com/images/fullpdf/1523548414_292IJARSE.pdf)\n- [https://bloggershrutidoshi.wordpress.com/2020/06/15/road-construction-electro-kinetic-road-ramp/](https://bloggershrutidoshi.wordpress.com/2020/06/15/road-construction-electro-kinetic-road-ramp/)",
        "author_name": "Gaurav Kumar",
        "slug": "electro-kinetic-road"
    },
    {
        "layout": "post",
        "title": "Introduction To Deep Learning",
        "author_github": "harry7337",
        "date": "2022-07-07T16:26:23",
        "image": "/assets/img/intro-to-dl",
        "description": "Introductory post on the fascinating field of Deep Learning and its Application",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Machine Learning",
            "Deep Learning",
            "AI"
        ],
        "categories": "CompSoc",
        "github_username": "harry7337",
        "text": "[{% load static %}]()\n# Introduction to Deep Learning\n\n\n## Introduction?\nWhat inspired me to write a blog on this topic was how popular the terms AI and ML were, compared to \u201cDeep Learning\u201d or \"DL\" which very few people had ever heard of, despite it making up a significant part of humanity's  journey toward achieving AI.\n\n\nOn an average in India, there are a _higher number_* of Google searches for __\u201cAI\u201d__, __\u201cML\u201d__ and __\u201cCloud\u201d__ than there is for __\"Deep Learning\"__.\n\n \nDL is not an abstract concept that has not been implemented yet. Google is using deep learning in its voice and image recognition algorithms (such as face classification in Google Photos) whereas Netflix and Amazon are using it to understand the behavior of their customer. Now, imagine the potential DL has in revolutionizing the world. \n\n## Brief Summary of AI and ML\n\nThe term AI was coined in 1956 by John McCarthy, who is also referred to as Father of Artificial Intelligence. The idea behind AI is fairly simple, which is to make machines that can take decisions on its own i.e give them intelligence. \nMachine Learning was the first step towards AI. ML does not have a fixed definition and it is fairly common for researchers to have their own definitions. This is one such definition that is often used and most widely accepted:\n\n> \u201cA computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measured by P, improves with experience E.\u201d \u2014 Tom Mitchell, Carnegie Mellon University.\n\nSo, let us take the example where our program is playing chess. Then the variable definitions are as follows:\n\n\n*T* = task of playing chess\n\n\n*E* = experience of playing many games of chess\n\n\n*P* = probability that the program will win the next game\n\n\nSo in ML, we are trying to get the program to learn without being explicitly programmed to do so.\n\n\n\n## Relationship between DL,ML,AI\n\nOf course, the three terms are interlinked and the picture below summarizes their relationship.\n\n![Relationship Between DL AI ML]({% static '/blog/assets/img/intro-to-dl/relationship.png' %})\n\n*Source: Nvidia*\n\n\nHere, the expansion of AI is clearly outlined- where each domain takes precedence over the other in solving certain types of tasks.\n\n\nMachine Learning is a subset of AI, where we provide a dataset which is often well structured to our model and it learns its craft based on that particular dataset. So if we provide the ML model a dataset of house prices in New Delhi,India, with the objective of predicting them then it will \u201clearn\u201d how to predict the prices specific to the dataset provided.\nDeep Learning is a subset of Machine Learning where similar Machine Learning Algorithms are used to train Deep Neural Networks so as to achieve better accuracy in those cases where former was not performing up to the mark. But why is ML not enough? This is discussed in the next section.\n\n## Drawbacks of the Machine Learning\n\n![Drawback]({% static '/blog/assets/img/intro-to-dl/drawback.png' %})\n\nSouce: edureka\n\n\nTraditional ML algorithms are not useful while working with high dimensional data, that is where we have a large number of inputs and outputs. For example, in the case of facial recognition, there are copious amounts of data that need to be dealt with. ML simply cannot provide a satisfactory result relative to the amount of data that is being provided to the program.\n\n\nAnother challenge is to extract the features or parameters in our data that play a vital role in predicting the outcome. This very process is referred to as feature extraction. ML does not seem to be able to efficiently extract features from the data we provide it. Let us take the example of image recognition, the patterns present in the image cannot be extracted satisfactorily with ML, and it fails to efficiently recognize important features in an image, like a face, scenery,etc\u2026\n\n\n## How does DL work\n### Intuition\n\nAt the heart of deep learning lie Artificial Neural Networks(ANNs) or simply Neural Networks. These are the data structures or computational models that help us perform tasks such as feature extraction, as mentioned above. So why are they called Neural Networks? That is because they were inspired by the way information flows through a human- Neurons.\n\n![Neuron]({% static '/blog/assets/img/intro-to-dl/neuron.jpg' %})\n\nSource: openbooks.lib.msu.edu\n\n\nOur human brain is a neural network, which is full of neurons and each neuron is connected to multiple neurons. Again, neurons have several Dendrites. Dendrites collect input signals which are summed up in the Cell body and later are transmitted to the next neuron through Axon.\n\n\nSimilarly, in an ANN, a __perceptron__(the basic computational unit in an ANN analogous to a human neuron) receives multiple inputs which are then processed through functions to get an output. But in the case of ANN, weights are assigned to various neurons. Then in the final layer everything is put together to come up with an answer.\n\n### Working\n\n![Perceptron]({% static '/blog/assets/img/intro-to-dl/perceptron.jpeg' %})\n\n*A single perceptron*\n\n*Source: towardsdatascience*\n\n\n1. On the left side data input represented by xi.\n2. We multiply each of the input by a weight wi, along the arrow (also called a synapse). So w1 * x1, w2 * x2, w3 * x3 and so on. The larger the weight, the more influential the corresponding input is.\n3. Once all the inputs are multiplied by a weight, we sum all of them up and add a bias to it bi.\n4. Now, we have an activation function. This particular activation f(x) outputs 1 if the sum obtained before is greater than or equal to zero. It outputs 0 otherwise. \n\n*Activation functions helps us map the input to a discrete output*.\n\n\nSay we are trying to recognize if the image of an animal is a dog, then we pass different pixel values of the image as input(x) and the activation function in the end tells us whether the animal is a dog(y=1) or not(y=0).\nNote how the results from each step moves towards the right, in a sort of forward direction. This type of flow of data is called forward propagation.\n\n\n\nThe working we saw just now is an example of a single-layer perceptron. Let us look at something a bit more complex\n\n#### Multi-Layer Perceptron:\n\nNeural networks with two or more layers are called multi-layer perceptron. This type of neural network has greater processing power. In this, the algorithm consists of two phases: \n\n- __Forward phase__: Where the activations are propagated from the input to the output layer\n- __Backward phase__: Where the error between the observed actual and the ideal value in the output layer is used to modify the weights and bias values.\n\n![Multi-Level Perceptron]({% static '/blog/assets/img/intro-to-dl/multi-perceptron.png' %})\n\n*Source: edureka*\n\n\t\n#### Deep Neural Network:\nDeep neural network refers to neural networks with multiple hidden layers and multiple non-linear transformations.\n\n![DNN]({% static '/blog/assets/img/intro-to-dl/dnn.png' %})\n\n*Source: edureka*\n\n\nAs you can imagine, the power and capability of Deep Neural Networks are much higher than a single-layer Perceptron, and it has a wide range of applications.\n\n## Applications of Deep Learning:\n### Healthcare:\nDeep Learning and its innovations are advancing the future of precision medicine and health management. Breast Cancer, Skin Cancer diagnostics are just a few examples of DL in Health Care. \n\nIn the coming years computer-aided diagnosis will play a major role in healthcare which not only speeds up the process but also may provide more accurate results than human diagnosis.\n\n### Computer vision and pattern recognition:\nComputer vision has given rise to some very interesting applications such as autonomous driving. Mobileye is one such company. It is a global leader in the development of vision technology for Advanced Driver Assistance Systems (ADAS) and autonomous driving.\n\n\nMobileye\u2019s philosophy is that:\n> If a human can drive a car based on vision alone \u2013 so can a computer. \n \nThis philosophy reflects the culture of thinking that deep learning has developed among companies.\n\n\nTheir systems can identify an imminent collision and brake without any driver intervention, center the vehicles on the lane, and assist in traffic jam situations. \n\n__Note__ that none of these require __human intervention__!\n\nPixel restoration, automatic color-filling in B&W photos and videos are also creative applications of DL.\n\n### Voice-activated intelligent assistants:\nApple\u2019s Siri, Google Now, Microsoft Cortana are a few examples of deep learning is voice search & voice-activated intelligent assistants.\n\n### Smart Advertising:\nPublishers and ad networks can now leverage their content to create data-driven predictive advertising, precisely targeted advertising and much more. There have been ethical concerns about this such as privacy in the past few years.\n\n### Finance:\nFrom predicting stock prices to analyzing trading strategy, and reviewing commercial loans- all come under the wings of DL.\n\n### Computer games:\n#### Go\nThe standard Go board has a 19\u00d719 grid of lines, containing 361 points. Despite its relatively simple rules, Go is extremely complex. \nCompared to chess, Go has both a larger board with more scope for play and longer games and, on average, many more alternatives to consider per move. \nThe number of legal board positions in Go has been calculated to be approximately 2.1\u00d710170, which is vastly greater than the number of atoms in the observable universe, estimated to be of the order of 1080.\n\nDeepMind\u2019s AlphaGo defeated Lee Sedol, one of the best Go players in the world!\n\n![GO]({% static '/blog/assets/img/intro-to-dl/go.jpeg' %})\n\n\n*Lee Sedol vs. AlphaGo in 2016*\n\n*Source: The New Yorker*\n\n\n\n\n## Conclusion\nDeep learning is not here to take part in the AI revolution, in fact it is taking over. The capabilities are limitless and its potential to help society progress is enormous. \n\nDespite its positives, we must remember not to misuse AI such as this; to cause detrimental effects to the rest of humanity goes against the principles of why AI was pioneered in the first place- to help people. \n\nI hope I inspired you to further pursue deep learning and use it to add something tangible to the community. With that, this is Harish Gumnur signing off. \n<br>_ _ _<br>\n*According to Keyword Planner, a tool provided by Google to analyze the number of keyword searches on a monthly basis over a particular period of time. The time period referenced in this blog is April 2020 to March 2022.*",
        "author_name": "Harish Gumnur",
        "slug": "intro-to-deep-learning"
    },
    {
        "layout": "post",
        "title": "Introduction to object oriented programming",
        "author_github": "AmandeepSingh285",
        "date": "2022-07-07T00:00:00",
        "image": "/assets/img/",
        "description": "Introduction to object oriented programming",
        "tags": [
            "IEEE NITK",
            "Blog",
            "OOPS"
        ],
        "categories": "CompSoc",
        "github_username": "AmandeepSingh285",
        "comments": true,
        "text": "[{% load static %}]()\n# Introduction to Object Oriented Programming\n\nObject oriented programming is a programming paradigm based on the concept of objects that contain data and instructions or code. Object oriented programming (OOP) can also be referred to as a programming concept working on the principles of abstraction, encapsulation, inheritance and polymorphism. It allows users to create objects they want and create methods to handle those objects. The basic concept of OOPs is to create objects, re-use them throughout the program, and manipulate these objects to get results. The popular object oriented languages are C++, Java, Python, JavaScript etc.\u00a0\n\n## Objects\n\nAny entity that has a state and a behavior is referred to as an object. For example a pen, chair, bottle etc. Objects can be physical or logical. An object can be defined as an instance of a class. An object contains an address and takes up some space in memory. When an object of a class is created, the class is said to be instantiated. All the instances share the attributes and the behavior of the class. But the values of those attributes, that is, the state are unique for each object. A single class may have any number of instances.\u00a0\n\n## Class\n\nA class can be defined as a blueprint from which objects are defined or created. Classes don't consume any space in the memory. It represents the set of properties or methods that are common to all objects of one type. Class declarations include the following components:\n\n- Class keyword\n- Class name: Generally written in sentence case.\u00a0\n- Modifiers: A class can either be public, private or protected.\n- Body: Consists of methods and data fields. The body is generally surrounded by curly brackets in languages like C++ and Java.\u00a0\u00a0\n\n\nThere are four pillars of object oriented programming namely:\n\n- Abstraction\n- Inheritance\n- Encapsulation\n- Polymorphism\n\n## Abstraction:\n\nData Abstraction is the property by virtue of which only the essential details are displayed to the user. The trivial or the non-essentials units are not displayed to the user. We can understand more about abstraction by taking a simple example. While driving a car the driver knows that pressing the accelerator would increase the speed of the car but the driver does not know about the internal functioning of the car or the implementation of the accelerator.\u00a0\n\n## Encapsulation:\n\nBinding data and code together into a single unit is referred to as encapsulation. We can also consider it to be a protective shield that prevents data access by code outside this shield. Technically, the data or variables of a class are hidden from other classes and can only be accessed by member functions of the own class. Hence clearly data is hidden from other classes. Encapsulation can be achieved by setting all the variables in a class as private and writing public member functions to access the variables.\u00a0\n\n## Inheritance:\n\nThe capability of a class to derive properties and characteristics from another class is referred to as inheritance. Inheritance is the process in which two classes have an is-a relationship among each other and objects of one class acquire properties and features of the other class. The class which inherits the features is known as the child class, and the class whose features it inherited is called the parent class.\u00a0 For example, Class Vehicle is the parent class, and Class Bus, Car, and Bike are child classes.\n\n## Polymorphism:\n\nPolymorphism refers to the ability of a variable, object or function to take multiple forms. For example, in English, the verb run has a different meaning if you use it with a laptop, a foot race, and business. Here, we understand the meaning of run based on the other words used along with it. The same also applied to Polymorphism.\n\n## Advantages of OOPs:\n\n- OOPs makes development and maintenance easier, whereas, in a procedure-oriented programming language, it is not easy to manage if code grows as project size increases.\n- It enhances program modularity because every object exists independently.\n- Objects created for Object-Oriented Programs can be reused in other programs. Thus it saves significant development cost.\n- OOPs provide data hiding, whereas, in a procedure-oriented programming language, global data can be accessed from anywhere.",
        "author_name": "Amandeep Singh",
        "slug": "intro-to-oops"
    },
    {
        "layout": "post",
        "title": "ZigBee Technology",
        "author_github": "Ashrith D R",
        "date": "2022-07-07T00:00:00",
        "Description": "Zigbee is a wireless technology developed as an open global standard to address the unique needs of low-cost, low-power wireless IoT networks",
        "tags": [
            "IEEE NITK",
            "Blog",
            "ZigBee"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "ashrithdr",
        "text": "[{% load static %}]()\n# What is ZigBee Technology? #\n\nZigBee is a wireless standard technology developed to enable low cost and low power wireless transmission in machine-to-machine and internet of things (IOT) networks.\n\nZigBee is specifically designed for control and sensor networks based on the IEEE standard for wireless personal area networks (WPANs). It has communication standards that specifies the physical and MAC layers required to handle many devices at low data rates.\n\nZigbee WPANS operate at frequencies of 8MHz, 902-928MHz, and 2.4GHz. The 250-kbps data are ideal for periodic transmission and intermediate two data transmission between controllers and sensors. \n\n![image 1]({% static '/blog/assets/img/zigbee-technology/Picture-1.jpg' %})\n\n# Advantages of ZigBee Technology #\n\n### 1)\tSupport for high- level nodes ###\nMany thousands of nodes can be connected using Zigbee technology. A single network can support up to 65000 nodes using Zigbee technology. With this the transmission range can be continued to expand. This can be made in a large space in a building with many floors.\n\n### 2)\tSuitable for low power devices ###\nThe Zigbee technology is generally ideal for low-power devices. It supports small battery-operated devices, sensors, object tags, and other similar devices. Since small devices does not need a lot of bandwidth, Zigbee technology is ideal for these devices. \n\n### 3)\tRange ###\nZigbee is a mesh network and hence Zigbee has a network that is expandable. When compared to a single router, Wi-Fi, or Bluetooth, Zigbee technology provides more stability. \n\n### 4)\tFlexible ###\nThe Zigbee technology\u2019s structure is very flexible. Zigbee technology has a simple installation process and is also less expensive. This technology has a short working period and long battery life and hence is less power consumption.\n\n### 5)\tMonitoring  ###\nThe Zigbee technology is simple to track and control, it is simple to maintain with the help of remote because it is mainly used for home appliances.\n\n![image 2]({% static '/blog/assets/img/zigbee-technology/Picture-3.webp' %})\n\n# Disadvantages of ZigBee Technology #\n\n### 1)\tChannel Noise ###\nNetwork interference is a major problem with Zigbee technology. This is due to channel noise and network\u2019s overcrowding. One of the main reasons for this is because it uses 2.4GHz bandwidth. This band is also used by microwaves, Bluetooth devices, cordless phones, and other wireless devices and technology is susceptible to interference.\n\n### 2)\tThe Rate of transmission is low ###\nZigbee is designed for low-rate data transmissions and because of this the technology used In it has low bit rate. As a result, Zigbee technology has a lower transmission rate than Bluetooth and Wi-Fi.\n\n### 3)\tCompatibility and security ###\nThere are many security risks associated with Zigbee technology. There are intentional interferences that can result in the loss of network services. This may also cause data theft from a node or the theft of an entire node because it is susceptible to interferences. By this a hacker can gain access to a node.\n\n### 4)\tExpensive ###\nThe installation of Zigbee technology is very costly. The cost of Zigbee implementation changes with the range and size of the network. \n\n\n# Applications of ZigBee Technology #\n\n### 1)\tAutomation in the Workplace ###\nA correspondence link is used in assembling and manufacturing businesses, which helps to track various parameters and basic types of equipment. As a result, Zigbee lowers the cost of communication and also it improves the control procedure for improved consistency.\n### 2)\tHome Automation ###\nZigbee is ideal for controlling home apparatuses remotely and can control devices such as lighting, machine control, heating, and cooling system control.\n### 3)\tSmart Metering ###\nZigbee remote tasks in smart metering include energy utilization reaction, estimation support and provides security against power theft from others.\n\n![image 3]({% static '/blog/assets/img/zigbee-technology/Picture-2.png' %})\n\n\n# References #\n1. [https://www.knowledgenile.com/blogs/zigbee-on-the-internet-of-things-advantages-and-disadvantages/](https://www.knowledgenile.com/blogs/zigbee-on-the-internet-of-things-advantages-and-disadvantages/)\n2. [https://blog.kct.ac.in/zigbee-technology/](https://blog.kct.ac.in/zigbee-technology/)\n3. [https://internetofthingsagenda.techtarget.com/](https://internetofthingsagenda.techtarget.com/)",
        "author_name": "Ashrith D R",
        "description": "",
        "slug": "zigbee-technology"
    },
    {
        "layout": "post",
        "title": "Cosmic Bit Flip",
        "author_github": "harishA3",
        "date": "2022-07-15T00:00:00",
        "description": "A bit flip is a change in a bit of the computer memory  from 0 to 1 or vice versa. Bit Flips can be caused by many factors,one of them bein Cosmic rays.This blog talks about Cosmic Bit Flips and why it is essential in the design of Avionic Systems.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Cosmic Bit Flip"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "harishA3",
        "comments": false,
        "text": "[{% load static %}]()\n# Cosmic Bit Flip\n\n## Abstract\n\nHas your laptop or PC ever crashed into a blue screen,just like the one shown below.\n\n![image]({% static '/blog/assets/img/cosmic-bit-flip/image-1.png' %})\n\nHave you ever wondered what the reason behind the system crash could be. Computers are employed everywhere today, and in some critical areas, such as a spacecraft, having the system crash could have quite unpleasant consequences.\nA likely reason for the system crash can be Cosmic Bit Flip. Spacecraft must be protected from Bit Flips to operate safely.\n\n## What is Cosmic Bit Flip?\n\nDigital data is stored as a sequence of 0s and 1s. Each 0 or 1 is called a bit. A bit-flip is an _inadvertent change_ of state of a bit that is different from its initial state.Cosmic ray is a highly energetic atomic nucleus or other particle travelling through space at a speed approaching that of light. Cosmic radiation from exploding stars outside our solar system can cause electronics to malfunction by flipping bits in a computer's memory,causing glitches.\n\n![image]({% static '/blog/assets/img/cosmic-bit-flip/image-2.png' %})\n\nCosmic Bit Flips, on rare occasions, cause a single bit of information to \u201cflip\u201d(single bit flip). This can be enough to force a computer to reboot, knock a passenger jet out of its autopilot mode\n\n## Single-Evnet Upset(SEU)\n\nA single event upset (SEU) is a bit flip in a memory element of a semiconductor device.The result of upsets is data corruption. Many systems can tolerate some level of soft errors(A temporary error that is not caused by a persistent hardware fault, typically being remedied by rebooting the system).\n  \nIn 2013, a speedrunner of the Super Mario 64 video game using the Nintendo 64 console experienced a glitch which teleported Mario higher up in the \"Tick Tock Clock\" stage. This has been hypothesized to have been caused by an SEU, flipping the least significant bit of Mario\u2019s height's most significant byte. Click [here](https://www.youtube.com/watch?v=bhBf5crp0i8) to watch the video for the same.\n\n## SEU in Aircraft Electronics\n\nOver the last ten years,the occurrence of single-event upset (SEU) in aircraft electronics has evolved from a series of interesting anecdotal incidents to accepted fact. Once avionics SEU was shown to be an actual effect, it had to be dealt with in avionics designs. The major concern is in random access memories (RAMs), both static (SRAMs) and dynamic (DRAMs), because these microelectronic devices contain the largest number of bits, but other parts, such as microprocessors, are also potentially susceptible to upset.The most common way of dealing with SEU in RAM\u2019s is by means of error detection and correction (EDAC). \n\nToday, a number of commercially available computer systems for upgrading military aircraft to incorporate EDAC in their designs are available. This trend of building EDAC directly into the design is likely to continue as larger quantities of memory (from megabits to gigabits) are incorporated into avionic(electronic equipment fitted in an aircraft) systems.  \n\n## Fatal Crashes of the Boeing 737 MAX \n\nOctober 29, 2018: Lion Air Flight 610, a 737 MAX 8, on a flight from Jakarta, Indonesia to Pangkal Pinang, Indonesia, crashed into the sea 13 minutes after takeoff, with 189 people on board the aircraft: 181 passengers (178 adults and three children), as well as six cabin crew and two pilots. All on board died.\n\n![image]({% static '/blog/assets/img/cosmic-bit-flip/lionairflightcrash.png' %})\n\nMarch 10, 2019: Ethiopian Airlines Flight 302, a 737 MAX 8, on a flight from Addis Ababa, Ethiopia to Nairobi, Kenya, crashed six-minutes after takeoff; all 157 people aboard (149 passengers and 8 crew members) died. The plane was only four months old at the time of the accident.\n\n![image]({% static '/blog/assets/img/cosmic-bit-flip/EthiopianAirlinesFlight302Crash.png' %})\n\nAfter two deadly crashes of Boeing\u2019s 737 MAX and the ensuing heavy criticism of the FAA for its limited oversight of the jet\u2019s original certification, the agency has been reevaluating and recertifying Boeing\u2019s updated flight-control systems.\n\n## Boeing 737 MAX Redesign\n\nBoeing announced on June 26,2019 that a new potential flaw had been discovered on the MAX \u2014 this time in a microprocessor in the jet\u2019s flight-control computer. the specific fault that showed up has \u201cnever happened in 200 million flight hours on this same flight-control computer in [older model] 737 NGs.\u201d\n\nIn sessions in a Boeing flight simulator in Seattle, two FAA engineering test pilots, typically ex-military test pilots, and a pilot from the FAA\u2019s Flight Standards Aircraft Evaluation Group (AEG), typically an ex-airline pilot, set up a session to test 33 different scenarios that might be sparked by a rare, random microprocessor fault in the jet\u2019s flight-control computer. This was standard testing that\u2019s typically done in certifying an airplane, but this time it was deliberately set up to produce specific effects similar to what happened on the Lion Air and Ethiopian flights.\n\nThe fault occurs when bits inside the microprocessor are randomly flipped from 0 to 1 or vice versa(Bit Flip). This is a known phenomenon that can happen due to cosmic rays striking the circuitry. Electronics inside aircraft are particularly vulnerable to such radiation because they fly at high altitudes and high geographic latitudes where the rays are more intense.\n\nfor example, a value of 1 on a single bit might indicate that the jet\u2019s wing flaps are up, while a 0 would mean they are down. A value of 1 on a different bit might tell the computer that the MAX\u2019s problematic flight-control system called MCAS(Maneuvering Characteristics Augmentation System) is engaged, while a 0 would indicate it is not. FAA regulations require that Bit Flips be accounted for in the design of all critical electronics on board aircraft.\n\nDuring the tests, 33 different scenarios were artificially induced by deliberately flipping five bits on the microprocessor, an error rate determined appropriate by prior analysis. For these simulations, the five bits flipped were chosen in light of the two deadly crashes to create the worst possible combinations of failures to test if the pilots could cope.\n\nIn one scenario, the bits chosen first told the computer that MCAS was engaged when it wasn\u2019t. This had the effect of disabling the cut-off switches inside the pilot-control column, which normally stop any uncommanded movement of the horizontal tail if the pilot pulls in the opposite direction.\n\nA second bit was chosen to make the horizontal tail, also known as the stabilizer, swivel upward uncommanded by the pilot, which has the effect of pitching the plane\u2019s nose down. Other bits were flipped to add three more complications.\nEven though the MCAS system that pushed the nose down on the two crash flights had not been activated, these changes in essence gave the FAA test pilots in the simulator an emergency situation similar to what transpired on those flights.\n\nBoeing could have just rewritten the software governing what functions are monitored within the flight-control computer to eliminate this failure scenario. Instead, it\u2019s decided to make a much more radical software redesign, one that will not only fix this problem but make the MAX\u2019s entire flight-control system \u2014 including MCAS \u2014 more reliable, according to three sources. This change means the flight-control system will take input from both of the airplane\u2019s flight computers and compare their outputs. This goes beyond what Boeing had previously decided to do, which is to adjust the MCAS software so that it took input from two angle of attack sensors instead of one.\n\n## Conclusion\n\nCosmic Bit Flips need to be considered during the desinging of Avionic Systems. The aircraft softwares need to be built in way such that the probable number of bit flips that can occur in the aircraft software should not affect it's flight.\n  \nEDAC Systems need to be incorported into avionics to prevent SEUs(Single event upsets)in the RAM or the microprocessor installed in the airplane. There is a need for Stringent testing of airplanes to prevent crashes.\n\n## Recourses\n\n- [What are bit flips and how are spacecraft protected from them](https://www.scienceabc.com/innovation/what-are-bit-flips-and-how-are-spacecraft-protected-from-them.html#:~:text=Cosmic%20Bit%2DFlip&text=Again%2C%20as%20the%20name%20suggests,been%20stripped%20of%20their%20electrons)\n- [software redesign of Boeing\u2019s 737 MAX flight controls](https://www.seattletimes.com/business/boeing-aerospace/newly-stringent-faa-tests-spur-a-fundamental-software-redesign-of-737-max-flight-controls/)\n- [List of accidents and incidents involving the Boeing 737](https://en.wikipedia.org/wiki/List_of_accidents_and_incidents_involving_the_Boeing_737)",
        "author_name": "Harish R Kale",
        "slug": "cosmic-bit-flip"
    },
    {
        "layout": "post",
        "title": "Exploring Web Exploitation Techniques",
        "author_github": "kavyabhat02",
        "date": "2022-07-15T00:00:00",
        "image": "/assets/img/",
        "description": "Solving labs based on web exploitation techniques, and discussing methods of prevention",
        "tags": [
            "IEEE NITK",
            "CompSoc"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "kavyabhat02",
        "text": "[{% load static %}]()\n# What is Web Exploitation? \n\nWeb exploitation involves taking advantage of flaws or weaknesses that are present in a web application. When an application uses data inputs that are not sanitized or validated, a hacker can easily exploit it and compromize the security of the web application.\n\nWhat parts of a web application are susceptible to attack? A web application usually has a web server, an application server and a database. Any of these components can be exploited if they are not secure. With websites handling sensitive data regarding users across the world, it is important to recognize vulnerabilities in the setup and guard against them. \n\nThere are several different web exploitation techniques. Here, we take a look at SQL injection and cross site scripting labs, and methods of guarding against these attacks.\n\n## SQL Injection\n\nSQL Injection is a common web vulnerability that occurs when an application makes a modifiable and insecure query to its database. This allows attackers to view data which they should typically be unable to access. \n\n[SQL Injection - Portswigger Lab](https://portswigger.net/web-security/sql-injection/lab-retrieve-hidden-data)\n\nLet\u2019s take a look at the above lab to get a better understanding of SQL injection. \n\nNote: You\u2019ll need a Portswigger account in order to solve the lab. Burpsuite is another helpful tool that you can use to scan web applications for vulnerabilities. \n\nThe goal here is to obtain the details of all released and unreleased products.\nNavigate to the proxy tab on Burpsuite, and open Burp\u2019s browser. Once you access the lab using the above link, turn on the \u2018intercept\u2019 option in Burpsuite. This will intercept the HTTP request to the server, and allow you to examine the manner in which requests and data are handled.\n\nNow that intercept is on, try choosing one of the categories from the different options listed(shown here):\n\n![On accessing the lab]({% static '/blog/assets/img/exploring-web-exploitation/1.png' %})\n\nThe request is intercepted by Burpsuite, and you can now view the details of the request \u2013 the first line mentions the category. Notice that the data is not validated in any way. Is it possible to insert something here that would return details of unreleased items too?\n\n![Intercepting request with Burpsuite]({% static '/blog/assets/img/exploring-web-exploitation/2.png' %})\n\n1=1 is a condition that always evaluates to true. We can include that along with the category option using the OR operator in SQL. Now, the query is modified to select all items, as even if the category is different, the expression returns true since we have appended the \u20181=1\u2019 condition.\n[The hyphens (--) that you see in the screenshot are a comment indicator in SQL, which comments out the remainder of the query.]\n\n![Modified request]({% static '/blog/assets/img/exploring-web-exploitation/3.png' %})\n\nForward this request to the server, and navigate back to the browser. You\u2019ll notice that all the products are listed, and the lab is solved!\n\n![Result of request]({% static '/blog/assets/img/exploring-web-exploitation/4.png' %})\n\nAnother option is to directly modify the URL parameter from \n> https://ac461ff61fdf9067c04e58b1007e0038.web-security-academy.net/filter?category=Corporate+gifts\n\nto that given below:\n\n> https://ac461ff61fdf9067c04e58b1007e0038.web-security-academy.net/filter?category=Corporate+gifts%27+OR+1=1--\n\n \n## Preventing SQL Injection Attacks\nSQL injection is very common, due to the fact that it targets the database - where most sensitive information is stored. \nThere are a few simple methods to avoid SQL injection attacks in your application:\n\n### Stored Procedures\nStored procedures are codes that can be stored and used many times. The SQL statements are automatically parameterized, and the procedure is called instead of writing the same query over and over.\n\n### Parameterized Queries\nIn this method, the SQL statements are precompiled and the user input is supplied as parameters. With parameterized queries, the entire user input is quoted, so that the intent or behaviour of the SQL statement cannot be changed. \n\n## Cross Site Scripting (XSS)\nXSS attacks exploit vulnerable websites by injecting malicious scripts into responses to web requests. An attacker sends malicious code or scripts along with seemingly benign data requests.\n \nThere are two main types of XSS attacks: reflected and stored.\n\n### Reflected XSS\nReflected XSS is non-persistent, and one of the simplest types. It arises when an application accepts data through a request, and immediately uses the data (without validation) in the response to the user. \n\nOn to the next lab:\n[Reflected XSS - Portswigger Lab](https://portswigger.net/web-security/cross-site-scripting/reflected/lab-html-context-nothing-encoded)\n\nThis lab requires us to perform an XSS attack that calls the alert function. When we access the lab, we find that there are only a few articles, and a search bar. The immediate approach is to enter data  by searching the blog. Enter some random text (for example, hello), and observe how the request is handled in the URL. This is what we see:\n\nhttps://ac601fcb1e00eb94c0642475006600b7.web-security-academy.net/?search=hello\n \nInteresting. The data we provide is directly passed to the request without validation. Maybe we can embed javascript using script tags in the search box. \nOn entering the text as shown:\n\n![Script in search]({% static '/blog/assets/img/exploring-web-exploitation/5.png' %})\n\nit executes an alert \u2013 and we\u2019ve solved this lab. \n\nHere, the alert() function is fairly harmless, and used without malicious intent. If an attacker found this vulnerability in a web application and performed an XSS attack, it could change the behaviour of the application entirely. \n\n### Stored XSS\nStored XSS is a persistent form of attack. Stored attacks occur when the web application receive data from requests, and include this data in responses later on. \n\nLet\u2019s try another lab, based on stored XSS:\n[Stored XSS - Portswigger Lab](https://portswigger.net/web-security/cross-site-scripting/stored/lab-html-context-nothing-encoded)\n\n\nThe instructions say to submit a comment, which executes an alert whenever the blog is viewed. This tells us that it is a form of stored XSS, because the contents are stored once submitted and loaded each time.\n\nEach time the blog is viewed, the comments are also loaded. Select any of the blogs listed, and submit a comment. However, instead of plaintext, let us provide an alert() function within script tags: \n\n![Comment]({% static '/blog/assets/img/exploring-web-exploitation/6.png' %})\n\nPost your comment, and the lab is solved. When you click on \u201cBack to Blog\u201d, your alert is executed and a pop-up appears:\n\n![Result]({% static '/blog/assets/img/exploring-web-exploitation/7.png' %})\n\n## Preventing XSS Attacks\nTo prevent XSS attacks, try to make sure that dynamic content can't be used to inject Javascript in the application.\n\n### Whitelisting values\nIf some item has only a handful of valid values, make a list of the known 'safe' values, and deny permission to all other input. One example is asking the user to select their country from a drop-down menu, rather than allowing them to enter it in a text field.\n\n### Escaping Dynamic Content\nUsually, an application does not need to obtain raw HTML input from its users. In order to treat user input as contents of an HTML tag rather than raw HTML, we need to escape all the dynamic content provided. \nThis process consists of replacing some important characters with the respective HTML entity encoding, for example: \n> \" to &#34\\\n  ; to &#59\n\nThese are only a few types of web exploitation that can occur. There are many other methods of exploiting web vulnerabilities, such as denial-of-service (DoS), client-side request forgery(CSRF), and server-side request forgery(SSRF), to name a few. \n\n## References\n- [Portswigger - SQL Injection](https://portswigger.net/web-security/sql-injection)\n- [OWASP - Prevention of SQL Injection](https://cheatsheetseries.owasp.org/cheatsheets/SQL_Injection_Prevention_Cheat_Sheet.html)\n- [Portswigger - Cross site scripting](https://portswigger.net/web-security/cross-site-scripting)\n- [Reflected XSS](https://www.imperva.com/learn/application-security/reflected-xss-attacks/)",
        "author_name": "Kavya Bhat",
        "slug": "exploring-web-exploitation-techniques"
    },
    {
        "layout": "post",
        "title": "Text Generation using RNN",
        "author_github": "ChinmayaSharma-hue",
        "date": "2022-07-15T00:00:00",
        "description": "A text generation model that trains over a book and generates text in the style of writing of the book.",
        "tags": [
            "IEE NITK",
            "Blog",
            "Deep Learning",
            "Sequential Models",
            "RNNs",
            "LSTMs"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "ChinmayaSharma-hue",
        "text": "[{% load static %}]()\n# Text Generation using RNN\nLong-short-term memory models, or LSTMs, employ gates to control the flow of information \nto tackle the problem of short-term memory. These models feature procedures for deciding \nwhether or not to maintain the information, allowing them to keep vital data for a long period. \nThey are widely employed in machine translation, speech recognition, handwriting recognition \nand creation, language modeling and translation, speech synthesis, and many other deep \nlearning applications due to their capacity to learn long-term dependencies.\n\nUsing LSTM, we can build a model and train it to generate text in the style of the text used\nto train the model.\n\nTo build, train and run the model, the following things need to be done.\n1. Data Pre-processing involves the following steps,\n    - Take a novel or any other form of literature in the form of a text file, and load it into the program in the form of a string. By assigning numbers to each character, we can represent the text in the form of numbers so that it can be passed through the model. This is what is done in tokenization. \n    - Make mini-batches out of the pre-processed data for batch learning. \n2. Define the LSTM model that can be trained on the data, and then train the model for a desired number of epochs. Tune the hyperparameters to get the best result out of the model.\n3. Define a predict function that instantiates the model, takes in the argument string (the initial word that the user provides), and then predicts the desired number of characters succeeding the initial word.\n\nAll in all, the data is pre-processed to be represented in the form of numbers, then the model is trained on this data in the form of mini-batches, and then a prompt word is sent to the model which then predicts the next character based on the data it has been trained on.\n\n### Import Resources\n```buildoutcfg\nimport numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\n```\n\n### Load in Data\n```buildoutcfg\n# open text file and read in data as `text`\nwith open('data/anna.txt', 'r') as f:\n    text = f.read()\n```\nThe novel Anna Karenina is used to train this character-wise RNN. That needs to be loaded. All the characters from the text file are loaded onto the string, txt.\n\n### Tokenization\n```buildoutcfg\nchars = tuple(set(text))\nint2char = dict(enumerate(chars))\nchar2int = {ch: ii for ii, ch in int2char.items()}\n\n# encode the text\nencoded = np.array([char2int[ch] for ch in text])\n```\n1. ``set(text)`` casts the string datatype which is ``text`` into a set, which removes all duplicate characters, and then this is cast into a tuple.\n2. `enumerate(chars)` assigns a number for each character in the tuple, which is used as the numerical equivalent of each character to be sent through the model after encoding, and this is cast into a dictionary as casting it into a list gives a list of tuples with the number and the corresponding character. This gives a dictionary mapping from numbers to characters.\n3. Through dictionary comprehension, another dictionary is created which gives a mapping from character to number. Using these two dictionaries, it is possible to obtain the number equivalent for a character and vice versa\n4. The characters in the text string are encoded by obtaining their numerical equivalent from the character to integer dictionary, which is then cast into a `numpy` array.\n\n### Pre-processing the data\nTo send the data through the model, each input needs to be one-hot encoded,\n```buildoutcfg\ndef one_hot_encode(arr, n_labels):\n    \n    # Initialize the encoded array\n    one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)\n    \n    # Fill the appropriate elements with ones\n    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n    \n    # Finally reshape it to get back to the original array\n    one_hot = one_hot.reshape((*arr.shape, n_labels))\n    \n    return one_hot\n```\n1. In `one_hot = np.zeros((arr.size, n_labels), dtype=np.float32)`, a zero matrix is created that has **as many rows as the number of elements in the encoded array**, each row having as many elements as the number of labels. Each row can be changed to a one-hot vector for each of the corresponding elements in the encoded array. \n2. In `one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1`, \n    1. `np.arange(one_hot.shape[0])`, \n        1. `one_hot.shape[0]` gives the number of rows in the one-hot vector created in the last step,\n        2. `np.arange()` is used to obtain evenly spaced values in a given interval. For eg., `np.arange(3)` gives `[0,1,2]`. As the argument given is the number of elements in the encoded array/number of rows in the one-hot vector, it gives a list of integers from 0 to that number. \n    2. `arr.flatten()` returns a copy of the array collapsed into one dimension. Therefore, the output will have a single dimension, i.e., a single row with all the elements of the matrix in that row.\n    \n    Through indexing, the first argument is used to invoke each of the rows, the second argument is used to invoke the element in that row which has an index equal to the numerical values in $arr$, which is supposed to be the encoded array when this function is used for one-hot encoding.\n    \n3. In `one_hot = one_hot.reshape((*arr.shape, n_labels))` , the shape of the matrix with a one-hot vector is reshaped to the shape of the encoded array.\n\n### Making Training mini-batches\n```buildoutcfg\ndef get_batches(arr, batch_size, seq_length):\n    '''Create a generator that returns batches of size\n       batch_size x seq_length from arr.\n       \n       Arguments\n       ---------\n       arr: Array you want to make batches from\n       batch_size: Batch size, the number of sequences per batch\n       seq_length: Number of encoded chars in a sequence\n    '''\n    \n    batch_size_total = batch_size * seq_length\n    # total number of batches we can make\n    n_batches = len(arr)//batch_size_total\n    \n    # Keep only enough characters to make full batches\n    arr = arr[:n_batches * batch_size_total]\n    # Reshape into batch_size rows\n    arr = arr.reshape((batch_size, -1))\n    \n    # iterate through the array, one sequence at a time\n    for n in range(0, arr.shape[1], seq_length):\n        # The features\n        x = arr[:, n:n+seq_length]\n        # The targets, shifted by one\n        y = np.zeros_like(x)\n        try:\n            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n        except IndexError:\n            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n        yield x, y\n```\n1. **The first thing we need to do is discard some of the text so we **only have full mini-batches**.**\n    1. `batch_size_total` which is the total batch size is the product of the number of sequences \n       in a batch and the number of elements in each sequence is calculated first.\n    2. `n_batches`  which is used to calculate how many batches can be made from the data \n       by getting an integer as the quotient of the division of the total size of the data \n       and the number of elements in each batch.\n    3. The data is redefined to contain only as many elements as can be used to make the \n       required number of batches. The elements, in the end, are omitted.\n2. **After that, we need to split\u00a0`arr`\u00a0into\u00a0`N`\u00a0batches.**\n    1. `N` is the number of sequences in each batch which is represented by `batch_size` that\n       is sent to the function. This is taken as the first dimension of the data by reshaping\n       the data to divide it into as many rows as the number of sequences. \n3. **Now that we have this array, we can iterate through it to get our mini-batches.** <br>\n       Since this is a generator function, we can iterate over the data and send a single \n       minibatch each time. We can use the range function for the division of the data into mini-batches. We can set a variable $n$ that ranges between 0 and the total number \n       of elements in each (full) sequence/each 'batch', and increments by the sequence \n       length, `seq_length` .\n### Defining the network\n```buildoutcfg\nclass CharRNN(nn.Module):\n    \n    def __init__(self, tokens, n_hidden=256, n_layers=2,\n                               drop_prob=0.5, lr=0.001):\n        super().__init__()\n        self.drop_prob = drop_prob\n        self.n_layers = n_layers\n        self.n_hidden = n_hidden\n        self.lr = lr\n        \n        # creating character dictionaries\n        self.chars = tokens\n        self.int2char = dict(enumerate(self.chars))\n        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n        \n        ## define the LSTM\n        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n                            dropout=drop_prob, batch_first=True)\n        \n        ## define a dropout layer\n        self.dropout = nn.Dropout(drop_prob)\n        \n        ## define the final, fully-connected output layer\n        self.fc = nn.Linear(n_hidden, len(self.chars))\n      \n    \n    def forward(self, x, hidden):\n        ''' Forward pass through the network. \n            These inputs are x, and the hidden/cell state `hidden`. '''\n                \n        ## Get the outputs and the new hidden state from the lstm\n        r_output, hidden = self.lstm(x, hidden)\n        \n        ## pass through a dropout layer\n        out = self.dropout(r_output)\n        \n        # Stack up LSTM outputs using view\n        # you may need to use contiguous to reshape the output\n        out = out.contiguous().view(-1, self.n_hidden)\n        \n        ## put x through the fully-connected layer\n        out = self.fc(out)\n        \n        # return the final output and the hidden state\n        return out, hidden\n    \n    \n    def init_hidden(self, batch_size):\n        ''' Initializes hidden state '''\n        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n        # initialized to zero, for hidden state and cell state of LSTM\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n        \n        return hidden\n```\n1. In the `_init_` method,\n    1. The arguments that are taken in the `_init_` method are tokens which are the inputs, \n       `n_hidden` which is the dimension of the hidden state, `n_layers` which is the \n       number of LSTM layers that are stacked upon one another, `drop_prob` which is the \n       dropout probability to be used in dropout layer, `lr` which is the learning rate used\n       in training.\n    2.  The `super().__init__()` **super** corresponds to the methods and attributes of\n        the parent class, it is used to initialize the methods and attributes of the parent \n        class `(nn.Module())`.\n    4. The character dictionaries are created to convert characters to integers and vice\n       versa. \n    5. The LSTM layer is defined as a network parameter.\n    6. The dropout layer is defined using the dropout parameter.\n    7. A fully connected layer, which is the last layer, is defined, with the arguments as \n       the input dimension which is the hidden state dimension which is the output of the \n       LSTM layers, and the output dimension which is the same as the dimension of the \n       inputs passed into the network.\n2. In the `forward` method,\n    1. The input and the initial hidden state are passed through the LSTM layer, and the \n       output and the final hidden state are obtained.\n    2. The output is then passed through the dropout layer.\n    3. The output is reshaped before it is passed through the fully connected layer, as the \n       output is of the dimension `(self.n_layers,batch_size,self.n_hidden)` and it is \n       squished. \n        \n    4. The reshaped output is passed through the fully connected layer to obtain the final \n       output.\n    5. The final output and the final hidden state are returned from this method. \n3. In the `init_hidden` method,\n    1. The hidden tuple which consists of the hidden state and the cell state is initialized to zero.\n\n### Training the model\n```buildoutcfg\ndef train(net, data, epochs=10, batch_size=10, seq_length=50, lr=0.001, clip=5, val_frac=0.1, print_every=10):\n    ''' Training a network \n    \n        Arguments\n        ---------\n        \n        net: CharRNN network\n        data: text data to train the network\n        epochs: Number of epochs to train\n        batch_size: Number of mini-sequences per mini-batch, aka batch size\n        seq_length: Number of character steps per mini-batch\n        lr: learning rate\n        clip: gradient clipping\n        val_frac: Fraction of data to hold out for validation\n        print_every: Number of steps for printing training and validation loss\n    \n    '''\n    net.train()\n    \n    opt = torch.optim.Adam(net.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    \n    # create training and validation data\n    val_idx = int(len(data)*(1-val_frac))\n    data, val_data = data[:val_idx], data[val_idx:]\n    \n    if(train_on_gpu):\n        net.cuda()\n    \n    counter = 0\n    n_chars = len(net.chars)\n    for e in range(epochs):\n        # initialize hidden state\n        h = net.init_hidden(batch_size)\n        \n        for x, y in get_batches(data, batch_size, seq_length):\n            counter += 1\n            \n            # One-hot encode our data and make them Torch tensors\n            x = one_hot_encode(x, n_chars)\n            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n            \n            if(train_on_gpu):\n                inputs, targets = inputs.cuda(), targets.cuda()\n\n            # Creating new variables for the hidden state, otherwise\n            # we'd backprop through the entire training history\n            h = tuple([each.data for each in h])\n\n            # zero accumulated gradients\n            net.zero_grad()\n            \n            # get the output from the model\n            output, h = net(inputs, h)\n            \n            # calculate the loss and perform backprop\n            loss = criterion(output, targets.view(batch_size*seq_length).long())\n            loss.backward()\n            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n            nn.utils.clip_grad_norm_(net.parameters(), clip)\n            opt.step()\n            \n            # loss stats\n            if counter % print_every == 0:\n                # Get validation loss\n                val_h = net.init_hidden(batch_size)\n                val_losses = []\n                net.eval()\n                for x, y in get_batches(val_data, batch_size, seq_length):\n                    # One-hot encode our data and make them Torch tensors\n                    x = one_hot_encode(x, n_chars)\n                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n                    \n                    # Creating new variables for the hidden state, otherwise\n                    # we'd backprop through the entire training history\n                    val_h = tuple([each.data for each in val_h])\n                    \n                    inputs, targets = x, y\n                    if(train_on_gpu):\n                        inputs, targets = inputs.cuda(), targets.cuda()\n\n                    output, val_h = net(inputs, val_h)\n                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n                \n                    val_losses.append(val_loss.item())\n                \n                net.train() # reset to train mode after iterationg through validation data\n                \n                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n                      \"Step: {}...\".format(counter),\n                      \"Loss: {:.4f}...\".format(loss.item()),\n                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n```\n1. The model is set to training mode using `net.train()` which allows using the gradients\n   for backpropagation.\n2. The optimizer and the loss functions are defined, and the loss function is taken to be a cross-entropy loss.\n3. Some amount of data is set aside to calculate the validation loss at each epoch.\n4. Inside the for loop that iterates through the number of epochs,\n    1. The initial hidden state is obtained from the `init_hidden` method that sets all the\n       elements to zero.\n    2. Inside the for loop that is used to iterate over all the batches of data, \n       (`get_batches()` is a generator function so at each iteration the next batch \n       is used)\n        1. The input data is one-hot encoded using the function that has been already \n           defined,\n        2. The input and target arrays are cast into tensors to be sent through the PyTorch-defined network,\n        3. The hidden variable is redefined to **avoid backpropagating through the entire \n           training history**,\n        4. The accumulated gradients are set to zero,\n        5. The output and the next hidden state are obtained from passing the inputs and \n           the previous hidden state through the model,\n        6. The loss is calculated by calling the loss function and passing the outputs and \n           the target tensors as arguments, (As the outputs have the squished shape from \n           before, the targets are also squished to make the batch size and sequence length \n           into one dimension)\n        7. Backpropagation is done using the loss calculated,\n        8. As RNN has the problem of exploding gradients, they are clipped when they go \n           past a certain value that is defined as 5 here, using the \n           `clip_grad_norm_()` function that takes in the network parameters and the value \n           of the clip as arguments,\n        9. The optimizer is used to upgrade the weights by calling the `opt.step()` function,\n        10. When the counter value equals the number of steps for which the training details \n            need to be printed,\n            All the steps as training are done for the validation data, only \n            backpropagation and up-gradation of weights are not done. Note that the network \n            needs to be set to evaluation mode by `net.eval()` and then back to training \n            mode by `net.train()`. In evaluation mode, the gradients do not need to be used.\n            \n            All the training details are then printed.\n\n### Instantiating the model\n\n```buildoutcfg\nn_hidden=512\nn_layers=2\n\nnet = CharRNN(chars, n_hidden, n_layers)\n\nbatch_size = 128\nseq_length = 100\nn_epochs = 5 # start smaller if you are just testing initial behavior\n\n# train the model\n\ntrain(net, encoded, epochs=n_epochs, batch_size=batch_size, seq_length=seq_length, lr=0.001, print_every=10)\n```\n\n### Making Predictions\n```buildoutcfg\ndef predict(net, char, h=None, top_k=None):\n        ''' Given a character, predict the next character.\n            Returns the predicted character and the hidden state.\n        '''\n        \n        # tensor inputs\n        x = np.array([[net.char2int[char]]])\n        x = one_hot_encode(x, len(net.chars))\n        inputs = torch.from_numpy(x)\n        \n        if(train_on_gpu):\n            inputs = inputs.cuda()\n        \n        # detach hidden state from history\n        h = tuple([each.data for each in h])\n        # get the output of the model\n        out, h = net(inputs, h)\n\n        # get the character probabilities\n        p = F.softmax(out, dim=1).data\n        if(train_on_gpu):\n            p = p.cpu() # move to cpu\n        \n        # get top characters\n        if top_k is None:\n            top_ch = np.arange(len(net.chars))\n        else:\n            p, top_ch = p.topk(top_k)\n            top_ch = top_ch.numpy().squeeze()\n        \n        # select the likely next character with some element of randomness\n        p = p.numpy().squeeze()\n        char = np.random.choice(top_ch, p=p/p.sum())\n        \n        # return the encoded value of the predicted char and the hidden state\n        return net.int2char[char], h\n```\n### Priming and Generating Text\n```buildoutcfg\ndef sample(net, size, prime='The', top_k=None):\n        \n    if(train_on_gpu):\n        net.cuda()\n    else:\n        net.cpu()\n    \n    net.eval() # eval mode\n    \n    # First off, run through the prime characters\n    chars = [ch for ch in prime]\n    h = net.init_hidden(1)\n    for ch in prime:\n        char, h = predict(net, ch, h, top_k=top_k)\n\n    chars.append(char)\n    \n    # Now pass in the previous character and get a new one\n    for ii in range(size):\n        char, h = predict(net, chars[-1], h, top_k=top_k)\n        chars.append(char)\n\n    return ''.join(chars)\n```\n```buildoutcfg\nprint(sample(net, 1000, prime='Anna', top_k=5))\n```\nGenerated text,\n![image](https://github.com/ChinmayaSharma-hue/Image_repo/blob/main/Untitled.png?raw=true)\n\nAs you can see, the generated text has some gibberish words, but all in all, it reflects a sort \nof efficiency of RNNs in generating text through sequential learning.",
        "author_name": "Chinmaya Sharma",
        "slug": "text-generation-using-rnn"
    },
    {
        "layout": "post",
        "title": "Introduction to 3D Biometrics",
        "author_github": "gayatri1705",
        "date": "2022-07-21T00:00:00",
        "description": "Biometrics is the science of recognizing the identity of a person based on the physical or behavioral attributes of the individual such as the face, fingerprints, etc. This blog gives more insight about 3D biometrics.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "3D Biometrics"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "gayatri1705",
        "text": "[{% load static %}]()\n# 3D Biometrics\n\n## Abstract \n\nWith the development in technology, automatic personal authentication is being used in many fields such as security, forensics, banking, etc. Many kinds of authentications techniques are available based on the different biometric characteristics. Mostly all the developed techniques use 2d Biometrics. Recently, the method of 3D biometrics for the use of authentication has been in use. Some of them are the face, palm, and fingerprint recognition.\n\n## What is 3D Biometrics?\nBiometrics is the science of identifying a person based on the physical features of a person. There are 2 types of biometrics: 2D and 3D. In 2D biometrics, the data acquired is 2D. 2D data does not construct a depth map instead, it captures a 2D image and stores it as the data. In 3D biometrics, a detailed 3D cloud map of the target features is made. \n\nThe newly developed 3D biometrics could overcome the limitations existing in 2D. Compared to 2D; much more geometric information can be contained in 3D, which can substantially improve the accuracy. For this reason, many researchers have turned their attention to this newly developing field.\n\nA general flow diagram for 3D biometrics is shown:\n\n![IMG 1](https://user-images.githubusercontent.com/78913478/166979211-82a499f8-77c3-4b21-8b39-91f79e4baf1a.png)\n\n\nThe workflow can be split up into 2 phases and 5 stages.\n\nIn the first phase, the 3D data is acquired and pre-processed to obtain clean 3D faces. Then the acquired data is processed to find features that could be used to differentiate faces. This is done by extraction algorithms. The features are then stored in each feature database. Then comes the testing phase. Where the acquisition, pre-processing, and extraction happened, were almost similar to the first stage. Next is the feature matching stage, where the features are matched with the feature database. If the matching percentage is sufficiently high, then we say that the face is matched.\n\n## 3D face acquisition\nFor the acquisition of face samples, special hardware is required. This hardware can be classified into 2 parts; Active and Passive Systems according to the technology used.\n\nThe active system emits nonvisible infrared light beams and measures their reflection. The 3D face can be constructed using this reflected data. According to the different illumination methods, the active system can be further classified into triangular or structured light-based.\n\nIn the triangular method, the emitting and receiving angles are measured. Using that data, the exact point of reflection is calculated. Using this process, a precise map is formed with many points. This is a high-precision method, but the only disadvantage is that it is slow. Therefore, it would be required for the target person to hold still for a minute. Therefore, this method can\u2019t be used for video authentication.\n \nCompared to triangular systems, structured light-based systems are more popular on consumer levels. In this method, a system emits a light grid. It then measures the deformation of that light grid to measure the surface structure. This method is much faster than the triangular method but is not as accurate as compared to triangular systems. \n\n## Preprocessing\nThe acquired data cannot be directly used for the authentication, because there are many extra features such as hair, ear, and eyeglasses. When we humans want to identify each other, these extra features and jewelry can be helpful. Although these features would be helpful for humans to identify each other, they would serve as a distraction or mislead the 3D face recognition system and hence should be removed. This is done through pre-processing.  \n\nThe first step of pre-processing is to detect the correct position and orientation of the human face. Different algorithms are used to turn the face to directly face the camera axis. Then clearly identifiable features such as the nose, and eyes, are used to isolate the human face area out of these distracting features, this operation is called segmentation.\n\nThere are different model formats such as depth image, point cloud, and mesh.\n\n![IMG 2](https://user-images.githubusercontent.com/78913478/166979331-d3338e88-a975-41db-b399-48c571ebd913.png)\n\n## Feature extraction, feature database, and feature matching\nOne of the most straightforward way of feature extraction is to take the entire face as a single feature vector. This is called the Global approach. In this approach, the entire face is stored in the database. In the matching stage, the target faces are compared with the database statistical classification functions. \n\nApart from the Global approach, the other one is called the Component-based approach. In this approach, important facial characteristics such as the eyes, and nose are extracted. These recognizable features are stored in the database. During the matching process, the features on the target face are compared with the database. \n\nThere are also hybrid approaches that combine both these methods to provide greater accuracy and security. \n\n\n## Newer Technologies \n\nEven as the methods of these biometric methods are still developing, a new method of 3D biometric using finger veins is being developed. Such type of authentication which uses unique anatomical features is soon replacing tradition methods of security. \n\nAlthough this new technique involving finger veins is being developed, it is based on 3D biometrics. The addition data and depth given by 3D images will make it more difficult to fake an identity. \n\nTo accomplish this 3D biometrics involving finger veins, a new method called photoacoustic tomography, an imaging technique that combines light and sound id being developed. In this method, a light source is used to illuminate the target. When the light hits the target surface, it creates a unique sound. This sound is then detected by a sensor, then a 3D image is constructed using this data. \n\n## Resources and links\n\n- [3D Biometric Authentication Based on Finger Veins Almost Impossible to Fool](https://www.optica.org/en-us/about/newsroom/news_releases/2020/3d_biometric_authentication_based_on_finger_veins/)\n- [3D face recognition: a survey](https://hcis-journal.springeropen.com/articles/10.1186/s13673-018-0157-2#Sec1)",
        "author_name": "Gayatri Kattimani",
        "slug": "introduction-to-3d-biometrics"
    },
    {
        "layout": "post",
        "title": "Powering EV in India",
        "author_github": "himanshu",
        "date": "2022-07-21T00:00:00",
        "description": "Electric vehicles sales have almost quadrupled in the last few years. This has become more evident with the steep decline in the batteries prices along with the EV friendly policies that have been introduced as a countermeasure to tighten the fuel emission standards. This has set up a bar for EV in the Indian Market with a backing from rising fuel prices.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Powering EV in India"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "himanshu",
        "comments": false,
        "text": "[{% load static %}]()\n# Powering EV in India\n\n## Background\n\nIf it is one fact that the entire world has understood in the last few years, it is the fact that we need to start making amends so that the environment that we have destroyed so much has some scope of recuperating. With almost all developed nations now paving the way towards a clean mobility revolution, India has also begun to make the needed changes.\n\n## EV in Indian Markets!!!\n\nTesla and OLA are yet to sell a single vehicle in India, but they have managed to whip up much excitement already. Earlier this year Tesla had asked the Indian government to lower import duties on fully imported EVs \u2013 they are currently taxed at the same rate as IC engine cars.\n\nNotably, Hyundai too backed this request, but OLA feels the government should not do so, with CEO Bhavish Aggarwal saying we should \u201chave confidence in our ability to build indigenously and also attract global OEMs to build in India, not just import.\u201d\n\nNaturally, this has sparked much nationalistic debate about allowing a foreign player easy entry into our market. After all, why should outside firms be allowed an easier entry when they should instead \u2018Make in India\u2019. And this is a very valid point, as we must continue to increase our manufacturing economy; luxury brands like Mercedes and Audi have for long been making cars in India and so this isn\u2019t something new.\n\nHowever, getting brands to make their goods in India isn\u2019t as easy as putting out a tweet or statement asking them to do so. The company\u2019s background and size, the volume of local market demand, export potential, current manufacturing feasibility and support infrastructure and many other factors have to be considered. This holds true for EVs too, as they use completely different propulsion technology and require a completely different support ecosystem.\n\n## Rise of EV in India\n\nThus, to really kick-start EV production in India, a tweaked import policy with lower duties could be just what the doctor ordered. And speaking of doctors, Dr. Pawan K. Goenka, former Mahindra MD \u2013 and according to his Twitter bio, \u2018committed to Aatmanirbhar Bharat\u2019 \u2013 also voiced his support, saying that he thinks the government should consider Tesla\u2019s request and lower the 100 percent slab to 60 percent for EVs. For the record, there are two slabs \u2013 import duties on cars priced below $40,000 is 60 percent, and it is 100 percent for cars priced above $40,000. Tesla had asked for a reduction to 40 percent.\n\nAs stated earlier, Hyundai too backed Tesla's request, with the company MD and CEO, S S Kim, saying he thinks that with lower duties and more investment on charging infrastructure we can \"in about two years reach a meaningful point in terms of scale for EV production\". Being a large local manufacturer, Hyundai\u2019s backing should also be taken seriously.\n\nAlso, interestingly, the government already offers some import concessions: it allows an import of up to 2,500 units per year without them having to be homologated in India, provided of course that they are RHD and homologated in other recognised countries. This is done to make it easier for international automakers to test-market their vehicles in India, and it also offers customers a wider choice of high-end vehicles.\n\n## Conclusion\n\nA volume-based concession or perhaps even a time-based one, or a combination of both, can easily be thought of in the case of EVs as well. These small concessions won't kill local automakers, but it can give a fillip to the EV movement, which only requires some concerted planning and effort rather than unilateral statements. In the end, India will be a global EV superpower not because we said so, but because we cleverly planned and did so.\n\n## Resources\n\n- [https://www.etrio.in/blog/revolution-of-the-EV-sector.html](https://www.etrio.in/blog/revolution-of-the-EV-sector.html)\n- [http://electric-vehicles-in-india.blogspot.com/?m=1](http://electric-vehicles-in-india.blogspot.com/?m=1)\n- [India's Electric Vehicles Sales Trend for 2021](https://evreporter.com/ev-sales-trend-in-india-in-2021/#:~:text=In%202021%2C%203%2C29%2C190%20electric,sales%20of%201%2C22%2C607%20units.&text=With%20a%20combined%20market%20share,the%20most%20popular%20electric%20vehicles)",
        "author_name": "Himanshu S Garud",
        "slug": "Powering-EV-in-India"
    },
    {
        "layout": "post",
        "title": "Trees in Competitive Programming",
        "author_github": "Dhruvil-Lakhtaria",
        "date": "2022-07-21T00:00:00",
        "image": "/assets/img/",
        "description": "An introduction to Trees and Competitive Programming with reference to Trees.",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Competitive Programming",
            "Trees",
            "Data Structures"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "Dhruvil-Lakhtaria",
        "use_math": true,
        "text": "[{% load static %}]()\n## Introduction\n\nA tree is a nonlinear hierarchial data structure that consist of nodes connected by edges.\n\n### Why Trees?\n\nOther data structures such as arrays, linked list, stack, and queue are linear data structures that store data sequentially. In order to perform any operation in a linear data structure, the time complexity increases with the increase in the data size. But, it is not acceptable in today's computational world.\n\nDifferent tree data structures allow quicker and easier access to the data as it is a non-linear data structure.\n  \n### Tree Terminologies\n\n **1. Node**:\n A node is an entity that contains a key or value and pointers to its child nodes. \nThe last nodes of each path are called  **leaf nodes or external nodes**  that do not contain a link/pointer to child nodes. \nThe node having at least a child node is called an  **internal node**.\n\n**2. Edge** :\n\tIt is the link between any two nodes.\n\t\n**3. Root**:\n\tIt is the topmost node of a tree.\n\t\n**4. Height and Depth of a Node**:\n\tThe height of a node is the number of edges from the node to the deepest leaf (ie. the longest path from the node to a leaf node).\n\tThe depth of a node is the number of edges from the root to the node.\n\n### Types of Trees\n\n 1. Binary Tree\n 2. Binary Search Tree\n 3. AVL Tree\n 4. Red-Black Tree\n 5. B-Tree\n \n > There are many more types of trees which one can search and read about ;).\n > Most common are the binary tree which have exactly two children. \n\n## Back To Competitive Programming\n\n## Common Tree Algorithms\n1. DFS and BFS\n2. Traversals\n3. Height of Trees \nand Many More\n\nLets talk about the DFS:\n- Parent-child convention: For understanding DFS or any topic related to Tree, we use this convention, in this, we fix a root node and then we add children to it, then if the child also has a child we call it grandchild of root node and child of the node which is connected to.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/6.png' %})\n\n- In the above example node, 1 is the root node, node 2 and 3 are children of root 1, node 4 and 5 are children of root 2 and grandchildren of root 1, and similarly, 6 is the child of root 3 and grandchild of root 1. It is often misunderstood that a node cannot have more than 2 children.  \nOnly in a Binary Tree, we can have at most 2 children, but in general, it\u2019s ok if a node has more than 2 children.\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/7.png' %})\n- So in the above figure , if we do dfs of root node 1, the order will be\n- 1 \u2192 2 \u2192 5 \u2192 3 \u2192 4\u2192 6\u2192 7 \u2192 8\n- So overall in DFS we finish all the children before moving to the next child, and this is true for every node we visit.\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/8.png' %})\n- First, 1 is Parent Node and node 2 is its child, so before visiting 3 we have to complete 2.\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/9.png' %})\n- Similarly, now node 2 is its parent node and root 4 and its children have to be completed before we visit 5. Let\u2019s take an example:\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/10.png' %})First we visit the root node.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/11.png' %})Then we move on to its first child.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/12.png' %})Then we visit the child of node 2.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/13.png' %})after we are done with all the children of node 2 we move onto node 3.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/14.png' %})Since there are no children of 3 we move to the next child which is 4.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/15.png' %})then we move to first child of 4 That is node 6.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/16.png' %})Then we complete the node 7.\n\n![alt_text]({% static '/blog/assets/img/DP_on_Trees/17.png' %})and we finish it with node 8.\n\n#### Other algorithms\nRefer the following link \n[Common Tree Algorithms - I](https://www.journaldev.com/44201/breadth-first-search-depth-first-search-bfs-dfs)\n[Common Tree Algorithms - II](https://www.tutorialspoint.com/data_structures_algorithms/tree_data_structure.htm) \n\n## Lets Solve a Problem \nThe problem statement is in the link below - \n[XOR-Tree](https://codeforces.com/problemset/problem/429/A)\n\nSo the problem states that on flipping the value of a node all the subsequent alternate children also get flipped.\nWe need to change it in such a way that we get the desired values of all nodes.\nFirst thing to notice is flipping a node only affects the node itself and any of its children. So we need to start from top and follow a greedy approach by flipping a node value wherever it is required.\n\nSecond thing to be noticed is that we will use a DFS with some modification.\n\nNow, coming to the main catch of the problem, suppose you already flipped a node ,all subsequent alternate nodes also get flipped, now suppose a child of a child of a node who is flipped had an opposite value in the target value, then we already have flippped it but we have not monitored or taken care of , of it. The case can be of a even deeper node and we can't manually update that everytime because it will costly.\n\nSo what we can do?\nWe can maintain three extra parameters that is number of flips so far on even and odd positions and another parameter as the current level paramter.\nBelow is the code for modified dfs with the main() function for the input.\nPut on the thinking caps and try to figure out the logic behind the even,odd and level parameters and the if-else conditions,  \n\n    vector<int>init,goal;\n    vector<int>flip;\n    void dfs(vector<vector<int>>&g,vector<int>&visited,int root,int even,int odd,int level)\n    {\n\t    if(init[root] == goal[root];\n\t    {\n\t\t    if(level%2 == 0)\n\t\t    {\n\t\t\t    if(even%2)\n\t\t\t    {\n\t\t\t\t    flip.push_back(root+1);\n\t\t\t\t    even++;\n\t\t\t    }\n\t\t\t}\n\t\t\telse\n\t\t\t{\n\t\t\t    if(odd%2)\n\t\t\t    {\n\t\t\t\t    flip.push_back(root+1);\n\t\t\t\t    odd++;\n\t\t\t    }\n\t\t    }\n\t\t    visited[root] = 1;\n\t\t    for(auto nodes : g[root])\n\t\t    {\n\t\t\t    if(!visited[nodes])\n\t\t\t    dfs(g,visited,nodes,even,odd,level+1);\n\t\t    }\n\t    }\n\t    else\n\t    {\n\t\t    if(level%2 == 0)\n\t\t    {\n\t\t\t    if(even%2 == 0)\n\t\t\t    {\n\t\t\t    flip.push_back(root+1);\n\t\t\t    even++;\n\t\t\t    }\n\t\t    }\n\t\t    else\n\t\t    {\n\t\t\t    if(odd%2 == 0)\n\t\t\t    {\n\t\t\t    flip.push_back(root+1);\n\t\t\t    odd++;\n\t\t\t    }\n\t\t    }\n\t\t    visited[root] = 1;\n\t\t    for(auto nodes : g[root])\n\t\t    {\n\t\t\t    if(!visited[nodes])\n\t\t\t\t    dfs(g,visited,nodes,even,odd,level+1); \n\t\t    }\n\t    } \n    }\n    \n    int main()\n    {\n\t\tint t = 1;\n\t    while(t--)\n\t    {\n\t\t    int n;\n\t\t    cin>>n;\n\t\t    vector<vector<int>>g(n,vector<int>());\n\t\t    for(int i = 1;i<n;i++)    \n\t\t    {\n\t\t\t    int  u,v;\n\t\t\t    cin>>u>>v;\n\t\t\t    u--;v--;\n\t\t\t    g[u].push_back(v);\n\t\t\t    g[v].push_back(u);\n\t\t    }\n\t\t    \n\t\t    for(int i = 0;i<n;i++)\n\t\t    {\n\t\t\t    int  val;\n\t\t\t    cin>>val;\n\t\t\t    init.push_back(val);\n\t\t    }\n\t\t    for(int i = 0;i<n;i++)\n\t\t    {\n\t\t\t    int  val;\n\t\t\t    cin>>val;\n\t\t\t    goal.push_back(val);\n\t\t    }\n\t\t    vector<int>visited(n);\n\t\t    dfs(g,visited,0,0,0,0);\n\t\t    cout<<flip.size()<<\"\\n\";\n\t\t    for(auto  flipper  :  flip)\n\t\t    {\n\t\t\t    cout<<flipper<<\"\\n\";\n\t\t    }\n\t    }\n\t    return  0;\n    }\n\nThanks for reading ;) .",
        "author_name": "Dhruvil Lakhtaria",
        "slug": "trees-in-competitive-programming"
    },
    {
        "layout": "post",
        "title": "Huffman encoding and text file compression",
        "date": "2022-07-31T00:00:00",
        "image": "/assets/img/",
        "description": "A post on Huffman encoding algorithm and its use in text file compression",
        "tags": [
            "IEEE NITK",
            "CompSoc"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "VikasKBhat",
        "text": "[{% load static %}]()\n![thumbnail]({% static '/blog/assets/img/Huffman/thumbnail.png' %})\n\n#### Prerequisites\nTrees, DFS, Min-heaps\n\n## Introduction\n\nIn this era of the internet, we constantly need to send and receive files. It is in our best interest that these files are as small as possible so that we can save on bandwidth and network usage. Hence, it is important to compress or encode files to minimize their transmission size.\n\nFor text files, one of the most popular compression algorithms is Huffman encoding. In Huffman encoding, each character is assigned a variable-length code, and this assignment is sent as a header file. The codes are in binary, and they are prefix codes, i.e., the code of one character is never the prefix for the code of another character. The most frequent character has the shortest code, and the least frequent has the longest code. The difficulty is in assigning codes to these characters, which is discussed below.\n\n## Huffman encoding algorithm and Huffman tree\n\nThe first part of the Huffman encoding algorithm is building a Huffman tree from the given characters. From the Huffman tree, we can give codes to the characters which will be used for the compression.\n\nWe will use the below line of text as the source string:\n\n```\nacbaabbacaaacaccbacaaabcbccaccacaacaacabbabcacabacaacbaaaacaabbacaaaaabaabbaaaabbbccacaaccbaaaa\n```\n\nThe following are the steps to make a Huffman tree:\n\n```\n\nFor each unique character in the source text:\n\n\tMake a leaf node in the tree storing the frequency of the character\n\nBuild a min-heap of nodes, which compares based on the frequency of the nodes\n\nwhile the size of the head is greater than one:\n\n\tExtract 2 minimum nodes from the heap\n\n\tCreate a node with frequency as sum of frequencies of extracted nodes\n\t\n\tSet extracted nodes as left and right children of newly created node\n\n\tInsert the new node in the min-heap\n\n```\n\nAfter these steps, the Huffman tree is complete, and the last node is treated as the root of the tree.\n\nConsider the character frequencies in the example:\n\na: 50\n\nb: 20\n\nc: 25\n- Three leaf nodes are created, one for each character.\n- A min-heap is created with these nodes.\n\nMin-heap state: 20 25 50\n\n- 20 (b) and 25 (c\\) are extracted from the min-heap. A new node with value 45 is created, the two nodes are added as its children and it is placed in the heap.\n\t\n![img1]({% static '/blog/assets/img/Huffman/img1.png' %})\n\nMin-heap state: 45 50\n\n- 45 and 50 (a) are extracted from the min-heap. A new node with value 95 is created, the two nodes are added as its children and it is placed in the heap.\n\n![img2]({% static '/blog/assets/img/Huffman/img2.png' %})\nMin-heap state: 95\n\n- The min-heap has only one element, and the Huffman tree is complete.\n\n- - - \nThe second part, as mentioned earlier, is to assign codes based on the Huffman tree. To do so, we do the following:\n\n```\nStart at the root node, and maintain a binary string (initially empty)\n\nPerform DFS on the tree starting from the root\n\nIf left child is being visited:\n\t0 is appended to the binary string\n\t\nIf right child is being visited:\n\t1 is appended to the binary string\n\t\nIf DFS is completed on a node and the node is being exited:\n\tThe binary string\u2019s last character is popped\n\nIf we reach a leaf node (associated with a character):\n\tThe Huffman code of that character is the binary string\n```\n\nIn the previous example, if we do the above, we end up with the following assignment:\n\na: 0\n\nb: 10\n\nc: 11\n\n![img3]({% static '/blog/assets/img/Huffman/img3.png' %})\n\nTo get an idea of why this method gives an optimal code, we have to consider the original objectives listed below:\n\n- The lengths of character codes are dynamic, and more frequent characters get shorter codes.\n- The code of one character is never the prefix of the code of another character.\n\nIt should be clear from the algorithm that the Huffman tree will have more frequent characters closer to the root because higher frequency characters are extracted later in the algorithm. The distance of a character node from the root(in terms of edges) is the length of the character code. Hence, more frequent characters will have shorter codes and the first objective is achieved.\n\nIn the Huffman tree, the exact character code depends on the path from the root to the node with the character. If the code of some character A has to be the prefix of some other character B, A will have to lie on the path from the root to B. Since characters are always stored in a leaf node, the path from the root to character B cannot contain the node for character A. Hence, the prefix code condition is achieved.\n\n#### Note\nAs the reader may have noticed, there exists an edge case to this algorithm, which is if the text file contains only 1 character. If the above algorithm is followed, the character ends up with no binary code.\n\nThis edge case has to be dealt with in the decoding program only since the header file containing the frequencies of the characters will make it clear that the file consists of a single character repeated several times. The header file is discussed in more detail below.\n\n## The final encoding\n\nNow that we have the codes for each character, we can start the encoding to reduce the file size. The procedure is simple. We replace each character with its code to get a binary string for the entire file. Since the code is a prefix code, there can be no ambiguity in the translation. After encoding the example string, we get:\n\n```\n01110001010011000110111110011000101110111101111011001100110101001011011010011001110000011001010011000001000101000001010101111011001111100000\n```\n\nThis binary string has to be stored in a binary file to achieve compression, i.e., the inner bits of the file have to correspond to the binary string. To do so, the simplest method is to divide the string into parts of 8 bits and store each 8 bit part in a single character which is written to the compressed file. After this, the 140 character binary string can be written in 18 bytes of memory, compared to the 95 bytes of the original text.\n\nThis solution, however, is not complete. That is because the last character to be generated this way need not contain all 8 bits, but the translation program, while decoding, would not be able to tell how many bits to use. \n\nOne simple solution to this is to include End of File (EOF) as a character in the Huffman encoding algorithm. When the decoding program reads EOF, it can immediately terminate translation, and hence garbage bits would not be read.\n\nFinally, just the binary file is not sufficient for translation. We need another file (often called the header file) to tell the decoding program about the character codes. The character codes can be given directly, or to further optimize space, the characters and their frequencies can be given. This can be achieved by storing the characters and frequencies in a suitable data structure and writing the entire structure to the header file. The decoding program can then retrieve the frequencies and run the same Huffman algorithm to generate character codes.\n\nIn the given example, there are 3 characters whose frequencies have to be given in the header file. If we optimize space properly, the header file should contain 3 characters and 3 ints, which is ~15 bytes. Hence the overall size will be 15 + 19 = 34 bytes, which is much smaller than the initial size of 95 bytes. \n\nHence our objective of text file compression is complete.\n\n## Conclusion\nHuffman Algorithm is a popular way of compressing text files. We have discussed the algorithm, the problems faced in its implementation for file compression as well as possible solutions. With this blog as a reference, the reader should be able to write their own program for text file compression.\n\n## References\n[Huffman Coding - GeeksForGeeks](https://www.geeksforgeeks.org/huffman-coding-greedy-algo-3/)\n\n[Huffman Coding - Wikipedia](https://en.wikipedia.org/wiki/Huffman_coding)",
        "author_name": "Vikas Keshavamurthy Bhat",
        "slug": "huffman-encoding"
    },
    {
        "layout": "post",
        "title": "Missile Homing Systems",
        "author_github": "DharaneedaranKS",
        "date": "2022-07-31T00:00:00",
        "description": "\u2018Missile guidance refers to a variety of methods of guiding a missile or a guided bomb to its intended target. The missile\u2019s target accuracy is a critical factor for its effectiveness.\u2019",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Sensors"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "DharaneedaranKS",
        "text": "[{% load static %}]()\n# MISSILE HOMING SYSTEMS\n\n## INTRODUCTION\n\nWhen we think about modern-day warfare, we mostly see missiles flying everywhere. In action Hollywood movies, we see the protagonists dodge heat-seeking missiles and fire an RPG at the antagonist to win the battle. We often see the case where these missiles can tail or rather chase an enemy aircraft or tank and reach their target destination accurately, but how does that happen? This is where homing missiles come into the picture.\n\n![genric missile]({% static '/blog/assets/img/missile-homing-systems/Picture1.jpg' %})\n\nA homing missile was a particular type of missile. The term was used to describe many types of weapons, including brilliant missiles and many types of concussion missiles, as it simply designated a guided missile. Homing missiles are those missiles Homing missiles have played an increasingly important role in warfare since the end of World War II. In contrast to inertially guided long-range ballistic missiles, homing missiles guide themselves to intercept targets that can maneuver unpredictably, such as enemy aircraft or anti-ship cruise missiles. Intercepting such threats requires an ability to sense the target location in real-time and respond rapidly to changes so that a target intercept can occur.\n\nThere are different ways a missile can be guided, and through years of research, trial and error, five basic guidance methods came to be used, either alone or in combination:\n* command\n* inertial\n* active\n* semiactive\n* passive.\nLets us now look into each one of them separately\n\n#### Command Guidance\nCommand guidance involved tracking the projectile from the launch site or platform and transmitting commands by radio, radar, or laser impulses or along thin wires or optical fibers. Tracking might be accomplished by radar or optical instruments from the launch site or by radar or television imagery relayed from the missile.\n\n ![command guidance]({% static '/blog/assets/img/missile-homing-systems/Picture2.jpg' %})\n\n#### Inertial Guidance\nInertial guidance was first used in long-range ballistic missiles in the 1950s, but it became more popular in tactical weapons after the 1970s, thanks to developments in miniaturized circuits, microcomputers, and inertial sensors. Inertial systems involved using small, highly accurate gyroscopic platforms to continuously determine the missile\u2019s position in space. These served as inputs to guidance processors, which calculated velocity and direction using the location data and inputs from accelerometers or integrating circuits. The flight path was programmed into the guidance computer, which then issued directives to keep the plane on course.\n\n![Inertial Guidance system gyroscope]({% static '/blog/assets/img/missile-homing-systems/Picture3.png' %})\n\n#### Active guidance\nThe missile would track its target with active guidance by emitting emissions that it created. For terminal homing, active guidance was extensively utilized. For example, self-contained radar systems were employed to track anti-ship, surface-to-air, and air-to-air missiles. The downside of active guidance is that it relies on emissions that can be traced, blocked, or fooled by decoys.\n\n\n#### Semi-Active Guidance\nSemiactive guidance involved illuminating or designating the target with energy emitted from a source other than the missile; a seeker in the projectile that was sensitive to the reflected energy then homed onto the target. Like active guidance, semiactive guidance was commonly used for terminal homing.\n\n![ Semi- Active guidance]({% static '/blog/assets/img/missile-homing-systems/Picture5.jpg' %})\n\n#### Passive guidance\nPassive guidance systems neither emitted energy nor received commands from an external source; rather, they \u201clocked\u201d onto an electronic emission coming from the target itself. The earliest successful passive homing munitions were \u201cheat-seeking\u201d air-to-air missiles that homed onto the infrared emissions of jet engine exhausts.\n\n ![Passive guidance]({% static '/blog/assets/img/missile-homing-systems/Picture6.jpg' %})\n\nIn the present day world and warfare, the primary focus is to develop passive homing systems. Why so? Well, it is so much more convenient for the pilot or soldier operating it to let the computer decide where and when to go and how to attack the enemy while you sit back and just watch the missile hit the target. Passive homing systems are also more accurate in comparison to other homing systems.\nThere are multiple passive homing systems such as ARM ( Anti-Radiation Missiles) and Heat Seeking Missiles.\n\n## ARM (ANTI-RADIATION MISSILE) :\n\n### INTRODUCTION:\n\nAnti-radiation missiles are used to detect the source of radio emissions from an enemy. The majority of these are made to be used against hostile radar. These missiles are meant to detect and track a radio emission source from an enemy. Typically, these are designed to be used against an enemy radar, but they can also be used to target jammers and even communications radios.\n\nRadar, which stands for radio detection and ranging, was one of the most important inventions of the twentieth century. In the military context, radar allowed for increased communication across the conflict zone and for tracking enemy and non-enemy weapons, planes, boats, submarines, and more.\nThe objective with which these missiles are designed is to break the enemy defence during the first leg of the battle so that the chances of surviving the further waves by the strike aircraft could be doubled.\n\n\n### WORKING:\n\nA traditional anti-radiation missile is attracted to the radar\u2019s mainlobe emission and its horizontal sidelobes and backlobes emission, depending on the distance between the radar and the missile. However, in the case of older radars, the primary target is their extremely high-level horizontal sidelobes and backlobes, which emit continuously. This permits the missile to maintain continuous radar tracking and prevents the passive anti-radiation homing receiver from becoming saturated. Modern radars with very low horizontal sidelobes and backlobes emissions provide a \u201cblinking\u201d target for missiles, with the \u201cblinking\u201d caused by the pauses in receiving the radar mainlobe emission as the antenna rotates.\n\nIn this case, missiles without GPS are obliged to estimate the position of the radar-based on an occasionally received emission. The guidance system of the missile is supported by its inertial system when the antenna turn speed is low (long intervals in receiving the emission), especially during the final phase of flight, which often results in a more significant margin of error (a few meters) in detecting the position of the radar than was assumed beforehand.\n\nMissiles without GPS are forced to approximate the position of the radar-based on a rarely received emission in this circumstance. When the antenna turn speed is low (long intervals in receiving the emission), especially during the final phase of flight, the missile\u2019s guidance system is supported by its inertial system, which often results in a larger margin of error (a few metres) in detecting the position of the radar than was assumed beforehand.\n\n## INFRARED HOMING\nInfrared homing refers to a passive missile guidance system that uses the emission from a target of electromagnetic radiation in the infrared part of the spectrum to track and follow it. Missiles that use infrared seeking are often called \u201cheat-seekers\u201d, since infrared (IR) is just below the visible spectrum of light in frequency and is radiated strongly by hot bodies. Many objects such as people, vehicle engines, and aircraft generate and retain heat and are especially visible in the infra-red wavelengths of light compared to objects in the background.\n\n![Infrared guidance]({% static '/blog/assets/img/missile-homing-systems/Picture7.jpg' %})\n\nThe infrared sensor package on the tip or head of a heat-seeking missile is known as the seeker's head. The NATO brevity code for an air-to-air infrared-guided missile launch is Fox Two.\n\n![Seeker head]({% static '/blog/assets/img/missile-homing-systems/Picture8.jpg' %})\n\n#### SEEKER TYPES\nThe three main materials used in the infrared sensor are lead(II) sulfide (PbS), indium antimonide (InSb), and mercury cadmium telluride (HgCdTe). Older sensors tend to use PbS; newer sensors use InSb or HgCdTe. All perform better when cooled, as they are both more sensitive and able to detect cooler objects.\nEarly infrared seekers were particularly successful in detecting shorter wavelength infrared radiation, such as the 4.2-micrometer emissions of a jet engine\u2019s carbon dioxide outflow. Single-color seekers are now used to describe such seekers, which are particularly sensitive in the 3 to 5 micrometer range. Modern infrared seekers also function in the wavelength range of 8 to 13 micrometers, which is the least absorbed by the environment. Two-color systems are a type of seeker. Flares and other countermeasures are less effective against two-color seekers.\n\n\n\n#### SCANNING PATTERNS AND MODULATION\nThe way by which the space in front of a missile is examined for targets can also determine a missile\u2019s resistance to decoys. Early missiles employed spin scanning, while later seekers used conical scanning, which improves decoy discrimination and overall sensitivity for longer-range tracking. There have also been missiles developed employing rosette scanning techniques. Modern heat-seeking missiles use imaging infrared (IIR), in which the IR/UV sensor is a focal plane array that can \u201csee\u201d in infrared, similar to a digital camera\u2019s CCD. This necessitates a lot more signal processing, but it can be a lot more accurate and difficult to deceive with decoys. In addition to being more flare-resistant, newer seekers are also less likely to be fooled into locking onto the sun, another common trick for avoiding heat-seeking missiles.\n\nPrior to imaging infrared sensors, there was also the issue of sensor modulation; earlier seekers used amplitude modulation (AM) to determine how far off-center the target was and thus how hard the missile had to turn to centre it, but this resulted in increased error as the missile approached the target and the target\u2019s image became relatively larger, which led to increased error as the missile approached the target and the target\u2019s image became relatively larger (creating an artificially stronger signal). This problem was rectified by switching to frequency modulation (FM), which is better able to identify distance without being confused by image size.\n\n### BIBLIOGRAPHY:\n- [https://www.britannica.com/technology/rocket-and-missile-system/Strategic-missiles](https://www.britannica.com/technology/rocket-and-missile-system/Strategic-missiles)\n- [https://www.britannica.com/technology/passive-guidance-system](https://www.britannica.com/technology/passive-guidance-system)\n- [What is an anti-radiation missile](https://en.as.com/en/2022/03/09/latest_news/1646866040_641804.html#:~:text=Anti%2Dradiation%20missiles%20work%20by,military%20enemy%20during%20a%20conflict.&text=Radar%2C%20which%20stands%20for%20radio,inventions%20of%20the%20twentieth%20century)\n- [https://military-history.fandom.com/wiki/Infrared_homing](https://military-history.fandom.com/wiki/Infrared_homing)",
        "author_name": "Dharaneedaran K S",
        "slug": "missile-homing-systems"
    },
    {
        "layout": "post",
        "title": "Sessions and JWT",
        "author_github": "famousfive1",
        "date": "2022-07-31T00:00:00",
        "image": "/assets/img/",
        "description": "A simple explanation about sessions and JWT used in authorization",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Web Dev",
            "JWT",
            "Sessions"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "famousfive1",
        "text": "[{% load static %}]()\n# __Sessions and JWT__\n\n## __Intro__\n\nSuppose you are developing a web application. Chances are that you want the user to be able to create an account or sign in and then access certain restricted parts of the site, such as their profile page or the forums/comment section. The HTTP protocol is stateless, meaning that it retains no information about the previous state or requests made. Thus, you need some other mechanism to track whether a user is logged in or not. There are two main methods to achieve this: Sessions and JSON Web Tokens (JWT).\n\nBefore going into their details, there are two terms that are often misused or interchanged.\n1. Authentication: the process of verifying a user and granting access.\n2. Authorization: the process of making sure that a person has permission to access a requested resource.\n\nAuthentication is done before authorization. Authentication usually involves a user typing in his/her username and password. Whereas in authorization, the server makes sure that the user who is requesting access is actually who he/she is claiming to be and that he has permission to request.\n\nHere, I am not going to explain the process of securely logging in a user and ensuring data security. That is a topic for another post.\n\n## __Sessions__\n\nAssume that you are a customer who has some issues and reach out to\u00a0tech support. You tell the attendant your name and contact details to establish that you are a past customer and then explain your problem. The attendant notes down your problem, gives you a token/complaint number, and tells you to mention it to the technician who will help you.\n\nWhat happened here is the basic idea behind sessions. It is a method to maintain the state of a user. In this case, the token number is a way to uniquely identify you. The server stores all the details. Whenever you go to the technician, you can give him your token number and he will be able to access your details and your complaint without you having to repeat yourself.\n\n## __JWT__\n\nThe attendant above could also have chosen a different approach. Instead of providing you with a token number, he could have signed the paper containing all of your information and complaint and instructed you to hand it to the technician. The technician then would have all the details available to him on the paper.\n\nThis is the idea behind JWT. None of your data is actually stored with the attendant. Instead, its given to you on a signed piece of paper. The signing process here is of huge significance. It ensures that the paper you are handing to the technician is legit and that you didn't change or\u00a0forge any details. If it wasn't signed by the attendant or if there is any suspicion of tampering, the technician could just reject it and ask you to start from the beginning again.\n\n## __Technicalities__\n\nWhat I have described here is only the core concepts behind the two major methods of dealing with statefulness in a stateless protocol like HTTP. Although there are some technicalities that have not been explained yet.\n\n### __Sessions__\n\nAfter authorization, the server needs to generate a session ID and send it to the client. This is usually done by setting it to a cookie. The server stores the session ID along with the user ID for further reference. This is stored either in a database or in an in-memory store like [Redis](https://redis.io/). The client then sends the session token on further requests to verify itself. Whenever there is a request, the server looks up the session ID and retrieves the data from the database. If a user logs out, then the session ID is removed from the storage.\n\n\nOWASP provides a detailed article ([Link](https://cheatsheetseries.owasp.org/cheatsheets/Session_Management_Cheat_Sheet.html)) on sessions, which is a must-read for anyone thinking of implementing sessions. Even if you end up using third-party libraries, having this knowledge is essential. Here are some key points:\n\n- Session IDs are supposed to be long, random strings of at least 128 bits.\n- They must not reveal any information about the architecture of your server (such as default names used by frameworks) or about the user itself.\n- They must be unpredictable (random enough) to prevent brute force attacks. Using a good cryptographically secure random number generator is a must.\n- Usage of proper cookie flags for storing and transmitting the ID token is recommended.\n\n\n### __JWT__\n\nJWT stands for JSON Web Token. It is because the data is formatted as a JSON (JavaScript Object Notation) object. There are 3 parts to a typical JWT, each separated by a dot (`.`):\n\n1. Header - contains the signing algorithm used\n2. Payload - contains the actual data being transmitted\n3. Signature - contains the header and payload signed together using a secret\n\nOWASP also has a cheatsheet for JWT ([Link](https://cheatsheetseries.owasp.org/cheatsheets/JSON_Web_Token_for_Java_Cheat_Sheet.html)).\n\nWhen used only for authorization, you should use a signing algorithm such as [HMAC](https://en.wikipedia.org/wiki/HMAC) (JWT also supports public/private key encryption like [RSA](https://en.wikipedia.org/wiki/RSA_(cryptosystem)) or [ECDSA](https://en.wikipedia.org/wiki/Elliptic_Curve_Digital_Signature_Algorithm) when two way communication is needed). When you use a token for authorization, it should only be modifiable by the server. Thus, the server needs to sign the token with a strong and secret key. Any time the content of the token is changed, it needs to be re-signed.\n\nThe secret used for signing should also be long and random.\n\n\n## __Sessions Vs. JWT - Advantages and Disadvantages__\n\nThere is no benefit to using one over the other. Each has its own tradeoffs. Depending on the application, one might be suitable over the other. Usually, token-based authorization is preferred when there is no continuous session being maintained, such as in API services. Sessions is good for situations where the user stays logged in for some time while working.\n\nThat being said, here are some tradeoffs:\n\n### _Sessions_\n\nThe client holds no sensitive data. Invalidation of a session is as simple as removing it from the storage.\n\nUsually, servers are sitting behind a load balancer which splits the incoming traffic. In this case, a user's request may be sent to server 1 for logging in but be sent to server 2 for the next. To make sure that the session persists across the server instances, either the session ID must be stored in shared memory or the load balancer must make sure to send the user's request to the same server every time.\n\n\n### _JWT_\n\nThe server doesn't have to maintain a state. All the necessary details are contained within the JWT itself. This also avoids the load balancer problem since the server doesn't have to remember anything.\n\nSigning out or invalidating a user/user's token is a bit hard. Separate storage needs to be maintained with the expired tokens, and each time a token is submitted, it needs to be checked for validity.\n\nIt is also a bit risky since some of the clients' details are stored on the token itself, which can be read by anyone. So, it requires more security measures.\n\nOn a side note, JWT has many other uses than just authorization.\n\n\n## __Conclusion__\n\nI hope this article gives a good introduction to sessions and JWT and helps you choose what authorization method you want to use. It can be a bit tricky to choose the right method. But both are equally capable.",
        "author_name": "Tejaswi Hegde",
        "slug": "sessions-and-jwt"
    },
    {
        "layout": "post",
        "title": "Shape-Shifting Polymers",
        "date": "2022-07-31T00:00:00",
        "author_github": "JuvvaSrinithya",
        "description": "Shape-Shifting Polymers are the polymers that have different ranges of reversible shapes on changes in temperature. For the fabrication of reconfigurable materials knowledge of Shape-Shifting materials is crucial.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Shape-Shifting Polymers"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "JuvvaSrinithya",
        "comments": false,
        "text": "[{% load static %}]()\n# Shape-Shifting Polymers\n\n## Abstract\n\nIn the present era, there is a wide range of progress in the polymeric materials which are related to external stimuli and used in the application of robotics, biomedical engineering and optical devices. These materials are classified into two classes (1) Shape-Changing Materials in which a certain type of shape-shifting is encoded in the original material. (2) Shape-Memory materials which do not have any fixed Shape-Shifting as prepared, but allow programming of complex shape transformations. Even though we know shape shifting polymers from Decades, till now there is no proper material that is spotted on the basic molecular behaviour responsible for the materials properties, so we need to adapt them to new applications on trial-and-error approaches. The goal of this review is to highlight recent developments in reversible shape-shifting by introducing novel mechanisms, materials, and applications.\n\n## What are Shape-Shifting Polymers?\n\nShape-shifting material changes upon external stimuli, converting stored potential energy into motion as actuators. Polymeric Materials have been intensively explored due to their low density, high deformability and wide range of synthetic strategies in molecular design and functionalization. Shape-shifting polymers can transform under a variety of stimuli including heat, electricity, light, solvent and magnetic field.\n\nShape-Shifting Materials are divided into two types:\n\na) Shape memory polymers can be programmed to hold a particular temporary conformation which reverts to a different, permanent shape when heat is applied. To do this the temporary shape is formed within a phase transition of the polymer\n\nExample: The glass transition where the molecules of the polymer change from a \u2019frozen\u2019 glassy state to a more mobile \u2019rubbery\u2019 state upon heating.\n\nb) Shape-changing materials undergo reversible shape alterations upon various stimuli. Generally speaking, nearly all objects change their dimensions in response to minute changes in the surrounding environment.\n\nExample: Thermal expansion and swelling.\n\nShape-shifting materials can perform shape variations from macroscopic to micro/nanoscales. Dynamic change in surface topologies and microstructures can affect physical properties including wetting, adhesion and propulsion effects.\n\n## Applications\n\n**Micro Robotics**\n\nThe shape shifting of polymers is followed by force generation, transforming stimuli into mechanical energy. With complex designing, the samples have the ability to perform remote manipulated self-loco motions as micro robotics. By applying the concept of cyclic stimuli, the reversible deformation of bending and unbending can be altered into motions such as swimming, walking, etc.\n\n\n**Biomedical Devices and Artificial Muscles**\n\nMechanically smart polymers which shift shape have large capability in biomedical applications. For example, in minimally invasive surgery, vasospasms can be prevented by specially designed shape shifting stents. The stent can be inserted or removed as thin wires and can be expanded in situ under stimuli.\n\n\n## Conclusion\n\nThis review presents a summary of different concepts of shape-shifting in polymeric materials. Demands in potential applications from micro robotics to biomedical devices require materials that are capable of reversibly actuating between sophisticated shapes, from macro to nanoscales and within a variety of environments under different stimuli.  The test was met with advancement through both substance plans with artificially new materials and novel instruments and conventions that grow the uses of existing polymers. The advances show a promising future for smart materials that could change our day-to-day routine.\n\n## Resources\n  \n- [Reversible shape\u2010shifting in polymeric materials - Zhou - 2016 - Journal of Polymer Science Part B: Polymer Physics - Wiley Online Library](https://onlinelibrary.wiley.com/doi/full/10.1002/polb.24014)\n- [Shape-shifting polymers, News, Chemistry World](https://www.chemistryworld.com/news/shape-shifting-polymers/3003607.article)\n- [Shape-memory polymers - ScienceDirect](https://www.sciencedirect.com/science/article/pii/S1369702107700470#!)",
        "author_name": "Juvva Srinithya",
        "slug": "Shape-Shifting-Polymers"
    },
    {
        "layout": "post",
        "title": "The Qubit Report",
        "date": "2022-08-10T00:00:00",
        "author_github": "anirudhprabhakaran3",
        "description": "An introduction to the world of Quantum Computing, the physics that powers it, and what it will power in the future.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Quantum"
        ],
        "categories": [
            "Compsoc"
        ],
        "github_username": "anirudhprabhakaran3",
        "comments": false,
        "text": "[{% load static %}]()\n# The Qubit Report\n\nIn [Part 1 of the Qubit Report](https://istenitk.wordpress.com/?p=2973), we looked at quantum computing and its workings. Now, we\u2019re going to look at a few places where this magical technology is being used!\n\nQuantum computing has been around for some time now. The first quantum computer was made in 1998 as a collaboration between three fellows:  Isaac Chuang (Los Alamos National Laboratory), Neil Gershenfeld (Massachusetts Institute of Technology), and Mark Kubinec (University of California, Berkeley). This primitive machine had two qubits and was the first quantum computer that could load some data and give some output. Since then, the field has not hit the breaks yet. The most powerful quantum computer currently has 216 qubits.\n\n![alt_text]({% static '/blog/assets/img/qubit-report/image1.png' %})\n\n## Why Quantum Computing?\n\nBefore we look into where all quantum computers are used, we first must understand why we use them. And the answer to that question is simple - for some tasks, the supercomputers we have are not \u201csuper\u201d enough.\n\n![alt_text]({% static '/blog/assets/img/qubit-report/image2.png' %})\n\nSome systems are very complex and involve lots of variables. Modelling such systems is difficult for even the supercomputers that we have. In such cases, quantum computers might be of help.\n\nBut beware! Quantum computing is not a magic wand that can be applied to any problem. There are specific types of problems that can be handled with the power of quantum computing. But it will not offer much of an advantage for unoptimised problems.\n\nOne field in which this technology performs well is modelling the behaviour of individual atoms in a molecule. The infinite complexity of this system is well modelled and handled by the powers of quantum computers.\n\nLet's look into the other fields in which these powerful machines can be deployed.\n\n## Cryptography\n\nImagine one fine day you wake up. When you check your phone, you see an SMS stating that your bank account has been wiped out. When you check your mail, you find out most of your accounts have been compromised. Sounds scary, right?\n\nThe technology that keeps us and our data safe on the Internet. Be it backups of our files, cat videos, or financial transactions. Cryptography is the most critical component of the modern internet - with billions of users, and trillions of requests, making sure your data or request is read only by the intended recipient is critical.\n\nThe core of such an essential aspect of modern computing is a very simple concept - factorisation. Risking oversimplification, cryptography is just multiplication. Yes, the numbers multiplied are large prime numbers (nearly 300 digits long), which makes it impossible for classical computers to break.\n\nHowever, this is a type of task at which quantum computers excel. These machines excel at factorisation and wouldn\u2019t have difficulty breaking modern encryption. Because of this, post-quantum cryptography is now an active research field. This aims to find encryption methods that are not based on prime factorisation or logarithmic arithmetic, thus rendering quantum computers ineffective against this. The following illustration briefly describes one way that quantum cryptography can be leveraged.\n\n![alt_text]({% static '/blog/assets/img/qubit-report/image3.png' %})\n\nAnother group researching this is going on another path. They are trying to find a *quantum* method of encryption. This is analogous to the current system - making an encryption algorithm impossible for even quantum computers to break. Similar to the classical case, these algorithms will also resist quantum hacking.\n\n## Machine Learning\n\n![alt_text]({% static '/blog/assets/img/qubit-report/image4.png' %})\n\nAt its core, all machine learning is matrix operations. Whether using a standard linear regression or a deep neural network, linear algebra is the essential math underlying machine learning. And this is why we are in luck.\n\nTurns out that most quantum computation techniques use the same mathematics required for artificial intelligence - linear algebra. Because of this, machine learning algorithms and models can be run on such computers, leveraging quantum mechanics' processing power. Popular machine learning frameworks in Python, like TensorFlow, are actively developing quantum counterparts to their normal features. There is an ongoing project for making a quantum version of the PyTorch library, headed by the Massachusetts Institute of Technology (MIT)\u2019s Hardware, AI and Neural Networks (HAN) Lab.\n\nThe most recent experiments display the significant speedup and time saved for implementing a reinforcement learning model on a quantum computer compared to a classical one, with either the same or better results and accuracies. More research is being done every day in quantum neural networks and reinforcement learning, and it is a booming research space. The introduction of quantum computing might provide the necessary firepower to make great leaps in machine learning and artificial intelligence. It might even work as the stepping stone to a generalised artificial intelligence (GAI).\n\n![alt_text]({% static '/blog/assets/img/qubit-report/image5.png' %})\n\n<center>The above image is of a quantum chip created by D-Wave. This particular chip acts as a 128-qubit processor. The first machine learning application to run on a quantum computer was an object detection model that could identify cars from digital images. Although not run on this particular machine, the model was first to run on an adiabatic D-Wave quantum computer in 2009.</center>\n\n## Computational Biology\n\nIt is pretty fitting - poetic even - that the most advanced computer made by mankind is to be used to improve knowledge about mankind. Living organisms are undoubtedly some of the most complex machines created by nature, whose secrets remain to be revealed. And quantum computers are helping us take a step in that direction.\n\nOne of the major domains that computational biologists are looking to leverage is computational genomics. This field tries to find the properties of various organisms using their genome. The computation of the human genome is one of the \u201cgrand challenges\u201d of modern biology. With their immense processing capabilities, Quantum computers are helping us get to the bottom of these problems faster.\n\n![alt_text]({% static '/blog/assets/img/qubit-report/image6.png' %})\n\nBut this is far from the only application. Some research into MicroRNA functioning is also very active, and there is a scope for many computational biology problems to be tackled by quantum computers.\n\n## Computer-Aided Drug Design\n\nWe all are still acutely aware of the situation during COVD-19. Hundreds of scientists worldwide spent hours painstakingly trying to find a vaccine and cure for the virus. After a few candidates were designed, they went through a system of trials to ensure that the drugs worked and were released for public use.\n\n![alt_text]({% static '/blog/assets/img/qubit-report/image7.png' %})\n\nImagine if, instead of trying so many different combinations of chemicals manually, you could run a software program that would tell the properties of a particular drug and how effective it would be against a particular disease. It would make the lives of every person involved in the process much easy. Also, it would drastically reduce the time needed to design drugs, allowing the process to go to the trial stage much quicker.\n\nSince drug design needs a computer to model the various interactions that molecules and atoms can have with other biological organisms, the processing power of quantum computers is a significant boost for this field. Some algorithms are being developed, like quantum deep neural networks and Quantum Variational Autoencoders (QVA), to help with this process.\n\n## So what is stopping us?\n\nWe mentioned many advantages that quantum computers would provide and their real-world implications. But you might have a very logical question after that - why hasn\u2019t this happened yet? Why have the real-world implications stated happened yet? What is stopping us?\n\nWell, the answer to that question is simple. These applications depend on quantum computers, which is the bottleneck. It is extremely difficult to make, run and maintain a quantum computer. The most prominent manufacturers and maintainers of quantum computers, Google and IBM, have around 20 quantum computers each. This is very low compared to the availability of classical computers and hardware. This is very low compared to the availability of classical computers and hardware. The superconducting cables required to build these machines are made only by one company in the world - a Japanese company called Coax Co.\n\n![alt_text]({% static '/blog/assets/img/qubit-report/image8.png' %})\n<center>One of Google's Quantum Computers</center>\n\nApart from this, there are also a few difficulties relating to the physics of these computers. It is a challenge to scale the number of qubits physically. As the number of qubits increases, it also becomes challenging to initialise these with random values. As the number increases, we also need some quantum controller that enables interfacing with them.\n\n![alt_text]({% static '/blog/assets/img/qubit-report/image9.png' %})\n<center>One of IBM's Quantum Computers</center>\n\nThere are difficulties. However, there is also a lot of work and research in this field to make it more viable to build more such machines. Once we can make more devices like this, human technology will get a considerable boost. Now, it is only a matter of time!",
        "author_name": "Anirudh Prabhakaran",
        "slug": "the-qubit-report"
    },
    {
        "layout": "post",
        "title": "All about Road Train",
        "author_github": "kshamaa24",
        "date": "2022-08-28T00:00:00",
        "description": "A road train, also called truck train, is a trucking vehicle used to move road cargo more efficiently than semi-trailer trucks. It consists of two or more semi-trailer trucks hauled by a prime mover.",
        "tags": [
            "IEEE NITK",
            "Piston",
            "Road train"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "kshamaa24",
        "text": "[{% load static %}]()\n# Road Train\n\n## Abstract\n\nBeginner drivers think thrice before overtaking a truck. Just imagine a giant vehicle in front of them. Road trains, once popular in Australia, are these nasty giant vehicles consisting of very powerful trucks, or a tractor, pulling two or three trailers at a time.  In Australia, they were used to transport everything from livestock to consumer goods to fuel.\n\n## What does it contain?\nEach trailer has 12 tires plus a dolly with eight tires making a total of 20, except for the truck and first trailer, which has 24. So, each road train has 64 tires not including spares!!\n\n## How big is it?!\n\nIn the United States, the average truck and trailer combo is about 21.3 to 24.4 meters long, and it weighs around 40 tons. In Australia, you can attach two of these trailers to a truck, which is known as a B-double. Add a third trailer, and it's called --you guessed it-- a B-triple. But that still isn't a road train. Those are classified as \"long vehicles.\"\n\n![image]({% static '/blog/assets/img/All-about-Road-Train/image-1.jpg' %})\n\nSo just imagine how big a road train is! A road train, rightly called as Australian Monster, has two or three trailers connected by converter dollies, which add a couple of extra axles to the rig. This combo can be up to 50 meters long - more than twice what we see on American roads. You can even attach a couple of extra trailers to a B-double setup with a converter dolly. This monster can be up to 53.5 meters - the maximum allowed on normal public roads. But there's nothing to limit the length of a road train on private roads.\n\n## You know what?!\n\nThough it is banned in Australia, a few years back, the Indian Government was planning to introduce them in India!! So it's better to know how to play nice with road trains.\n\n## How to play nice with road trains?\n\nIf you are on a road trip with friends and find this huge monster limiting you from becoming Verstappen, then just think it's not your day today!! No dude, don't overtake or else you'll be one of the pancakes we used to see in TV cartoons in our childhood!!  \nHere are a few driving tips to help you not end up looking like a cartoon pancake:\n- Don't ever cut off a road train. Simple physics tells you it will take for-freaking-ever for the brakes to stop so much mass.\n- Don't pass as they turn. If the road train ahead of you has its blinker on, don't try to scoot around in the empty lane. These rigs need so much space to maneuver a turn that they'll use everybody's lane. Everybody's!\n- If you're approaching a road train, you need to not only stay in your own lane, you need to stay as far left in your lane as possible. Chances are that the road train is using its entire lane -- and some of yours, too.\n- Don't pass unless you can see 3 kilometers. Passing a road train 51.8 meters long, even one averaging less than 96 kilometers per hour, is going to take you a long, long, long time. I am just repeating again, before tucking into the opposite lane, make sure you can see at least 3 kilometers ahead before mashing the gas like your life depends on it.\n- Don't slow down. If you're driving along the roads and a road train appears in your rearview mirror, don't slow down so you can get a good look. The truck can't slow down as quickly as you, and there you have it. You're a pancake.\n- As a matter of fact, just stop. If a road train is going to overtake you, rather than dealing with the turbulence and the dust it's going to kick up, just pull over. Way over. If you choose to let the road train blow past you, pull way off the road and wait for the dust to settle.\n\n![image]({% static '/blog/assets/img/All-about-Road-Train/image-2.jpg' %})\n\n## Conclusion\n\nAlthough road trains have the advantage of carrying huge amounts, I mean really huge loads at a time, is it worth it?!\n\nI don't have an exact answer, but I really found this monster interesting!! And if you have made it till here, I hope you did too :)\n\nDo eat pancakes, don't be one!\n\n\n## Recourses\n\n- [https://auto.howstuffworks.com/road-trains.htm](https://auto.howstuffworks.com/road-trains.htm)\n- [https://www.mbandf.com/en/parallel-world/australian-road-trains](https://www.mbandf.com/en/parallel-world/australian-road-trains)",
        "author_name": "Kshamaa Acharya B",
        "slug": "all-about-road-train"
    },
    {
        "layout": "post",
        "title": "Ethical Hacking",
        "author_github": "Meghna-U",
        "date": "2022-08-28T00:00:00",
        "description": "Ethical hacking is a process of finding out and fixing vulnerabilities in an application or software that a potential attacker can use to exploit an individual or organization.This blog gives more insight on Ethical Hacking.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Ethical Hacking"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Meghna-U",
        "text": "[{% load static %}]()\n# Ethical Hacking\n\n## Abstract\n\nThe word hacking has a negative impact on the reader or listener. It reminds us of people with stealing vital information and identities and using them for illegal causes. But that's not always the case. Hacking can also bring a positive change in the society and reduce the negative impacts. This is called ethical hacking where the hacker complies with the rules and regulations. There are various tools and techniques used by hackers.\n\n## What is Ethical Hacking\n\nEthical hacking is a process of finding out vulnerabilities in an application or an organization's infrastructure that a potential attacker can use to exploit an individual or organization.\n\nA hacker is a person who steals the private information of an individual or a business and gains advantages from it from their digital device or network using a variety of hacking tools.\n\n## Types of hackers\n\n1. Black hat hackers: also called criminal hackers, these hackers are cybercriminals who breaks into computer systems with criminal and hostile intention. They are skilled in advanced technical knowledge and have the ability to find vulnerabilities and cause serious harm.\n2. White hat hackers: also called authorized hackers, white hat hackers are cybersecurity professionals who have the authority to hack systems and use their skills to find vulnerabilities in organizational networks and computer systems. They are usually hired by businesses and governments.\n3. Grey hat hackers: They hack into systems and networks without following the rules nor with any malicious intention. They hack just for fun. If they find any vulnerabilities, they'll inform the owner.\n4. Script kiddies: amateur hackers who don't have advanced skills or experience. They use malware created by other hackers to carry out attacks.\n5. Green hat hackers: also called as hackers-in-training, they are just introduced to the world of hacking and focuses on improving their skills and expertise.\n6. Blue hat hackers: Blue hat or authorized software hackers are employed by businesses to test and check a new software for bugs before it's released. Their job is to find vulnerabilities and rectify them.\n7. Red hat hackers: they are government-hired hackers entitled to spot vulnerabilities in security systems and also to hunt for black hat hackers. They use similar tactics as black hat hackers to track them down.\n8. Nation sponsored hackers: these people are hackers are appointed by the government of one country to acquire access to another nation's computer systems..\n9. Whistleblower hackers: are hackers who perform a cyberattack from within the company or business they work for. Ex: to expose an organization's illegal activity, etc.\n10. Botnets: Botnet hackers are hackers who make bots to carry-out high intensity attacks across as many devices as possible. They usually target IoT (Internet of Things) devices like routers and cameras. These bots look for unsecured devices to plant themselves in.\n11. Gaming hackers: are hackers who use their skills to launch attack on their competitors or cause DDoS (Distributed Denial-of-Service) attacks to push them out a game.\n12. Cryptojackers: are hackers who use their skills to find vulnerabilities and exploit resources as a method to mine for cryptocurrencies.\n13. Elite hackers: hackers who have the highest skillset and expertise and considered pioneers and experts in the world of hacking.\n14. Hacktivists: a blend of a hacker and activist, hacktivists utilizes their skills to attack government systems to emphasize or point out towards a certain social or political cause.\n\n## Penetration Testing\n\nPenetration testing is an activity that requires testing applications for security vulnerabilities. Penetration tests can be used to oppose and capture hackers before they cause any harm or damage. Penetration testers design various tests and tools and conduct and run security tests on a variety of networks and servers. They also pin down potential hacking methods used by hackers in near future. They're responsible for spotting security breaches and loopholes.\n\n## SQL Injection\n\nSQL injection could be a code injection approach for attacking data-driven systems that involves inserting malicious SQL statements into an entry field and executing them. SQL injection must profit of a software security flaw, like when user input is erroneously checked for string literal escape characters encoded in SQL statements or when user input isn't strongly typed and executed unexpectedly. SQL injection attacks allow attackers to spoof identity, tamper with existing data, cause repudiation issues like voiding transactions or changing balances, allow complete disclosure of all data on the system, destroy the info or make it otherwise unavailable, and gain access to the database server's administrators.\n\n## Footprinting\n\nThis method, aslo known as reconaissance is used to collect information about the computer systems. To obtain this information, hackers use various techniques like DNS queries, Network enumeration, Network queries and Operating system identification. One software used by hackers to practice footprinting is Wireshark.\n\n## Conclusion\n\nEthical hacking strengthens the computer and network security by using various testing methods. It helps one to take preventive measures against illegal hackers and their exploitations.\n\n## References\n\n- [10 Types Of Hackers To Be Aware Of In 2022](https://www.jigsawacademy.com/blogs/cyber-security/different-types-of-hackers/)\n- [The Importance of Penetration Testing and What it takes to be a Penetration Tester](https://techjobs.sulekha.com/techpulse/the-importance-of-penetration-testing-and-what-it-takes-to-be-a-penetration-tester_17534)\n- [TYPES OF HACKERS](https://www.google.com/url?sa=i&url=https%3A%2F%2Fipcisco.com%2Ftypes-of-hackers%2F&psig=AOvVaw0rXxA3yPjsKekaIJr0s9xn&ust=1653317484415000&source=images&cd=vfe&ved=0CAwQjRxqFwoTCMCfgeGt8_cCFQAAAAAdAAAAABAD)\n- [The Ultimate Guide to Ethical Hacking](https://www.google.com/url?sa=i&url=https%3A%2F%2Fwww.vpnmentor.com%2Fblog%2Fultimate-guide-to-ethical-hacking%2F&psig=AOvVaw3aOCSi9XTX6YpUPf4c34Kk&ust=1653317531796000&source=images&cd=vfe&ved=0CAwQjRxqFwoTCIjS2fOt8_cCFQAAAAAdAAAAABAD)",
        "author_name": "Meghna Uppuluri",
        "slug": "ethical-hacking"
    },
    {
        "layout": "post",
        "title": "ML in Game Beautification",
        "date": "2022-08-28T00:00:00",
        "image": "/assets/img/",
        "description": "A better look into the advent of ML and its use cases in the field of Game Development, specifically Game Beautification.",
        "tags": [
            "IEEE NITK",
            "Compsoc",
            "Computer Vision"
        ],
        "categories": [
            "Compsoc"
        ],
        "github_username": "MistaAsh",
        "text": "[{% load static %}]()\n## **Introduction**\nMachine Learning is currently one of the world's fastest-growing technologies with an increasingly broader range of applications. Apart from the evident Artificial Intelligence, ML has various applications ranging from procedural automation to language translation to data analysis. The advent of the digital age has further increased the use of ML in different fields of work.\n\nBut one of the more unexpected applications of ML has been its increasing use in video games.\n\n## **Image Upscaling**\n\nHistorically, video games have often used ML techniques to create AI-bots to compete against the players. This has been a common practice but has been replaced by more advanced techniques such as Reinforcement Learning.\n\nHowever, in recent times, fans have discovered that machine learning is the perfect tool to improve the graphics of classic games. The technique is known as \"AI Upscaling,\" which uses GANs (Generative Adversarial Networks) trained on millions of low-res and high-res images. This algorithm is then used to convert low-res to high-res images with minimum loss of accuracy. Upscaling, as a general technique, has been around for a long time, but AI has drastically improved the speed and quality of results.\n\n![Doom image enhanced]({% static '/blog/assets/img/ml-in-game-beautification/doom-enhanced.png' %})\n\n### Case Study\nThe same can be seen in the case of the game *Final Fantasy VII*.\n\nOne of the big reasons a game like Final Fantasy VII looks so ugly today is because of its textures. Developers paint these JPG images on polygons so that a box looks like a wall of bricks instead. But when SquareSoft was trying to fit Final Fantasy VII on a handful of CDs, it squashed down the resolution of these assets and threw away the originals. And that posed a big problem for the prerendered background images in Final Fantasy VII.\n\nDevelopers (and fan modders) have a lot of methods to improve the visuals of an old game. But nothing could make a 320-by-240 low-resolution CG background look better short of remaking them from scratch. That process is too time-consuming for fans to do for free. And Square Enix decided it was easier to remake the game instead of doing so.\n\nThis is where machine learning can make a difference. Fans used software AI Gigapixel which worked on the same principle to improve the resolution of Final Fantasy VII's backdrops. The program works through a process of deep learning. Developers feed the system an extremely high resolution of an image and a low-resolution version of the same picture. The AI Gigapixel team then programs its neural network to attempt to make adjustments to the image.\n\n![Final Fantasy VII]({% static '/blog/assets/img/ml-in-game-beautification/ff7-enhanced.png' %})\n\nFurthermore, big graphic-oriented companies like Square Enix have been using machine learning to improve the visuals of their games. <blockquote>\u201cNvidia has been inventing new ways to generate interactive graphics for 25 years, and this is the first time we can do so with a neural network,\u201d said Bryan Catanzaro, who led the team and is also vice president of Nvidia\u2019s deep learning research arm. \u201cNeural networks \u2014 specifically generative models \u2014 will change how graphics are created. This will enable developers to create new scenes at a fraction of the traditional cost.\u201d</blockquote>\n\nTo achieve this, the team based their approach on previous work like Pix2Pix, an open-source image-to-image translation tool that uses neural networks. In addition, the researchers utilized a particular type of unsupervised deep learning algorithm called generative adversarial networks (GANs), which designates one neural network as a \"generator\" and another neural network as a \"discriminator.\" These two networks play a zero-sum game \u2014 with the generator network aiming to produce a synthesized video that the discriminator network cannot ultimately determine as fake.\n\nTraining data was taken from videos of driving sequences, culled from autonomous vehicle research data in various cities, and segmented into multiple categories, such as buildings, cars, trees, etc. The GAN is then fed these data segments to synthesize a variety of fresh and different iterations of these objects to eliminate any perceived sense of d\u00e9j\u00e0 vu.\n\nThe team then used a conventional game engine to produce a virtual urban environment, using the GAN to generate and overlay the synthesized images in real-time. Moreover, to prevent the system from making a video where things might completely change appearance from one frame to the next, the team had to incorporate a kind of short-term memory that would enable the model to remember the attributes of objects consistently.\n\n### Future\nWith the advent of technologies and increasing processing power, machine learning in games has become a viable option. As a result, nowadays, game developers are looking toward ML to improve multiple facets of the games, such as-\n- using AI to create a scenes a little different every time so that user feels that they are in reality which changes over time rather than a game which was created by someone and always look the same.\n- reducing time to render 3D models and animations by using GANs and RL techniques.\n- improving the graphics of the game by using AI Upscaling techniques.\n\n## Resources and Going Further\n* [AI Upscaling](https://www.theverge.com/2019/4/18/18311287/ai-upscaling-algorithms-video-games-mods-modding-esrgan-gigapixelC) - A better understanding of AI Upscaling.\n* [Automating Model Rendering](https://www.youtube.com/watch?v=FlgLxSLsYWQ&t=489s) - A video about how GANs could potentially replace 3D desginers and introduction of procedural workflows.",
        "author_name": "Ashish Bharath",
        "slug": "ml-in-game-beautification"
    },
    {
        "layout": "post",
        "title": "MicroProcessor 8085 Architecture and Features",
        "author_github": "HrishiCoolkarni",
        "date": "2022-09-04T00:00:00",
        "image": "/assets/img/",
        "description": "You do not have conversations with microprocessors. You tell them what to do, then helplessly watch the disaster when they take you literally!",
        "tags": [
            "IEEE NITK",
            "Blog",
            "MicroProcessor"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "HrishiCoolkarni",
        "text": "[{% load static %}]()\nTechnology, like nature, is continuously evolving. At a time when our lives are becoming increasingly tech-centric, we must take a close look at this crucial component. After all, be it laptops, self-driving cars, smartphones, or smart home devices, they all have microprocessors.\n\n### INTRODUCTION\nAn electronic device that runs some sort of program contains a microprocessor or a microcontroller. Microprocessors can be found in laptops, which are an excellent example of their use. Let us analyze with the example of an AC remote if we need a microprocessor or microcontroller, and if yes, what code is being executed. When we press a key on the AC remote, the corresponding function should be enabled. We should also be able to figure out which key was pressed and which code was executed. Also, for the display, there is code running in the background. Wireless data transmission through remote to AC also requires code to be executed. Similarly, there are many devices we come across daily in which we use either a microprocessor or a microcontroller.\n\n### Difference between microprocessor and microcontroller\n\n#### Based on the structure:\nWe need a storage/ memory device to store and execute the programs. We also need input/output ports to receive or transmit the data. The uses of microprocessors vary from light to intense applications, and the memory needed also depends on the application. Hence, the microprocessor has only a CPU, and all memory elements(RAM/ROM) and i/o ports are externally connected. These devices are made up of a single chip and are used for specific applications\n\n### 8085- Introduction and features\n\nToday we are using i5 and i7 processors in our computers. However, still, we start the study with basic microprocessors to understand more advanced processors as these are products of continuous development over several decades.\n\n8085 is an 8-bit microprocessor designed by Intel in 1977. The configurations of 8085 microprocessor mainly include data bus-8-bit, address bus-16 bit, program counter-16-bit, stack pointer-16 bit, registers 8-bit, +5V voltage supply, and operates at 3.2 MHz single segment CLK. The applications of 8085 microprocessors are involved in microwave ovens, washing machines, gadgets, etc.\n\n**The features of the 8085 microprocessor are:**\n\nThis microprocessor is an 8-bit device that receives, operates, or outputs 8-bit information in a simultaneous approach.\nThe processor consists of 16-bit and 8-bit address and data lines, and so the capacity of the device is 216, which is 64KB of memory.\n\nA total of 246 operational codes and 80 instructions are present\nIt has an internal clock generator and operates at a 50% duty cycle.\nThis microprocessor is constructed of a single NMOS chip device and has 6200 transistors.\nThis microprocessor is available in a DIP package of 40 pins\nIt has a mechanism by which it is possible to increase its interrupt handling capacity\nThe 8085 can share the system bus with the Direct Memory Access controller. This Features of the 8085 Microprocessor allow transferring a large amount of data from I/O device to memory or from memory to I/O device with high speeds.\n\n\n### Architectural representation of 8085 microprocessor\n\n![\"image1\"]({% static '/blog/assets/img/architecture-8085/Architecture_8085.jpg' %})\n\n#### Working of 8085 microprocessor\nAs we know, the function of the microprocessor is to execute the instructions. Since the instructions are present in the memory, they must first be fetched from memory. To store the location of the instruction, it uses a  special-purpose register Program counter(PC).\n\nOnce the instruction is fetched from memory, it is sent to the instruction register(IR) via internal data buses. At this point, the fetching of the instruction is finished, and the PC increments itself. Thus, it provides sequencing of the program to be executed. The instruction register sends the instruction to the instruction decoder, where it decodes the information present in the IR and sends it to the timing and control circuit, where it gives the timing and control signals which control the register, the data buffers, ALU, and external peripheral signals depending on the nature of the instruction. Suppose the instruction is ADD  B. It simply tells ALU to add the data present in the B register to that of the accumulator, i.e., register A and store the result in the accumulator; due to such cumulative action, register A is termed an accumulator. After every instruction execution performed by the ALU, the status of the flag register (The flag register holds the status of the current result generated by the ALU and not the generated result) gets changed. So, ALU produces the result and its status simultaneously after each operation. \n\nThis process of fetch, decode, and execute completes the execution of 1 instruction.\n\n\n### Resources:\n\n- [https://electronicsdesk.com/8085-microprocessor.html](https://electronicsdesk.com/8085-microprocessor.html)\n- [https://www.elprocus.com/8085-microprocessor-architecture/](https://www.elprocus.com/8085-microprocessor-architecture/)\n- [https://www.eeeguide.com/8085-microprocessor-architecture-and-its-operations/](https://www.eeeguide.com/8085-microprocessor-architecture-and-its-operations/)\n- [https://www.eeeguide.com/features-of-8085-microprocessor/](https://www.eeeguide.com/features-of-8085-microprocessor/)",
        "author_name": "Hrishikesh Kulkarni",
        "slug": "architecture-8085"
    },
    {
        "layout": "post",
        "title": "SEEFOOD",
        "author_github": "vinayakj02",
        "date": "2022-09-04T00:00:00",
        "image": "/assets/img/",
        "description": "Recreating the SEE-FOOD app from Silicon Valley using Transfer Learning in PyTorch",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Deep Learning",
            "PyTorch",
            "Transfer Learning",
            "CNNs"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "vinayakj02",
        "text": "[{% load static %}]()\nSEEFOOD is an application which basically predicts if an image is an image of a hot-dog or not. This is based on the model used in the [HBO's Silicon Valley ](https://www.youtube.com/watch?v=vIci3C4JkL0).\n\n<p align=\"center\">\n  <img src=\"https://user-images.githubusercontent.com/74676945/176158167-70dacb97-35f2-455b-8cfb-4b32c0caca57.gif\" alt = \"img\">\n</p>\n\nThis blog is a short tutorial on building this classfier model. Knowing the basics of Deep Learning, CNNs and Transfer learning would be helpful in following along. \n\n<br>\n<p align=\"center\">\n<img src=\"https://user-images.githubusercontent.com/74676945/176158166-e24d4065-aae8-4cd8-a6e5-3002ebf15117.gif\" alt = \"img\">\n</p>\n\n### How to start ? \nFirst, we need to get a dataset for training the model. For this, there is a [dataset on kaggle ](https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog) which perfectly matches our needs. So now that we have a dataset, we need to build and train the actual model.  \n \n### Let's start coding. \nThe notebook is [here](https://github.com/vinayakj02/SEEFOOD-classifier/blob/master/seefood_notebook.ipynb) if you want to follow through as you're reading the article. \n\nFirst, off we import all the required libraries.\n```python\n\nimport torch \nfrom torchvision import datasets,transforms, models\nfrom torch import optim , nn\nfrom torch.utils.data import DataLoader\nimport torch.nn.functional as F \nimport matplotlib.pyplot as plt\n```\n\n\nBefore importing the dataset we can use a simple transform to ensure all our images and resized, center cropped, and convert into tensors. \n```python\ntransforms = transforms.Compose([transforms.Resize(256),\n                                 transforms.CenterCrop(224),\n                                 transforms.ToTensor(),])\n\ndataset = datasets.ImageFolder(root='gdrive/My Drive/seefood/train/',transform=transforms)\nvalidation_set = datasets.ImageFolder(root='gdrive/My Drive/seefood/test/',transform=transforms)\n```\n<br>\n\nNow we can import both the test and validation sets. Since the number of images here is not very high we can use a batch size of 20.\n\nOnce the dataset is loaded we can view the images. \n```python\ntrainLoader = DataLoader(dataset, batch_size=20,shuffle= True)\nvalLoader = DataLoader(validation_set,batch_size=20,shuffle=True )\n\ndata = iter(trainLoader)\nimages ,labels = next(data)\nplt.imshow(images[0][0,:,:],cmap='gray')\nplt.title(f\"Label : {labels[0]}\")\n```\n<p align=\"center\" >\n<img src=\"https://user-images.githubusercontent.com/74676945/187061960-e3844d01-b890-4671-bb4f-82e7a51a633f.png\" alt = \"img\" width=\"300\" height=\"300\" />\n </p>\n \n### Building the model\nSince we are dealing with images we would need to use a model which is based on Convolutional Neural Networks (CNNs). The model itself would be a binary classifier to detect whether the food item is a \"hot dog\" or not. Smaller models with lower layers do not tend to work because they are simply not powerful enough to detect the different features, so we can see a good use case of transfer learning here. \n\nFor this purpose we can use the densenet121 model, you can read more about Densely Connected Convolutional Networks in this paper [here](https://arxiv.org/pdf/1608.06993.pdf).\n. Since we are only going to train the last block of the model we can freeze all the other parameters of the model.  \n```python3\nmodel = models.densenet121(pretrained=True)\nfor params in model.parameters():\n  params.require_grad = False\n#model.classifier -> Linear(in_features=1024, out_features=1000, bias=True)\n```\nIf we take a look at the classifier currently we can see it's a classifier that has 1024 input features and 1000 output features. \nWe can redefine this classifier for our case here. \n\n```python\nclassifier = nn.Sequential(nn.Linear(1024,1024),nn.ReLU(),nn.Dropout(p=0.3),\n                           nn.Linear(1024,512),nn.ReLU(),nn.Dropout(p=0.3),\n                           nn.Linear(512,2),nn.LogSoftmax(dim=1))\nmodel.classifier = classifier \n\n```\nThe classifier now, \n```\nmodel.classifier -> Sequential(\n  (0): Linear(in_features=1024, out_features=1024, bias=True)\n  (1): ReLU()\n  (2): Dropout(p=0.3, inplace=False)\n  (3): Linear(in_features=1024, out_features=512, bias=True)\n  (4): ReLU()\n  (5): Dropout(p=0.3, inplace=False)\n  (6): Linear(in_features=512, out_features=2, bias=True)\n  (7): LogSoftmax(dim=1)\n)\n```\n\n\n### Training the model\nFor the loss function here we can use the negative log-likelihood loss. Since we are training only the classifier part of the model, we need to include only the parameters from that block in the optimizer.\n```python\nloss_function = nn.NLLLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.003)\n```\n\n\nIf you are trying to train the model, do make use of GPUs on google colab or kaggle. This speeds up the training process a lot. \nThe standard training loop, \n```python\nloss_graph , val_loss_graph , acc = [] , [] , []\nfor _ in range(6):\n  running_loss = 0\n  val_loss = 0\n  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n  model.train()\n\n  for images, labels in trainLoader:\n\n    images , labels = images.to(device), labels.to(device)\n    optimizer.zero_grad()\n    logits = model(images)\n    loss = loss_function(logits,labels )\n    running_loss += loss.item()\n    loss.backward()\n\n    optimizer.step()\n\n  with torch.no_grad(): \n    model.eval()\n    cor = 0\n    total = 0\n\n    for images,labels in valLoader:\n      images , labels = images.to(device), labels.to(device)\n      predictions = model(images)\n      loss = loss_function(predictions, labels)\n      val_loss += loss.item()\n\n      for p,l in zip(torch.argmax(predictions,dim=1 ),labels):\n        if p==l:\n          cor +=1\n        total +=1 \n\n  loss_graph.append(running_loss/len(trainLoader))\n  val_loss_graph.append(val_loss/len(valLoader))\n  acc.append(cor*100/total)\n  print(f'training loss : {running_loss/len(trainLoader)} , validation loss : {val_loss/len(valLoader)} , Accuracy : {cor*100/total}')\n ```\n\n```training loss : 0.8344280552864075 , validation loss : 0.38366479575634005 , Accuracy : 87.4\ntraining loss : 0.3701903349161148 , validation loss : 0.4923225581645966 , Accuracy : 76.2\ntraining loss : 0.40178473711013796 , validation loss : 0.26429639220237733 , Accuracy : 90.2\ntraining loss : 0.29359916508197786 , validation loss : 0.2639751332998276 , Accuracy : 89.6\ntraining loss : 0.23448901653289794 , validation loss : 0.26386004567146304 , Accuracy : 89.6\n```\n### Result\nOn training it for about 6 epochs I got around 86% accuracy. \nYou can try this [here](https://seefood-nothotdog.herokuapp.com/) \n\n<p align=\"center\" >\n<img src = \"https://user-images.githubusercontent.com/74676945/187063085-454ab22f-b26b-4695-af2b-397f0e808744.png\" alt = \"img\"> \n<img src = \"https://user-images.githubusercontent.com/74676945/187063231-877d2ecb-8dc3-4d3d-8231-53ef749f24c3.png\" alt = \"img\">\n </p>\n \n### Interesting question ?\nWhy do you think densenet121 was used for this model ?\n\n## Links and resources\n- [Github repo containing all the code and model weights](https://github.com/vinayakj02/SEEFOOD-classifier)\n- [Dataset on Kaggle](https://www.kaggle.com/datasets/dansbecker/hot-dog-not-hot-dog)\n- [A practical example to learn Transfer learning with PyTorch](https://towardsdatascience.com/a-practical-example-in-transfer-learning-with-pytorch-846bb835f2db)",
        "author_name": "Vinayak Vatsalya J",
        "slug": "seefood"
    },
    {
        "layout": "post",
        "title": "The screen you are reading on",
        "author_github": "madhanraj13579",
        "date": "2022-09-04T00:00:00",
        "description": "Liquid Crystal Display (LCD) is a type of flat panel display which uses liquid crystals in its primary form of operation. LCDs have a large and varying set of use cases for consumers and businesses, as they can be commonly found in smartphones, televisions, computer monitors, and instrumental panels.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Liquid Crystal Display categories",
            "Piston"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "madhanraj13579",
        "text": "[{% load static %}]()\n# Liquid Crystal Display\n\n## Abstract\n\nLiquid Crystal Display(LCD) is a flat panel display which is used in almost every screen which we are currently using. LCDs were a big leap in terms of the technology they replaced, which include light emitting diodes(LED) and gas-plasma displays. LCDs allowed displays to be much thinner than cathode ray tube technology. LCDs consume much less power than LED and gas-display displays. In short, LCDs are much more efficient than any of the previous generation inventions.\n\n## How does Liquid Crystal Display work?\n\nA display is made up of millions of pixels. The quality of a display commonly refers to the number of pixels; for example, a 4K display is made up of 3840 x2160 or 4096 x2160 pixels. A pixel is made up of three subpixels; a red, blue and green commonly called RGB. When the subpixels in a pixel change color combinations, a different color can be produced.\n\nWith all the pixels on a display working together, the display can make millions of different colors. When the pixels are rapidly switched on and off, a picture is created.\n\nThe way a pixel is controlled is different in each type of display. In short, LCDs are lit by a backlight, and pixels are switched on and off electrically while using liquid crystals to rotate polarized light. A polarized glass filter is placed in front and behind all the pixels, the front filter is placed at 90 degree. In between both filters are the liquid crystals, which can be electrically switched on and off.\n\n![image]({% static '/blog/assets/img/the-screen-you-are-reading-on/image-1.png' %})\n\n## Types of LCDs:\n\n- **Twisted Nematic (TN) :** which are inexpensive while having high response times. However, TN displays have low contrast ratios, viewing angles and color contrasts. \n- **In Panel Switching Displays (IPS Panels) :** which boast much better contrast ratios, viewing angles and color contract when compared to TN LCDs. \n- **Vertical Alignment Panels (VA Panels) :** which are seen as a medium quality between TN and IPS displays.\n- **Advanced Fringe Field Switching (AFFS) :** which is a top performance compared IPS display in color reproduction range.\n    \n\n![image]({% static '/blog/assets/img/the-screen-you-are-reading-on/image-2.jpg' %})\n\n## Advantages of LCDs\n\n- **Energy Efficient :** LCDs are known for their energy-efficient properties. When compared to cathode ray tubes, for example, a typical LCD will use about 25% less power. As a result, LCDs offer cost saving benefits in the form of cheaper utility bills.\n- **Long Lasting :** LCDs have a longer lifespan than that of other display devices. A typical LCD may last for up to 60,000 hours.\n- **LED Backlighting :** LED is the most common type of backlighting used in their construction.\n- **NO Screen Burn-In :** screen burn-in is a phenomenon that occurs in display devices with phosphor-based pixels. Traditional cathode ray tubes fall under this category because their pixels are made of phosphor compounds. LCDs, however, use pixels made of organic materials.\n- **Supports Small and Low-Profile sizes :** LCD devices come in all shapes and sizes. While some of them are large, others are small with a narrow and low-profile design. Smartphones and tablets, for example, often use LCD technology. It allows for small and low-profile designs that isn't possible with other, older display technology.\n    \n\n## Disadvantages of LCDs\n\n- For an LCD the aspect ratio and resolution are fixed and cant be changed. \n- **Viewing Angle :** Restricted viewing angles affect the brightness, contract and colors shown. Wide angles can lead to contrast and color reversal. \n- **Cost :** Considerably more expensive purchase price than comparable cathode ray tubes. \n- **White Saturation :** Saturation and compression can occur due to the bright-end of the intensity scale becoming overloaded. Contract control must be carefully adjusted.\n    \n\n## Some facts on LCDs\n\nTo say LCDs have a lot of pixels would be an understatement. While the exact number varies, most LCDs are now made with over 6 million pixels. You may discover display devices marketed and sold as light-emitting diodes (LED). Based on the name alone, conventional wisdom should lead you to believe they are powered LED display technology. However, LED displays are actually LCD displays. They are only called LEDs because they use LEDs for backlighting.\n\nAccording to Statista, over 200 million LCD TVs are sold each year and that's just one type of LCD device. Others include tablets, smartphones,watches, alarm clocks, smart appliances, human machine interfaces (HMis) and more.\n\nAnd moreover the LCD displays are not affected by the magnets. So, you don't have to worry about magnets damaging your LCD.\n\n![image]({% static '/blog/assets/img/the-screen-you-are-reading-on/image-3.png' %})\n\n## Conclusion\n\nFrom TVs and computer monitors to smartphones, tablets and more, Liquid Crystal Display (LCD) is used to power countless display devices. LCD has become an indispensable part of your life.\n\n## Recourses\n\n- [MOLECULAR AND BIOMOLECULAR ELECTRONICS - AMERICAN CHEMICAL SOCIETY](https://docecity.com/molecular-and-biomolecular-electronics-american-chemical-soc-5f1073cfe2083.html)\n- [Molecular electronics: Some views on transport junctions and beyond](https://www.pnas.org/doi/10.1073/pnas.0500075102)\n- [Advances in Molecular Electronics: A Brief Review](https://www.sciencedirect.com/science/article/pii/S2095809918306453)",
        "author_name": "Madhan Raj R",
        "slug": "The-screen-you-are-reading-on"
    },
    {
        "layout": "post",
        "title": "Cuda Cores",
        "author_github": "Jayasrivastava345",
        "date": "2022-09-11T00:00:00",
        "description": "CUDA is a proprietary technology developed by NVIDIA and stands for Compute Unified Device Architecture ,these are mainly used for parallel computing",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Cuda cores"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Jayasrivastava345",
        "text": "[{% load static %}]()\n# Parallel processing -Cuda Cores\n\n## What are cores? \n\nYou've probably come across the term **CUDA cores** if you've spent any time looking at graphic card specifications.\nIn this article, I'll go over CUDA cores in further detail.\nBut first, let's figure out **what are cores?**\n\nA core is the beating heart of any CPU or GPU, allowing it to focus on only one task at a time while performing it fast and efficiently. The number of cores present in a CPU can determine its overall processing power. The ability of a computer to manipulate data is referred to as processing power.\n\nA single-core CPU is analogous to a human who can either breathe or talk, but not both at the same time. To extend the example, a single-core CPU stops talking when it needs to breathe. It also stops breathing when it needs to talk. Multi-core CPUs, on the other hand, can perform everything at once. Having several cores allows the computer to do a particular type of multitasking called parallel processing.\n\n## What is parallel processing and why parallel processing?\n\nWe must first comprehend the concept of parallel processing before writing more on cores. \n\nParallel processing is a computing technique that uses two or more processors (CPUs) to perform different sections of a larger operation. Breaking up different parts of a task among multiple processors can help reduce the amount of time to run a program. Typically, a computer scientist will use a software tool to break a complex work into many sections and allocate each part to a processor. Each CPU will solve its section, and the data will be reassembled by a software tool in order to read the answer or do the task. Usually, each CPU will normally run and conduct activities in parallel as directed, retrieving data from the computer's memory. Processors will also use software to communicate to stay in sync with one another. Assuming that all of the processors are in sync, the software will combine all of the data components at the end of a task. Computers without multiple processors can still be used in parallel processing if they are networked together to form a cluster.\n\nParallel processing can take many different forms. SIMD and MIMD are two of the most widely utilized varieties. SIMD (single instruction multiple data) is a parallel processing technique. Two or more processors in a computer will follow the same instruction set while handling distinct data. SIMD is most commonly used to examine big data sets with the same set of benchmarks. Another frequent type of parallel processing is MIMD or multiple instructions for multiple data. Each computer contains two or more processors and receives data from several different data streams.\n\nMISD, or multiple instruction single data, is a less common type of parallel processing in which each processor uses a separate method with the same input data. Serial processing (also known as sequential processing) can only perform one task at a time using one processor, but parallel processing can complete numerous tasks using two or more processors. A computer that uses serial processing will take longer to finish a complex task than one that uses parallel processing.\n \n## Cuda cores description\n\nA single-core CPU is quick, but it has its limitations. A multi-core CPU is slower per task, but it can perform multiple tasks simultaneously. \n\nImagine, instead of a CPU with a few cores, a processor with thousands of cores all operating in parallel for specific tasks rather than the more generalized duties that a conventional CPU is faced with. GPUs are equipped with this capability. That's why GPUs are so much slower than CPUs for serial computing yet so much faster for parallel computation.\nThe cores on a GPU about which this blog is focussed on are **CUDA Cores**.\n\nCPU cores and GPU cores have many commonalities, but they also have numerous distinctions. CPU cores are built to execute many instructions at the same time. They're made for general-purpose calculations and can be used for various tasks. GPU cores are only used for one thing: graphics processing.CPUs have a small number of powerful cores, but GPUs have many cores that are substantially less powerful. A GPU excels at parallel tasks, such as calculating the appearance of hundreds of pixels in fractions of a second. The difference between CPUs and GPUs is that each is purpose-built to do different types of processing.\n\n**CUDA** is a proprietary technology developed by NVIDIA and stands for Compute Unified Device Architecture. CUDA Cores are used for many things, but they\u2019re mainly used for enabling efficient parallel computing. A single CUDA core is analogous to a single CPU core. The main distinction is that it is less powerful but has many implementations, allowing for parallel computation.\nA normal CPU has anything from two to sixteen cores, yet even the most fundamental current NVIDIA GPUs have hundreds of CUDA cores. High-end cards, on the other hand, now have thousands of them. CUDA is an interface for accessing those cores and communicating with the rest of your system, not just a group of cores. CUDA cores are the processors that execute those instructions.\n\n## Resources\n\n- [Nice to know: What is CUDA?](https://www.spo-comm.de/en/blog/know-how/what-is-cuda)\n- [CUDA Cores](https://www.gamersnexus.net/dictionary/2-cuda-cores#:~:text=CUDA%20Cores%20are%20parallel%20processors,visually%20to%20the%20end%2Duser)",
        "author_name": "Jaya Srivastava",
        "slug": "cuda-cores"
    },
    {
        "layout": "post",
        "title": "Intro to Blockchain",
        "date": "2022-09-11T00:00:00",
        "image": "/assets/img/",
        "description": "Intro to BlockChain",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "BlockChain"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "anuragkumar2121",
        "text": "[{% load static %}]()\n## **Introduction**\n\n![alt_text]({% static '/blog/assets/img/BlockChain/1.png' %})\n\nA blockchain is a distributed software network that functions both as a digital ledger and a mechanism enabling the secure transfer of assets without an intermediary. Just as the internet is a technology that facilitates the digital flow of information, blockchain is a technology that facilitates the digital exchange of units of value. Anything from currencies to land titles to votes can be tokenized, stored, and exchanged on a blockchain network.\n\n## **The root of it all\u2026 Bitcoin**\n\n![alt_text]({% static '/blog/assets/img/BlockChain/2.png' %})\n\n* In 2008, after the financial crisis, an anonymous programmer by the pseudonym Satoshi Nakamoto invented Bitcoin, a peer-to-peer electronic cash system based on a new form of database technology called blockchain.\n* While traditional currencies are backed by trust in 3rd parties, like the bank, bitcoin is backed by mathematics. It helps in creating a trustless environment, because the mathematics itself creates the trust.\n* It uses the concept of a distributed ledger system.\n\n## **Distributed Ledger System**\n\n![alt_text]({% static '/blog/assets/img/BlockChain/3.png' %})\n\n* Each player in the bitcoin system maintains their own personal ledger with all the set of transactions till date. The transactions are immutable in nature, i.e, once written they cannot be changed.\n* Whenever a new transaction is made, it is hashed via a special function called cryptographic hash function. This is a special function \nf(Message, Private Key) which returns the hash of the message using the private key of user.\n* The cryptographic hash function is such that it is difficult to invert and thus going from the hashed output back to the message is incredibly difficult. For example, the CHF called SHA256 generates a 256 bit unique hash for each input, and therefore the probability of a hash corresponding to a message is 2^-256  , which is practically 0.\n* When this message is received by Alice, she can verify that it was indeed sent by Bob by using her public key using a function verify (Message, Hash, Public Key).\n\n## **Nonce and Proof of Work**\n\n![alt_text]({% static '/blog/assets/img/BlockChain/4.png' %})\n\nOk, so that's it for theory. Now let's see where we can implement it. In the supply chain, it has great utility. But how? So let's take example of mushrooms. Let's say I buy mushrooms from some farmer in Udupi, I will make a block with that information of when, where and how much mushrooms were bought from which farmer. So in the next step, this goes in some processing plant so even at this stage I will store the information related to processing and this way I have all the information of all steps travelled by mushroom before reaching to customer. Now in case of any defect, in just a matter of minutes, we can go back and see where the issue is.\n\nIn Bitcoin\u2019s Proof of Work mining process, the goal is to solve a mathematical puzzle in order to discover the next block hash and receive Bitcoin rewards. Miners must find a nonce value that, when plugged into the hashing algorithm, generates a hash value that is lower than the target difficulty.\n\nIt can be said that the nonce is the missing piece of the puzzle needed to discover the next block and miners receive the block reward.  So basically, mining revolves around brute-forcing the nonce through a mathematical algorithm and finding that lucky number that will reward the miners for their efforts.",
        "author_name": "Anurag Kumar",
        "slug": "into-to-blockchain"
    },
    {
        "layout": "post",
        "title": "Review of our Existing Power",
        "author_github": "mohiteManthan",
        "date": "2022-09-11T00:00:00",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Energy"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "mohiteManthan",
        "text": "[{% load static %}]()\n## Energy: Forward and Beyond\n\nThe one thing that is constantly the most talked about in science is the current forms of energy generation and sourcing. The world has seen its transition from non-renewable to renewable sources of energy through the last decades. However, the growth in the consumption of renewable sources of energy has been limited to certain countries and regions of the world. Hence, there have been attempts to create methodologies to harness even more different types of renewable sources of energy. We will be reviewing some of the recent developments in renewable sources of energy and the extent to which they are used in the current times.\n\n## The contemporary sources of renewable energy\n\n1. **Solar energy** - This is probably the most common renewable source of energy that is currently being harnessed in the world. Solar energy comes with its own advantages and disadvantages. Hence, the main disadvantage being the constant need for a storage device required with it. However, after an intervention by a lot of major energy companies and especially after newer names like SolarCity making it big in the industry, the number of houses running on solar energy has increased exponentially.\n2. **Wind Energy** - Wind energy is again one of the most common renewable sources of energy, it also comes with the similar disadvantages like those of solar energy. Wind farms have been ever increasing in developed countries like the USA and the UK, however the reach of it is reduced due to the high initial cost of installation of the wind turbine. The wind turbine also requires some specific locations to be installed, they cannot be installed anywhere and everywhere, a certain demographic study of the wind patterns is needed to check whether a wind turbine can be installed or not.  \n3. **Hydro energy** - In the areas of renewable energy, hydro energy is one of the most commercially developed renewable source of energy. Due to the increase in the number of water reservoirs and dams in recent times the number of hydro electric turbines installed has increased drastically. However, the use of hydro energy is restriction so certain areas, which limits its use.  \n  \nNow we will take a look at the newer forms of renewable energy sources, and some recent developments in the ways in which renewable energy sources are harnessed.  \n  \n## Recent developments in renewable energy sources \n  \n1. **Algal biofuels** - Considering the current state of fossil fuels, algae offers a commercial potential. SInce in algae the carbon dioxide has recently been taken from the atmosphere by photosynthesis, the environmental impact is very minimal. Algal fuel production has very little to no impact on the land and water resources used. Hence, algal fuel could be the future fuel for our vehicles.  \n2. **Bioalcohols** - As we have seen in the case of algal fuels, bioalcohols provide similar advantages. Along with that bio alcohols like biodiesel and ethanol are already commercially available which means that they have high commercial potential in the market in near future.\n3. **Piezoelectricity** - Piezoelectric devices the energy developed by pressure, that is, piezoelectricity is the process of using crystals to convert mechanical energy into electrical energy, or vice versa. There has been extensive research going on in the areas of harnessing energy from daily activities and piezoelectricity has been one of the successful attempts in doing so. Piezoelectric devices can be installed anywhere, like dancefloors, railway stations and hence when people step on these platforms the mechanical energy transferred to the platform by walking is converted into electrical energy.  \n  \n\n## Other new harnessable forms of energy\n\n1. **Body heat** - The research in the area of harnessing energy from body heat is still scarce and a lot of developments are needed to make it commercially successful.  \n2. **Solar wind** - Solar winds can reach upto 500 miles per hour of speed and hence could be a major source of energy. If we find a way to harness solar wind energy, the energy produced in one hour by solar wind would be even more than the energy used by the whole world in one year. However, the research in this domain is still scarce and hence its very difficult to see the use of solar wind energy in the coming years.",
        "author_name": "Mohite Manthan Ravindra",
        "slug": "review-of-our-energy"
    },
    {
        "layout": "post",
        "title": "Transformers",
        "author_github": "madhavkumar1523",
        "date": "2022-09-11T00:00:00",
        "image": "/assets/img/",
        "description": "An introduction to transformers in machine learning",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Deep learning"
        ],
        "categories": [
            "compsoc"
        ],
        "github_username": "madhavkumar1523",
        "text": "[{% load static %}]()\n<figure>\n  <img src=\"/blog/assets/img/transformers/transformers_1.jpg\" alt=\"Optimus Prime\" style=\"width:100%\">\n  <figcaption>Photo by \u00a9Paramount Pictures </figcaption>\n</figure>\nDeep learning is progressing at a rapid pace and everyday there are some breakthroughs in the field of artificial intelligence. Recently, OpenAI came up with their DALL.E2 which creates realistic images and art from a description in natural language. It is all the talk right now. There are so many amazing models that it is hard to keep track of all of them. One particular neural network model changed the way common natural language processing tasks are done. It is called a Transformer which used the attention mechanism to do effective natural language processing tasks.\n\n## A brief introduction to sequence-to-sequence(seq2seq) models\n<figure>\n  <img src=\"/blog/assets/img/transformers/seq2seq.png\" alt=\"A basic seq-2-seq model\" style=\"width:100%\">\n  <figcaption>A basic seq-2-seq model</figcaption>\n</figure>\nSeq2seq models convert one type of sequence to some other type of sequence. It may be converting sentence in one language to a sentence in some other language. They can also be converting audio to text. I will be focusing on the text2text sequence models in this blog. Generally, in seq2seq models, we use LSTM(long short term memory) models which keeps track of the sequence while remembering(or forgetting) the important(or unimportant) parts of the sequence. There are other models like GRU and RNN which can be used in specific usecases.\n\nSeq2seq models consist of two components, the encoder and the decoder. The encoder operates on the input and converts it into encodings. The decoder takes these encodings and decode them to give an output. This output is then compared with the expected output and accordingly the parameters of encoder and decoder changes and the model learns. If the last few sentences seemed too complex to you, think of it this way, the encoder and decoder know a common language. The encoder converts the input into that common language and the decoder then converts this common language into the output language, sort of like translators.\n\n## Attention is all you need\nThe paper [\"Attention is all you need\"](https://arxiv.org/abs/1706.03762) introduced the novel architecture called Transformers. It uses attention mechanism for sequence transduction. But what exactly is attention mechanism? Let's understand it.\n\nAttention mechanism goes through an input and at each step decides which other parts of the sentence are important. Let me explain it with a real world example. While reading a sentence, you go through the entire sentence but hold on to the keywords to have the context of the sentence. Attention mechanism does something similar. It gives more attention to the words or parts of the sequence which gives context of the whole sequence.\n\nTransformer is also a seq2seq model but it differs from the existing seq2seq models as it does not use recurrent neural networks. Let's take a look at the transformer model architecture.\n\n<img src=\"/blog/assets/img/transformers/transformers_2.png\" alt=\"Transformer Model\" style=\"width:100%\">\n\nThe encoder is made of modules that can be stacked on top of each other multiple times. Each module mainly consists of multi-head attention and feed forward layers. We can see a positional encoding at the bottom of both encoder and decoder. This is to give each word a relative position to remember the order of the sequence. The input and output sentences are first embedded into an n-dimensional space. Let's define the attention equation first.\n\n<img src=\"/blog/assets/img/transformers/transformers_3.png\" alt=\"Attention\" style=\"width:100%\">\n\nHere, Q is query matrix that contains the query (vector representation of one word in the sequence), K are all the keys (vector representations of all the words in the sequence) and V are the values, which are again the vector representations of all the words in the sequence. In simple terms, think of Q as asking a question for a particular word of the sentence as to how close or far is it from the other words of the sentence. K represents the words with which we are comparing our query word. Each word has a K vector. Now, once we have found out how close each word is with the query word, we multiply this similarity with the value(V) of each word to get a proper number. For the encoder and decoder, multi-head attention modules, V consists of the same word sequence than Q. However, for the attention module that is taking into account the encoder and the decoder sequences, V is different from the sequence represented by Q. Here, d_k is the dimension of the space into which the input and output are embedded.\n\nTaking the analogy of linear regression, we can think that the values in V are multiplied and summed with weights which are defined as follows:\n\n<img src=\"/blog/assets/img/transformers/transformers_4.png\" alt=\"Weights\" style=\"width:100%\">\n\nIt means that weights a is defined as how each word (represented by Q) is influenced by all the other words in the sequence (represented by K). Because of the softmax function, the weights will have values from 0 to 1. These weights are then applied to all the words in the sequence that are introduced in V(same vectors than Q for encoder and decoder but different for the module that has encoder and decoder inputs).\n\n<img src=\"/blog/assets/img/transformers/transformers_5.png\" alt=\"multi-head attention\" style=\"width:100%\">\n\nIn the multi-head attention model, the attention mechanism is parallelized into multiple mechanisms. The attention mechanism is repeated multiple times with linear projections of Q, K and V. Hence the model learns with different values of Q,K,V which inceases the accuracy of the model.\n\nNow, let's see how the training of this big model takes place. We put the input into the encoder and it gives out the encodings. We then put the output into the decoder and it gives out the encodings too, but there is a catch here. It shift the positions of the words in the decoder layer by 1 to the right. This allows the model to learn the words on its own instead of just copying the output sentence's words. It tries to guess what comes next based on the part of the sentence it has seen till now. Also, it applies a mask to the input in the first multi-head attention, the reason being that if we input all the words together, the model can see the possible future sequence elements and hence not learn properly. The training of this model is extremely fast because it does not go through the sentences sequentially, it puts all of them together and parallelizes learning. This parallelization speeds up the whole training process a lot.\n\nThis blog is just a small introduction to Transformers. There are many usecases and amazing models based on this architecture. I would encourage you to go and check them out and probably make a model of your own :)",
        "author_name": "Madhav Kumar",
        "slug": "transformers"
    },
    {
        "layout": "post",
        "title": "Complexity Theory",
        "author_github": "Rohan-Kamat",
        "date": "2022-09-18T00:00:00",
        "image": "/assets/img/",
        "description": "Post on the basic ideas of complexity theory",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Algorithm analysis"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "Rohan-Kamat",
        "text": "[{% load static %}]()\n# COMPLEXITY THEORY\nComplexity theory is a field focusing on classifying computational problems based on the  resources utilized, and relating these classes to each other. Complexity of a problem refers to the amount of resources utilized by it. A computational problem is one which can be solved by the mechanical and electronic implementation of mathematical steps. To accomplish this, we need an algorithm, which is a step-by-step procedure to solve a problem. Complexity theory deals with the complexity of problems which can\u2019t get a better complexity regardless of the algorithm used for solving it.\n\nResources that a computational problem might require:\n- Computation time- depends on various factors such as the CPU clock cycle speed, Algorithm used, Computer Architecture and Organisation.\n- Number of system level steps it takes to solve the problem \u2013 Depends on the Instruction Set Architecture of the sytem.\n- Memory Space\n- Storage\n\nIt is generally noticed that as the input size of a problem gets larger, the resources utilized by the problem also increase. Taking all the above mentioned resources into account and the fact that the resource utilization depends on the input size, a tool was developed to effectively represent the complexity of an algorithm as a function of its input size called \u2013 the **asymptotic notation**\n\n## ASYMPTOTIC NOTATION\nWhile defining the asymptotic running time of an algorithm, we assume that the input size is always given as an integer value. In other words, the domain of any function that defines the running time of an algorithm is the set of natural numbers (N = {1, 2, 3 \u2026.})\n\nSay T(n) represents the running time of an algorithm, which has been calculated taking all the above mentioned facts into consideration. But the exact running time of an algorithm is of little use to any programmer and it also takes enough time to find it. Instead, we can just know the abstract running time, using which we can easily compare two algorithms and tell which one is better. This is the asymptotic notation, which is applied to running times of algorithms.\n\n- **The Big O notation**:\nA function f(n) belongs to O(g(n)) if f(n) < cg(n) for some c>0 and all n> some N0. This defines the worst case time of the algorithm.\n![image_1]({% static '/blog/assets/img/complexity-theory/Big_O.png' %})\n- **The Big theta notation:**\nA function f(n) belongs to \u0398(g(n)) if c1g(n) < f(n) < c2g(n) for c1 > 0 and c2 > 0 and all n > some N0. This defines average case time of the algorithm.\n![image_2]({% static '/blog/assets/img/complexity-theory/Big_theta.png' %})\n- **The Big Omega notation:**\nA function f(n) belongs to \u2126(g(n)) if cg(n) < f(n) for c>0 and all n>some N0. This defines the Best case time of the algorithm.\n![image_3]({% static '/blog/assets/img/complexity-theory/Big_omega.png' %})\n\n\nPOLYNOMIAL TIME\n---------------\n\nPolynomial time or polytime is the time represented by O(nk), i.e. any problem whose worst case running time is of the order of nk is said to execute in polytime.\n\nCLASSES OF COMPUTATIONAL PROBLEMS\n---------------------------------\n\n- **P class of problems:** These are the class of problems which can be solved in polytime, i.e. there exists an algorithm to solve these problems in O(nk)time.\n- **NP class of problems:** These are the class of problems which can be verified in polytime. This means that they may or may not have a polytime solution but given a solution to the problem, it can be verified in polytime whether the solution given is valid or not. Evidently the NP class of problems are decision problems, i.e their output is either yes or no. P is a subset of NP, as any problem that has a polytime solution can also be verified in polytime. Whether it is a proper subset is still an open question.\n\n### POLYNOMIAL TIME REDUCTIONS\n\n    This concept is used to solve problems using the solutions to other problems. If an algorithm to solve one problem is known to us and we need to solve another problem, we can convert/reduce the second problem to an input of the first one and use the output of the first problem hence obtained as the output to the second problem as well. However,\n\n    1.  The reduction must be carried out in polytime\n    2.  The number of times the first problem has to be solved to solve the second one also must be polynomial.\n\n    Then the second problem is said to be polytime reducible to the first one. This reduction indicates that the second problem is no harder than the first one, i.e. if we can efficiently solve the first problem we can solve the second one as well.\n\n    An instance of a problem is the problem along with its inputs(Say if A is our problem A(a,b,c) where a,b,c are inputs, is an instance). Technically speaking, an instance of problem A has to be reduced to an instance of problem B to solve problem A using B.\n\n![image_4]({% static '/blog/assets/img/complexity-theory/Polytime_reduction.png' %})\n\n* **NP-Complete(NPC):** These are the class of problems which are also a subset of NP and all problems in the class NP can be polytime reduced to these problems. NPC problems are of great significance as they have the power to answer the \u201cquestion of the millennium\u201d \u2013 is P=NP or P\u2260NP. This is because, if a polytime algorithm can be generated to solve any of the NPC problems, it would imply that all NP problems can also be solved in polytime, since they are polytime reducible to the NPC problem. Hence P=NP. On the other hand if it can be proved that no polytime algorithm exists for any of the NPC problems, it would also imply that no polytime algorithm exists for any NP problem, i.e. P\u2260NP.\n* **NP-hard:** NP-hard problems are those which may or may not have a polytime verifier but all NP problems can be polytime reduced to NP-hard problems. NPC = NP \u0548 NP-hard\n\n![Complexity hierarchy]({% static '/blog/assets/img/complexity-theory/Complexity_hierarchy.png' %})",
        "author_name": "Rohan Kamat",
        "slug": "complexity-theory"
    },
    {
        "layout": "post",
        "title": "Hydrogel and its usage",
        "author_github": "raghawendra",
        "date": "2022-09-18T00:00:00",
        "description": "A hydrogel is a crosslinked hydrophilic polymer that does not dissolve in water. They are highly absorbent yet maintain well defined structures. These properties underpin several applications, especially in the biomedical area. Many hydrogels are synthetic, but some are derived from nature.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Hydrogel and its usage"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "raghawendra",
        "comments": false,
        "text": "[{% load static %}]()\n# Hydrogel and its usage\n\n## Abstract\n\nOver the past decades, science has come up with a wide array of peculiar materials that offered solutions to the existing problems all while creating new ones. An exciting example of these peculiar materials is hydrogels. Hydrogels are crosslinked polymer networks with an excellent absorption capacity. Hydrogels exhibit the properties of elastic solids with deformability and softness.\n\nHydrogels show great swelling and deswelling behaviours in response to the changes in environmental conditions such as pH, temperature, solvent composition, enzymes, electric fields, and light and interesting thing is that we can manipulate properties of hydrogels through different preparation methods and using different types of polymers in the network structure. So, that we can use hydrogel at many desired place by controlling its property but this area is yet to be explored more by science.\n\nMost importantly, hydrogels are biocompatible materials since they are composed of polymer chains. The responsive nature and biocompatibility of hydrogel to environmental changes of hydrogels are useful in many different application areas such as drug delivery systems, tissue engineering, optics, diagnostics, and imaging.\n  \n## What are hydrogels?\n\nHydrogels are three-dimensional (3D) cross-linked polymer networks, which can absorb and retain large amount of water. The formation of hydrogels involves a cross-linking process of polymer chains. Such process is also known as the \u201cgelation\u201d process. Based on different gelation mechanisms, gelation can take place either by physical cross-linking (physical gelation) or by chemical cross-linking (chemical gelation) of polymer chains. In the case of physically cross-linked hydrogels, the formation of hydrogel network results from various strong/weak intermolecular interactions between the polymer chains  e.g., hydrogen bonds, electrostatic interactions, hydrophobic interactions, crystallization etc.\n\n## Properties of Hydrogel\n\nGenerally, the properties of the hydrogel can be characterized from about its five aspects,  Which are :\n\n**A. Physical**\n\nIt includes  swelling ratio, degradability, thermal stability, porosity, etc\n\n**B. Chemical**\n\nThe chemical properties of hydrogels include the chemical composition and the types of functional groups present in the network.\n\n**C. Mechanical**\n\nThe mechanical properties of hydrogels include Young\u2019s modulus, tensile/compression strength, tensile/compression strain, toughness, etc\n\n**D. Rheological**\n\nWhich includes loss/storage modulus, viscosity of the hydrogel. as hydrogels can retain large amount of water within the cross-linked polymer network, they exhibit unique viscoelastic characteristics when undergoing deformation. Using rheometer we can find its viscoelasticity.\n\n**E. Biological**\n\nWhich includes biocompatibility properties.  So that they can be used in drug delivery system or tissue healing properties or tissue-engineering.\n\nOne of important properties are \u201cself-healing\u201d which refers to the ability of a material to heal the damage autonomously and  regain its original properties. Developing advanced self-healing hydrogels has attracted much research interests since the self-healing property could not only prolong the lifespan but also improve the reliability and durability of the hydrogels in various biomedical and engineering\n\napplications. Its self-healing performance can be studied by evaluating the recovery of mechanical properties using tensile test\n\n## Advantages\n  \nThe main advantage current and potential advantages of hydrogels are that they possess a degree of flexibility very similar to natural tissue, due to their significant water content. They are biocompatible, biodegradable and can be injected. Hydrogels also possess good transport properties and easy to modify. Environmentally sensitive hydrogels have the ability to sense changes of pH, temperature, or the concentration of metabolite and release their load as result of such a change.\n    \n## Disadvantages\n\nHydrogel are expensive, these are hard to handle and usually weak in nature. In medical application it is difficult to sterlise and some time it react with and cause coagulation in drug delivery system.\n  \n## Conclusion\nConventionally people think that hydrogel to be mechanically weak with low toughness and having fracture energy (<10J/m2)  Such weak mechanical properties limit their uses in a wide range of applications, such as sensors and actuators, soft robotics, and artificial cartilage. These mentioned application have again their biological significance in treatment.\nHydrogel materials generally exhibit a number of properties including permeability to oxygen and nutrients, which make these materials attractive for use in biological applications. Hydrogel is thus a promising candidate for TE. Scientists are researching to grow replacement body parts in hydrogels in the future.\n\n## Resources\n\n- [https://nanografi.com/blog/explained-hydrogels/](https://nanografi.com/blog/explained-hydrogels/)\n- [Persistent link using digital object identifier](https://doi.org/10.1016/B978-0-12-820541-9.00008-9)\n- [https://doi.org/10.1016/B978-0-12-816806-6.00010-8](https://doi.org/10.1016/B978-0-12-816806-6.00010-8)",
        "author_name": "Raghwendra Pratap Yadav",
        "slug": "Hydrogel-as-Conductor"
    },
    {
        "layout": "post",
        "title": "Photonic Integrated Circuits",
        "author_github": "gagana-05",
        "date": "2022-09-18T00:00:00",
        "description": "Breif on Photonic Integrated Circuits (PICs) and it's Advantages",
        "tags": [
            "IEEE NITK",
            "ICs",
            "Digital Electronics"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "gagana-05",
        "text": "[{% load static %}]()\n# PHOTONIC INTEGRATED CIRCUITS\n\nA photonic integrated circuit (PIC), also known as an integrated optical circuit, is a device that combines numerous (at least two) photonic functions, comparable to an electronic IC. A PIC provides functionality for information signals imposed on optical wavelengths commonly in the visible spectrum or near-infrared (850 nm-1700 nm).\nIn 2005, an issue with quantum noise emerged during the development of laser light through silicon in an electronic integrated circuit, preventing the generation. This challenge was easily solved by a photonic integrated circuit, which produced laser light within the circuit as a single medium and at a higher bandwidth. As a result, the significance of PIC was recognised.\n\n\n## Comparison to electronic integration\n\nThe primary distinction between PICs and Electronic ICs is the type of material employed in their manufacture. Silicon is the most common material used in electronic ICs. However, in the case of PIC, the fabrication material is primarily determined by the device's purpose. That is, the material will be determined by the device's intended function. For example, silica (silicon dioxide) based PICs have very desirable properties for passive photonic circuits such as AWGs due to their comparatively low losses and low thermal sensitivity, GaAs or InP based PICs allow direct integration of light sources, and Silicon PICs allow photonics and transistor-based electronics to be integrated together.\n\nTake a look at a GaAs-InP Photonic integrated circuit.\n\n![pic](https://user-images.githubusercontent.com/82756709/160186144-0e5e40ba-7a54-4bc4-ab88-373f0b521121.gif)\n\n## Fabrication Methods\nBoth devices are made using the same techniques. The etching and material deposition procedures used in photolithography are the same. The fundamental difference is in the fabrication device itself. The transistor is the most important component in an electronic IC. However, there is no single main device that dominates the fabrication in PIC. The ranges of fabrication devices vary depending on the application, as the devices to be integrated are larger than those utilised in an electronic integrated circuit. Optical amplifiers, filters, low loss-high efficiency interconnect waveguides, detectors, power splitters, modulators, and lasers are among the devices. The techniques and steps get more challenging as different materials are required to build all of these devices on a single chip.\nHowever, utilising the resonant photonic interferometry process, researchers have recently devised methods to create PICs. We can simply and cheaply develop ultraviolet light-emitting diodes (LED) using this technology. We can easily overcome optical computing issues by using such LEDs.\n\n## Photonic Integration Methods\nThere exists mainly two kinds of photonic integration methods. They are\n- Hybrid Photonic Integration\n- Monolithic Photonic Integration\n\nThe developed integrated IC for Hybrid Photonic Integration will be a single package. This kit will include several photonic devices that all do the same task. Because of this benefit, many ICs are produced using this technology in order to combine a large number of integrated optic devices.\n\n![41377_2020_272_Fig1_HTML](https://user-images.githubusercontent.com/82756709/160185775-974e358c-6074-4b12-b0d1-394013a787f8.png)\n\nMonolithic Photonic Integration combines a number of optical devices with distinct functions to build a single integrated circuit. The construction of such a gadget will be difficult due to the large number of materials required. To manufacture and integrate onto a common substrate, all of these materials must be fabricated. As a result, a single chip can perform multiple functions.\n\n![image-59-1024x413](https://user-images.githubusercontent.com/82756709/160186015-b082d71a-51f0-42e8-97de-48d14ccc1fc9.png)\n\n## Applications of Photonic Integrated Circuits\nExternally Modulated Lasers (EML) which combine a distributed feedback laser diode with an electro-absorption modulator on a single Indium-Phosphide [InP] chip, are used in fibre-optic communication.\nIt has a lot of potential in Wavelength Division Multiplexed (WDM) fibre-optic communication systems, where an Arrays Waveguide Grating (AWG) must be constructed. Optical multiplexers and demultiplexers often use AWG.\nBiomedical and photonic computing applications\nOptical sensors and metrology are examples of applications.\n\n## Advantages of Photonic Integrated Circuits\nThe employment of optical devices helps to make the entire system more discreet, small, and high-performing.\nThe chip can also be integrated with simple electronic circuits, allowing it to perform a wider range of tasks.\nThough rare, the neutron flux impact can harm PICs, causing some of their functionalities to be lost. When it comes to the effects of electromagnetic pulse [EMP], however, there will be no issues like there are with electrical ICs.\nFuture\nThis technology has not yet reached its pinnacle of development. People are working to make this technology more common, low-cost, and efficient through continuous study. However, practically all electronic ICs will be replaced by PICs in the future. There may also be instances where both electronics and optics can be integrated.\n\n## References\n1. [https://opg.optica.org/oe/fulltext.cfm?uri=oe-12-8-1583&id=79592](https://opg.optica.org/oe/fulltext.cfm?uri=oe-12-8-1583&id=79592)\n2. [https://ieeexplore.ieee.org/abstract/document/81373](https://ieeexplore.ieee.org/abstract/document/81373)\n3. [https://www.circuitstoday.com/photonic-integrated-circuit](https://www.circuitstoday.com/photonic-integrated-circuit)",
        "author_name": "K A Gaganashree",
        "slug": "photonic-integrated-circuits"
    },
    {
        "layout": "post",
        "title": "Evolution of Microprocessor",
        "author_github": "rishee-k",
        "date": "2022-10-02T00:00:00",
        "description": "Microprocessor is a control unit of a microcomputer, made on a small chip capable of performing ALU (arithmetic logic unit) operations and communicating with  other devices connected to it. Let\u2019s see about its evolution.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Evolution of Microprocessor"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "rishee-k",
        "text": "[{% load static %}]()\n# Evolution of Microprocessor\n\n## Abstract\n\nLike the evolution of the human being, from ape to a well-developed and educated gentleman; Computer processors have also undergone massive changes during their journey from 1971 to 2013, and advancements in this area are set to explode in the near future.\n\nServer, workstation, etc. they are used in seismic data processing. When designing a processor, there are always some things that focus on: \n1. Small size\n2. High speed\n3. Better performance and efficiency\n4. Low power consumption, in turn low heat dissipation. \n\n## What is Microprocessor?\n\n\nThe evolution of microprocessors has become the brains of millions of gadgets since 1971, which means that without processors these wonderful innovations of millions of gadgets would not have  been possible. Come now to discover gadgets where the CPU has played a central role for more than 40 long years.\n\n## Types of Microprocessors\n\nThere are basically 5 types of microprocessors:\n1. Complex Instruction Set Microprocessors\n2. Reduced Instruction Set Microprocessor\n3. Superscalar Processors\n4. Application Specific Integrated Circuit\n5. Digital Signal Multiprocessors\n\n\n\n![istockphoto-1296135863-170667a](https://user-images.githubusercontent.com/78703003/166898405-15dbbeb3-d51e-4cbc-983f-c0a89c95b728.jpg)\n\n\n\n\n\n## Complex Instruction Set Microprocessors\n\nThey are also called CISM for short and classify a microprocessor in which commands can be executed alongside other low-level tasks. It mainly performs the task of uploading, downloading and recovering data to and from  memory card. In addition to that, it also performs complex mathematical calculations within a single command.\n\n## Reduced Instruction Set Microprocessor\n\nThis processor is also called RISC. These types of chips are made based on the function where the microprocessor can do small things in a given command. This way it executes more commands at  faster speed.\n\n## Application Specific Integrated Circuit\n\n\n![images](https://user-images.githubusercontent.com/78703003/166898600-70826849-6290-41e0-93e5-4fb3f181b413.jpg)\n\n\n\nThis processor is also known as ASIC. They are used for specific purposes that comprise automotive emissions control or personal digital assistants\u2019 computer. \n\n\n![download](https://user-images.githubusercontent.com/78703003/166898902-603b3f3b-62fe-4fcd-ae9c-0f7b6533b930.jpg)\n\n\n\n## Digital Signal Multiprocessors\n\nAlso called DSPs, they are used to encode and decode video or  convert  digital and video to analog and analog to digital. They need an excellent microprocessor for mathematical calculations. Chips from this processor are used in SONAR, RADAR, home theater audio equipment, cell phones, and TV set-top boxes.\n\n## Future of microprocessor\nMicroprocessor technology has improved performance by three orders of magnitude over the past two decades, so continuing this trajectory would require at least a 30-fold increase in performance  by 2020. Microprocessor performance scaling is facing new challenges .Innovation of energy-efficient microarchitecture innovations developed over the past two decades. Additionally, chip architects  face these challenges with a continued industry expectation of a 30x performance increase over the next decade and 1000x  by 2030.\n\nAs the transistor shrinks, the supply voltage decreases, as does the threshold voltage of the transistor (when the transistor begins to conduct). But the transistor is not a perfect switch, it loses a small amount of current when turned off, increasing exponentially as the threshold voltage decreases. Additionally, the integrating capacitance of exponentially rising transistors exacerbates the effect; therefore, a substantial part of the energy consumption is due to losses.To keep losses under control, the threshold voltage cannot be lowered any further and, in fact, must rise, which reduces transistor performance. As transistors have reached atomic size, lithography and variability pose further scaling issues, which affect supply voltage scaling. With limited scaling of the supply voltage, energy and power reduction are limited, which negatively affects the subsequent integration of transistors. Therefore, the ability to integrate transistors will continue with scaling, albeit with limited performance and power benefits. The challenge for chip architects is to use this integration capability to continue to improve performance.\n\n## Resourses\n\n- [The Future of Microprocessors](https://cacm.acm.org/magazines/2011/5/107702-the-future-of-microprocessors/fulltext#:~:text=Multiple%20cores%20and%20customization%20will,customization%20can%20reduce%20execution%20latency.)",
        "author_name": "Rishee K",
        "slug": "evolution-of-microprocessor"
    },
    {
        "layout": "post",
        "title": "Object Detection with Deep Learning",
        "author_github": "RadhikaChhabra17",
        "date": "2022-10-02T00:00:00",
        "image": "/assets/img/",
        "description": "Have you ever wondered how self driving cars detect obstacles on their way or how image captioning works? All this is possible due to object detection. This blog explains how deep learning techniques are used in object detection.",
        "tags": [
            "Deep Learning",
            "Computer Vision"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "RadhikaChhabra17",
        "text": "[{% load static %}]()\n## INTRODUCTION\n\nObject detection, in simple terms, is a method that is used to recognize and detect different objects present in an image or video and label them to classify these objects. It provides a much better understanding of the object as a whole, rather than just basic object classification. The important difference is the \"variable\" part. In contrast with problems like classification, the output of object detection is variable in length, since the number of objects detected may change from image to image. All in all, it answers the question: \"What object is where and how much of it is there?\".\n\n## Terminology\n\nObject detection is commonly confused with image recognition, so before we proceed, it's important that we clarify the distinctions between them.\n\n**Image recognition** : It assigns a label to an image. A picture of a dog receives the label 'dog'. A picture of two dogs, still receives the label 'dog'.\n\n**Object detection** : It draws a box around each dog and labels the box \u201cdog\u201d. The model predicts where each object is and what label should be applied. In that way, object detection provides more information about an image than recognition.\n<br>\n<img src=\"/blog/assets/img/Object-Detection/detect_vs_recognition.jpg\" alt=\"Compare detection vs recognition\" style=\"zoom:68%;\"/>\n\n## HOW OBJECT DETECTION WORKS?\n\nThe simplest approach is to pass the image to the series of convulation and pooling layer and we get object class as an output. But how we will do that for multiple objects? For multiple objects, we can divide the images into various regions and take different regions as different image. Pass all these regions (images) to the CNN and classify them into various classes. Once we have divided each region into its corresponding class, we can combine all these regions to get the original image with the detected objects. The problem with using this approach is that the objects in the image can have different aspect ratios and spatial locations. To overcome all these problems, there are multiple DL Algorithm. Some of them are discussed below:\n\n## DEEP LEARNING ALGORITHMS\n\n### 1. R-CNN\n\nIn region convolution neural network, selective search algorithm is used to efficiently determine the region proposals. This selective search algorithm proposes approximately 2000 region proposals per image. These region proposals are then passed to the CNN model. This CNN model then outputs feature vector from each region proposal. This vector is passed to SVM model for classification of object and bounding box regressor for localization.\n\n**Disadvantage** :\n\n- In R-CNN we pass each region proposal one by one in the CNN architecture and selective search generated around 2000 region proposal for an image. So, it is computationally expensive to train and even test the image using R-CNN.\n- It takes a large space to store feature map of each proposal.\n- It takes enough time to detect object in image.\n\n### 2. Fast R-CNN\n\nTo deal with the problem of R-CNN, Fast R-CNN was proposed, It takes the whole image and region proposals as input in its CNN architecture in one forward propagation. It also combines different parts of architecture (such as ConvNet, pooling, and classification layer) in one complete architecture. That also removes the requirement to store a feature map and saves disk space. It also uses the softmax layer instead of SVM in its classification of region proposal which proved to be faster and generate better accuracy than SVM.\n\n**DISADVANTAGES**:\n\n- Selective search algorithm takes a lot of time.\n\n### 3. Faster R-CNN\n\nAfter the Fast R-CNN, the bottleneck of the architecture is selective search. In Faster R-CNN, it was replaced by the region proposal network. RPN takes image feature maps as an input and generates a set of object proposals, each with an objectness score as output. RPN uses a sliding window over these feature maps, and at each window, it generates k Anchor boxes of different shapes and sizes. Anchor boxes are fixed sized boundary boxes that are placed throughout the image and have different shapes and sizes. For each anchor, RPN predicts two things:\n\n1. The probability that an anchor is an object\n2. The bounding box regressor for adjusting the anchors to better fit the object\n\n**DISADVANTAGES**\n\n- The algorithm requires many passes through a single image to extract all the objects\n- As there are different systems working one after the other, the performance of the systems further ahead depends on how the previous systems performed\n\n### 4. YOLO(YOU LOOK ONLY ONCE):\n\nYOLO is a one-step process for object detection. It is a neural network model that requires just one pass of an image through its network to conduct object detection. An obvious benefit is that YOLO proposes greater efficiency when compared to RCNNs.\nYOLO algorithm works using the following three techniques:\n\n1.  **Residual blocks** : First, the image is divided into various grids. Every grid cell will detect objects that appear within them. For example, if an object center appears within a certain grid cell, then this cell will be responsible for detecting it.\n    <br>\n    <img src=\"/blog/assets/img/Object-Detection/grids.png\" alt=\"RESIDUAL BLOCK\" style=\"zoom:67%;\"/>\n2.  **Bounding box regression** : A bounding box is an outline that highlights an object in an image. YOLO uses a single bounding box regression to predict the height, width, center, and class of objects.\n    <br>\n    <img src=\"/blog/assets/img/Object-Detection/box.png\" alt=\"BOUNDING BOX\" style=\"zoom:67%;\"/>\n3.  **Intersection over union (IOU)** : Intersection over union (IOU) is a phenomenon in object detection that describes how boxes overlap. YOLO uses IOU to provide an output box that surrounds the objects perfectly. Each grid cell is responsible for predicting the bounding boxes and their confidence scores. The IOU is equal to 1 if the predicted bounding box is the same as the real box.\n    <br>\n    <img src=\"/blog/assets/img/Object-Detection/iou.jpeg\" alt=\"IOU\" style=\"zoom:67%;\"/>\n\n## Object Detection Dataset\n\nThere are various dataset used for training the model\n\n1. The COCO Dataset\n2. Open Images\n3. Pascal VOC\n\n## APPLICATIONS\n\n1. **FACE RECOGNITION** : Most face recognition systems are powered by object detection. It can be used to detect faces, classify emotions or expressions, and feed the resulting box to an image-retrieval system to identify a specific person out of a group. It is also used in unlocking phone with face lock.\n   <br>\n   <img src=\"/blog/assets/img/Object-Detection/face_recognition.webp\" alt=\"FACE RECOGNITION\" style=\"zoom:67%;\"/>\n2. **AUTONOMOUS VEHICLES** : Self-driving cars use object detection to spot pedestrians, other cars, and obstacles on the road in order to move around safely. For example, Tesla\u2019s Autopilot AI heavily utilizes object detection to perceive environmental and surrounding threats such as oncoming vehicles or obstacles.\n   <br>\n   <img src=\"/blog/assets/img/Object-Detection/cars.jpg\" alt=\"AUTONOMOUS VEHICLES\" style=\"zoom:67%;\"/>\n3. **People detection in Security** : A wide range of security applications in video surveillance are based on object detection, for example, to detect people in restricted or dangerous areas, suicide prevention, or to automate inspection tasks on remote locations with computer vision.\n4. **Pose Estimation** : Finding the location of the main joints of a body from images, videos, or a sequence of images. Forms of pose estimation are present in applications such as Action recognition, Human interactions, creation of assets for virtual reality and 3D graphics games, robotics and more\n5. **Crowd Counting** : Crowd counting is another valuable application of object detection. For densely populated areas like theme parks, malls, and city squares, object detection can help businesses and municipalities more effectively measure different kinds of traffic\u2014whether on foot, in vehicles, or otherwise.\n\n## REFERENCES\n\n1. [https://github.com/amusi/awesome-object-detection](https://github.com/amusi/awesome-object-detection)\n2. [https://towardsdatascience.com/beginners-guide-to-object-detection-algorithms-6620fb31c375\"](https://towardsdatascience.com/beginners-guide-to-object-detection-algorithms-6620fb31c375)\n3. [https://towardsdatascience.com/object-detection-with-10-lines-of-code-d6cb4d86f606](https://towardsdatascience.com/object-detection-with-10-lines-of-code-d6cb4d86f606)\n4. [https://www.pyimagesearch.com/2018/05/14/a-gentle-guide-to-deep-learning-object-detection/](https://www.pyimagesearch.com/2018/05/14/a-gentle-guide-to-deep-learning-object-detection/)\n5. [https://paperswithcode.com/task/object-detection](https://paperswithcode.com/task/object-detection)",
        "author_name": "Radhika Chhabra",
        "slug": "object-detection"
    },
    {
        "layout": "post",
        "title": "Prefabrication in Civil Engineering Construction",
        "author_github": "RajeevBhat026",
        "date": "2022-10-02T00:00:00",
        "description": "Prefabrication is the designing and manufacturing of assemblies under factory conditions, then transporting them to\u2014and assembling them on a construction site.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Prefabrication in Civil Engineering Construction"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "RajeevBhat026",
        "text": "[{% load static %}]()\n# Prefabrication in Civil Engineering Construction\n\n## Abstract\n\nNo doubt Prefabrication is the Boom in the modern world.\nIt should become the present and future of construction industry that too with the advancement in the 3D printing, Material Science and other technologies, it will revolutionise the industry and can solve many problems. Young minds of this generation should look forward to adopt these techniques and young intellects should collaborate with each other to build start-ups, business models and companies in the prefabrication Technologies and Methods.\n\n## What is Prefabrication\n\nPrefabrication is the method of designing and manufacturing of the assemblies and component of a building or structure at a location other than building site under factory conditions, then transporting them to and assembling them on construction site. Prefabricated units may include components like doors, window panels, steel frames and structures, wall panels, floor panels, trusses, or sometimes even rooms and buildings.\n\n## How Prefabrication is Efficient and why it is recommended\n\nWith the advancement in in the technology and transportation systems, prefabrication methods have great scope in modernising the construction process.\n\nThough prefabrication existed in practice from long back, they were only limited to the small scale precast concrete slabs and steel structures but with the advancements in field of material science and new technologies like 3D printing, it has gained the potential to revolutionise the construction industry by decreasing the time taken for the construction, by providing the environmental benefits such as minimising the construction wastes and carbon emissions and less interruption to the neighbours and building sites by minimising the noise and dust.  \n\nAs the main construction work is done in the assembly line and under factory conditions, numerous activities can be carried out parallelly thus increasing the efficiency\n\nThe quality of construction is much superior when components are manufactured in a steady and stable environment\nThe wastes and left out materials can be reused ensuring the efficient usage of the resources\n  \n\n## Useful Applications of Prefabrication Method\n\n- This method comes in great use when something should be built within a very small span of time. For example, in the time of covid crisis, many isolation wards were built within days or week which could accommodate large number of people and could serve the purpose. Similar buildings can also be built during floods, earthquakes Tsunamis etc. So, prefabrication construction method plays a major role in disaster managements\n- A company which built a full-fledged 10 storey apartment building within 24 hours in China became famous talk throughout the world. This was made possible by assembling the prefabricated rooms, frames, walls and other components by cranes and skilled labours at the construction site. These buildings proved to be more efficient economically and structurally than convectional methods. In fact, they showed more structural strength, stability and durability than the convectional buildings.\n- Prefabrication has the great potential to solve the problem of housing crisis. On 25th June 2015 The Pradhan Mantri Awas Yojna (PMAY) was launched which involve manufacture of about 20 million houses by 2022 for urban poor while rural housing which envisages 10 million houses in next three years was launched on 1st April, 2016. In order to attain this enormous task, Ministry of Housing and Urban Development, Government of India has adopted numerous innovative pre-fabrication construction systems, developed within the country.\n- As it requires the proper planning and collaboration between businessmen, manufacturers, architects and civil engineers, it provides the space for further research and development and tackle the scarcity of the labour at construction site.\n  \n\n## Why prefabrication is unpopular in India\n\n- Most of the people and civil engineers prefer to go by traditional method of construction for which lack of knowledge about modern technologies and methods is also a reason\n- Lack of appropriate transport systems and undulating topography of the country is one of the main hindrances to the booming of pre-fab construction methods in India\n- Contractors prefer to employ low-cost labours as against high capital investment\n- Lack of advancement in technology and administrative support\n\n## Disadvantages of Prefabrication Construction\n\n- Prefabrication technology method requires high initial capital investment, it works as a business model and increased production volume is necessary to make sure the affordability through prefabrication\n- It needs skilled labour and heavy-duty cranes with accuracy and precision to assemble the components at the site.\n- Transportation costs may be high for huge prefabricated sections\n- It requires high levels of collaboration among project parties, and manufacturers, architect, civil engineers etc.\n\n## Recourses\n\n- [MOLECULAR AND BIOMOLECULAR ELECTRONICS - AMERICAN CHEMICAL SOCIETY](https://docecity.com/molecular-and-biomolecular-electronics-american-chemical-soc-5f1073cfe2083.html)\n- [Molecular electronics: Some views on transport junctions and beyond](https://www.pnas.org/doi/10.1073/pnas.0500075102)\n- [Advances in Molecular Electronics: A Brief Review](https://www.sciencedirect.com/science/article/pii/S2095809918306453)",
        "author_name": "Rajeev Bhat",
        "slug": "Prefabrication-in-Civil-Engineering-Construction"
    },
    {
        "layout": "post",
        "title": "Semi Supervised Learning",
        "author_github": "Vaish-922",
        "date": "2022-10-02T00:00:00",
        "image": "/assets/img/",
        "description": "",
        "tags": [
            "IEEE NITK",
            "Blog"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "Vaish-922",
        "text": "[{% load static %}]()\nMachine Learning is broadly classified into three main categories -\n\n1. **Supervised Learning** - used when all the available data is annotated\n2. **Unsupervised Learning** - no annotated data is available\n3. **Semi Supervised Learning**.\n\nSupervised Learning includes models like linear regression, whereas unsupervised has logistic regression. Semi-supervised Learning falls between unsupervised Learning (with no labelled training data) and supervised Learning (with only labelled training data). It is a remarkable instance of weak supervision.\n\nSemi Supervised Learning (SSL) is an upcoming field that is gaining popularity day by day. It is an approach to machine learning that combines a small amount of labelled data with a large amount of unlabeled data during training. When labelled data isn\u2019t available due to various reasons, unlabeled data when used in conjunction with a small amount of labelled data can produce considerable improvement in training accuracy. The collection of labelled data for a learning problem often requires a skilled human agent or a physical experiment. Since the cost of acquiring labelled data is very high, it renders these training sets infeasible, whereas the collection of unlabeled data is relatively inexpensive. The best example of this is medical images such as X-Rays. Getting labelled X-ray images involves a lot of time to be spent by the doctors which isn\u2019t feasible. In such situations, semi-supervised Learning can be of great practical value. Semi-supervised Learning is also of theoretical interest in machine learning and as a human learning model.\n\nA Semi-Supervised algorithm assumes the following about the data:\n\n1.  **Continuity Assumption:** The algorithm assumes that the points which are closer to each other are more likely to have the same output label.\n2.  **Cluster Assumption:** The data can be divided into discrete clusters and points in the same group are more likely to share an output label.\n3.  **Manifold Assumption:** The data lie approximately on a manifold of a much lower dimension than the input space. This assumption allows the use of distances and densities which are defined on a manifold.  \n\nThe basic working of an SSL model is as given:\n\n-   Train the model with the small amount of labelled training data just like you would in supervised Learning until it gives you good results.\n-   Then use it with the unlabeled training dataset to predict the outputs, which are pseudo labels since they may not be entirely accurate.\n-   Link the labels from the labelled training data with the pseudo labels created in the previous step.\n-   Link the data inputs in the labelled training data with the inputs in the unlabeled data.\n-   Then, train the model the same way you did with the labelled set in the beginning in order to decrease the error and improve the model\u2019s accuracy.\n\nThe above method is commonly known as the self training method or psuedo label method. However, this method becomes heavily dataset dependent and in some cases might reduce accuracy by a great extent. \n\nThe algorithm for psuedo label is as follows:\n\nPseudo-Label are target classes for unlabeled data as if they were true labels. The class, which has maximum predicted probability predicted using a network for each unlabeled sample, is picked up:\n\n![image]({% static '/blog/assets/img/semi-supervised-learning/2022-09-02-16-29-09.png' %})\n\nPseudo-Label is used in a fine-tuning phase with Dropout. The pre-trained network is trained in a supervised fashion with labeled and unlabeled data simultaneously:\n\n![image]({% static '/blog/assets/img/semi-supervised-learning/2022-09-02-16-29-54.png' %})\n\nwhere n is the number of samples in labeled data for SGD, n\u2019 is the number of samples in unlabeled data; C is the number of classes;\nfmi is the output for labeled data, ymi is the corresponding label;\nf\u2019mi for unlabeled data, y\u2019mi is the corresponding pseudo-label;\n\n\u03b1(t) is a coefficient balancing them at epoch t. If \u03b1(t) is too high, it disturbs training even for labeled data. Whereas if \u03b1(t) is too small, we cannot use benefit from unlabeled data.\n\n\u03b1(t) is slowly increased, to help the optimization process to avoid poor local minima:\n\n![image]({% static '/blog/assets/img/semi-supervised-learning/2022-09-02-16-30-39.png' %})\n\nOne more popular algorithm used for SSL is known as the TMean Teacher Model\n\nIn the mean teacher model, two identical models are trained with two different strategies called the student and teacher model. In which, only the student model is trained. And, a very minimal amount of weights of student model is assigned to the teacher model at every step called exponential moving average weights that's why we call it as Mean teacher.\n\nThere are two cost functions to be calculated here which is important during back propogation. The cost functions are:\n1. **Classification cost(C(\u03b8))** - binary cross-entropy between label predicted by student model and original label. \n2. **Consistency Cost(J(\u03b8))** - mean squared difference between the predicted outputs of the student (weights \u03b8 and noise \u03b7) and teacher model (weights \u03b8\u2032 and noise \u03b7\u2032).\n\n\n![image]({% static '/blog/assets/img/semi-supervised-learning/2022-09-02-16-36-42.png' %})\n\nConsistency cost is actually the distribution difference between two predictions (student and teacher prediction) and the original label is not required. During training, the model tries to minimize the distribution difference between the student and teacher model. So, instead of labeled data, we may utilize unlabelled data here. The mathematical declaration of consistency cost is as follows.\n\n![image]({% static '/blog/assets/img/semi-supervised-learning/2022-09-02-16-44-23.png' %})\n\nWhile back-propagating in the student model, the overall cost\n(O(\u03b8)) is calculated with the given formula:\n\n![image]({% static '/blog/assets/img/semi-supervised-learning/2022-09-02-16-45-11.png' %})\n\nDuring training, the exponential moving average(EMA) weights of the student model are assigned to the teacher model at every step and the proportion of weights assigned is controlled by parameter alpha(\u03b1). As mentioned in equation 3, while assigning weights, the teacher model holds its previous weights in alpha(\u03b1) proportion and (1\u2212\u03b1) portion of student weights.\n\n![image]({% static '/blog/assets/img/semi-supervised-learning/2022-09-02-16-46-44.png' %})\n \nWith the amount of data growing exponentially, there is no time to label and annotate them. This is where semi supervised learning comes into the picture. \nSSL is used in various aspects of our lives nowadays. Its most common practical uses include Speech Analysis, Web Content Classification, Protein Sequence Classification, Text Document Classifier, etc. \n\n## References\n\n- [Pseudo-Labeling to deal with small datasets \u2014 What, Why & How?](https://towardsdatascience.com/pseudo-labeling-to-deal-with-small-datasets-what-why-how-fd6f903213af)\n- [Semi-supervised learning - Wikipedia](https://en.wikipedia.org/wiki/Semi-supervised_learning#:~:text=Semi%2Dsupervised%20learning%20is%20an,with%20only%20labeled%20training%20data))\n- [Semi-Supervised Learning - GFG](https://www.geeksforgeeks.org/ml-semi-supervised-learning/)",
        "author_name": "VAISHALI S",
        "slug": "semi-supervised-learning"
    },
    {
        "layout": "post",
        "title": "AI in Medicine",
        "author_github": "swetha-thomas",
        "date": "2023-03-08T00:00:00",
        "image": "/assets/img/",
        "description": "Explore the Applications of AI in Medicine",
        "tags": [
            "IEEE NITK",
            "Blog"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "swetha-thomas",
        "text": "[{% load static %}]()\n# AI IN MEDICINE\n\nArtificial intelligence finds application in many fields and is now being increasingly used in healthcare. A large percentage of healthcare companies have already deployed AI technologies and are satisfied with the results\n\nWhy do you think AI is becoming so popular in the healthcare industry?\n\nVast amounts of medical data is available but how do we process all the data efficiently without the risk of missing out information which is vital for quality of patient care. This is where Artificial intelligence comes to the rescue. Artificial intelligence(AI) in medicine is the use of machine learning models to search medical data and uncover insights. AI, machine learning (ML), natural language processing (NLP) and deep learning (DL) enable medical professionals and healthcare stakeholders to identify healthcare needs and solutions faster with more accuracy. This helps to improve health outcomes and patient experience.\n\nIn this blog, I would like to explore 4 main applications of AI in medicine\n\n## 1. Diagnosing diseases\n\nRadiological imaging is of paramount importance as doctors often use them to confirm the diagnosis of a health condition and start the required treatment. With megapixels of data packed into the various scans like X-rays, CAT scans, MRIs, and other testing modalities, combing through very high-resolution images is often challenging even for very experienced clinicians. Studies have shown that AI tools can perform just as well, if not better, than human clinicians at analyzing features in scans.\n\nSome examples of AI used in Radiological Imaging are:\n\n![Radiology]({% static '/blog/assets/img/ai-in-medicine/radiology.png' %})\n\n- **Detecting lung and breast cancer based on CT scans** - Lung and breast cancer is known to be two of the leading causes of cancer-related deaths. Hence, advancements in screening processes are of utmost importance. So, AI has been implemented in nodule identification from scan images and has shown promising results in terms of sensitivity and specificity compared to radiologist performance without AI.\n- **Assessing cardiac health from Electrocardiograms** - Researchers found that by combining AI with imaging techniques like electrocardiograms and cardiac MRI images, physicians could improve predictive models indicating a patient\u2019s risk for heart attacks. \n- **Classifying skin lesions from images of the skin** - AI is making progress in the field of dermatology, particularly in improving the sensitivity and accuracy of screening of skin lesions including malignancies.\n- **Finding indicators of diabetic retinopathy in eye images**\n\nNow let us take a quick dive into how AI is actually used in detection of lung cancer from CT scans.\n\nWhich ML technique is used for this? Deep Learning. \n\nDeep Learning is a technique that is inspired by our brain\u2019s own network of neurons. In this specific situation, we\u2019ll be using the deep learning algorithm called Convolutional Neural Network (CNN).\n\n![CNN]({% static '/blog/assets/img/ai-in-medicine/cnn.png' %})\n\nThe CNN architecture generally consists of 3 main types of layers: convolutional layers, pooling layers, and a fully connected layer.\n\nA dataset consisting of a large number of images (matrix of pixel values) of lung CT scans, both benign and malignant, is fed as input into a series of convolutional and pooling. The convolutional layers are responsible for eventually detecting objects like nodules in the lung in the images. The pooling layers are responsible for reducing the spatial size of the matrices outputted by each convolutional layer before it is sent as input into the next convolutional layer. This will help to reduce the amount of computation performed in the network.\n\nBased on the activation values of the final convolutional layer, the fully connected layer outputs a set of confidence scores (probability) that specify how likely the image is to belong to the \u2018benign\u2019 class or the \u2018malignant\u2019 class.\n\n## 2. Developing drugs faster\n\nDeveloping drugs is a very laborious process involving years of work and huge amounts of investment.\n\n![Drug]({% static '/blog/assets/img/ai-in-medicine/drug.png' %})\n\nThere are 4 main stages in drug development and AI is used in all the stages.\n\n__Stage 1:__ Identify targets for intervention\n\nThe first step is to identify the biological molecules with which potential drugs may interact to alter disease activity. Traditionally targets were discovered through experiments, however you wouldn\u2019t know whether there is better target out there because you are limited by the targets that you have screened for. This is where ML can help.\n\n__Stage 2:__ Discover drug candidates \n\nNext, you need to identify a compound that can interact with the identified target molecule in the desired manner. This often involves screening many thousands or even millions of potential compounds for to observe their effect on the target which can be done easily with the help of ML models.\n\n__Stage 3:__ Speed up clinical trials\n\nIt\u2019s quite hard to select suitable candidates for clinical trials and if you choose the wrong candidates, it prolongs the trial, costing a lot of time and resources. ML can speed up the  process by automatically identifying suitable candidates  They also act as an early warning system for a clinical trial that is not producing conclusive results.\n\n__Stage 4:__ Find Biomarkers for diagnosing the disease\n\nBiomarkers are molecules that are found in bodily fluids which can be used to diagnose a disease, to assess disease progression, disease risk and disease prognosis. AI algorithms classify molecules into good and bad candidates \u2013 which helps clinicians focus on analyzing the best prospects.\n\n## 3. Personalized disease treatment\n\nNo patient is the same. Sounds obvious, yet often doctors treat patients with the same diagnosis in a similar way. ML models can help doctors to personalize treatments for different patients with the exact same diagnosis. \n\n## 4. Improve gene editing\n\n![Gene]({% static '/blog/assets/img/ai-in-medicine/gene.png' %})\n\nGene editing has been gaining popularity as potential treatment for genetic diseases. CRISPR-Cas9 system for gene editing is a big revolution in gene editing as it uses single guide RNA (sgRNA) to target and edit a specific location on the DNA. However, the limitation is that the sgRNA can fit multiple DNA locations which can lead to off target effects. ML models can reliably predict the degree of both sgRNA \u2013 target interactions and off target effects for a given sgRNA. This helps to develop appropriate shRNA for all regions of human DNA with precision. \n\n## What are the challenges of AI in healthcare?\n\nIn order for an AI solution to be successful, it requires a vast amount of patient data to train and optimise the performance of the algorithms. In healthcare, getting access to these datasets poses a wide range of issues such as:\n\n- Patient privacy and the ethics of data ownership \u2013 accessing personal medical records is strictly protected. \n- Quality and usability of data \u2013 in other industries, vast amounts of data are generally reliable and accurately measured \u2013 for example, aircraft engine sensors or car location and velocity data to predict highway traffic.  However, in the healthcare industry, data can be subjective, and often inaccurate.\n\n\n## The future outlook for AI\n\nOver the next few years, hybrid models will be adopted, where clinicians are supported in diagnosis, treatment planning, and identifying risk factors, but retain ultimate responsibility for the patient\u2019s care.  This will result in faster adoption by healthcare providers by mitigating perceived risk and start to deliver measurable improvements in patient outcomes and operational efficiency at scale.  \n\n## References\n\n- [Artificial Intelligence in Medicine \\| IBM](https://www.ibm.com/topics/artificial-intelligence-medicine)\n- [A Comprehensive Guide to Convolutional Neural Networks](https://towardsdatascience.com/a-comprehensive-guide-to-convolutional-neural-networks-the-eli5-way-3bd2b1164a53)\n- [AI IN DRUG DISCOVERY](https://www.merckgroup.com/en/research/science-space/envisioning-tomorrow/precision-medicine/generativeai.html)\n- [CRISPR.ML - Machine learning meets gene editing](https://www.youtube.com/watch?v=p5G5aMnExpI)",
        "author_name": "Swetha Mary Thomas",
        "slug": "ai-in-medicine"
    },
    {
        "layout": "post",
        "title": "The Math Behind Reinforcement Learning",
        "author_github": "nikhilreddy2002",
        "date": "2023-03-08T00:00:00",
        "description": "Reinforcement Learning is feedback based Machine Learning, where an agent seeks to maximize their cumulative reward by choosing a series of actions in a given environment.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Reinforcement Learning",
            "Machine Learning"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "nikhilreddy2002",
        "text": "[{% load static %}]()\n# The Math Behind Reinforcement Learning #\n\n>*Reinforcement learning is learning what to do, and how to map situations to actions, to maximize a numerical reward signal. The learner is not told which actions to take but instead must discover which actions yield the most reward by trying them. In the most interesting and challenging cases, actions may affect not only the immediate reward but also the next situation and, through that, all subsequent rewards. These two characteristics, trial-and-error search, and delayed reward are the two most important distinguishing features of reinforcement learning.*\n\n\n## Markov Decision Process - The Environment\n\n> *A Markov Decision Process is a set of rules that describe the environment for reinforcement learning to take place.* Almost every task can be broken down into an MDP. In an MDP there are mainly three aspects:\n\n- **Agent**: The learner and decision-maker\n- **Environment**: The agent interacts with it; everything outside the agent; comprises states, which contain all the information for the agent to choose the best action in a given situation.\n- **Reward**: Special numerical values given to the agent by the environment, the agent seeks to maximize this over time through its choice of actions. They can be positive or negative, describing desirable and undesirable actions respectively of an agent.  \n\nEvery MDP must follow the Markov Property:\n\t\n*The state must include information about all aspects of the past agent\u2013environment interaction that makes a difference in the future.* i.e. the future is independent of the past given the present, and all information required to make decisions is given to the agent in the current state.\n\n![MDP]({% static '/blog/assets/img/math-behind-reinforcement-learning/mdp.png' %})\n\nThe agent and environment interact at each of a sequence of discrete-time steps, t = 0, 1, 2, 3, . . . . . \n\nAt each time step t, the agent receives some representation of the environment\u2019s state, S<sub>t</sub> &isin; S, and on that basis selects an action, A<sub>t</sub> &isin; A(s). One\ntime step later, in part as a consequence of its action, the agent receives a numerical\nreward,  R<sub>t</sub> &isin; R, and finds itself in a new state, S<sub>t+1</sub>. The MDP and agent together thereby give rise to a sequence or trajectory that begins like this:\n\nS<sub>0</sub>, A<sub>0</sub>, R<sub>1</sub>, S<sub>1</sub>, A<sub>1</sub>, R<sub>2</sub>, S<sub>2</sub>, A<sub>2</sub>, R<sub>3</sub>, . . .\n\n\nIn a finite MDP, the sets of states, actions, and rewards (S, A, and R) all have a finite\nnumber of elements. In this case, the random variables R<sub>t</sub> and S<sub>t</sub> have well-defined discrete probability distributions dependent only on the preceding state and action. That\nis, for particular values of these random variables, s' &isin; S and r &isin; R, there is a probability of those values occurring at time t, given particular values of the preceding state and action:\n\n## **p(s', r | s, a) = P {S<sub>t</sub> = s', R<sub>t</sub> = r | S<sub>t-1</sub> = s, A<sub>t-1</sub> = a}**\n\n**The above formula describes what is called the dynamics of an MDP,** essentially it's a conditional probability that tells us that upon taking an action in the state s what is the probability that we end up in the state s' with a reward of r.\n\nFor every MDP the following should be true\n\n![sum over all]({% static '/blog/assets/img/math-behind-reinforcement-learning/dynamics_sigma_formula.png' %})\n\nThis tells us that for a state s and its set of actions a &isin; A(s), the probabilties set of exhausitve actions all add up.\n\n\n## Rewards and Discounting\n\nThe agent\u2019s goal is to maximize the total amount of reward it receives. This means maximizing not immediate reward, but the cumulative reward in the long run. \n\nReward hypothesis:\n\n>That all of what we mean by goals and purposes can be well thought of as the maximization of the expected value of the cumulative sum of a received scalar signal (called reward).\n\nWe seek to maximize the expected return, where the return, denoted  G<sub>t</sub>, is defined as some specific function of the reward sequence. In the simplest case the return\nis the sum of the rewards:\n## **G<sub>t</sub> =. R<sub>t+1</sub> + R<sub>t+2</sub> + R<sub>t+3</sub> + \u00b7 \u00b7 \u00b7 + R<sub>T</sub>**\nwhere T is a final time step.\n\nHowever we quickly notice that just summing up all the rewards is cumbersome for the tasks which have long episode lengths, moreover, for continuous tasks, in which the task does not break down into natural sequences and goes on forever, the final time step R<sub>T</sub> is not well defined to calculate return. Hence we use Discounting to overcome these challenges.\n\nUsing **Discounting factor &gamma;** we can give the most immediate rewards more value than those that come later on. This makes sense intuitively as well, furthermore, it helps in breaking down tasks that don't fall into discrete timesteps.\n\n![Discounted Return]({% static '/blog/assets/img/math-behind-reinforcement-learning/dicounted_return.png' %})\n\n\n## Policies\nA policy is a mapping from states to probabilities of selecting each possible\naction. If the agent is following policy &pi; at time t, then &pi;(a|s) is the probability that A<sub>t</sub> = a if S<sub>t</sub> = s.(Note &pi; is simply a probablitity function.)\n\n\nSimply put a policy is the behavior of the agent, i.e. it tells us what actions the agent will take when presented a particular siutuation (state).\n\n## Value Functions\n\nA value function tells us how good a particular state or state-action pair is for an agent. Value functions are always defined for a particular policy.\n\n### State Value function\nThe value function of a state s under a policy &pi;, denoted v<sub>&pi;</sub>(s), is the expected return when starting in s and following &pi; thereafter.\n\n![State Value Function]({% static '/blog/assets/img/math-behind-reinforcement-learning/state_value_function.png' %})\n\n\n### Action Value function\n\nvalue of taking action a in state s under a policy &pi;, denoted v<sub>&pi;</sub>(s), is the expected return when starting in s and taking an action a, and following &pi; thereafter.\n\n![Action Value Function]({% static '/blog/assets/img/math-behind-reinforcement-learning/action_value_function.png' %})\nhere E denotes the expectation of a random variable\n\nWe can estimate the values of state function or action function using Monte Carlo methods, by averaging over many random samples of actual returns.\n\n\n## Optimal Policies and Optimal Value Functions\nSolving a reinforcement learning task means, roughly, finding a policy that achieves a lot of rewards over the long run. For finite MDPs, we can precisely define an optimal policy in the following way. Value functions define a partial ordering over policies. A policy &pi; is defined to be better than or equal to a policy &pi;' if its expected return is greater than or equal to that of &pi; for all states. In other words, &pi; &geq; &pi;' if and only if v<sub>&pi;</sub>(s) &geq; v<sub>&pi;'</sub>(s) for all s &isin; S. There is always at least one policy that is better than or equal to all other policies. This is an optimal policy. Although there may be more than one, we denote all the optimal policies by &pi;<sup>\\*</sup> . They share the same state-value function, called the optimal\nstate-value function, denoted v<sup>\\*</sup>\n\n![Action Value Function]({% static '/blog/assets/img/math-behind-reinforcement-learning/optimal_state_value_function.png' %})\n\nOptimal policies also share the same optimal action-value function.\n\n![Action Value Function]({% static '/blog/assets/img/math-behind-reinforcement-learning/optimal_action_value_function.png' %})\n\nWe can form backup diagrams to calculate both the state and action value functions.\n\n![Back up diagram for State Value Function]({% static '/blog/assets/img/math-behind-reinforcement-learning/state_backup.png' %})\n![Back up diagram for Action Value Function]({% static '/blog/assets/img/math-behind-reinforcement-learning/action_backup.png' %})\n\n### Bellman Equation \nOne fundamental property we exploit is that the value functions, can be represented as recursive calls, so they can be bootstrapped to the value of  the previous state or state-action pair.\n\n![Recursive Representation of State Value Function]({% static '/blog/assets/img/math-behind-reinforcement-learning/state_value_function_recursive.png' %})\n\n### An intuitive example (Gridworld)\nThe below picture shows a rectangular gridworld representation of a simple finite MDP. The cells of the grid correspond to the states of the environment. At each cell, four actions are possible: north, south, east, and west, which deterministically cause the agent to move one cell in the respective direction on the grid. Actions that would take the agent off the grid leave its location unchanged, but also result in a reward of -1. Other actions result in a reward of 0, except those that move the agent out of the special states A and B. From state A, all four actions yield a reward of +10 and take the agent to A'. From state B, all actions yield a reward of +5 and take the agent to B'.\n\n![Grid World]({% static '/blog/assets/img/math-behind-reinforcement-learning/grid_world.png' %})\n\nSuppose the agent selects all four actions with equal probability in all states.The below picture shows the value function, v<sub>&pi;</sub>(s), for this policy, for the discounted reward case with &gamma; = 0.9. This value function was computed by solving the system of linear equations. Notice the negative values near the lower edge; these are the result of the high probability of hitting the edge of the grid there under the random policy. State A is the best state to be in under this policy, but its expected return is less than 10, its immediate reward because from A the agent is taken to A', from which it is likely to run into the edge of the grid. State B, on the other hand, is valued more than 5, its immediate reward,\nbecause from B the agent is taken to B', which has a positive value. From B' the expected penalty (negative reward) for possibly running into an edge is more than compensated for by the expected gain for possibly stumbling onto A or B.\n\n![Grid World]({% static '/blog/assets/img/math-behind-reinforcement-learning/value_function_grid_world.png' %})\n\nUsing the Bellman equation for the same Grid World for v <sup>\\*</sup> and solving the set of linear equations that are obtained thereafter we get the optimal policy &pi;<sup>\\*</sup>. The picture below shows the optimal value function and  the corresponding optimal policies. Where there are multiple arrows in a cell, all of the corresponding actions are optimal.\n\n![Grid World]({% static '/blog/assets/img/math-behind-reinforcement-learning/optimal_grid_world.png' %})\n\nYou may notice that there is more than one optimal action the agent can take in a particular given state, this leads to more than one optimal policy.\nHowever, the optimal value function is unique for a given task/environment.\n\nYou may notice that there is more than one optimal action the agent can take in a particular given state, this leads to more than one optimal policy.\nHowever, the optimal value function is unique for a given task/environment. \n\nThere are multiple methods to obtain the optimal value functions, and based on these multiple reinforcement learning algorithms have been proposed. For example, in Monte-Carlo methods, the value function is determined by taking repeated samples via experience and then averaging them. These kinds of algorithms perform well when the dynamics of the environment is not given to us. Other methods use two policies, one for exploring the environment and another for updating the value functions, these methods are called off-policy methods as they learn the optimal policy regardless of the agent\u2019s motivation. Similarly, we can find different algorithms that perform well in different situations, I suggest that you explore these for yourself. \n\n**This is the fundamental math behind Reinforcement Learning. Based on the situation we exploit different fundamental math properties and use the formulas to our advantage.**\n\n**If you think about it reinforcement learning is rote learning for computers.**\n\n## References\n\n1. [Reinforcement learning - Wikipedia](https://en.wikipedia.org/wiki/Reinforcement_learning)\n2. Richard S. Sutton and Andrew G. Barto. Reinforcement Learning: An Introduction, Second edition. The MIT Press, Cambridge Massachusetts, 2018.",
        "author_name": "Nikhil P Reddy",
        "slug": "math-behind-reinforcement-learning"
    },
    {
        "layout": "post",
        "title": "Neuralink",
        "date": "2023-03-08T00:00:00",
        "image": "/assets/img/",
        "description": "A deep dive into one of Elon Musk's latest ventures which aims to develop an extension for the human brain",
        "tags": [
            "IEEE NITK",
            "Compsoc",
            "Neuralink"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "kid-116",
        "text": "[{% load static %}]()\n<img src=\"/blog/assets/img/neuralink/hero.png\" width=\"500\" alt=\"Brain waves\">\n\n## **Introduction**\nThe valley startup, [Neuralink](https://neuralink.com/), has been in the news lately following a clinical trial on monkeys gone wrong. Co-founded by Elon Musk, the company aims to develop Brain-Machine Interfaces (BMIs), an ambitious project, and we\u2019ve come to expect nothing less from the billionaire business magnate. Human cognition consists of two major systems - the Limbic system processes our needs and emotions and is also the storehouse for our memories; the Cortex handles problem-solving, critical thinking, and the ability to plan. In the long run, Neuralink plans on being the third layer atop our brains allowing us to augment ourselves, ushering in the era of cyborgs (not to get too excited as it definitely won\u2019t be the kind shown in movies and video games). In reality, this third layer already exists as mobile devices, laptops and the internet, which have become an essential part of our lives but the way we interact with these devices is a bottleneck. Think what it would be like if we were to integrate the functionality of these devices with the super-intelligent human brain. It could be revolutionary but let\u2019s not get ahead of ourselves. Neuralink has a three-step approach towards achieving its ambitions - understanding, interface, engineering.\n\n#### Understanding\nThe human brain is the most complicated organ, and extensive research programs are trying to unearth its mysteries. It is a web of communication allowing us to move, think, feel and sense. There are 86 billion neurons in our brain which send and receive information. They generally have three parts: a dendrite which receives a signal, a cell body called a soma which computes the signal, and an axon that sends a signal out. These neurons are connected through synapses and communicate via electric signals. The change in electric potential associated with the passage of an impulse along the membrane of the neurons, a.k.a \u2018action potentials\u2019, causes synapses to release neurotransmitters. These small molecules bind to receptors on dendrites, opening channels that cause current to flow across the neuron\u2019s membrane. The idea is to place electrodes near neurons to detect action potentials. Collecting these signals from many neurons allows us to decode the information represented by these cells. In the movement-related areas of the brain, for example, neurons represent intended movements. There are neurons in the brain that carry information about everything we see, feel, touch, or think, which brings us to interfacing.\n\n#### Interfacing\nInterfacing involves mapping the network of neurons and the data it holds to computers. The answer to that is \u2018The Link\u2019 - a sealed neural implant that processes, simulates and transmits neural signals. It consists of tiny flexible neural threads containing thousands of electrodes to detect neural signals. The threads on the Link are so fine and flexible that they can\u2019t be inserted by human hands and requires a robotic system that the neurosurgeon can use to reliably and efficiently insert these threads exactly where they need to be.\n\n<p align=\"center\">\n    <img src=\"/blog/assets/img/neuralink/link.png\" width=\"250\" alt=\"The Link\">\n    <i>The Link</i>\n</p>\n\n#### Engineering\nEngineering is where the technology applications come to light. The ultimate vision is a direct link between the brain and everyday technology. Still, the initial goal of our Neuralink is to help people with paralysis to regain independence through the control of computers and mobile devices. The Link is designed to give people the ability to communicate more easily via text or speech synthesis, follow their curiosity on the web, or express their creativity through photography, art, or writing apps. As the underlying technology develops, the interface between The Link and the human brain will grow in size and become more sophisticated, allowing access to more brain areas and new neural information. The technology can treat a wide range of neurological disorders, restore sensory and movement function and eventually expand how we interact with each other and the digital world.\n\nA tiny neural implant with a wide array of applications is what Neuralink promised to deliver during its inception in 2016. Fast forward to 2022, the company has come under fire after the disastrous clinical trial consisting of 23 monkey test subjects, of which 15 are now dead. The company has also been accused of animal cruelty. Neuralink\u2019s brain chips were implanted in monkeys during tests at the University of California, Davis, from 2017 to 2020, which did not bear fruitful results. In one extreme case, a monkey was allegedly found missing some of its fingers and toes \"possibly from self-mutilation or some other unspecified trauma\". \u201cPretty much every single monkey that had had implants put in their head suffered from pretty debilitating health effects,\u201d Jeremy Beckham from PCRM (Physicians Committee for Responsible Medicine) told the New York Post, adding that \u201cthey were, frankly, maiming and killing the animals.\u201d\n\nThe trial\u2019s outcome is a significant setback for the company, but it is still in its early days. While the trial result was very unfortunate, such studies are vital for companies such as Neuralink. Some of its other test implants in animals have shown varying degrees of success and are worth looking at - [Monkey MindPong](https://neuralink.com/blog/monkey-mindpong/). What Neuralink is proposing isn\u2019t entirely new; primitive BMIs have been around since the 1950s, but what Neuralink is trying can help it achieve its complete form and become a natural extension to the human brain.",
        "author_name": "Mehul Todi",
        "slug": "neuralink"
    },
    {
        "layout": "post",
        "title": "Planes without Pilots",
        "author_github": "rupnkrdas",
        "date": "2023-03-08T00:00:00",
        "description": "Self-Flying planes are here, and Autonomous Aircrafts are the way of the Future.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Self-Flying Planes"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "rupnkrdas",
        "text": "[{% load static %}]()\n# Self-Flying Planes\n\n## Abstract\n\nI find it fascinating how, as they gain a better understanding of the level of autonomy in current planes, people become more comfortable with concepts like self-flying planes. Modern planes are self-flying in the sense that they can execute a flight plan devised by pilots on their own. These planes, on the other hand, can't think for themselves; they'll stick to the flight plan unless it's changed. When pilots remove the seatbelt indicator, for example, you're probably on a self-flying plane. With this information, and the increasing number of successful flights every day, autonomous aircraft appears to be a safe and feasible breakthrough. When a commercial aircraft is still on the ground, the flight plan is entered into the flight management system (FMS). The autopilot is normally turned on a few minutes after takeoff by the pilots. Until a few minutes before landing, the autopilot is usually turned on. This isn't always the case, though. If the plane and runway are certified for autonomous landings in low visibility, the plane can direct itself to a safe, smooth touchdown. Pilots are on hand to deal with changes and potentially dangerous conditions such as diversions, turbulence, and emergency situations. They're a safety feature in the cockpit that keeps an eye on what's going on with the plane. For example, it is the pilots' responsibility to keep the autopilot on track; if it fails, they must assume control.\n\n\n## The Difference Between Autonomous Aircraft and Self-Flying Planes\n\nAlthough self-flying planes are highly automated, they still require two competent, well-trained pilots.\n\nModern autopilot systems are built to respond to commands from the pilot or the flight director computer. At cruising altitudes, the autopilot keeps the plane on a predetermined path. It can even follow instructions for climbs, descents, and curves. You can imagine the autopilot following an airway, which is an unseen highway.\n\nPlanes are unlikely to encounter birds or other obstructions at cruising altitudes. As a result, the autopilot does not require freedom of movement.\n\n![image]({% static '/blog/assets/img/planes-without-pilots/img-1.webp' %})\n\nAircraft will no longer be partially automated; instead, they will be entirely autonomous, obviating the necessity for a pilot.\n\nUrban air mobility is one of the most common uses for autonomous aircraft. These self-driving flying taxis, as they're known in the industry, can transform personal transport and shipping in megacities by lowering traffic and improving safety. Designing these autonomous planes to operate in such a crowded environment, on the other hand, presents engineering challenges far more difficult than those overcome by the advent of existing autopilot systems.\n\nAside from flying a predetermined route, autonomous aircraft will need to safely fulfil three very crucial roles in order to operate in an urban environment:\n1. Landing and taking off without a runway\n2. Identifying potential roadblocks (like vehicles, buildings and birds)\n3. Changing course to deal with unforeseen circumstances (like wind gusts, engine failures and obstacles)\n\nSensors, embedded software, and artificial intelligence (AI) systems that continuously detect unsafe conditions, determine a safe path of motion, and execute those actions are required for this level of autonomy. In any area or weather situation, these systems will need to be able to distinguish between buildings, birds, and other aircraft. Such a system necessitates extensive engineering development, including simulation, software, and hardware-in-the-loop testing.\n\n## What Will the Future of Aviation and Autonomous Aircraft Look Like?\n\nMoving to single pilot operations will be the first step toward autonomous aircraft in the future.\n\nThere have already been whispers of launching single-pilot operations for freight and short-haul aircraft.\n\nFlights for urban air mobility will also require a single pilot or complete autonomy.\n\nFully autonomous systems will be more qualified than humans to tackle city skies crowded with impediments due to their speed and reliability.\n\nThese compact, light urban vehicles will also be capable of vertical takeoff and landing (VTOL). As a result, they may be able to transport four people. Short-distance flights between municipal airports could carry up to 14 passengers. As a result, businesses cannot allocate space or weight to two pilots and expect to break even financially.\n\nAside from autonomous features, urban aircraft will differ significantly from existing aircraft, necessitating:\n\n1. Electric propulsion to reduce pollution in cities\n2. Lightweight frames/loads for maximum manoeuvrability and efficiency\n3. Quiet systems to minimise urban noise to a minimum\n4. Comfortable experiences to enter into the urban mobility market \n\nMany major and startup firms are currently competing to supply these self-driving vehicles in cities.\n\n## How to Get from Self-Flying Planes to Autonomous Aircraft\n\nTo make autonomous aircraft a reality, a lot of work will have to be done on sensors. In an urban area, you can't just place a camera on an aircraft and expect it to see everything. Cameras, like the human eye, would be rendered ineffective in certain situations such as fog and glare.\n\nTo comprehend the environment, engineers will need to build radar, lidar, cameras, sound, and infrared sensors. They'll next need to use sensor fusion approaches to combine the data from various devices to get an accurate picture of the environment for each flight and weather situation.\n\nThe perceived data must then be sent to the aircraft's internal software and AI systems so that a safe flight may be planned. To ensure functional safety, these systems will need to be created and tested. To put it another way, engineers must ensure that the autonomous aircraft makes the safest decision possible in every circumstance. These testing may take decades, if not longer, without simulation.",
        "author_name": "Rupankar Das",
        "slug": "planes-wihtout-pilots"
    },
    {
        "layout": "post",
        "title": "Cold War: How it changed Aviation forever",
        "author_github": "SpandanPatkar",
        "date": "2023-03-28T00:00:00",
        "description": "The blog aims on enlightening the readers how the 2 superpowers facing each other during Cold War brought a remarkable change in the history of aviation and pretty much kick start series of revolutions in the very same sector.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Aviation",
            "Aerodynamics"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "SpandanPatkar",
        "text": "[{% load static %}]()\n# Cold War: How it changed Aviation forever\n\n## Abstract\n\nThe Post World War 2 era saw the clash of 2 giants defending their ideologies at respective ends: The United States of America (USA) and The Union of Soviet Socialist Republics (USSR). There was a tremendous expenditure of assets which was carried out on both sides in order to develop new technologies, consolidate existing and build new allies, espionage, and research and development purposes. One such setor where R&D resulted in development of truly extraordinary pieces of technology which till date are considered as the \u2018Van Gogh\u2019 equivalent of the aviation sector. This piece of article will discuss about the Aircrafts which went into service for their respective countries and pretty much steered the course of the cold war.\n\n## Why the need for such heavy expenditure?\n\nIt is the process of encoding,manipulating and retrieving information at a molecular or macroscopic level. It represents the final stage for miniaturization of electronic circuitry.It has provided new methods for high speed communication and signal processing and neural architectures.The most advanced ideas in this field involves the use of molecules as single logic or memory units.Current work in molecular electronics involves molecular junction transport(switch and transistors).\n\n## The BlackBird\n\nThe 1960 U2 spy plane incident was a major embarrassment as well as a huge reconnaissance blunder by the United States ,as it was an open sign of Capitalist aggression by the west. This prompted it to develop an aircraft like never before: The SR71 Blackbird. Blackbird was actually part of a project under a contract with Lockheed Martin called the \u2018A-12\u2019. There were a few variants of the project like the original **A-12 (CIA)**, **YF-12 (USAF)**, **M21 (CIA)** for various purposes, but the most popular and widely build was the **SR-71 (USAF)** aka The Blackbird variant which was used for the longest period of time before retirement.\n\nThe variant names are followed by the operator names in parentheses. Except for the YF-12,which was built as an interceptor,the other variants were meant for reconnaissance missions.\n\n## Speciality of Blackbird:\n\nBlackbird was one of a kind spy plane which could cruise at extraordinary speeds of Mach 3.2 for hours on. No other aircraft in history was and still is capable of that. Its engine was one of a kind, which could act as a normal afterburner at lower speeds,such as during takeoff and landing, but towards the top speed limits, it would act as a Turbo-Ramjet engine. This made it an ideal candidate for reconnaissance purposes, for which it was used in North Korea and Vietnam. It lacked the agility and maneuverability of a fighter jet, but what it possessed was raw speed. It used a special di-borane fuel and its body had to be designed from scratch by the scientists at Lockheed Martin. Its majority of body was made from Titanium, which could resist softening due to heat while flying at the edge of space. The funny part was that the Titanium required for the SR-71 was imported to the USA from the USSR via a series of shell companies.\n\nNo longer in service, the SR-71 is a fond aircraft to ponder upon now for the visitors at a handful of Aviation Museums throughout the USA.\n\n## Mikoyan Gurevich-25, MIG-25\n\nUnlike the SR-71, MIG-25 was not build as a reconnaissance aircraft, but as an interceptor fighter aircraft, meaning it was built for defensive interceptions of oncoming enemy aircrafts\n\n\n## Biomolecular electronics\n\nBiomolecular electronics is a subfield of molecular electronics that involves the use of native as well as modified biological molecules in the place of organic synthesized molecules.Biomolecular electronics deals with biomolecules which are able to transfer electrons between molecular partners as a result of redox reactions. These molecules may be redox metalloproteins, proteins with disulphide bonds and quinone based molecules.\n\n## Potential applications for Biomolecular electronics.\n\n- A single molecule biosensor for food borne bacteria\n- Electrical measurements of biomolecular interactions at a single molecular level\n- Measuring Ag-Ab interactions\n- Electrical cancer detection in liquid biopsies.\n\n## Conclusion\n\nThe discussion has outlined the advantages and disadvantages of implementing molecular electronics. If we ignore the commercial viability, the concept is justified based on the basic research done in this field. However, commercial viability remains as a question in many areas of proposed implementation.Molecular based holographics,nonlinear optical materials,LCD and microwave assemblies have proven to be the major accomplishments in this field. Only by giving enough time and effort can one expect a considerable level of success from molecular electronic device candidates.\n\n## Recourses\n\n- [Molecular and biomolecular electronics - AMERICAN CHEMICAL SOCIETY](https://docecity.com/molecular-and-biomolecular-electronics-american-chemical-soc-5f1073cfe2083.html)\n- [Molecular electronics: Some views on transport junctions and beyond](https://www.pnas.org/doi/10.1073/pnas.0500075102)\n- [Advances in Molecular Electronics: A Brief Review](https://www.sciencedirect.com/science/article/pii/S2095809918306453)",
        "author_name": "Spandan Patkar",
        "slug": "cold-war"
    },
    {
        "layout": "post",
        "title": "Introduction to Deep Reinforcement Learning",
        "date": "2023-03-28T00:00:00",
        "image": "/assets/img/",
        "description": "Introduction to Deep Reinforcement Learning",
        "tags": [
            "IEEE NITK",
            "CompSoc",
            "Reinforcement Learning",
            "Deep Learning"
        ],
        "categories": [
            "CompSoc"
        ],
        "github_username": "anirudhprabhakaran3",
        "text": "[{% load static %}]()\nIn March 2016, a small company based out of London shook the computer science world. They had built a program that defeated the world champion, Lee Sedol, at the ancient Chinese game of Go. The game Go is considered the most challenging game for artificial intelligence to master, because of its complex nature. Scientists said that a breakthrough would be at least ten years in the making. That was, until AlphaGo came along.\n\nNow, I could talk on about how great the five-match tournament was, or how AlphaGo has now matured into MuZero. Instead, today we'll talk about what powers AlphaGo and all its successors - Deep Reinforcement Learning.\n\n## Deep Reinforcement Learning\n\nDeep Reinforcement Learning is basically what it says on the box - it is a mixture of Deep Learning and Reinforcement Learning. While RL focuses on solving tasks and finding solutions based on trial-and-error, DL focuses on getting information about the state from the input. Combining these two together give us a great tool, DRL, which allows the agents to make decisions based on the unstructured input data without explicitly programming it.\n\nDRL has been used for a varied set of applications like robotics, video games, natural language processing, computer vision, education, transportation, finance and healthcare. It is one of the hottest topics in the ML/AI world, has lots of research being poured into it and a lot of scope for it in the future.\n\n## History\n\nOnce common thread lies between all the researchers in different subfields of artificial intelligence - to find out, model and simulate the human brain. With the rise in neural networks in the 1980s, we came a step closer to achieving that goal, and the obvious next step was to take a look at DRL.\n\nAlong with the rise in usage of neural networks and deep learning, the next part of the puzzle was placed when in 2012, there was a revival of the Deep Learning revolution. People started looking into using deep neural networks to learn and understand the policy, value and/or Q functions for reinforcement learning algorithms.\n\n## Types\n\nThere are primary two types of DRL algorithms.\n\nIn **model-based** DRL algorithms, a forward model of the environment is estimated, usually using supervised learning. Future actions are obtained by model predictive control using the learned model. This leads to some interaction of the agent with the environment, causing a change in the environment, which leads to a new future action; and the loop continues.\n\nIn **model-free** DRL algorithms, a policy is learnt without explicitly modelling the forward dynamics. The policy can be optimised to maximize returns by estimating the policy gradient, but that comes with its own drawbacks. Another class of algorithms are rely on dynamic programming, inspired by Q-learning.\n\n\n## Case Study - AlphaGo\n\n![AlphaGo vs Lee Sedol]({% static '/blog/assets/img/deep-reinforcement-learning/alphago-vs-lee-sedol.jpeg' %})\n\nLet's return back to AlphaGo. We'll try to examine how DeepMind used DRL to make their program, and what made it possible to make the program so amazing.\n\nAlphaGo takes in the Go board as the input. The neural networks try to learn and extract information from the input. One of the neural networks, called the _policy network_, selects the next move to play. The other network, called the _value network_, predicts the winner of the game. Sounds simple, right? This approach is sort of how our brain also processes information and takes a decision - one part thinks about what we should do next, and the other part figures out what the outcome would be if we made that move, and if the outcome was favourable to us. This bicameralism makes the program very powerful, and sort of mimics the human brain.\n\nAlphaGo was trained with some amateur games, so that it could learn the rules and get more information about Go. Then, DeepMind had it play with different versions of itself, thousands of times. This lead to AlphaGo becoming stronger and stronger, until it was proficient enough to beat the current word champion, Lee Sedol.\n\n## Where do we go from here?\n\nDeep Reinforcement Learning is an exploding subfield, with research being done at a massive scale and pace. There are lots of topics on which research is happening, including but not limited to off-policy RL, Inverse RL and Multi-agent RL. The real world applications include robotics, sustainability, and helping make scientific and mathematical advancements that were thought to be years away.\n\nDRL might be the closest that we have come to modelling the function of human brain, and taking inspiration from nature to create something magnificent.",
        "author_name": "Anirudh Prabhakaran",
        "slug": "introduction-to-deep-reinforcement-learning"
    },
    {
        "layout": "post",
        "title": "One Micro Bot, One Macro Revolution",
        "author_github": "SunainaSunil",
        "date": "2023-03-28T00:00:00",
        "description": "Let\u2019s take it down a notch and discover what robotics has to offer when it comes to the field of microbotics.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Microbotics",
            "Robotics"
        ],
        "categories": [
            "Piston"
        ],
        "github_username": "SunainaSunil",
        "text": "[{% load static %}]()\n# One Micro Bot, One Macro Revolution\n\n## Abstract\n\n### History and Inspiration\nLike most other technology, the science of microbotics is inspired by nature. It was only after the revolutionary discovery of microorganisms that we could even fathom the extent to which systems could function at a microscopic scale. The movement of tiny hair-like organelles called cilia and flagella attached to the cell surface has become the reason for the inspiration behind various microbotic innovations that dominate today\u2019s world.\n![image]({% static '/blog/assets/img/microbotics/Microbots.jpg' %})\nThe idea of microbots came up way back in the 1960s-1970s with raw research being conducted by the U.S. intelligence and imaginative theories being presented in science-fiction cinema. However, the earliest concepts of building them were properly initiated after advancements in microelectromechanical systems (MEMS) when technologies such as microcontrollers and miniature mechanical systems were being tested.\n### Applications\nMicrobotics has since been researched extensively and its applications have widened over the years. Its emergence has made significant impacts in medical, cleaning and rehabilitation robots. In the field of targeted medicine, microbots are used to travel through pathways of the body and receive real-time feedback from inside the body while being operated externally. Within the field of medicine, microbots are also used for drug delivery to specified parts of the body. Applications of microbotics also extend to high-precision operations that can be controlled by robots. Building of nanomaterials with accurate measurements can be achieved using microbots.\n### Wide Field of Microbotics\nFor the sake of definitions, one would refer to microbotics as the science of miniature robots of dimensions less than a millimeter. But the field of microbotics extends to the topics of micromanipulation as well as microfabrication. Micromanipulation refers to the robotic manipulation of objects in the dimensional range of one millimeter to one micrometer. Meanwhile, microfabrication is the design and fabrication of robotic agents of a size within the same range.\n### Challenges\nThe evolution of microbotics hasn\u2019t been an easy journey. As one scales down to microbotics, there arises the challenge of scaling physical effects too. While the microscopic world works on the same laws of physics as the visual macroscopic world does, the relative importance of laws related to electrostatic forces and fluid mechanics creates issues. Traditional fabrication methods fail miserably when it comes to building microbots. Taking into consideration the changes caused by scaling physical effects, designing microbots has to be tackled with new technologies.\nMoreover, there is the other obstacle of a consistent and ample power supply for a robot of miniscule dimensions. Conventionally, miniature copies of existing power supplies such as lightweight battery sources (coin cells) are used to power the bots. Other technology such as absorbing mechanical energy from the environment to power the bot has also been innovated. This still poses to be the greatest challenge of the field and research is continued for feasible power generation for microbots.\n### Innovation\nThe field of microbotics has evolved with many successful innovations, the most prominent one being microbot swarms. A bio-inspired swarm algorithm called the BEECLUST algorithm was extracted from the collective behavior of honeybees and tested on swarms of autonomous robots. One popular example of a microbot swarm is the RoboBees. Developed by Harvard, this swarm of microbotic \u201cbees\u201d have the ability to traverse land, air and water. To save power for longer flights, these RoboBees have been designed to perch on surfaces using static electricity.\n![image]({% static '/blog/assets/img/microbotics/RoboBees.jpg' %})\nAlso inspired from nature is Harvard's Ambulatory Microbot, also fondly called HAMR. Inspired by one of the most invincible species of the earth- the cockroach, HAMR is a microbot that can perform operations like jumping, carrying miniature items about twice its weight and traveling with a speed of about 19 inches per second.\n![image]({% static '/blog/assets/img/microbotics/HAMR.jpeg' %})\n\n## Resources\n\n-\tMicrobotics - Wikipedia\n-\tAll about Microbotics\n-\tSwarm Robotics\n-\tInnovative Microbot Inventions",
        "author_name": "Sunaina Sunil",
        "slug": "microbotics"
    },
    {
        "layout": "post",
        "title": "Stock Prediction with Neural Networks",
        "author_github": "Palgun7",
        "date": "2023-03-28T00:00:00",
        "description": "Stock market prediction is the act of trying to determine the future value of company stock or other financial assets that are traded. This can be achieved through deep learning models.",
        "tags": [
            "IEEE NITK",
            "Blog",
            "Neural Networks",
            "Deep Learning",
            "Stocks"
        ],
        "categories": [
            "Diode"
        ],
        "github_username": "Palgun7",
        "text": "[{% load static %}]()\n# Stock Prediction with Neural Networks\n\n## Introduction\n\nA stock market is a public market where you can buy and sell shares for publicly listed companies. The stocks, also known as equities, represent ownership in the company. The stock exchange is the mediator that allows the buying and selling of shares. Trading in the stock market means that we are buying one or more shares of the stock. These represent a segment of the whole stock market exchanges that happen in a day.\n\n## Stock Price Prediction\n\nStock Price Prediction using machine learning helps you discover the future value of company stock and other financial assets traded on an exchange. The idea of predicting stocks is such that we should be able to predict how and which stocks will perform well based on the previous trends. Predicting how the stock market will perform is a hard task to do. There are several factors involved in this prediction, such as physical and psychological factors, rational and irrational behavior. All these factors combine to make share prices dynamic and volatile. This makes it very difficult to predict stock prices with high accuracy.\n\nThere are multiple variables in the dataset \u2013 date, open, high, low, last, close, total_trade_quantity, and turnover.\n\n- The columns Open and Close represent the starting and final price at which the stock is traded on a particular day.\n- High, Low, and Last represent the maximum, minimum, and last price of the share for the day.\n- Total Trade Quantity is the number of shares bought or sold in the day and Turnover (Lacs) is the turnover of the particular company on a given date.\n  The profit or loss calculation is usually determined by the closing price of a stock for the day; hence we will consider the closing price as the target variable. Let\u2019s plot the target variable to understand how it\u2019s shaping up in our data.\n\n## Linear Regression\n\nThe most basic machine learning algorithm that can be implemented on this data is linear regression. The linear regression model returns an equation that determines the relationship between the independent variables and the dependent variable.\nThe equation for linear regression can be written as:\n\nY= \u23751x1 + \u23752x2 + \u2026+ \u2375nxn\n\nHere x1,x2,...,xn represents the independent variables, and the coefficients \u23751,\u23752,...,\u2375n represent the weights.\n\nFor the case of stock prediction, we do not have a set of independent variables. We have only the dates instead. Let us use the date column to extract features like \u2013 day, month, year, Mon/Fri, etc., and then fit a linear regression model.\n\nLinear regression is a simple technique and quite easy to interpret, but there are a few obvious disadvantages. One problem in using regression algorithms is that the model overfits the date and month column. Instead of taking into account the previous values from the point of prediction, the model will consider the value from the same date a month ago, or the same date/month a year ago.\n\n## Understanding Long Short-Term Memory Network\n\nLTSMs are a type of Recurrent Neural Network for learning long-term dependencies. It is commonly used for processing and predicting time-series data. The reason they work so well is that LSTM can store past important information and forget the information that is not. LSTM has three gates:\n\n- The input gate: The input gate adds information to the cell state\n- The forget gate: It removes the information that is no longer required by the model\n- The output gate: Output Gate at LSTM selects the information to be shown as output\n\nFrom the image on the top, you can see LSTMs have a chain-like structure. General RNNs have a single neural network layer. LSTMs, on the other hand, have four interacting layers communicating extraordinarily.\n\n## Working of LSTMs\n\nLSTMs work in a three-step process.\n\n- The first step in LSTM is to decide which information to be omitted from the cell in that particular time step. The sigmoid function looks at the previous state and the current input, it then interprets which data is important and keeps that.\n- There are two functions in the second layer. The first is the sigmoid function, and the second is the tanh function. The sigmoid function decides which values to let through (0 or 1). The tanh function gives the weightage to the values passed, deciding their level of importance from -1 to 1.\n- The third step is to decide what will be the final output. First, you need to run a sigmoid layer which determines what parts of the cell state make it to the output. Then, you must put the cell state through the tanh function to push the values between -1 and 1 and multiply it by the output of the sigmoid gate.\n\n# Google Stock Price Prediction Using LSTM\n\n## 1. Import the Libraries.\n\n## 2. Load the Training Dataset.\n\nThe Google training data has information from 3 Jan 2012 to 30 Dec 2016. There are five columns. The Open column tells the price at which a stock started trading when the market opened on a particular day. The Close column refers to the price of an individual stock when the stock exchange closed the market for the day. The High column depicts the highest price at which a stock is traded during a period. The Low column tells the lowest price of the period. Volume is the total amount of trading activity during a period.\n\n## 3. Use the Open Stock Price Column to Train Your Model.\n\n## 4. Normalizing the Dataset.\n\n## 5. Creating X_train and y_train Data Structures.\n\n## 6. Reshape the Data.\n\n## 7. Building the Model\n\n## 8. Fitting the Model.\n\n## 9. Extracting the Actual Stock Prices of Jan-2017.\n\n## 10. Preparing the Input for the Model.\n\n## 11. Predicting the Values\n\n## 12. Plotting the Actual and Predicted Prices\n\n## Conclusion\n\nAs you can see above, the model can predict the trend of the actual stock prices very closely. The accuracy of the model can be enhanced by training with more data and increasing the LSTM layers.\n\nThe LSTM model can be tuned for various parameters such as changing the number of LSTM layers, adding dropout value, or increasing the number of epochs.\nThe stock market plays a remarkable role in our daily lives. It is a significant factor in a country's GDP growth.\nTime series forecasting is a very intriguing field to work with and can help in predicting more about such fields closely.\n\n## Bibliography\n\n- [Stock market](https://www.wikiwand.com/en/Stock_market)\n- [Stock Price Prediction Using Machine Learning: An Easy Guide!](https://www.simplilearn.com/tutorials/machine-learning-tutorial/stock-price-prediction-using-machine-learning)\n- [Stock Prices Prediction Using Machine Learning and Deep Learning Techniques (with Python codes)](https://www.analyticsvidhya.com/blog/2018/10/predicting-stock-price-machine-learningnd-deep-learning-techniques-python/#h2_12)",
        "author_name": "Palgun N P",
        "slug": "stock-prediction"
    }
]